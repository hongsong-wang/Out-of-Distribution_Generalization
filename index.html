<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.00237.pdf' target='_blank'>https://arxiv.org/pdf/2510.00237.pdf</a></span>   <span><a href='https://github.com/XiaofengLin7/debunking-sft-generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Lin, Hejian Sang, Zhipeng Wang, Xuezhou Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00237">Debunk the Myth of SFT Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A prevailing view holds that supervised fine-tuning (SFT) memorizes training data and fails to generalize, whereas reinforcement learning (RL) attains broader robustness. We revisit this claim through a systematic evaluation on two decision-making benchmarks, Sokoban and General Points, and arrive at a different conclusion. We show that much of SFT's perceived failure stems from frozen-prompt artifacts: when trained on fixed instruction templates, SFT models cling to training semantics rather than adapting to new ones. Introducing prompt diversity during training breaks this shortcut and yields strong generalization to unseen instruction variants without harming in-distribution performance. Beyond instruction shifts, we ask whether SFT can generalize to strictly harder tasks. Here, chain-of-thought (CoT) supervision provides an algorithmic scaffold that markedly improves transfer to more difficult regimes, such as larger Sokoban grids with additional boxes and arithmetic with out-of-distribution values or five-card compositions that increase combinatorial complexity. Finally, combining prompt diversity with CoT achieves the best of both worlds: robust generalization across both instruction-variant and difficulty-variant settings, matching or surpassing RL baselines on our benchmarks while retaining SFT's simplicity and stability. These findings challenge the narrative that SFT is inherently inferior to RL and support a data-centric perspective: with appropriately curated demonstrations, vanilla SFT can generalize as strongly as RL. Code reproducing the results in the paper can be found at: https://github.com/XiaofengLin7/debunking-sft-generalization.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2509.26062.pdf' target='_blank'>https://arxiv.org/pdf/2509.26062.pdf</a></span>   <span><a href='https://github.com/wyf23187/DyFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, Arman Cohan, Xiangliang Zhang, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26062">DyFlow: Dynamic Workflow Framework for Agentic Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible. To address these limitations, we propose DyFlow, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization. DyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning. We systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation. Results demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains. The code is publicly available at https://github.com/wyf23187/DyFlow.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2509.23616.pdf' target='_blank'>https://arxiv.org/pdf/2509.23616.pdf</a></span>   <span><a href='https://github.com/flzeng1/GraphIFE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanlong Zeng, Wensheng Gan, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23616">GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The class imbalance problem refers to the disproportionate distribution of samples across different classes within a dataset, where the minority classes are significantly underrepresented. This issue is also prevalent in graph-structured data. Most graph neural networks (GNNs) implicitly assume a balanced class distribution and therefore often fail to account for the challenges introduced by class imbalance, which can lead to biased learning and degraded performance on minority classes. We identify a quality inconsistency problem in synthesized nodes, which leads to suboptimal performance under graph imbalance conditions. To mitigate this issue, we propose GraphIFE (Graph Invariant Feature Extraction), a novel framework designed to mitigate quality inconsistency in synthesized nodes. Our approach incorporates two key concepts from graph invariant learning and introduces strategies to strengthen the embedding space representation, thereby enhancing the model's ability to identify invariant features. Extensive experiments demonstrate the framework's efficiency and robust generalization, as GraphIFE consistently outperforms various baselines across multiple datasets. The code is publicly available at https://github.com/flzeng1/GraphIFE.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2509.21320.pdf' target='_blank'>https://arxiv.org/pdf/2509.21320.pdf</a></span>   <span><a href='https://github.com/open-sciencelab/SciReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21320">SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2509.18165.pdf' target='_blank'>https://arxiv.org/pdf/2509.18165.pdf</a></span>   <span><a href='https://github.com/XiudingCai/SIM-pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuding Cai, Yaoyao Zhu, Linjie Fu, Dong Miao, Yu Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18165">Self Identity Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Regularization is essential in deep learning to enhance generalization and mitigate overfitting. However, conventional techniques often rely on heuristics, making them less reliable or effective across diverse settings. We propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic regularization framework that leverages an inverse mapping mechanism to enhance representation learning. By reconstructing the input from its transformed output, SIM reduces information loss during forward propagation and facilitates smoother gradient flow. To address computational inefficiencies, We instantiate SIM as $ ρ\text{SIM} $ by incorporating patch-level feature sampling and projection-based method to reconstruct latent features, effectively lowering complexity. As a model-agnostic, task-agnostic regularizer, SIM can be seamlessly integrated as a plug-and-play module, making it applicable to different network architectures and tasks. We extensively evaluate $ρ\text{SIM}$ across three tasks: image classification, few-shot prompt learning, and domain generalization. Experimental results show consistent improvements over baseline methods, highlighting $ρ\text{SIM}$'s ability to enhance representation learning across various tasks. We also demonstrate that $ρ\text{SIM}$ is orthogonal to existing regularization methods, boosting their effectiveness. Moreover, our results confirm that $ρ\text{SIM}$ effectively preserves semantic information and enhances performance in dense-to-dense tasks, such as semantic segmentation and image translation, as well as in non-visual domains including audio classification and time series anomaly detection. The code is publicly available at https://github.com/XiudingCai/SIM-pytorch.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2509.17925.pdf' target='_blank'>https://arxiv.org/pdf/2509.17925.pdf</a></span>   <span><a href='https://github.com/baiyou1234/SmaRT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhan Wang, Yifei Chen, Shuo Jiang, Wenjing Yu, Mingxuan Liu, Beining Wu, Jinying Zong, Feiwei Qin, Changmiao Wang, Qiyuan Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17925">SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable brain tumor segmentation in MRI is indispensable for treatment planning and outcome monitoring, yet models trained on curated benchmarks often fail under domain shifts arising from scanner and protocol variability as well as population heterogeneity. Such gaps are especially severe in low-resource and pediatric cohorts, where conventional test-time or source-free adaptation strategies often suffer from instability and structural inconsistency. We propose SmaRT, a style-modulated robust test-time adaptation framework that enables source-free cross-domain generalization. SmaRT integrates style-aware augmentation to mitigate appearance discrepancies, a dual-branch momentum strategy for stable pseudo-label refinement, and structural priors enforcing consistency, integrity, and connectivity. This synergy ensures both adaptation stability and anatomical fidelity under extreme domain shifts. Extensive evaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT consistently outperforms state-of-the-art methods, with notable gains in Dice accuracy and boundary precision. Overall, SmaRT bridges the gap between algorithmic advances and equitable clinical applicability, supporting robust deployment of MRI-based neuro-oncology tools in diverse clinical environments. Our source code is available at https://github.com/baiyou1234/SmaRT.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2509.15194.pdf' target='_blank'>https://arxiv.org/pdf/2509.15194.pdf</a></span>   <span><a href='https://github.com/YujunZhou/EVOL-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15194">Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing self-improvement approaches primarily rely on self-confirmation signals (e.g., confidence, entropy, or consistency) to generate rewards. This reliance drives models toward over-confident, majority-favored solutions, causing an entropy collapse that degrades pass@n and reasoning complexity. To address this, we propose EVOL-RL, a label-free framework that mirrors the evolutionary principle of balancing selection with variation. Concretely, EVOL-RL retains the majority-voted answer as an anchor for stability, but adds a novelty-aware reward that scores each sampled solution by how different its reasoning is from other concurrently generated responses. This majority-for-stability + novelty-for-exploration rule mirrors the variation-selection principle: selection prevents drift, while novelty prevents collapse. Evaluation results show that EVOL-RL consistently outperforms the majority-only baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents in-domain diversity collapse but also improves out-of-domain generalization (from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The code is available at: https://github.com/YujunZhou/EVOL-RL.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2509.13172.pdf' target='_blank'>https://arxiv.org/pdf/2509.13172.pdf</a></span>   <span><a href='https://github.com/WHU-USI3DV/WHU-STree' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13172">WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Street trees are vital to urban livability, providing ecological and social benefits. Establishing a detailed, accurate, and dynamically updated street tree inventory has become essential for optimizing these multifunctional assets within space-constrained urban environments. Given that traditional field surveys are time-consuming and labor-intensive, automated surveys utilizing Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing MMS-acquired tree datasets are limited by small-scale scene, limited annotation, or single modality, restricting their utility for comprehensive analysis. To address these limitations, we introduce WHU-STree, a cross-city, richly annotated, and multi-modal urban street tree dataset. Collected across two distinct cities, WHU-STree integrates synchronized point clouds and high-resolution images, encompassing 21,007 annotated tree instances across 50 species and 2 morphological parameters. Leveraging the unique characteristics, WHU-STree concurrently supports over 10 tasks related to street tree inventory. We benchmark representative baselines for two key tasks--tree species classification and individual tree segmentation. Extensive experiments and in-depth analysis demonstrate the significant potential of multi-modal data fusion and underscore cross-domain applicability as a critical prerequisite for practical algorithm deployment. In particular, we identify key challenges and outline potential future works for fully exploiting WHU-STree, encompassing multi-modal fusion, multi-task collaboration, cross-domain generalization, spatial pattern learning, and Multi-modal Large Language Model for street tree asset management. The WHU-STree dataset is accessible at: https://github.com/WHU-USI3DV/WHU-STree.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2509.09674.pdf' target='_blank'>https://arxiv.org/pdf/2509.09674.pdf</a></span>   <span><a href='https://github.com/PRIME-RL/SimpleVLA-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09674">SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $π_0$ on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2509.09572.pdf' target='_blank'>https://arxiv.org/pdf/2509.09572.pdf</a></span>   <span><a href='https://github.com/dyzy41/PeftCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijun Dong, Yuxuan Hu, LiBo Wang, Geng Chen, Xiaoliang Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09572">PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2509.05592.pdf' target='_blank'>https://arxiv.org/pdf/2509.05592.pdf</a></span>   <span><a href='https://github.com/inclusionConf/MFFI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changtao Miao, Yi Zhang, Man Luo, Weiwei Feng, Kaiyuan Zheng, Qi Chu, Tao Gong, Jianshu Li, Yunfeng Diao, Wei Zhou, Joey Tianyi Zhou, Xiaoshuai Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05592">MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advances in Artificial Intelligence Generated Content (AIGC) have enabled increasingly sophisticated face forgeries, posing a significant threat to social security. However, current Deepfake detection methods are limited by constraints in existing datasets, which lack the diversity necessary in real-world scenarios. Specifically, these data sets fall short in four key areas: unknown of advanced forgery techniques, variability of facial scenes, richness of real data, and degradation of real-world propagation. To address these challenges, we propose the Multi-dimensional Face Forgery Image (\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation Operations. MFFI integrates $50$ different forgery methods and contains $1024K$ image samples. Benchmark evaluations show that MFFI outperforms existing public datasets in terms of scene complexity, cross-domain generalization capability, and detection difficulty gradients. These results validate the technical advance and practical utility of MFFI in simulating real-world conditions. The dataset and additional details are publicly available at {https://github.com/inclusionConf/MFFI}.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2509.00311.pdf' target='_blank'>https://arxiv.org/pdf/2509.00311.pdf</a></span>   <span><a href='https://github.com/hikmatkhan/MorphGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hikmat Khan, Syed Farhan Alam Zaidi, Pir Masoom Shah, Kiruthika Balakrishnan, Rabia Khan, Muhammad Waqas, Jia Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00311">MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization in computational histopathology is hindered by heterogeneity in whole slide images (WSIs), caused by variations in tissue preparation, staining, and imaging conditions across institutions. Unlike machine learning systems, pathologists rely on domain-invariant morphological cues such as nuclear atypia (enlargement, irregular contours, hyperchromasia, chromatin texture, spatial disorganization), structural atypia (abnormal architecture and gland formation), and overall morphological atypia that remain diagnostic across diverse settings. Motivated by this, we hypothesize that explicitly modeling biologically robust nuclear morphology and spatial organization will enable the learning of cancer representations that are resilient to domain shifts. We propose MorphGen (Morphology-Guided Generalization), a method that integrates histopathology images, augmentations, and nuclear segmentation masks within a supervised contrastive learning framework. By aligning latent representations of images and nuclear masks, MorphGen prioritizes diagnostic features such as nuclear and morphological atypia and spatial organization over staining artifacts and domain-specific features. To further enhance out-of-distribution robustness, we incorporate stochastic weight averaging (SWA), steering optimization toward flatter minima. Attention map analyses revealed that MorphGen primarily relies on nuclear morphology, cellular composition, and spatial cell organization within tumors or normal regions for final classification. Finally, we demonstrate resilience of the learned representations to image corruptions (such as staining artifacts) and adversarial attacks, showcasing not only OOD generalization but also addressing critical vulnerabilities in current deep learning systems for digital pathology. Code, datasets, and trained models are available at: https://github.com/hikmatkhan/MorphGen
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2508.20096.pdf' target='_blank'>https://arxiv.org/pdf/2508.20096.pdf</a></span>   <span><a href='https://github.com/OpenIXCLab/CODA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20096">CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2508.18440.pdf' target='_blank'>https://arxiv.org/pdf/2508.18440.pdf</a></span>   <span><a href='https://github.com/lars76/pitch-benchmark' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lars76/swift-f0,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lars Nieradzik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18440">SwiftF0: Fast and Accurate Monophonic Pitch Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and real-time monophonic pitch estimation in noisy conditions, particularly on resource-constrained devices, remains an open challenge in audio processing. We present \emph{SwiftF0}, a novel, lightweight neural model that sets a new state-of-the-art for monophonic pitch estimation. Through training on diverse speech, music, and synthetic datasets with extensive data augmentation, SwiftF0 achieves robust generalization across acoustic domains while maintaining computational efficiency. SwiftF0 achieves a 91.80\% harmonic mean (HM) at 10 dB SNR, outperforming baselines like CREPE by over 12 percentage points and degrading by only 2.3 points from clean audio. SwiftF0 requires only 95,842 parameters and runs approximately 42x faster than CREPE on CPU, making it ideal for efficient, real-time deployment. To address the critical lack of perfectly accurate ground truth pitch in speech corpora (which typically rely on algorithmic estimators or laryngograph signals), we introduce \emph{SpeechSynth}. This synthetic speech dataset, generated by a phoneme-level TTS model, provides exact, on-demand ground-truth pitch curves, enabling more robust model training and evaluation. Furthermore, we propose a unified metric, combining six complementary performance measures for comprehensive and reliable pitch evaluation, and release an open-source pitch benchmark suite. A live demo of SwiftF0 is available at https://swift-f0.github.io/, the source code at https://github.com/lars76/swift-f0, and the benchmark framework at https://github.com/lars76/pitch-benchmark.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2508.17054.pdf' target='_blank'>https://arxiv.org/pdf/2508.17054.pdf</a></span>   <span><a href='https://github.com/Kin-Zhang/DeltaFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingwen Zhang, Xiaomeng Zhu, Yushan Zhang, Yixi Cai, Olov Andersson, Patric Jensfelt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17054">DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($Î$Flow), a lightweight 3D framework that captures motion cues via a $Î$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2 and Waymo datasets show that $Î$Flow achieves state-of-the-art performance with up to 22% lower error and $2\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2508.15215.pdf' target='_blank'>https://arxiv.org/pdf/2508.15215.pdf</a></span>   <span><a href='https://github.com/Ben1001409/SleepDIFFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Wei Hao Chin, Yuin Torng Yew, Haocheng Wu, Lanxin Liang, Chow Khuen Chan, Norita Mohd Zain, Siti Balqis Samdin, Sim Kuan Goh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15215">Multi-Channel Differential Transformer for Cross-Domain Sleep Stage Classification with Heterogeneous EEG and EOG</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classification of sleep stages is essential for assessing sleep quality and diagnosing sleep disorders. However, manual inspection of EEG characteristics for each stage is time-consuming and prone to human error. Although machine learning and deep learning methods have been actively developed, they continue to face challenges arising from the non-stationarity and variability of electroencephalography (EEG) and electrooculography (EOG) signals across diverse clinical configurations, often resulting in poor generalization. In this work, we propose SleepDIFFormer, a multi-channel differential transformer framework for heterogeneous EEG-EOG representation learning. SleepDIFFormer is trained across multiple sleep staging datasets, each treated as a source domain, with the goal of generalizing to unseen target domains. Specifically, it employs a Multi-channel Differential Transformer Architecture (MDTA) designed to process raw EEG and EOG signals while incorporating cross-domain alignment. Our approach mitigates spatial and temporal attention noise and learns a domain-invariant EEG-EOG representation through feature distribution alignment across datasets, thereby enhancing generalization to new domains. Empirically, we evaluated SleepDIFFormer on five diverse sleep staging datasets under domain generalization settings and benchmarked it against existing approaches, achieving state-of-the-art performance. We further conducted a comprehensive ablation study and interpreted the differential attention weights, demonstrating their relevance to characteristic sleep EEG patterns. These findings advance the development of automated sleep stage classification and highlight its potential in quantifying sleep architecture and detecting abnormalities that disrupt restorative rest. Our source code and checkpoint are made publicly available at https://github.com/Ben1001409/SleepDIFFormer
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2508.12137.pdf' target='_blank'>https://arxiv.org/pdf/2508.12137.pdf</a></span>   <span><a href='https://github.com/nikosips/infusing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos-Antonios Ypsilantis, Kaifeng Chen, AndrÃ© Araujo, OndÅej Chum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12137">Infusing fine-grained visual knowledge to Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale contrastive pre-training produces powerful Vision-and-Language Models (VLMs) capable of generating representations (embeddings) effective for a wide variety of visual and multimodal tasks. However, these pretrained embeddings remain suboptimal for fine-grained open-set visual retrieval, where state-of-the-art results require fine-tuning the vision encoder using annotated domain-specific samples. Naively performing such fine-tuning typically leads to catastrophic forgetting, severely diminishing the model's general-purpose visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve optimal balance between fine-grained domain adaptation and retention of the pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual learning literature, we systematically analyze standard regularization techniques aimed at knowledge retention and propose an efficient and effective combination strategy. Additionally, we address the commonly overlooked yet critical aspects of validation set design and hyperparameter tuning to ensure reproducibility and robust generalization across datasets and pretrained models. We extensively evaluate our method on both fine-grained and coarse-grained image-image and image-text retrieval benchmarks. Our approach consistently achieves strong results, notably retaining the visual-text alignment without utilizing any text data or the original text encoder during fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2508.11898.pdf' target='_blank'>https://arxiv.org/pdf/2508.11898.pdf</a></span>   <span><a href='https://github.com/1mather/omnid.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jilei Mao, Jiarui Guan, Yingjuan Tang, Qirui Hu, Zhihang Li, Junjie Yu, Yongjie Mao, Yunzhe Sun, Shuang Liu, Xiaozhu Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11898">OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The visuomotor policy can easily overfit to its training datasets, such as fixed camera positions and backgrounds. This overfitting makes the policy perform well in the in-distribution scenarios but underperform in the out-of-distribution generalization. Additionally, the existing methods also have difficulty fusing multi-view information to generate an effective 3D representation. To tackle these issues, we propose Omni-Vision Diffusion Policy (OmniD), a multi-view fusion framework that synthesizes image observations into a unified bird's-eye view (BEV) representation. We introduce a deformable attention-based Omni-Feature Generator (OFG) to selectively abstract task-relevant features while suppressing view-specific noise and background distractions. OmniD achieves 11\%, 17\%, and 84\% average improvement over the best baseline model for in-distribution, out-of-distribution, and few-shot experiments, respectively. Training code and simulation benchmark are available: https://github.com/1mather/omnid.git
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2508.11695.pdf' target='_blank'>https://arxiv.org/pdf/2508.11695.pdf</a></span>   <span><a href='https://github.com/Anonymous-Name-139/RefAdgen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyun Chen, Weikai Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11695">RefAdGen: High-Fidelity Advertising Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Artificial Intelligence Generated Content (AIGC) techniques has unlocked opportunities in generating diverse and compelling advertising images based on referenced product images and textual scene descriptions. This capability substantially reduces human labor and production costs in traditional marketing workflows. However, existing AIGC techniques either demand extensive fine-tuning for each referenced image to achieve high fidelity, or they struggle to maintain fidelity across diverse products, making them impractical for e-commerce and marketing industries. To tackle this limitation, we first construct AdProd-100K, a large-scale advertising image generation dataset. A key innovation in its construction is our dual data augmentation strategy, which fosters robust, 3D-aware representations crucial for realistic and high-fidelity image synthesis. Leveraging this dataset, we propose RefAdGen, a generation framework that achieves high fidelity through a decoupled design. The framework enforces precise spatial control by injecting a product mask at the U-Net input, and employs an efficient Attention Fusion Module (AFM) to integrate product features. This design effectively resolves the fidelity-efficiency dilemma present in existing methods. Extensive experiments demonstrate that RefAdGen achieves state-of-the-art performance, showcasing robust generalization by maintaining high fidelity and remarkable visual results for both unseen products and challenging real-world, in-the-wild images. This offers a scalable and cost-effective alternative to traditional workflows. Code and datasets are publicly available at https://github.com/Anonymous-Name-139/RefAdgen.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2508.10729.pdf' target='_blank'>https://arxiv.org/pdf/2508.10729.pdf</a></span>   <span><a href='https://github.com/MyUniverse0726/EgoCross' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanjun Li, Yuqian Fu, Tianwen Qian, Qi'ao Xu, Silong Dai, Danda Pani Paudel, Luc Van Gool, Xiaoling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10729">EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Multimodal Large Language Models (MLLMs) have significantly pushed the frontier of egocentric video question answering (EgocentricQA). However, existing benchmarks and studies are mainly limited to common daily activities such as cooking and cleaning. In contrast, real-world deployment inevitably encounters domain shifts, where target domains differ substantially in both visual style and semantic content. To bridge this gap, we introduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate the cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four diverse and challenging domains, including surgery, industry, extreme sports, and animal perspective, representing realistic and high-impact application scenarios. It comprises approximately 1,000 QA pairs across 798 video clips, spanning four key QA tasks: prediction, recognition, localization, and counting. Each QA pair provides both OpenQA and CloseQA formats to support fine-grained evaluation. Extensive experiments show that most existing MLLMs, whether general-purpose or egocentric-specialized, struggle to generalize to domains beyond daily life, highlighting the limitations of current models. Furthermore, we conduct several pilot studies, \eg, fine-tuning and reinforcement learning, to explore potential improvements. We hope EgoCross and our accompanying analysis will serve as a foundation for advancing domain-adaptive, robust egocentric video understanding. Data and codes will be released at: \href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2508.10549.pdf' target='_blank'>https://arxiv.org/pdf/2508.10549.pdf</a></span>   <span><a href='https://github.com/boyiZheng99/PSScreen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyi Zheng, Qing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10549">PSScreen: Partially Supervised Multiple Retinal Disease Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging multiple partially labeled datasets to train a model for multiple retinal disease screening reduces the reliance on fully annotated datasets, but remains challenging due to significant domain shifts across training datasets from various medical sites, and the label absent issue for partial classes. To solve these challenges, we propose PSScreen, a novel Partially Supervised multiple retinal disease Screening model. Our PSScreen consists of two streams and one learns deterministic features and the other learns probabilistic features via uncertainty injection. Then, we leverage the textual guidance to decouple two types of features into disease-wise features and align them via feature distillation to boost the domain generalization ability. Meanwhile, we employ pseudo label consistency between two streams to address the label absent issue and introduce a self-distillation to transfer task-relevant semantics about known classes from the deterministic to the probabilistic stream to further enhance the detection performances. Experiments show that our PSScreen significantly enhances the detection performances on six retinal diseases and the normal state averagely and achieves state-of-the-art results on both in-domain and out-of-domain datasets. Codes are available at https://github.com/boyiZheng99/PSScreen.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2508.10549.pdf' target='_blank'>https://arxiv.org/pdf/2508.10549.pdf</a></span>   <span><a href='https://github.com/boyiZheng99/PSScreen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyi Zheng, Qing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10549">PSScreen: Partially Supervised Multiple Retinal Disease Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging multiple partially labeled datasets to train a model for multiple retinal disease screening reduces the reliance on fully annotated datasets, but remains challenging due to significant domain shifts across training datasets from various medical sites, and the label absent issue for partial classes. To solve these challenges, we propose PSScreen, a novel Partially Supervised multiple retinal disease Screening model. Our PSScreen consists of two streams and one learns deterministic features and the other learns probabilistic features via uncertainty injection. Then, we leverage the textual guidance to decouple two types of features into disease-wise features and align them via feature distillation to boost the domain generalization ability. Meanwhile, we employ pseudo label consistency between two streams to address the label absent issue and introduce a self-distillation to transfer task-relevant semantics about known classes from the deterministic to the probabilistic stream to further enhance the detection performances. Experiments show that our PSScreen significantly enhances the detection performances on six retinal diseases and the normal state averagely and achieves state-of-the-art results on both in-domain and out-of-domain datasets. Codes are available at https://github.com/boyiZheng99/PSScreen.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2508.09926.pdf' target='_blank'>https://arxiv.org/pdf/2508.09926.pdf</a></span>   <span><a href='https://github.com/owkin/histoplus/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Adjadj, Pierre-Antoine Bannier, Guillaume Horent, Sebastien Mandela, Aurore Lyon, Kathryn Schutte, Ulysse Marteau, Valentin Gaury, Laura Dumont, Thomas Mathieu, MOSAIC consortium, Reda Belbahri, Benoît Schmauch, Eric Durand, Katharina Von Loga, Lucie Gillet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09926">Towards Comprehensive Cellular Characterisation of H&E slides</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell detection, segmentation and classification are essential for analyzing tumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing methods suffer from poor performance on understudied cell types (rare or not present in public datasets) and limited cross-domain generalization. To address these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei covering 13 cell types. In external validation across 4 independent cohorts, HistoPLUS outperforms current state-of-the-art models in detection quality by 5.2% and overall F1 classification score by 23.7%, while using 5x fewer parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types and brings significant improvements on 8 of 13 cell types. Moreover, we show that HistoPLUS robustly transfers to two oncology indications unseen during training. To support broader TME biomarker research, we release the model weights and inference code at https://github.com/owkin/histoplus/.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2508.09547.pdf' target='_blank'>https://arxiv.org/pdf/2508.09547.pdf</a></span>   <span><a href='https://github.com/F1y1113/GoViG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengyi Wu, Yifei Dong, Zhi-Qi Cheng, Yilong Dai, Guangyu Chen, Hang Wang, Qi Dai, Alexander G. Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09547">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Goal-Conditioned Visual Navigation Instruction Generation (GoViG), a new task that aims to autonomously generate precise and contextually coherent navigation instructions solely from egocentric visual observations of initial and goal states. Unlike conventional approaches that rely on structured inputs such as semantic annotations or environmental maps, GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments. Our method addresses this task by decomposing it into two interconnected subtasks: (1) visual forecasting, which predicts intermediate visual states bridging the initial and goal views; and (2) instruction generation, which synthesizes linguistically coherent instructions grounded in both observed and anticipated visuals. These subtasks are integrated within an autoregressive multimodal large language model trained with tailored objectives to ensure spatial accuracy and linguistic clarity. Furthermore, we introduce two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, to mimic incremental human cognitive processes during navigation. To evaluate our method, we propose the R2R-Goal dataset, combining diverse synthetic and real-world trajectories. Empirical results demonstrate significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2508.08974.pdf' target='_blank'>https://arxiv.org/pdf/2508.08974.pdf</a></span>   <span><a href='https://github.com/Elman295/TCSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Elman Ghazaei, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08974">Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Earth's surface is constantly changing, and detecting these changes provides valuable insights that benefit various aspects of human society. While traditional change detection methods have been employed to detect changes from bi-temporal images, these approaches typically require expert knowledge for accurate interpretation. To enable broader and more flexible access to change information by non-expert users, the task of Change Detection Visual Question Answering (CDVQA) has been introduced. However, existing CDVQA methods have been developed under the assumption that training and testing datasets share similar distributions. This assumption does not hold in real-world applications, where domain shifts often occur. In this paper, the CDVQA task is revisited with a focus on addressing domain shift. To this end, a new multi-modal and multi-domain dataset, BrightVQA, is introduced to facilitate domain generalization research in CDVQA. Furthermore, a novel state space model, termed Text-Conditioned State Space Model (TCSSM), is proposed. The TCSSM framework is designed to leverage both bi-temporal imagery and geo-disaster-related textual information in an unified manner to extract domain-invariant features across domains. Input-dependent parameters existing in TCSSM are dynamically predicted by using both bi-temporal images and geo-disaster-related description, thereby facilitating the alignment between bi-temporal visual data and the associated textual descriptions. Extensive experiments are conducted to evaluate the proposed method against state-of-the-art models, and superior performance is consistently demonstrated. The code and dataset will be made publicly available upon acceptance at https://github.com/Elman295/TCSSM.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2508.05213.pdf' target='_blank'>https://arxiv.org/pdf/2508.05213.pdf</a></span>   <span><a href='https://github.com/ljm198134/TVGTANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianming Liu, Wenlong Qiu, Haitao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05213">Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-Shot Segmentation(FSS) aims to efficient segmentation of new objects with few labeled samples. However, its performance significantly degrades when domain discrepancies exist between training and deployment. Cross-Domain Few-Shot Segmentation(CD-FSS) is proposed to mitigate such performance degradation. Current CD-FSS methods primarily sought to develop segmentation models on a source domain capable of cross-domain generalization. However, driven by escalating concerns over data privacy and the imperative to minimize data transfer and training expenses, the development of source-free CD-FSS approaches has become essential. In this work, we propose a source-free CD-FSS method that leverages both textual and visual information to facilitate target domain task adaptation without requiring source domain data. Specifically, we first append Task-Specific Attention Adapters (TSAA) to the feature pyramid of a pretrained backbone, which adapt multi-level features extracted from the shared pre-trained backbone to the target task. Then, the parameters of the TSAA are trained through a Visual-Visual Embedding Alignment (VVEA) module and a Text-Visual Embedding Alignment (TVEA) module. The VVEA module utilizes global-local visual features to align image features across different views, while the TVEA module leverages textual priors from pre-aligned multi-modal features (e.g., from CLIP) to guide cross-modal adaptation. By combining the outputs of these modules through dense comparison operations and subsequent fusion via skip connections, our method produces refined prediction masks. Under both 1-shot and 5-shot settings, the proposed approach achieves average segmentation accuracy improvements of 2.18\% and 4.11\%, respectively, across four cross-domain datasets, significantly outperforming state-of-the-art CD-FSS methods. Code are available at https://github.com/ljm198134/TVGTANet.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2508.03982.pdf' target='_blank'>https://arxiv.org/pdf/2508.03982.pdf</a></span>   <span><a href='https://github.com/uponacceptance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinwei Zhang, Lianrui Zuo, Blake E. Dewey, Samuel W. Remedios, Yihao Liu, Savannah P. Hays, Dzung L. Pham, Ellen M. Mowry, Scott D. Newsome, Peter A. Calabresi, Aaron Carass, Jerry L. Prince
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03982">UNISELF: A Unified Network with Instance Normalization and Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated segmentation of multiple sclerosis (MS) lesions using multicontrast magnetic resonance (MR) images improves efficiency and reproducibility compared to manual delineation, with deep learning (DL) methods achieving state-of-the-art performance. However, these DL-based methods have yet to simultaneously optimize in-domain accuracy and out-of-domain generalization when trained on a single source with limited data, or their performance has been unsatisfactory. To fill this gap, we propose a method called UNISELF, which achieves high accuracy within a single training domain while demonstrating strong generalizability across multiple out-of-domain test datasets. UNISELF employs a novel test-time self-ensembled lesion fusion to improve segmentation accuracy, and leverages test-time instance normalization (TTIN) of latent features to address domain shifts and missing input contrasts. Trained on the ISBI 2015 longitudinal MS segmentation challenge training dataset, UNISELF ranks among the best-performing methods on the challenge test dataset. Additionally, UNISELF outperforms all benchmark methods trained on the same ISBI training data across diverse out-of-domain test datasets with domain shifts and missing contrasts, including the public MICCAI 2016 and UMCL datasets, as well as a private multisite dataset. These test datasets exhibit domain shifts and/or missing contrasts caused by variations in acquisition protocols, scanner types, and imaging artifacts arising from imperfect acquisition. Our code is available at https://github.com/uponacceptance.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2508.02314.pdf' target='_blank'>https://arxiv.org/pdf/2508.02314.pdf</a></span>   <span><a href='https://github.com/AI4Wireless/LAM4PHY_6G' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajia Guo, Yiming Cui, Shi Jin, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02314">Large AI Models for Wireless Physical Layer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large artificial intelligence models (LAMs) are transforming wireless physical layer technologies through their robust generalization, multitask processing, and multimodal capabilities. This article reviews recent advancements in LAM applications for physical layer communications, addressing limitations of conventional AI-based approaches. LAM applications are classified into two strategies: leveraging pre-trained LAMs and developing native LAMs designed specifically for physical layer tasks. The motivations and key frameworks of these approaches are comprehensively examined through multiple use cases. Both strategies significantly improve performance and adaptability across diverse wireless scenarios. Future research directions, including efficient architectures, interpretability, standardized datasets, and collaboration between large and small models, are proposed to advance LAM-based physical layer solutions for next-generation communication systems.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2508.01731.pdf' target='_blank'>https://arxiv.org/pdf/2508.01731.pdf</a></span>   <span><a href='https://github.com/YuxiangZhang-BIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Zhang, Wei Li, Mengmeng Zhang, Jiawei Han, Ran Tao, Shunlin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01731">SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Remote Sensing Foundation Models (RSFMs) have led to significant breakthroughs in the field. While many RSFMs have been pretrained with massive optical imagery, more multispectral/hyperspectral data remain lack of the corresponding foundation models. To leverage the advantages of spectral imagery in earth observation, we explore whether existing RSFMs can be effectively adapted to process diverse spectral modalities without requiring extensive spectral pretraining. In response to this challenge, we proposed SpectralX, an innovative parameter-efficient fine-tuning framework that adapt existing RSFMs as backbone while introducing a two-stage training approach to handle various spectral inputs, thereby significantly improving domain generalization performance. In the first stage, we employ a masked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) to extract attribute tokens from both spatial and spectral dimensions. Simultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA) that dynamically aggregates multi-attribute expert knowledge while performing layer-wise fine-tuning. With semantic segmentation as downstream task in the second stage, we insert an Attribute-refined Adapter (Are-adapter) into the first stage framework. By iteratively querying low-level semantic features with high-level representations, the model learns to focus on task-beneficial attributes, enabling customized adjustment of RSFMs. Following this two-phase adaptation process, SpectralX is capable of interpreting spectral imagery from new regions or seasons. The codes will be available from the website: https://github.com/YuxiangZhang-BIT.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2508.01667.pdf' target='_blank'>https://arxiv.org/pdf/2508.01667.pdf</a></span>   <span><a href='https://github.com/wloves/Rein' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixiang Wei, Xiaoxiao Ma, Ruishen Yan, Tao Tu, Huaian Chen, Jinjin Zheng, Yi Jin, Enhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01667">Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Foundation Models(VFMs) have achieved remarkable success in various computer vision tasks. However, their application to semantic segmentation is hindered by two significant challenges: (1) the disparity in data scale, as segmentation datasets are typically much smaller than those used for VFM pre-training, and (2) domain distribution shifts, where real-world segmentation scenarios are diverse and often underrepresented during pre-training. To overcome these limitations, we present Rein++, an efficient VFM-based segmentation framework that demonstrates superior generalization from limited data and enables effective adaptation to diverse unlabeled scenarios. Specifically, Rein++ comprises a domain generalization solution Rein-G and a domain adaptation solution Rein-A. Rein-G introduces a set of trainable, instance-aware tokens that effectively refine the VFM's features for the segmentation task. This parameter-efficient approach fine-tunes less than 1% of the backbone's parameters, enabling robust generalization. Building on the Rein-G, Rein-A performs unsupervised domain adaptation at both the instance and logit levels to mitigate domain shifts. In addition, it incorporates a semantic transfer module that leverages the class-agnostic capabilities of the segment anything model to enhance boundary details in the target domain. The integrated Rein++ pipeline first learns a generalizable model on a source domain (e.g., daytime scenes) and subsequently adapts it to diverse target domains (e.g., nighttime scenes) without any target labels. Comprehensive experiments demonstrate that Rein++ significantly outperforms state-of-the-art methods with efficient training, underscoring its roles an efficient, generalizable, and adaptive segmentation solution for VFMs, even for large models with billions of parameters. The code is available at https://github.com/wloves/Rein.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2507.21786.pdf' target='_blank'>https://arxiv.org/pdf/2507.21786.pdf</a></span>   <span><a href='https://github.com/Rain-Bus/MSGCoOp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaolong Wang, Tongfeng Sun, Mingzheng Du, Yachao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21786">MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language pre-trained models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, and prompt learning has emerged as an efficient alternative to full fine-tuning. However, existing methods often struggle with generalization to novel classes, a phenomenon attributed to overfitting on seen classes and forgetting general knowledge. Furthermore, recent approaches that improve generalization often introduce complex architectures or heavy computational overhead. In this paper, we propose a Multiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance few-shot generalization while maintaining computational efficiency. Our approach leverages an ensemble of parallel learnable context vectors to capture diverse semantic aspects. To enrich these prompts, we introduce a semantic guidance mechanism that aligns them with comprehensive class descriptions automatically generated by a Large Language Model (LLM). Furthermore, a diversity regularization loss encourages the prompts to learn complementary and orthogonal features, preventing them from collapsing into redundant representations. Extensive experiments on 11 benchmark datasets show that MSGCoOp significantly improves performance on base-to-novel generalization, achieving an average harmonic mean improvement of 1.10\% over the strong KgCoOp baseline. Our method also demonstrates enhanced robustness in cross-domain generalization tasks. Our code is avaliable at: \href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2507.21745.pdf' target='_blank'>https://arxiv.org/pdf/2507.21745.pdf</a></span>   <span><a href='https://github.com/aybora/FewShotReasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aybora Koksal, A. Aydin Alatan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21745">Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language and vision-language models have enabled strong reasoning capabilities, yet they remain impractical for specialized domains like remote sensing, where annotated data is scarce and expensive. We present the first few-shot reinforcement learning with verifiable reward (RLVR) framework for satellite imagery that eliminates the need for caption supervision--relying solely on lightweight, rule-based binary or IoU-based rewards. Adapting the "1-shot RLVR" paradigm from language models to vision-language models, we employ policy-gradient optimization with as few as one curated example to align model outputs for satellite reasoning tasks. Comprehensive experiments across multiple remote sensing benchmarks--including classification, visual question answering, and grounding--show that even a single example yields substantial improvements over the base model. Scaling to 128 examples matches or exceeds models trained on thousands of annotated samples. While the extreme one-shot setting can induce mild, task-specific overfitting, our approach consistently demonstrates robust generalization and efficiency across diverse tasks. Further, we find that prompt design and loss weighting significantly influence training stability and final accuracy. Our method enables cost-effective and data-efficient development of domain-specialist vision-language reasoning models, offering a pragmatic recipe for data-scarce fields: start from a compact VLM, curate a handful of reward-checkable cases, and train via RLVR.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2507.19950.pdf' target='_blank'>https://arxiv.org/pdf/2507.19950.pdf</a></span>   <span><a href='https://github.com/zhengcy-lambo/RARE.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyu Zheng, Jin Huang, Honghua Chen, Mingqiang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19950">RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research leveraging large-scale pretrained diffusion models has demonstrated the potential of using diffusion features to establish semantic correspondences in images. Inspired by advancements in diffusion-based techniques, we propose a novel zero-shot method for refining point cloud registration algorithms. Our approach leverages correspondences derived from depth images to enhance point feature representations, eliminating the need for a dedicated training dataset. Specifically, we first project the point cloud into depth maps from multiple perspectives and extract implicit knowledge from a pretrained diffusion network as depth diffusion features. These features are then integrated with geometric features obtained from existing methods to establish more accurate correspondences between point clouds. By leveraging these refined correspondences, our approach achieves significantly improved registration accuracy. Extensive experiments demonstrate that our method not only enhances the performance of existing point cloud registration techniques but also exhibits robust generalization capabilities across diverse datasets. Codes are available at https://github.com/zhengcy-lambo/RARE.git.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2507.17312.pdf' target='_blank'>https://arxiv.org/pdf/2507.17312.pdf</a></span>   <span><a href='https://github.com/pq-chen/CasP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiqi Chen, Lei Yu, Yi Wan, Yingying Pei, Xinyi Liu, Yongxiang Yao, Yingying Zhang, Lixiang Ru, Liheng Zhong, Jingdong Chen, Ming Yang, Yongjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17312">CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-dense feature matching methods have shown strong performance in challenging scenarios. However, the existing pipeline relies on a global search across the entire feature map to establish coarse matches, limiting further improvements in accuracy and efficiency. Motivated by this limitation, we propose a novel pipeline, CasP, which leverages cascaded correspondence priors for guidance. Specifically, the matching stage is decomposed into two progressive phases, bridged by a region-based selective cross-attention mechanism designed to enhance feature discriminability. In the second phase, one-to-one matches are determined by restricting the search range to the one-to-many prior areas identified in the first phase. Additionally, this pipeline benefits from incorporating high-level features, which helps reduce the computational costs of low-level feature extraction. The acceleration gains of CasP increase with higher resolution, and our lite model achieves a speedup of $\sim2.2\times$ at a resolution of 1152 compared to the most efficient method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority in geometric estimation, particularly with impressive cross-domain generalization. These advantages highlight its potential for latency-sensitive and high-robustness applications, such as SLAM and UAV systems. Code is available at https://github.com/pq-chen/CasP.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2507.15037.pdf' target='_blank'>https://arxiv.org/pdf/2507.15037.pdf</a></span>   <span><a href='https://github.com/Jerome-Young/OmniVTON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaotong Yang, Yuhui Li, Shengfeng He, Xinzhe Li, Yangyang Xu, Junyu Dong, Yong Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15037">OmniVTON: Training-Free Universal Virtual Try-On</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based Virtual Try-On (VTON) techniques rely on either supervised in-shop approaches, which ensure high fidelity but struggle with cross-domain generalization, or unsupervised in-the-wild methods, which improve adaptability but remain constrained by data biases and limited universality. A unified, training-free solution that works across both scenarios remains an open challenge. We propose OmniVTON, the first training-free universal VTON framework that decouples garment and pose conditioning to achieve both texture fidelity and pose consistency across diverse settings. To preserve garment details, we introduce a garment prior generation mechanism that aligns clothing with the body, followed by continuous boundary stitching technique to achieve fine-grained texture retention. For precise pose alignment, we utilize DDIM inversion to capture structural cues while suppressing texture interference, ensuring accurate body alignment independent of the original image textures. By disentangling garment and pose constraints, OmniVTON eliminates the bias inherent in diffusion models when handling multiple conditions simultaneously. Experimental results demonstrate that OmniVTON achieves superior performance across diverse datasets, garment types, and application scenarios. Notably, it is the first framework capable of multi-human VTON, enabling realistic garment transfer across multiple individuals in a single scene. Code is available at https://github.com/Jerome-Young/OmniVTON
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2507.14935.pdf' target='_blank'>https://arxiv.org/pdf/2507.14935.pdf</a></span>   <span><a href='https://github.com/haihuangcode/CMG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Huang, Yan Xia, Shulei Wang, Hanting Wang, Minghui Fang, Shengpeng Ji, Sashuai Zhou, Tao Jin, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14935">Open-set Cross Modal Generalization via Multimodal Unified Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper extends Cross Modal Generalization (CMG) to open-set environments by proposing the more challenging Open-set Cross Modal Generalization (OSCMG) task. This task evaluates multimodal unified representations in open-set conditions, addressing the limitations of prior closed-set cross-modal evaluations. OSCMG requires not only cross-modal knowledge transfer but also robust generalization to unseen classes within new modalities, a scenario frequently encountered in real-world applications. Existing multimodal unified representation work lacks consideration for open-set environments. To tackle this, we propose MICU, comprising two key components: Fine-Coarse Masked multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI enhances multimodal alignment by applying contrastive learning at both holistic semantic and temporal levels, incorporating masking to enhance generalization. CUJP enhances feature diversity and model uncertainty by integrating modality-agnostic feature selection with self-supervised learning, thereby strengthening the model's ability to handle unknown categories in open-set tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the effectiveness of our approach. The code is available at https://github.com/haihuangcode/CMG.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2507.12851.pdf' target='_blank'>https://arxiv.org/pdf/2507.12851.pdf</a></span>   <span><a href='https://github.com/bitPrincy/SRE-DG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wang, Zhi Gao, Jin Chen, Qingjie Zhao, Xinxiao Wu, Jiebo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12851">Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn a model from source domains and apply it to unseen target domains with out-of-distribution data. Owing to CLIP's strong ability to encode semantic concepts, it has attracted increasing interest in domain generalization. However, CLIP often struggles to focus on task-relevant regions across domains, i.e., domain-invariant regions, resulting in suboptimal performance on unseen target domains. To address this challenge, we propose an attention-refocusing scheme, called Simulate, Refocus and Ensemble (SRE), which learns to reduce the domain shift by aligning the attention maps in CLIP via attention refocusing. SRE first simulates domain shifts by performing augmentation on the source data to generate simulated target domains. SRE then learns to reduce the domain shifts by refocusing the attention in CLIP between the source and simulated target domains. Finally, SRE utilizes ensemble learning to enhance the ability to capture domain-invariant attention maps between the source data and the simulated target data. Extensive experimental results on several datasets demonstrate that SRE generally achieves better results than state-of-the-art methods. The code is available at: https://github.com/bitPrincy/SRE-DG.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2507.08340.pdf' target='_blank'>https://arxiv.org/pdf/2507.08340.pdf</a></span>   <span><a href='https://github.com/HopkinsKwong/MCCSDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08340">Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at https://github.com/HopkinsKwong/MCCSDG
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2507.08340.pdf' target='_blank'>https://arxiv.org/pdf/2507.08340.pdf</a></span>   <span><a href='https://github.com/HopkinsKwong/MCCSDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08340">Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at https://github.com/HopkinsKwong/MCCSDG
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2507.06230.pdf' target='_blank'>https://arxiv.org/pdf/2507.06230.pdf</a></span>   <span><a href='https://github.com/tum-vision/scenedino' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar JevtiÄ, Christoph Reich, Felix Wimbauer, Oliver Hahn, Christian Rupprecht, Stefan Roth, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06230">Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in a feed-forward manner. Through a novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of a current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards a strong foundation for single image 3D scene understanding.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2507.06146.pdf' target='_blank'>https://arxiv.org/pdf/2507.06146.pdf</a></span>   <span><a href='https://github.com/00why00/PFCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Lei Zhang, Wei Wei, Chen Ding, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06146">Prompt-Free Conditional Diffusion for Multi-object Image Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models has underpinned much recent advances of dataset augmentation in various computer vision tasks. However, when involving generating multi-object images as real scenarios, most existing methods either rely entirely on text condition, resulting in a deviation between the generated objects and the original data, or rely too much on the original images, resulting in a lack of diversity in the generated images, which is of limited help to downstream tasks. To mitigate both problems with one stone, we propose a prompt-free conditional diffusion framework for multi-object image augmentation. Specifically, we introduce a local-global semantic fusion strategy to extract semantics from images to replace text, and inject knowledge into the diffusion model through LoRA to alleviate the category deviation between the original model and the target dataset. In addition, we design a reward model based counting loss to assist the traditional reconstruction loss for model training. By constraining the object counts of each category instead of pixel-by-pixel constraints, bridging the quantity deviation between the generated data and the original data while improving the diversity of the generated data. Experimental results demonstrate the superiority of the proposed method over several representative state-of-the-art baselines and showcase strong downstream task gain and out-of-domain generalization capabilities. Code is available at \href{https://github.com/00why00/PFCD}{here}.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.04061.pdf' target='_blank'>https://arxiv.org/pdf/2507.04061.pdf</a></span>   <span><a href='https://github.com/ghh1125/DOCTOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanghui Guo, Weijie Shi, Mengze Li, Juncheng Li, Hao Chen, Yue Cui, Jiajie Xu, Jia Zhu, Jiawei Shen, Zhangze Chen, Sirui Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04061">Consistent and Invariant Generalization Learning for Short-video Misinformation Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Short-video misinformation detection has attracted wide attention in the multi-modal domain, aiming to accurately identify the misinformation in the video format accompanied by the corresponding audio. Despite significant advancements, current models in this field, trained on particular domains (source domains), often exhibit unsatisfactory performance on unseen domains (target domains) due to domain gaps. To effectively realize such domain generalization on the short-video misinformation detection task, we propose deep insights into the characteristics of different domains: (1) The detection on various domains may mainly rely on different modalities (i.e., mainly focusing on videos or audios). To enhance domain generalization, it is crucial to achieve optimal model performance on all modalities simultaneously. (2) For some domains focusing on cross-modal joint fraud, a comprehensive analysis relying on cross-modal fusion is necessary. However, domain biases located in each modality (especially in each frame of videos) will be accumulated in this fusion process, which may seriously damage the final identification of misinformation. To address these issues, we propose a new DOmain generalization model via ConsisTency and invariance learning for shORt-video misinformation detection (named DOCTOR), which contains two characteristic modules: (1) We involve the cross-modal feature interpolation to map multiple modalities into a shared space and the interpolation distillation to synchronize multi-modal learning; (2) We design the diffusion model to add noise to retain core features of multi modal and enhance domain invariant features through cross-modal guided denoising. Extensive experiments demonstrate the effectiveness of our proposed DOCTOR model. Our code is public available at https://github.com/ghh1125/DOCTOR.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2507.04061.pdf' target='_blank'>https://arxiv.org/pdf/2507.04061.pdf</a></span>   <span><a href='https://github.com/ghh1125/DOCTOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanghui Guo, Weijie Shi, Mengze Li, Juncheng Li, Hao Chen, Yue Cui, Jiajie Xu, Jia Zhu, Jiawei Shen, Zhangze Chen, Sirui Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04061">Consistent and Invariant Generalization Learning for Short-video Misinformation Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Short-video misinformation detection has attracted wide attention in the multi-modal domain, aiming to accurately identify the misinformation in the video format accompanied by the corresponding audio. Despite significant advancements, current models in this field, trained on particular domains (source domains), often exhibit unsatisfactory performance on unseen domains (target domains) due to domain gaps. To effectively realize such domain generalization on the short-video misinformation detection task, we propose deep insights into the characteristics of different domains: (1) The detection on various domains may mainly rely on different modalities (i.e., mainly focusing on videos or audios). To enhance domain generalization, it is crucial to achieve optimal model performance on all modalities simultaneously. (2) For some domains focusing on cross-modal joint fraud, a comprehensive analysis relying on cross-modal fusion is necessary. However, domain biases located in each modality (especially in each frame of videos) will be accumulated in this fusion process, which may seriously damage the final identification of misinformation. To address these issues, we propose a new DOmain generalization model via ConsisTency and invariance learning for shORt-video misinformation detection (named DOCTOR), which contains two characteristic modules: (1) We involve the cross-modal feature interpolation to map multiple modalities into a shared space and the interpolation distillation to synchronize multi-modal learning; (2) We design the diffusion model to add noise to retain core features of multi modal and enhance domain invariant features through cross-modal guided denoising. Extensive experiments demonstrate the effectiveness of our proposed DOCTOR model. Our code is public available at https://github.com/ghh1125/DOCTOR.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2507.01723.pdf' target='_blank'>https://arxiv.org/pdf/2507.01723.pdf</a></span>   <span><a href='https://github.com/amazon-science/Spherical_Diffusion_Policy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xupeng Zhu, Fan Wang, Robin Walters, Jane Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01723">SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion Policies are effective at learning closed-loop manipulation policies from human demonstrations but generalize poorly to novel arrangements of objects in 3D space, hurting real-world performance. To address this issue, we propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion policy that adapts trajectories according to 3D transformations of the scene. Such equivariance is achieved by embedding the states, actions, and the denoising process in spherical Fourier space. Additionally, we employ novel spherical FiLM layers to condition the action denoising process equivariantly on the scene embeddings. Lastly, we propose a spherical denoising temporal U-net that achieves spatiotemporal equivariance with computational efficiency. In the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization across transformed 3D scenes. SDP demonstrates a large performance improvement over strong baselines in 20 simulation tasks and 5 physical robot tasks including single-arm and bi-manual embodiments. Code is available at https://github.com/amazon-science/Spherical_Diffusion_Policy.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2506.23822.pdf' target='_blank'>https://arxiv.org/pdf/2506.23822.pdf</a></span>   <span><a href='https://github.com/shiming-chen/LaZSL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiming Chen, Bowen Duan, Salman Khan, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23822">Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale vision-language models (VLMs), such as CLIP, have achieved remarkable success in zero-shot learning (ZSL) by leveraging large-scale visual-text pair datasets. However, these methods often lack interpretability, as they compute the similarity between an entire query image and the embedded category words, making it difficult to explain their predictions. One approach to address this issue is to develop interpretable models by integrating language, where classifiers are built using discrete attributes, similar to human perception. This introduces a new challenge: how to effectively align local visual features with corresponding attributes based on pre-trained VLMs. To tackle this, we propose LaZSL, a locally-aligned vision-language model for interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal transport to perform interaction between visual regions and their associated attributes, facilitating effective alignment and providing interpretable similarity without the need for additional training. Extensive experiments demonstrate that our method offers several advantages, including enhanced interpretability, improved accuracy, and strong domain generalization. Codes available at: https://github.com/shiming-chen/LaZSL.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2506.23580.pdf' target='_blank'>https://arxiv.org/pdf/2506.23580.pdf</a></span>   <span><a href='https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yawen Zou, Guang Li, Duo Su, Zi Wang, Jun Yu, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23580">Dataset Distillation via Vision-Language Category Prototype</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dataset distillation (DD) condenses large datasets into compact yet informative substitutes, preserving performance comparable to the original dataset while reducing storage, transmission costs, and computational consumption. However, previous DD methods mainly focus on distilling information from images, often overlooking the semantic information inherent in the data. The disregard for context hinders the model's generalization ability, particularly in tasks involving complex datasets, which may result in illogical outputs or the omission of critical objects. In this study, we integrate vision-language methods into DD by introducing text prototypes to distill language information and collaboratively synthesize data with image prototypes, thereby enhancing dataset distillation performance. Notably, the text prototypes utilized in this study are derived from descriptive text information generated by an open-source large language model. This framework demonstrates broad applicability across datasets without pre-existing text descriptions, expanding the potential of dataset distillation beyond traditional image-based approaches. Compared to other methods, the proposed approach generates logically coherent images containing target objects, achieving state-of-the-art validation performance and demonstrating robust generalization. Source code and generated data are available in https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2506.23542.pdf' target='_blank'>https://arxiv.org/pdf/2506.23542.pdf</a></span>   <span><a href='https://github.com/davidweidawang/GIGA-ToF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weida Wang, Changyong He, Jin Zeng, Di Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23542">Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at \href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2506.23219.pdf' target='_blank'>https://arxiv.org/pdf/2506.23219.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/UrbanLLaVA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Feng, Shengyuan Wang, Tianhui Liu, Yanxin Xi, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23219">UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In $\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2506.21042.pdf' target='_blank'>https://arxiv.org/pdf/2506.21042.pdf</a></span>   <span><a href='https://github.com/heboyong/Fitness-Generalization-Transferability' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyong He, Yuxiang Ji, Zhuoyue Tan, Liaoni Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21042">Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detectors often suffer from performance drop due to domain gap between training and testing data. Recent methods explore diffusion models applied to domain generalization (DG) and adaptation (DA) tasks, but still struggle with large inference costs and have not yet fully leveraged the capabilities of diffusion models. We propose to tackle these problems by extracting intermediate features from a single-step diffusion process, improving feature collection and fusion to reduce inference time by 75% while enhancing performance on source domains (i.e., Fitness). Then, we construct an object-centered auxiliary branch by applying box-masked images with class prompts to extract robust and domain-invariant features that focus on object. We also apply consistency loss to align the auxiliary and ordinary branch, balancing fitness and generalization while preventing overfitting and improving performance on target domains (i.e., Generalization). Furthermore, within a unified framework, standard detectors are guided by diffusion detectors through feature-level and object-level alignment on source domains (for DG) and unlabeled target domains (for DA), thereby improving cross-domain detection performance (i.e., Transferability). Our method achieves competitive results on 3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO generalization benchmark demonstrate that our method maintains significant advantages and show remarkable efficiency in large domain shifts and low-data scenarios. Our work shows the superiority of applying diffusion models to domain generalized and adaptive detection tasks and offers valuable insights for visual perception tasks across diverse domains. The code is available at \href{https://github.com/heboyong/Fitness-Generalization-Transferability}.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2506.20599.pdf' target='_blank'>https://arxiv.org/pdf/2506.20599.pdf</a></span>   <span><a href='https://github.com/GeoX-Lab/RSTI/tree/main/SFNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Qi, Xinchang Zhang, Dingqi Ye, Yongjia Ruan, Xin Guo, Shaowen Wang, Haifeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20599">SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative artificial intelligence is producing fake remote sensing imagery (RSI) that is increasingly difficult to detect, potentially leading to erroneous intelligence, fake news, and even conspiracy theories. Existing forgery detection methods typically rely on single visual features to capture predefined artifacts, such as spatial-domain cues to detect forged objects like roads or buildings in RSI, or frequency-domain features to identify artifacts from up-sampling operations in adversarial generative networks (GANs). However, the nature of artifacts can significantly differ depending on geographic terrain, land cover types, or specific features within the RSI. Moreover, these complex artifacts evolve as generative models become more sophisticated. In short, over-reliance on a single visual cue makes existing forgery detectors struggle to generalize across diverse remote sensing data. This paper proposed a novel forgery detection framework called SFNet, designed to identify fake images in diverse remote sensing data by leveraging spatial and frequency domain features. Specifically, to obtain rich and comprehensive visual information, SFNet employs two independent feature extractors to capture spatial and frequency domain features from input RSIs. To fully utilize the complementary domain features, the domain feature mapping module and the hybrid domain feature refinement module(CBAM attention) of SFNet are designed to successively align and fuse the multi-domain features while suppressing redundant information. Experiments on three datasets show that SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art RS forgery detection methods and exhibits robust generalization capabilities. The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2506.17685.pdf' target='_blank'>https://arxiv.org/pdf/2506.17685.pdf</a></span>   <span><a href='https://github.com/Ashayan97/SeqDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirshayan Nasirimajd, Chiara Plizzari, Simone Alberto Peirone, Marco Ciccone, Giuseppe Averta, Barbara Caputo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17685">Domain Generalization using Action Sequences for Egocentric Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing human activities from visual inputs, particularly through a first-person viewpoint, is essential for enabling robots to replicate human behavior. Egocentric vision, characterized by cameras worn by observers, captures diverse changes in illumination, viewpoint, and environment. This variability leads to a notable drop in the performance of Egocentric Action Recognition models when tested in environments not seen during training. In this paper, we tackle these challenges by proposing a domain generalization approach for Egocentric Action Recognition. Our insight is that action sequences often reflect consistent user intent across visual domains. By leveraging action sequences, we aim to enhance the model's generalization ability across unseen environments. Our proposed method, named SeqDG, introduces a visual-text sequence reconstruction objective (SeqRec) that uses contextual cues from both text and visual inputs to reconstruct the central action of the sequence. Additionally, we enhance the model's robustness by training it on mixed sequences of actions from different domains (SeqMix). We validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement in cross-domain action recognition in unseen environments, and on EGTEA the model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action recognition.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2506.12413.pdf' target='_blank'>https://arxiv.org/pdf/2506.12413.pdf</a></span>   <span><a href='https://github.com/PerceptualAI-Lab/Awesome-Domain-Generalizable-Person-Re-ID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonseo Lee, Juhyun Park, Jihyong Oh, Chanho Eom
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12413">Domain Generalization for Person Re-identification: A Survey Towards Domain-Agnostic Person Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person Re-identification (ReID) aims to retrieve images of the same individual captured across non-overlapping camera views, making it a critical component of intelligent surveillance systems. Traditional ReID methods assume that the training and test domains share similar characteristics and primarily focus on learning discriminative features within a given domain. However, they often fail to generalize to unseen domains due to domain shifts caused by variations in viewpoint, background, and lighting conditions. To address this issue, Domain-Adaptive ReID (DA-ReID) methods have been proposed. These approaches incorporate unlabeled target domain data during training and improve performance by aligning feature distributions between source and target domains. Domain-Generalizable ReID (DG-ReID) tackles a more realistic and challenging setting by aiming to learn domain-invariant features without relying on any target domain data. Recent methods have explored various strategies to enhance generalization across diverse environments, but the field remains relatively underexplored. In this paper, we present a comprehensive survey of DG-ReID. We first review the architectural components of DG-ReID including the overall setting, commonly used backbone networks and multi-source input configurations. Then, we categorize and analyze domain generalization modules that explicitly aim to learn domain-invariant and identity-discriminative representations. To examine the broader applicability of these techniques, we further conduct a case study on a related task that also involves distribution shifts. Finally, we discuss recent trends, open challenges, and promising directions for future research in DG-ReID. To the best of our knowledge, this is the first systematic survey dedicated to DG-ReID.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2506.10675.pdf' target='_blank'>https://arxiv.org/pdf/2506.10675.pdf</a></span>   <span><a href='https://github.com/jwxsp1/ConStyX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Chen, Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10675">ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical images are usually collected from multiple domains, leading to domain shifts that impair the performance of medical image segmentation models. Domain Generalization (DG) aims to address this issue by training a robust model with strong generalizability. Recently, numerous domain randomization-based DG methods have been proposed. However, these methods suffer from the following limitations: 1) constrained efficiency of domain randomization due to their exclusive dependence on image style perturbation, and 2) neglect of the adverse effects of over-augmented images on model training. To address these issues, we propose a novel domain randomization-based DG method, called content style augmentation (ConStyX), for generalizable medical image segmentation. Specifically, ConStyX 1) augments the content and style of training data, allowing the augmented training data to better cover a wider range of data domains, and 2) leverages well-augmented features while mitigating the negative effects of over-augmented features during model training. Extensive experiments across multiple domains demonstrate that our ConStyX achieves superior generalization performance. The code is available at https://github.com/jwxsp1/ConStyX.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2506.09881.pdf' target='_blank'>https://arxiv.org/pdf/2506.09881.pdf</a></span>   <span><a href='https://github.com/anonymouse-9c53tp182bvz/Vireo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Chen, Ting Han, Chengzheng Fu, Changshe Zhang, Chaolei Wang, Jinhe Su, Guorong Cai, Meiliu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09881">Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary semantic segmentation (OVSS) and domain generalization in semantic segmentation (DGSS) highlight a subtle complementarity that motivates Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS aims to generate pixel-level masks for unseen categories while maintaining robustness across unseen domains, a critical capability for real-world scenarios such as autonomous driving in adverse conditions. We introduce Vireo, a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS and DGSS for the first time. Vireo builds upon the frozen Visual Foundation Models (VFMs) and incorporates scene geometry via Depth VFMs to extract domain-invariant structural features. To bridge the gap between visual and textual modalities under domain shift, we propose three key components: (1) GeoText Prompts, which align geometric features with language cues and progressively refine VFM encoder representations; (2) Coarse Mask Prior Embedding (CMPE) for enhancing gradient flow for faster convergence and stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH), which fuses refined structural and semantic features for robust prediction. Comprehensive evaluation on these components demonstrates the effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art performance and surpasses existing methods by a large margin in both domain generalization and open-vocabulary recognition, offering a unified and scalable solution for robust visual understanding in diverse and dynamic environments. Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2506.09460.pdf' target='_blank'>https://arxiv.org/pdf/2506.09460.pdf</a></span>   <span><a href='https://github.com/amir-khb/SSUDOSDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirreza Khoshbakht, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09460">Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-set domain generalization(OSDG) for hyperspectral image classification presents significant challenges due to the presence of unknown classes in target domains and the need for models to generalize across multiple unseen domains without target-specific adaptation. Existing domain adaptation methods assume access to target domain data during training and fail to address the fundamental issue of domain shift when unknown classes are present, leading to negative transfer and reduced classification performance. To address these limitations, we propose a novel open-set domain generalization framework that combines four key components: Spectrum-Invariant Frequency Disentanglement (SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network (DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning (EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty Disentanglement (SSUD) for reliable open-set classification. The SIFD module extracts domain-invariant spectral features in the frequency domain through attention-weighted frequency analysis and domain-agnostic regularization, while DCRN captures complementary spectral and spatial information via parallel pathways with adaptive fusion. EDL provides principled uncertainty estimation using Dirichlet distributions, enabling the SSUD module to make reliable open-set decisions through uncertainty-aware pathway weighting and adaptive rejection thresholding. Experimental results on three cross-scene hyperspectral classification tasks show that our approach achieves performance comparable to state-of-the-art domain adaptation methods while requiring no access to the target domain during training. The implementation will be made available at https://github.com/amir-khb/SSUDOSDG upon acceptance.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2506.09033.pdf' target='_blank'>https://arxiv.org/pdf/2506.09033.pdf</a></span>   <span><a href='https://github.com/ulab-uiuc/Router-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhen Zhang, Tao Feng, Jiaxuan You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09033">Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2506.08518.pdf' target='_blank'>https://arxiv.org/pdf/2506.08518.pdf</a></span>   <span><a href='https://github.com/sunnyinAI/FedTail' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Gupta, Nikita Jangid, Shounak Das, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08518">FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) seeks to train models that perform reliably on unseen target domains without access to target data during training. While recent progress in smoothing the loss landscape has improved generalization, existing methods often falter under long-tailed class distributions and conflicting optimization objectives. We introduce FedTAIL, a federated domain generalization framework that explicitly addresses these challenges through sharpness-guided, gradient-aligned optimization. Our method incorporates a gradient coherence regularizer to mitigate conflicts between classification and adversarial objectives, leading to more stable convergence. To combat class imbalance, we perform class-wise sharpness minimization and propose a curvature-aware dynamic weighting scheme that adaptively emphasizes underrepresented tail classes. Furthermore, we enhance conditional distribution alignment by integrating sharpness-aware perturbations into entropy regularization, improving robustness under domain shift. FedTAIL unifies optimization harmonization, class-aware regularization, and conditional alignment into a scalable, federated-compatible framework. Extensive evaluations across standard domain generalization benchmarks demonstrate that FedTAIL achieves state-of-the-art performance, particularly in the presence of domain shifts and label imbalance, validating its effectiveness in both centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2506.06664.pdf' target='_blank'>https://arxiv.org/pdf/2506.06664.pdf</a></span>   <span><a href='https://github.com/NVlabs/GTRS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenxin Li, Wenhao Yao, Zi Wang, Xinglong Sun, Joshua Chen, Nadine Chang, Maying Shen, Zuxuan Wu, Shiyi Lan, Jose M. Alvarez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06664">Generalized Trajectory Scoring for End-to-end Multimodal Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end multi-modal planning is a promising paradigm in autonomous driving, enabling decision-making with diverse trajectory candidates. A key component is a robust trajectory scorer capable of selecting the optimal trajectory from these candidates. While recent trajectory scorers focus on scoring either large sets of static trajectories or small sets of dynamically generated ones, both approaches face significant limitations in generalization. Static vocabularies provide effective coarse discretization but struggle to make fine-grained adaptation, while dynamic proposals offer detailed precision but fail to capture broader trajectory distributions. To overcome these challenges, we propose GTRS (Generalized Trajectory Scoring), a unified framework for end-to-end multi-modal planning that combines coarse and fine-grained trajectory evaluation. GTRS consists of three complementary innovations: (1) a diffusion-based trajectory generator that produces diverse fine-grained proposals; (2) a vocabulary generalization technique that trains a scorer on super-dense trajectory sets with dropout regularization, enabling its robust inference on smaller subsets; and (3) a sensor augmentation strategy that enhances out-of-domain generalization while incorporating refinement training for critical trajectory discrimination. As the winning solution of the Navsim v2 Challenge, GTRS demonstrates superior performance even with sub-optimal sensor inputs, approaching privileged methods that rely on ground-truth perception. Code will be available at https://github.com/NVlabs/GTRS.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2506.05957.pdf' target='_blank'>https://arxiv.org/pdf/2506.05957.pdf</a></span>   <span><a href='https://github.com/tianyao-aka/PrunE-GraphOOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianjun Yao, Haoxuan Li, Yongqiang Chen, Tongliang Liu, Le Song, Eric Xing, Zhiqiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05957">Pruning Spurious Subgraphs for Graph Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) often encounter significant performance degradation under distribution shifts between training and test data, hindering their applicability in real-world scenarios. Recent studies have proposed various methods to address the out-of-distribution generalization challenge, with many methods in the graph domain focusing on directly identifying an invariant subgraph that is predictive of the target label. However, we argue that identifying the edges from the invariant subgraph directly is challenging and error-prone, especially when some spurious edges exhibit strong correlations with the targets. In this paper, we propose PrunE, the first pruning-based graph OOD method that eliminates spurious edges to improve OOD generalizability. By pruning spurious edges, PrunE retains the invariant subgraph more comprehensively, which is critical for OOD generalization. Specifically, PrunE employs two regularization terms to prune spurious edges: 1) graph size constraint to exclude uninformative spurious edges, and 2) $ε$-probability alignment to further suppress the occurrence of spurious edges. Through theoretical analysis and extensive experiments, we show that PrunE achieves superior OOD performance and outperforms previous state-of-the-art methods significantly. Codes are available at: \href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2506.05814.pdf' target='_blank'>https://arxiv.org/pdf/2506.05814.pdf</a></span>   <span><a href='https://github.com/Aalto-QuML/PIPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yogesh Verma, Amauri H. Souza, Vikas Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05814">Positional Encoding meets Persistent Homology on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The local inductive bias of message-passing graph neural networks (GNNs) hampers their ability to exploit key structural information (e.g., connectivity and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged as two promising approaches to mitigate this issue. PE schemes endow GNNs with location-aware features, while PH methods enhance GNNs with multiresolution topological features. However, a rigorous theoretical characterization of the relative merits and shortcomings of PE and PH has remained elusive. We bridge this gap by establishing that neither paradigm is more expressive than the other, providing novel constructions where one approach fails but the other succeeds. Our insights inform the design of a novel learnable method, PiPE (Persistence-informed Positional Encoding), which is provably more expressive than both PH and PE. PiPE demonstrates strong performance across a variety of tasks (e.g., molecule property prediction, graph classification, and out-of-distribution generalization), thereby advancing the frontiers of graph representation learning. Code is available at https://github.com/Aalto-QuML/PIPE.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2506.03706.pdf' target='_blank'>https://arxiv.org/pdf/2506.03706.pdf</a></span>   <span><a href='https://github.com/adityagandhamal/OV-COAST/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Gandhamal, Aniruddh Sikdar, Suresh Sundaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03706">OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary semantic segmentation (OVSS) entails assigning semantic labels to each pixel in an image using textual descriptions, typically leveraging world models such as CLIP. To enhance out-of-domain generalization, we propose Cost Aggregation with Optimal Transport (OV-COAST) for open-vocabulary semantic segmentation. To align visual-language features within the framework of optimal transport theory, we employ cost volume to construct a cost matrix, which quantifies the distance between two distributions. Our approach adopts a two-stage optimization strategy: in the first stage, the optimal transport problem is solved using cost volume via Sinkhorn distance to obtain an alignment solution; in the second stage, this solution is used to guide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS models on the MESS benchmark, where our approach notably improves the performance of the cost-aggregation model CAT-Seg with ViT-B backbone, achieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 % mIoU. The code is available at https://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ .
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2505.23173.pdf' target='_blank'>https://arxiv.org/pdf/2505.23173.pdf</a></span>   <span><a href='https://github.com/s-enmt/PseudoDomainBed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shohei Enomoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23173">Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models often struggle to maintain performance when deployed on data distributions different from their training data, particularly in real-world applications where environmental conditions frequently change. While Multi-source Domain Generalization (MDG) has shown promise in addressing this challenge by leveraging multiple source domains during training, its practical application is limited by the significant costs and difficulties associated with creating multi-domain datasets. To address this limitation, we propose Pseudo Multi-source Domain Generalization (PMDG), a novel framework that enables the application of sophisticated MDG algorithms in more practical Single-source Domain Generalization (SDG) settings. PMDG generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques, creating a synthetic multi-domain dataset that can be used with existing MDG algorithms. Through extensive experiments with PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze the effectiveness of PMDG across multiple datasets and architectures. Our analysis reveals several key findings, including a positive correlation between MDG and PMDG performance and the potential of pseudo-domains to match or exceed actual multi-domain performance with sufficient data. These comprehensive empirical results provide valuable insights for future research in domain generalization. Our code is available at https://github.com/s-enmt/PseudoDomainBed.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2505.22465.pdf' target='_blank'>https://arxiv.org/pdf/2505.22465.pdf</a></span>   <span><a href='https://github.com/zobia111/SDG-Alzheimer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zobia Batool, Huseyin Ozkan, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22465">Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although Alzheimer's disease detection via MRIs has advanced significantly thanks to contemporary deep learning models, challenges such as class imbalance, protocol variations, and limited dataset diversity often hinder their generalization capacity. To address this issue, this article focuses on the single domain generalization setting, where given the data of one domain, a model is designed and developed with maximal performance w.r.t. an unseen domain of distinct distribution. Since brain morphology is known to play a crucial role in Alzheimer's diagnosis, we propose the use of learnable pseudo-morphological modules aimed at producing shape-aware, anatomically meaningful class-specific augmentations in combination with a supervised contrastive learning module to extract robust class-specific representations. Experiments conducted across three datasets show improved performance and generalization capacity, especially under class imbalance and imaging protocol variations. The source code will be made available upon acceptance at https://github.com/zobia111/SDG-Alzheimer.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2505.21828.pdf' target='_blank'>https://arxiv.org/pdf/2505.21828.pdf</a></span>   <span><a href='https://github.com/YuehHanChen/SAGE-Eval/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Yueh-Han, Guy Davidson, Brenden M. Lake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21828">SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Do LLMs robustly generalize critical safety facts to novel situations? Lacking this ability is dangerous when users ask naive questions. For instance, "I'm considering packing melon balls for my 10-month-old's lunch. What other foods would be good to include?" Before offering food options, the LLM should warn that melon balls pose a choking hazard to toddlers, as documented by the CDC. Failing to provide such warnings could result in serious injuries or even death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic GEneralization evaluation, the first benchmark that tests whether LLMs properly apply well established safety facts to naive user queries. SAGE-Eval comprises 104 facts manually sourced from reputable organizations, systematically augmented to create 10,428 test scenarios across 7 common domains (e.g., Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet, passes only 58% of all the safety facts tested. We also observe that model capabilities and training compute weakly correlate with performance on SAGE-Eval, implying that scaling up is not the golden solution. Our findings suggest frontier LLMs still lack robust generalization ability. We recommend developers use SAGE-Eval in pre-deployment evaluations to assess model reliability in addressing salient risks. We publicly release SAGE-Eval at https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available at https://github.com/YuehHanChen/SAGE-Eval/tree/main.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2505.20653.pdf' target='_blank'>https://arxiv.org/pdf/2505.20653.pdf</a></span>   <span><a href='https://github.com/Lynn0925/RoGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyu Qiu, Ke Jiang, Xiaoyang Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20653">RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in domain generalization for deepfake detection have attracted significant attention, with previous methods often incorporating additional modules to prevent overfitting to domain-specific patterns. However, such regularization can hinder the optimization of the empirical risk minimization (ERM) objective, ultimately degrading model performance. In this paper, we propose a novel learning objective that aligns generalization gradient updates with ERM gradient updates. The key innovation is the application of perturbations to model parameters, aligning the ascending points across domains, which specifically enhances the robustness of deepfake detection models to domain shifts. This approach effectively preserves domain-invariant features while managing domain-specific characteristics, without introducing additional regularization. Experimental results on multiple challenging deepfake detection datasets demonstrate that our gradient alignment strategy outperforms state-of-the-art domain generalization techniques, confirming the efficacy of our method. The code is available at https://github.com/Lynn0925/RoGA.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2505.20446.pdf' target='_blank'>https://arxiv.org/pdf/2505.20446.pdf</a></span>   <span><a href='https://github.com/azencot-group/ImagenFew' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tal Gonen, Itai Pemper, Ilan Naiman, Nimrod Berman, Omri Azencot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20446">Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling of time series is a central challenge in time series analysis, particularly under data-scarce conditions. Despite recent advances in generative modeling, a comprehensive understanding of how state-of-the-art generative models perform under limited supervision remains lacking. In this work, we conduct the first large-scale study evaluating leading generative models in data-scarce settings, revealing a substantial performance gap between full-data and data-scarce regimes. To close this gap, we propose a unified diffusion-based generative framework that can synthesize high-fidelity time series across diverse domains using just a few examples. Our model is pre-trained on a large, heterogeneous collection of time series datasets, enabling it to learn generalizable temporal representations. It further incorporates architectural innovations such as dynamic convolutional layers for flexible channel adaptation and dataset token conditioning for domain-aware generation. Without requiring abundant supervision, our unified model achieves state-of-the-art performance in few-shot settings-outperforming domain-specific baselines across a wide range of subset sizes. Remarkably, it also surpasses all baselines even when tested on full datasets benchmarks, highlighting the strength of pre-training and cross-domain generalization. We hope this work encourages the community to revisit few-shot generative modeling as a key problem in time series research and pursue unified solutions that scale efficiently across domains. Code is available at https://github.com/azencot-group/ImagenFew.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2505.19659.pdf' target='_blank'>https://arxiv.org/pdf/2505.19659.pdf</a></span>   <span><a href='https://github.com/backpropagator/LangDAug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Piyush Tiwary, Kinjawl Bhattacharyya, Prathosh A. P
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19659">LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DAug). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DAug methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel $\textbf{Lang}$evin $\textbf{D}$ata $\textbf{Aug}$mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2505.17612.pdf' target='_blank'>https://arxiv.org/pdf/2505.17612.pdf</a></span>   <span><a href='https://github.com/Nardien/agent-distillation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17612">Distilling LLM Agent into Small Models with Retrieval and Code Tools</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2505.17017.pdf' target='_blank'>https://arxiv.org/pdf/2505.17017.pdf</a></span>   <span><a href='https://github.com/ZiyuGuo99/Image-Generation-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17017">Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2505.15734.pdf' target='_blank'>https://arxiv.org/pdf/2505.15734.pdf</a></span>   <span><a href='https://github.com/ctrl-gaurav/Debate-Train-Evolve' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Srivastava, Zhenyu Bi, Meng Lu, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15734">DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on seven reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities. Our framework code and trained models are publicly available at https://github.com/ctrl-gaurav/Debate-Train-Evolve
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2505.15545.pdf' target='_blank'>https://arxiv.org/pdf/2505.15545.pdf</a></span>   <span><a href='https://github.com/andrewcaunes/ia4markings' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Caunes, Thierry Chateau, Vincent Fremont
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15545">seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D semantic segmentation plays a pivotal role in autonomous driving and road infrastructure analysis, yet state-of-the-art 3D models are prone to severe domain shift when deployed across different datasets. We propose a novel multi-view projection framework that excels in both domain generalization (DG) and unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans into coherent 3D scenes and renders them from multiple virtual camera poses to create a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D segmentation model in-domain. During inference, the model processes hundreds of views per scene; the resulting logits are back-projected to 3D with an occlusion-aware voting scheme to generate final point-wise labels. Our framework is modular and enables extensive exploration of key design parameters, such as view generation optimization (VGO), visualization modality optimization (MODO), and 2D model choice. We evaluate on the nuScenes and SemanticKITTI datasets under both the DG and UDA settings. We achieve state-of-the-art results in UDA and close to state-of-the-art in DG, with particularly large gains on large, static classes. Our code and dataset generation tools will be publicly available at https://github.com/andrewcaunes/ia4markings
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2505.13233.pdf' target='_blank'>https://arxiv.org/pdf/2505.13233.pdf</a></span>   <span><a href='https://github.com/BIT-DA/ABS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lincan Cai, Jingxuan Kang, Shuang Li, Wenxuan Ma, Binhui Xie, Zhida Qin, Jian Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13233">From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive zero-shot capabilities on downstream tasks. Prior research highlights the crucial role of visual augmentation techniques, like random cropping, in alignment with fine-grained class descriptions generated by large language models (LLMs), significantly enhancing zero-shot performance by incorporating multi-view information. However, the inherent randomness of these augmentations can inevitably introduce background artifacts and cause models to overly focus on local details, compromising global semantic understanding. To address these issues, we propose an \textbf{A}ttention-\textbf{B}ased \textbf{S}election (\textbf{ABS}) method from local details to global context, which applies attention-guided cropping in both raw images and feature space, supplement global semantic information through strategic feature selection. Additionally, we introduce a soft matching technique to effectively filter LLM descriptions for better alignment. \textbf{ABS} achieves state-of-the-art performance on out-of-distribution generalization and zero-shot classification tasks. Notably, \textbf{ABS} is training-free and even rivals few-shot and test-time adaptation methods. Our code is available at \href{https://github.com/BIT-DA/ABS}{\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2505.13232.pdf' target='_blank'>https://arxiv.org/pdf/2505.13232.pdf</a></span>   <span><a href='https://github.com/alinlab/StarFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Younghyun Kim, Jongheon Jeong, Sangkyung Kwak, Kyungmin Lee, Juho Lee, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13232">StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning robust representations from data often requires scale, which has led to the success of recent zero-shot models such as CLIP. However, the obtained robustness can easily be deteriorated when these models are fine-tuned on other downstream tasks (e.g., of smaller scales). Previous works often interpret this phenomenon in the context of domain shift, developing fine-tuning methods that aim to preserve the original domain as much as possible. However, in a different context, fine-tuned models with limited data are also prone to learning features that are spurious to humans, such as background or texture. In this paper, we propose StarFT (Spurious Textual Alignment Regularization), a novel framework for fine-tuning zero-shot models to enhance robustness by preventing them from learning spuriosity. We introduce a regularization that aligns the output distribution for spuriosity-injected labels with the original zero-shot model, ensuring that the model is not induced to extract irrelevant features further from these descriptions. We leverage recent language models to get such spuriosity-injected labels by generating alternative textual descriptions that highlight potentially confounding features. Extensive experiments validate the robust generalization of StarFT and its emerging properties: zero-shot group robustness and improved zero-shot classification. Notably, StarFT boosts both worst-group and average accuracy by 14.30% and 3.02%, respectively, in the Waterbirds group shift scenario, where other robust fine-tuning baselines show even degraded performance.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2505.12697.pdf' target='_blank'>https://arxiv.org/pdf/2505.12697.pdf</a></span>   <span><a href='https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_Coder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaofan Li, Jianlyu Chen, Yingxia Shao, Defu Lian, Zheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12697">Towards A Generalist Code Embedding Model Based On Massive Data Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code embedding models attract increasing attention due to the widespread popularity of retrieval-augmented generation (RAG) in software development. These models are expected to capture the rich semantic relationships inherent to code, which differ significantly from those found in text. However, existing models remain severely limited due to the scarcity of high-quality training data. In this work, we introduce \textbf{CodeR} (\underline{Code} \underline{R}etrieval), a state-of-the-art embedding model for general-purpose code retrieval. The superior performance of CodeR is built upon CodeR-Pile, a large-scale synthetic dataset constructed under the DRU (Diversity, Reliability, Usability) principle via a novel data synthesis pipeline. To optimize training effectiveness, we propose Annealing, a curriculum learning strategy that enables effective knowledge transfer across heterogeneous sources of data. We evaluate CodeR based on 16 diverse code retrieval tasks, where it significantly outperforms existing baselines and exhibits strong out-of-domain generalization performance. We have publicly released our code and the well-trained model to facilitate further research in this critical area. https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_Coder.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2505.11883.pdf' target='_blank'>https://arxiv.org/pdf/2505.11883.pdf</a></span>   <span><a href='https://github.com/zihuanqiu/MINGLE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihuan Qiu, Yi Xu, Chiyuan He, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11883">MINGLE: Mixture of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual model merging integrates independently fine-tuned models sequentially without access to the original training data, offering a scalable and efficient solution for continual learning. However, existing methods face two critical challenges: parameter interference among tasks, which leads to catastrophic forgetting, and limited adaptability to evolving test distributions. To address these issues, we introduce the task of Test-Time Continual Model Merging (TTCMM), which leverages a small set of unlabeled test samples during inference to alleviate parameter conflicts and handle distribution shifts. We propose MINGLE, a novel framework for TTCMM. MINGLE employs a mixture-of-experts architecture with parameter-efficient, low-rank experts, which enhances adaptability to evolving test distributions while dynamically merging models to mitigate conflicts. To further reduce forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations, thereby suppressing activations on old tasks and preserving past knowledge. We further introduce an Adaptive Relaxation Strategy that adjusts constraint strength dynamically based on interference signals observed during test-time adaptation, striking a balance between stability and adaptability. Extensive experiments on standard continual merging benchmarks demonstrate that MINGLE achieves robust generalization, significantly reduces forgetting, and consistently surpasses previous state-of-the-art methods by 7-9\% on average across diverse task orders. Our code is available at: https://github.com/zihuanqiu/MINGLE
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2505.11849.pdf' target='_blank'>https://arxiv.org/pdf/2505.11849.pdf</a></span>   <span><a href='https://github.com/NellyW8/VeriReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiting Wang, Guoheng Sun, Wanghao Ye, Gang Qu, Ang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11849">VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating Register Transfer Level (RTL) code generation using Large Language Models (LLMs) offers substantial promise for streamlining digital circuit design and reducing human effort. However, current LLM-based approaches face significant challenges with training data scarcity, poor specification-code alignment, lack of verification mechanisms, and balancing generalization with specialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework integrating supervised fine-tuning with Guided Reward Proximal Optimization (GRPO) reinforcement learning for RTL generation. Using curated training examples and a feedback-driven reward model, VeriReason combines testbench evaluations with structural heuristics while embedding self-checking capabilities for autonomous error correction. On the VerilogEval Benchmark, VeriReason delivers significant improvements: achieving 83.1% functional correctness on the VerilogEval Machine benchmark, substantially outperforming both comparable-sized models and much larger commercial systems like GPT-4 Turbo. Additionally, our approach demonstrates up to a 2.8X increase in first-attempt functional correctness compared to baseline methods and exhibits robust generalization to unseen designs. To our knowledge, VeriReason represents the first system to successfully integrate explicit reasoning capabilities with reinforcement learning for Verilog generation, establishing a new state-of-the-art for automated RTL synthesis. The models and datasets are available at: https://huggingface.co/collections/AI4EDA-CASE Code is Available at: https://github.com/NellyW8/VeriReason
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2505.11099.pdf' target='_blank'>https://arxiv.org/pdf/2505.11099.pdf</a></span>   <span><a href='https://github.com/L1277471578/HyMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Liu, Chunyang Wang, Xuelian Liu, Bo Xiao, Guan Xi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11099">HyMamba: Mamba with Hybrid Geometry-Feature Coupling for Efficient Point Cloud Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud classification is one of the essential technologies for achieving intelligent perception of 3D environments by machines, its core challenge is to efficiently extract local and global features. Mamba leverages state space models (SSMs) for global point cloud modeling. Although prior Mamba-based point cloud processing methods pay attention to the limitation of its flattened sequence modeling mechanism in fusing local and global features, the critical issue of weakened local geometric relevance caused by decoupling geometric structures and features in the input patches remains not fully revealed, and both jointly limit local feature extraction. Therefore, we propose HyMamba, a geometry and feature coupled Mamba framework featuring: (1) Geometry-Feature Coupled Pooling (GFCP), which achieves physically interpretable geometric information coupling by dynamically aggregating adjacent geometric information into local features; (2) Collaborative Feature Enhancer (CoFE), which enhances sparse signal capture through cross-path feature hybridization while effectively integrating global and local contexts. We conducted extensive experiments on ModelNet40 and ScanObjectNN datasets. The results demonstrate that the proposed model achieves superior classification performance, particularly on the ModelNet40, where it elevates accuracy to 95.99% with merely 0.03M additional parameters. Furthermore, it attains 98.9% accuracy on the ModelNetFewShot dataset, validating its robust generalization capabilities under sparse samples. Our code and weights are available at https://github.com/L1277471578/HyMamba
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2505.10231.pdf' target='_blank'>https://arxiv.org/pdf/2505.10231.pdf</a></span>   <span><a href='https://github.com/Roypic/Aligner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Luo, Ziyu Zhou, Zixin Shu, AurÃ©lie Pahud de Mortanges, Robert Berke, Mauricio Reyes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10231">On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. Our code is available at https://github.com/Roypic/Aligner.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2505.09274.pdf' target='_blank'>https://arxiv.org/pdf/2505.09274.pdf</a></span>   <span><a href='https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fares Bougourzi, Abdenour Hadid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09274">Recent Advances in Medical Imaging Segmentation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical imaging is a cornerstone of modern healthcare, driving advancements in diagnosis, treatment planning, and patient care. Among its various tasks, segmentation remains one of the most challenging problem due to factors such as data accessibility, annotation complexity, structural variability, variation in medical imaging modalities, and privacy constraints. Despite recent progress, achieving robust generalization and domain adaptation remains a significant hurdle, particularly given the resource-intensive nature of some proposed models and their reliance on domain expertise. This survey explores cutting-edge advancements in medical image segmentation, focusing on methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and Universal Models. These approaches offer promising solutions to longstanding challenges. We provide a comprehensive overview of the theoretical foundations, state-of-the-art techniques, and recent applications of these methods. Finally, we discuss inherent limitations, unresolved issues, and future research directions aimed at enhancing the practicality and accessibility of segmentation models in medical imaging. We are maintaining a \href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub Repository} to continue tracking and updating innovations in this field.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2505.08459.pdf' target='_blank'>https://arxiv.org/pdf/2505.08459.pdf</a></span>   <span><a href='https://github.com/hsushuai/SAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Xu, Sijia Cui, Yanna Wang, Bo Xu, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08459">Strategy-Augmented Planning for Large Language Models via Opponent Exploitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently modeling and exploiting opponents is a long-standing challenge in adversarial domains. Large Language Models (LLMs) trained on extensive textual data have recently demonstrated outstanding performance in general tasks, introducing new research directions for opponent modeling. Some studies primarily focus on directly using LLMs to generate decisions based on the elaborate prompt context that incorporates opponent descriptions, while these approaches are limited to scenarios where LLMs possess adequate domain expertise. To address that, we introduce a two-stage Strategy-Augmented Planning (SAP) framework that significantly enhances the opponent exploitation capabilities of LLM-based agents by utilizing a critical component, the Strategy Evaluation Network (SEN). Specifically, in the offline stage, we construct an explicit strategy space and subsequently collect strategy-outcome pair data for training the SEN network. During the online phase, SAP dynamically recognizes the opponent's strategies and greedily exploits them by searching best response strategy on the well-trained SEN, finally translating strategy to a course of actions by carefully designed prompts. Experimental results show that SAP exhibits robust generalization capabilities, allowing it to perform effectively not only against previously encountered opponent strategies but also against novel, unseen strategies. In the MicroRTS environment, SAP achieves a $85.35\%$ performance improvement over baseline methods and matches the competitiveness of reinforcement learning approaches against state-of-the-art (SOTA) rule-based AI. Our code is available at https://github.com/hsushuai/SAP.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2505.07675.pdf' target='_blank'>https://arxiv.org/pdf/2505.07675.pdf</a></span>   <span><a href='https://github.com/erjui/DHO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seongjae Kang, Dong Bok Lee, Hyungjoon Jang, Sung Ju Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07675">Simple yet Effective Semi-supervised Knowledge Distillation from Vision-Language Models via Dual-Head Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised learning (SSL) has emerged as a practical solution for addressing data scarcity challenges by leveraging unlabeled data. Recently, vision-language models (VLMs), pre-trained on massive image-text pairs, have demonstrated remarkable zero-/few-shot performance that often surpasses SSL approaches due to their exceptional generalization capabilities. This gap motivates us to question: how can we effectively harness the powerful generalization capabilities of VLMs into task-specific models? Knowledge distillation (KD) offers a natural framework for transferring VLM capabilities, but we identify that it suffers from gradient conflicts between supervised and distillation losses. To address this challenge, we propose Dual-Head Optimization (DHO), which introduces dual prediction heads for each distinct signal. We observe that DHO resolves gradient conflicts, enabling improved feature learning compared to single-head KD baselines, with practical benefits of minimal computational overhead and test-time hyperparameter tuning without retraining. Extensive experiments across 15 datasets show that DHO consistently outperforms KD baselines, often outperforming teacher models with smaller student models. DHO also achieves new state-of-the-art performance on both in-distribution ImageNet semi-supervised learning and out-of-distribution generalization across ImageNet variants. We publicly release our code and model checkpoints to facilitate future research at https://github.com/erjui/DHO.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2505.07219.pdf' target='_blank'>https://arxiv.org/pdf/2505.07219.pdf</a></span>   <span><a href='https://github.com/qinhongda8/LDDS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/qinhongda8/LDDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongda Qin, Xiao Lu, Zhiyong Wei, Yihong Cao, Kailun Yang, Ningjiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07219">Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector's backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at https://github.com/qinhongda8/LDDS.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2505.03539.pdf' target='_blank'>https://arxiv.org/pdf/2505.03539.pdf</a></span>   <span><a href='https://github.com/MengfeiD/PanOoS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MengfeiD/PanOoS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengfei Duan, Kailun Yang, Yuheng Zhang, Yihong Cao, Fei Teng, Kai Luo, Jiaming Zhang, Zhiyong Li, Shutao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03539">Panoramic Out-of-Distribution Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic imaging enables capturing 360Â° images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to background clutter and pixel distortions. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2505.02515.pdf' target='_blank'>https://arxiv.org/pdf/2505.02515.pdf</a></span>   <span><a href='https://github.com/pizzareapers/FedSDAF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongze Li, Zesheng Zhou, Zhenbiao Cao, Xinhui Li, Wei Chen, Xiaojin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02515">FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Federated Domain Generalization (FedDG) methods focus on learning domain-invariant features or adapting to unseen target domains, often overlooking the unique knowledge embedded within the source domain, especially in strictly isolated federated learning environments. Through experimentation, we discovered a counterintuitive phenomenon.: features learned from a complete source domain have superior generalization capabilities compared to those learned directly from the target domain. This insight leads us to propose the Federated Source Domain Awareness Framework (FedSDAF), the first systematic approach to enhance FedDG by leveraging source domain-aware features. FedSDAF employs a dual-adapter architecture that decouples "local expertise" from "global generalization consensus". A Domain-Aware Adapter, retained locally, extracts and protects the unique discriminative knowledge of each source domain, while a Domain-Invariant Adapter, shared across clients, builds a robust global consensus. To enable knowledge exchange, we introduce a Bidirectional Knowledge Distillation mechanism that facilitates efficient dialogue between the adapters. Extensive experiments on four benchmark datasets (OfficeHome, PACS, VLCS, DomainNet) show that FedSDAF significantly outperforms existing FedDG methods.The source code is available at https://github.com/pizzareapers/FedSDAF.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2505.02182.pdf' target='_blank'>https://arxiv.org/pdf/2505.02182.pdf</a></span>   <span><a href='https://github.com/Purdue-M2/SP_CUP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yamini Sri Krubha, Aryana Hou, Braden Vester, Web Walker, Xin Wang, Li Lin, Shu Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02182">Robust AI-Generated Face Detection with Imbalanced Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfakes, created using advanced AI techniques such as Variational Autoencoder and Generative Adversarial Networks, have evolved from research and entertainment applications into tools for malicious activities, posing significant threats to digital trust. Current deepfake detection techniques have evolved from CNN-based methods focused on local artifacts to more advanced approaches using vision transformers and multimodal models like CLIP, which capture global anomalies and improve cross-domain generalization. Despite recent progress, state-of-the-art deepfake detectors still face major challenges in handling distribution shifts from emerging generative models and addressing severe class imbalance between authentic and fake samples in deepfake datasets, which limits their robustness and detection accuracy. To address these challenges, we propose a framework that combines dynamic loss reweighting and ranking-based optimization, which achieves superior generalization and performance under imbalanced dataset conditions. The code is available at https://github.com/Purdue-M2/SP_CUP.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2504.21063.pdf' target='_blank'>https://arxiv.org/pdf/2504.21063.pdf</a></span>   <span><a href='https://github.com/GongShuai8210/TRIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Gong, Chaoran Cui, Xiaolin Dong, Xiushan Nie, Lei Zhu, Xiaojun Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21063">Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization (FedDG) aims to learn a globally generalizable model from decentralized clients with heterogeneous data while preserving privacy. Recent studies have introduced prompt learning to adapt vision-language models (VLMs) in FedDG by learning a single global prompt. However, such a one-prompt-fits-all learning paradigm typically leads to performance degradation on personalized samples. Although the mixture of experts (MoE) offers a promising solution for specialization, existing MoE-based methods suffer from coarse image-level expert assignment and high communication costs from parameterized routers. To address these limitations, we propose TRIP, a Token-level prompt mixture with parameter-free routing framework for FedDG, which treats multiple prompts as distinct experts. Unlike existing image-level routing designs, TRIP assigns different tokens within an image to specific experts. To ensure communication efficiency, TRIP incorporates a parameter-free routing mechanism based on token clustering and optimal transport. The instance-specific prompt is then synthesized by aggregating experts, weighted by the number of tokens assigned to each. Additionally, TRIP develops an unbiased learning strategy for prompt experts, leveraging the VLM's zero-shot generalization capability. Extensive experiments across four benchmarks demonstrate that TRIP achieves optimal generalization results, with communication of only 1K parameters per round. Our code is available at https://github.com/GongShuai8210/TRIP.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2504.20571.pdf' target='_blank'>https://arxiv.org/pdf/2504.20571.pdf</a></span>   <span><a href='https://github.com/ypwang61/One-Shot-RLVR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ypwang61/One-Shot-RLVR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20571">Reinforcement Learning for Reasoning in Large Language Models with One Training Example</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the mathematical reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Furthermore, RLVR with only two examples even slightly exceeds these results (MATH500: 74.8%, average: 36.6%). Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. We also show the critical role of promoting exploration (e.g., by incorporating entropy loss with an appropriate coefficient) in 1-shot RLVR training. We also further discuss related observations about format correction, label robustness and prompt modification. These findings can inspire future work on RLVR efficiency and encourage a re-examination of recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2504.19737.pdf' target='_blank'>https://arxiv.org/pdf/2504.19737.pdf</a></span>   <span><a href='https://github.com/Abhishek19009/CoDEx' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhishek Kuriyal, Elliot Vincent, Mathieu Aubry, Loic Landrieu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19737">CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global variations in terrain appearance raise a major challenge for satellite image analysis, leading to poor model performance when training on locations that differ from those encountered at test time. This remains true even with recent large global datasets. To address this challenge, we propose a novel domain-generalization framework for satellite images. Instead of trying to learn a single generalizable model, we train one expert model per training domain, while learning experts' similarity and encouraging similar experts to be consistent. A model selection module then identifies the most suitable experts for a given test sample and aggregates their predictions. Experiments on four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent gains over existing domain generalization and adaptation methods. Our code is publicly available at https://github.com/Abhishek19009/CoDEx.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2504.19574.pdf' target='_blank'>https://arxiv.org/pdf/2504.19574.pdf</a></span>   <span><a href='https://github.com/sminhwang/DG-DETR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seongmin Hwang, Daeyoung Han, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19574">DG-DETR: Toward Domain Generalized Detection Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end Transformer-based detectors (DETRs) have demonstrated strong detection performance. However, domain generalization (DG) research has primarily focused on convolutional neural network (CNN)-based detectors, while paying little attention to enhancing the robustness of DETRs. In this letter, we introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple, effective, and plug-and-play method that improves out-of-distribution (OOD) robustness for DETRs. Specifically, we propose a novel domain-agnostic query selection strategy that removes domain-induced biases from object queries via orthogonal projection onto the instance-specific style space. Additionally, we leverage a wavelet decomposition to disentangle features into domain-invariant and domain-specific components, enabling synthesis of diverse latent styles while preserving the semantic features of objects. Experimental results validate the effectiveness of DG-DETR. Our code is available at https://github.com/sminhwang/DG-DETR.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2504.17515.pdf' target='_blank'>https://arxiv.org/pdf/2504.17515.pdf</a></span>   <span><a href='https://github.com/orange-czh/Mamba-Sea' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/orange-czh/Mamba-Sea' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Cheng, Jintao Guo, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17515">Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To segment medical images with distribution shifts, domain generalization (DG) has emerged as a promising setting to train models on source domains that can generalize to unseen target domains. Existing DG methods are mainly based on CNN or ViT architectures. Recently, advanced state space models, represented by Mamba, have shown promising results in various supervised medical image segmentation. The success of Mamba is primarily owing to its ability to capture long-range dependencies while keeping linear complexity with input sequence length, making it a promising alternative to CNNs and ViTs. Inspired by the success, in the paper, we explore the potential of the Mamba architecture to address distribution shifts in DG for medical image segmentation. Specifically, we propose a novel Mamba-based framework, Mamba-Sea, incorporating global-to-local sequence augmentation to improve the model's generalizability under domain shift issues. Our Mamba-Sea introduces a global augmentation mechanism designed to simulate potential variations in appearance across different sites, aiming to suppress the model's learning of domain-specific information. At the local level, we propose a sequence-wise augmentation along input sequences, which perturbs the style of tokens within random continuous sub-sequences by modeling and resampling style statistics associated with domain shifts. To our best knowledge, Mamba-Sea is the first work to explore the generalization of Mamba for medical image segmentation, providing an advanced and promising Mamba-based architecture with strong robustness to domain shifts. Remarkably, our proposed method is the first to surpass a Dice coefficient of 90% on the Prostate dataset, which exceeds previous SOTA of 88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2504.16433.pdf' target='_blank'>https://arxiv.org/pdf/2504.16433.pdf</a></span>   <span><a href='https://github.com/HariseetharamG/FrogDogNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hariseetharam Gunduboina, Muhammad Haris Khan, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16433">FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, large-scale vision-language models (VLMs) like CLIP have gained attention for their zero-shot inference using instructional text prompts. While these models excel in general computer vision, their potential for domain generalization in remote sensing (RS) remains underexplored. Existing approaches enhance prompt learning by generating visual prompt tokens but rely on full-image features, introducing noise and background artifacts that vary within a class, causing misclassification. To address this, we propose FrogDogNet, a novel prompt learning framework integrating Fourier frequency filtering and self-attention to improve RS scene classification and domain generalization. FrogDogNet selectively retains invariant low-frequency components while eliminating noise and irrelevant backgrounds, ensuring robust feature representation across domains. The model first extracts significant features via projection and self-attention, then applies frequency-based filtering to preserve essential structural information for prompt learning. Extensive experiments on four RS datasets and three domain generalization tasks show that FrogDogNet consistently outperforms state-of-the-art prompt learning methods, demonstrating superior adaptability across domain shifts. Our findings highlight the effectiveness of frequency-based invariant feature retention in generalization, paving the way for broader applications. Our code is available at https://github.com/HariseetharamG/FrogDogNet
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2504.14280.pdf' target='_blank'>https://arxiv.org/pdf/2504.14280.pdf</a></span>   <span><a href='https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Li, Yongguang Li, Yali Fu, Jiahong Liu, Yixin Liu, Menglin Yang, Irwin King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14280">CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for enhancing model robustness across diverse environments. Contrastive Language-Image Pretraining (CLIP) plays a significant role in these tasks, offering powerful zero-shot capabilities that allow models to perform effectively in unseen domains. However, there remains a significant gap in the literature, as no comprehensive survey currently exists that systematically explores the applications of CLIP in DG and DA, highlighting the necessity for this review. This survey presents a comprehensive review of CLIP's applications in DG and DA. In DG, we categorize methods into optimizing prompt learning for task alignment and leveraging CLIP as a backbone for effective feature extraction, both enhancing model adaptability. For DA, we examine both source-available methods utilizing labeled source data and source-free approaches primarily based on target domain data, emphasizing knowledge transfer mechanisms and strategies for improved performance across diverse contexts. Key challenges, including overfitting, domain diversity, and computational efficiency, are addressed, alongside future research opportunities to advance robustness and efficiency in practical applications. By synthesizing existing literature and pinpointing critical gaps, this survey provides valuable insights for researchers and practitioners, proposing directions for effectively leveraging CLIP to enhance methodologies in domain generalization and adaptation. Ultimately, this work aims to foster innovation and collaboration in the quest for more resilient machine learning models that can perform reliably across diverse real-world scenarios. A more up-to-date version of the papers is maintained at: https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2504.14260.pdf' target='_blank'>https://arxiv.org/pdf/2504.14260.pdf</a></span>   <span><a href='https://github.com/TorchRWKV/flash-linear-attention' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Xiao, Li Zhiyuan, Lin Yueyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14260">Cross-attention for State-based model RWKV-7</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce CrossWKV, a novel cross-attention mechanism for the state-based RWKV-7 model, designed to enhance the expressive power of text-to-image generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV) architecture, CrossWKV integrates text and image modalities in a single pass, utilizing a generalized delta rule with vector-valued gating and low-rank adaptations (LoRA) to achieve superior cross-modal alignment. Unlike Transformer-based models, CrossWKV's non-diagonal, input-dependent transition matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$ complexity class, including all regular languages, as demonstrated by its ability to perform state-tracking tasks like $S_5$ permutation modeling. Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance while offering robust generalization across diverse prompts. The model's enhanced expressivity, combined with constant memory usage and linear scaling, positions it as a powerful solution for advanced cross-modal tasks, with potential applications in high-resolution generation and dynamic state manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2504.09696.pdf' target='_blank'>https://arxiv.org/pdf/2504.09696.pdf</a></span>   <span><a href='https://github.com/aeroplanepaper/GRPO-LEAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jixiao Zhang, Chunsheng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09696">GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Group Relative Policy Optimization (GRPO), which is widely adopted by R1-like reasoning models, has advanced mathematical reasoning. Nevertheless, GRPO faces challenges in reward sparsity, verbosity, and inadequate focus on problem difficulty. We propose GRPO-LEAD, enhancing GRPO with: (1) length-regularized rewards to encourage conciseness while maintaining accuracy; (2) explicit penalties for incorrect solutions to improve model precision; and (3) difficulty-aware advantage reweighting for robust generalization on challenging problems. Comprehensive evaluations demonstrate that GRPO-LEAD significantly improves reasoning accuracy, conciseness, and efficiency. Our approach achieves state-of-the-art performance for 14B-scale models, underscoring the synergy of our methods with appropriate model scale and high-quality data. Our source code, generated dataset, and models are available at https://github.com/aeroplanepaper/GRPO-LEAD.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2504.09448.pdf' target='_blank'>https://arxiv.org/pdf/2504.09448.pdf</a></span>   <span><a href='https://github.com/LinLLLL/BayesCAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Zhu, Xinbing Wang, Chenghu Zhou, Nanyang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09448">Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large pre-trained models showed promising results in few-shot learning. However, their generalization ability on two-dimensional Out-of-Distribution (OoD) data, i.e., correlation shift and diversity shift, has not been thoroughly investigated. Researches have shown that even with a significant amount of training data, few methods can achieve better performance than the standard empirical risk minimization method (ERM) in OoD generalization. This few-shot OoD generalization dilemma emerges as a challenging direction in deep neural network generalization research, where the performance suffers from overfitting on few-shot examples and OoD generalization errors. In this paper, leveraging a broader supervision source, we explore a novel Bayesian cross-modal image-text alignment learning method (Bayes-CAL) to address this issue. Specifically, the model is designed as only text representations are fine-tuned via a Bayesian modelling approach with gradient orthogonalization loss and invariant risk minimization (IRM) loss. The Bayesian approach is essentially introduced to avoid overfitting the base classes observed during training and improve generalization to broader unseen classes. The dedicated loss is introduced to achieve better image-text alignment by disentangling the causal and non-casual parts of image features. Numerical experiments demonstrate that Bayes-CAL achieved state-of-the-art OoD generalization performances on two-dimensional distribution shifts. Moreover, compared with CLIP-like models, Bayes-CAL yields more stable generalization performances on unseen classes. Our code is available at https://github.com/LinLLLL/BayesCAL.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2504.06220.pdf' target='_blank'>https://arxiv.org/pdf/2504.06220.pdf</a></span>   <span><a href='https://github.com/VisionXLab/Earth-Adapter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxing Hu, Ziyang Gong, Yupei Wang, Yuru Jia, Gen Luo, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06220">Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt powerful Foundation Models (FMs) to diverse downstream tasks while preserving and unleashing their inherent capabilities. However, we have observed that existing PEFT methods, which are often designed with natural imagery in mind, struggle when applied to Remote Sensing (RS) scenarios. This is primarily due to their inability to handle artifact influences, a problem particularly severe in RS image features. To tackle this challenge, we introduce Earth-Adapter, the first PEFT method specifically designed for RS artifacts conquering. Earth-Adapter introduces a novel Mixture of Frequency Adaptation process that combines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT). By utilizing DFT, Earth-Adapter can decompose features into different frequency components, precisely separating artifacts from original features. The MoA then dynamically assigns weights to each adapter expert, allowing for the combination of features across various frequency domains. These simple-yet-effective approaches enable Earth-Adapter to more efficiently overcome the disturbances caused by artifacts than previous PEFT methods, significantly enhancing the FMs' performance on RS scenarios. Experiments on Domain Adaptation (DA), and Domain Generalization (DG) semantic segmentation benchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline Rein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG benchmarks. Our code will be released at https://github.com/VisionXLab/Earth-Adapter.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2504.04517.pdf' target='_blank'>https://arxiv.org/pdf/2504.04517.pdf</a></span>   <span><a href='https://github.com/jaychempan/ETS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiancheng Pan, Yanxing Liu, Xiao He, Long Peng, Jiahao Li, Yuze Sun, Xiaomeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04517">Enhance Then Search: An Augmentation-Search Strategy with Foundation Models for Cross-Domain Few-Shot Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models pretrained on extensive datasets, such as GroundingDINO and LAE-DINO, have performed remarkably in the cross-domain few-shot object detection (CD-FSOD) task. Through rigorous few-shot training, we found that the integration of image-based data augmentation techniques and grid-based sub-domain search strategy significantly enhances the performance of these foundation models. Building upon GroundingDINO, we employed several widely used image augmentation methods and established optimization objectives to effectively navigate the expansive domain space in search of optimal sub-domains. This approach facilitates efficient few-shot object detection and introduces an approach to solving the CD-FSOD problem by efficiently searching for the optimal parameter configuration from the foundation model. Our findings substantially advance the practical deployment of vision-language models in data-scarce environments, offering critical insights into optimizing their cross-domain generalization capabilities without labor-intensive retraining. Code is available at https://github.com/jaychempan/ETS.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2504.02298.pdf' target='_blank'>https://arxiv.org/pdf/2504.02298.pdf</a></span>   <span><a href='https://github.com/ethanxyluo/SPACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Luo, Kecheng Chen, Pao-Sheng Vincent Sun, Chris Xing Tian, Arindam Basu, Haoliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02298">SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility. However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios. Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior. To address these challenges, we propose SPike-Aware Consistency Enhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data. We evaluate SPACE on multiple datasets. Furthermore, SPACE exhibits robust generalization across diverse network architectures, consistently enhancing the performance of SNNs on CNNs, Transformer, and ConvLSTM architectures. Experimental results show that SPACE outperforms state-of-the-art ANN methods while maintaining lower computational cost, highlighting its effectiveness and robustness for SNNs in real-world settings. The code will be available at https://github.com/ethanxyluo/SPACE.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2504.02272.pdf' target='_blank'>https://arxiv.org/pdf/2504.02272.pdf</a></span>   <span><a href='https://github.com/longshaocong/GCDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Long, Qianyu Zhou, Xiangtai Li, Chenhao Ying, Yunhai Tong, Lizhuang Ma, Yuan Luo, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02272">Generative Classifier for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to improve the generalizability of computer vision models toward distribution shifts. The mainstream DG methods focus on learning domain invariance, however, such methods overlook the potential inherent in domain-specific information. While the prevailing practice of discriminative linear classifier has been tailored to domain-invariant features, it struggles when confronted with diverse domain-specific information, e.g., intra-class shifts, that exhibits multi-modality. To address these issues, we explore the theoretical implications of relying on domain invariance, revealing the crucial role of domain-specific information in mitigating the target risk for DG. Drawing from these insights, we propose Generative Classifier-driven Domain Generalization (GCDG), introducing a generative paradigm for the DG classifier based on Gaussian Mixture Models (GMMs) for each class across domains. GCDG consists of three key modules: Heterogeneity Learning Classifier~(HLC), Spurious Correlation Blocking~(SCB), and Diverse Component Balancing~(DCB). Concretely, HLC attempts to model the feature distributions and thereby capture valuable domain-specific information via GMMs. SCB identifies the neural units containing spurious correlations and perturbs them, mitigating the risk of HLC learning spurious patterns. Meanwhile, DCB ensures a balanced contribution of components in HLC, preventing the underestimation or neglect of critical components. In this way, GCDG excels in capturing the nuances of domain-specific information characterized by diverse distributions. GCDG demonstrates the potential to reduce the target risk and encourage flat minima, improving the generalizability. Extensive experiments show GCDG's comparable performance on five DG benchmarks and one face anti-spoofing dataset, seamlessly integrating into existing DG methods with consistent improvements.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2503.22748.pdf' target='_blank'>https://arxiv.org/pdf/2503.22748.pdf</a></span>   <span><a href='https://github.com/yin-gz/SPARK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gongzhu Yin, Hongli Zhang, Yi Luo, Yuchen Yang, Kun Lu, Chao Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22748">Ignite Forecasting with SPARK: An Efficient Generative Framework for Refining LLMs in Temporal Knowledge Graph Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal Knowledge Graph (TKG) forecasting is crucial for predicting future events using historical data. With the surge of Large Language Models (LLMs), recent studies have begun exploring their integration into TKG forecasting and achieved some success. However, they still face limitations such as limited input length, inefficient output generation, and resource-intensive refinement, which undermine their performance and practical applicability. To address these limitations, we introduce SPARK, a Sequence-level Proxy-Adapting framework for Refining LLMs in TKG forecasting. Inspired by inference-time algorithms adopted in controlling generation, SPARK offers a cost-effective, plug-and-play solution through two key innovations: (1) Beam Sequence-Level Generation, which reframes TKG forecasting as a top-K sequence-level generation task, using beam search for efficiently generating next-entity distribution in a single forward pass. (2) TKG Adapter for Refinement, which employs traditional TKG models as trainable proxy adapters to leverage global graph information and refine LLM outputs, overcoming both the input length and the resource-intensive fine-tuning problems. Experiments across diverse datasets validate SPARK's forecasting performance, robust generalization capabilities, and high efficiency. We release source codes at https://github.com/yin-gz/SPARK.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2503.18483.pdf' target='_blank'>https://arxiv.org/pdf/2503.18483.pdf</a></span>   <span><a href='https://github.com/joeyz0z/LanCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zequn Zeng, Yudi Su, Jianqiao Sun, Tiansheng Wen, Hao Zhang, Zhengjue Wang, Bo Chen, Hongwei Liu, Jiawei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18483">Explaining Domain Shifts in Language: Concept erasing for Interpretable Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Concept-based models can map black-box representations to human-understandable concepts, which makes the decision-making process more transparent and then allows users to understand the reason behind predictions. However, domain-specific concepts often impact the final predictions, which subsequently undermine the model generalization capabilities, and prevent the model from being used in high-stake applications. In this paper, we propose a novel Language-guided Concept-Erasing (LanCE) framework. In particular, we empirically demonstrate that pre-trained vision-language models (VLMs) can approximate distinct visual domain shifts via domain descriptors while prompting large Language Models (LLMs) can easily simulate a wide range of descriptors of unseen visual domains. Then, we introduce a novel plug-in domain descriptor orthogonality (DDO) regularizer to mitigate the impact of these domain-specific concepts on the final predictions. Notably, the DDO regularizer is agnostic to the design of concept-based models and we integrate it into several prevailing models. Through evaluation of domain generalization on four standard benchmarks and three newly introduced benchmarks, we demonstrate that DDO can significantly improve the out-of-distribution (OOD) generalization over the previous state-of-the-art concept-based models.Our code is available at https://github.com/joeyz0z/LanCE.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2503.18294.pdf' target='_blank'>https://arxiv.org/pdf/2503.18294.pdf</a></span>   <span><a href='https://github.com/Falmi/LGPS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fiseha B. Tesema, Alejandro Guerra Manzanares, Tianxiang Cui, Qian Zhang, Moses Solomon, Sean He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18294">LGPS: A Lightweight GAN-Based Approach for Polyp Segmentation in Colonoscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Colorectal cancer (CRC) is a major global cause of cancer-related deaths, with early polyp detection and removal during colonoscopy being crucial for prevention. While deep learning methods have shown promise in polyp segmentation, challenges such as high computational costs, difficulty in segmenting small or low-contrast polyps, and limited generalizability across datasets persist. To address these issues, we propose LGPS, a lightweight GAN-based framework for polyp segmentation. LGPS incorporates three key innovations: (1) a MobileNetV2 backbone enhanced with modified residual blocks and Squeeze-and-Excitation (ResE) modules for efficient feature extraction; (2) Convolutional Conditional Random Fields (ConvCRF) for precise boundary refinement; and (3) a hybrid loss function combining Binary Cross-Entropy, Weighted IoU Loss, and Dice Loss to address class imbalance and enhance segmentation accuracy. LGPS is validated on five benchmark datasets and compared with state-of-the-art(SOTA) methods. On the largest and challenging PolypGen test dataset, LGPS achieves a Dice of 0.7299 and an IoU of 0.7867, outperformed all SOTA works and demonstrating robust generalization. With only 1.07 million parameters, LGPS is 17 times smaller than the smallest existing model, making it highly suitable for real-time clinical applications. Its lightweight design and strong performance underscore its potential for improving early CRC diagnosis. Code is available at https://github.com/Falmi/LGPS/.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2503.13915.pdf' target='_blank'>https://arxiv.org/pdf/2503.13915.pdf</a></span>   <span><a href='https://github.com/dongkwani/UPCSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongkwan Lee, Kyomin Hwang, Nojun Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13915">Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of semi-supervised domain generalization (SSDG), where the distributions of train and test data differ, and only a small amount of labeled data along with a larger amount of unlabeled data are available during training. Existing SSDG methods that leverage only the unlabeled samples for which the model's predictions are highly confident (confident-unlabeled samples), limit the full utilization of the available unlabeled data. To the best of our knowledge, we are the first to explore a method for incorporating the unconfident-unlabeled samples that were previously disregarded in SSDG setting. To this end, we propose UPCSC to utilize these unconfident-unlabeled samples in SSDG that consists of two modules: 1) Unlabeled Proxy-based Contrastive learning (UPC) module, treating unconfident-unlabeled samples as additional negative pairs and 2) Surrogate Class learning (SC) module, generating positive pairs for unconfident-unlabeled samples using their confusing class set. These modules are plug-and-play and do not require any domain labels, which can be easily integrated into existing approaches. Experiments on four widely used SSDG benchmarks demonstrate that our approach consistently improves performance when attached to baselines and outperforms competing plug-and-play methods. We also analyze the role of our method in SSDG, showing that it enhances class-level discriminability and mitigates domain gaps. The code is available at https://github.com/dongkwani/UPCSC.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2503.13012.pdf' target='_blank'>https://arxiv.org/pdf/2503.13012.pdf</a></span>   <span><a href='https://github.com/Yore0/TTDG-MGM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingguo Lv, Xingbo Dong, Liwen Wang, Jiewen Yang, Lei Zhao, Bin Pu, Zhe Jin, Xuejun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13012">Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite domain generalization (DG) has significantly addressed the performance degradation of pre-trained models caused by domain shifts, it often falls short in real-world deployment. Test-time adaptation (TTA), which adjusts a learned model using unlabeled test data, presents a promising solution. However, most existing TTA methods struggle to deliver strong performance in medical image segmentation, primarily because they overlook the crucial prior knowledge inherent to medical images. To address this challenge, we incorporate morphological information and propose a framework based on multi-graph matching. Specifically, we introduce learnable universe embeddings that integrate morphological priors during multi-source training, along with novel unsupervised test-time paradigms for domain adaptation. This approach guarantees cycle-consistency in multi-matching while enabling the model to more effectively capture the invariant priors of unseen data, significantly mitigating the effects of domain shifts. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches on two medical image segmentation benchmarks for both multi-source and single-source domain generalization tasks. The source code is available at https://github.com/Yore0/TTDG-MGM.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2503.12797.pdf' target='_blank'>https://arxiv.org/pdf/2503.12797.pdf</a></span>   <span><a href='https://github.com/thunlp/DeepPerception' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12797">DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\% accuracy improvements on KVG-Bench and exhibiting +4.60\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2503.10996.pdf' target='_blank'>https://arxiv.org/pdf/2503.10996.pdf</a></span>   <span><a href='https://github.com/GaotangLi/JUICE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaotang Li, Yuzhong Chen, Hanghang Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10996">Taming Knowledge Conflicts in Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language Models (LMs) often encounter knowledge conflicts when parametric memory contradicts contextual knowledge. Previous works attribute this conflict to the interplay between "memory heads" and "context heads", attention heads assumed to promote either memory or context exclusively. In this study, we go beyond this fundamental assumption by uncovering a critical phenomenon we term the superposition of contextual information and parametric memory, where highly influential attention heads simultaneously contribute to both memory and context. Building upon this insight, we propose Just Run Twice (JuICE), a test-time attention intervention method that steers LMs toward either parametric beliefs or contextual knowledge without requiring fine-tuning. JuICE identifies a set of reliable attention heads and leverages a dual-run approach to mitigate the superposition effects. Extensive experiments across 11 datasets and 6 model architectures demonstrate that JuICE sets the new state-of-the-art performance and robust generalization, achieving significant and consistent improvement across different domains under various conflict types. Finally, we theoretically analyze knowledge conflict and the superposition of contextual information and parametric memory in attention heads, which further elucidates the effectiveness of JuICE in these settings. Our code is available at https://github.com/GaotangLi/JUICE.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2503.10615.pdf' target='_blank'>https://arxiv.org/pdf/2503.10615.pdf</a></span>   <span><a href='https://github.com/Fancy-MLLM/R1-onevision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10615">R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2503.10526.pdf' target='_blank'>https://arxiv.org/pdf/2503.10526.pdf</a></span>   <span><a href='https://github.com/zzezze/NeighborRetr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengrong Lin, Zheng Wang, Tianwen Qian, Pan Mu, Sixian Chan, Cong Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10526">NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal retrieval aims to bridge the semantic gap between different modalities, such as visual and textual data, enabling accurate retrieval across them. Despite significant advancements with models like CLIP that align cross-modal representations, a persistent challenge remains: the hubness problem, where a small subset of samples (hubs) dominate as nearest neighbors, leading to biased representations and degraded retrieval accuracy. Existing methods often mitigate hubness through post-hoc normalization techniques, relying on prior data distributions that may not be practical in real-world scenarios. In this paper, we directly mitigate hubness during training and introduce NeighborRetr, a novel method that effectively balances the learning of hubs and adaptively adjusts the relations of various kinds of neighbors. Our approach not only mitigates the hubness problem but also enhances retrieval performance, achieving state-of-the-art results on multiple cross-modal retrieval benchmarks. Furthermore, NeighborRetr demonstrates robust generalization to new domains with substantial distribution shifts, highlighting its effectiveness in real-world applications. We make our code publicly available at: https://github.com/zzezze/NeighborRetr .
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2503.10460.pdf' target='_blank'>https://arxiv.org/pdf/2503.10460.pdf</a></span>   <span><a href='https://github.com/Qihoo360/Light-R1' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Qihoo360/Light-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10460">Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.
  Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.
  Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.
  Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at https://github.com/Qihoo360/Light-R1.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2503.10216.pdf' target='_blank'>https://arxiv.org/pdf/2503.10216.pdf</a></span>   <span><a href='https://github.com/kk42yy/CoStoDet-DDPM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixiang Yang, Xin Li, Qiang Li, Zhiwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10216">CoStoDet-DDPM: Collaborative Training of Stochastic and Deterministic Models Improves Surgical Workflow Anticipation and Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anticipating and recognizing surgical workflows are critical for intelligent surgical assistance systems. However, existing methods rely on deterministic decision-making, struggling to generalize across the large anatomical and procedural variations inherent in real-world surgeries.In this paper, we introduce an innovative framework that incorporates stochastic modeling through a denoising diffusion probabilistic model (DDPM) into conventional deterministic learning for surgical workflow analysis. At the heart of our approach is a collaborative co-training paradigm: the DDPM branch captures procedural uncertainties to enrich feature representations, while the task branch focuses on predicting surgical phases and instrument usage.Theoretically, we demonstrate that this mutual refinement mechanism benefits both branches: the DDPM reduces prediction errors in uncertain scenarios, and the task branch directs the DDPM toward clinically meaningful representations. Notably, the DDPM branch is discarded during inference, enabling real-time predictions without sacrificing accuracy.Experiments on the Cholec80 dataset show that for the anticipation task, our method achieves a 16% reduction in eMAE compared to state-of-the-art approaches, and for phase recognition, it improves the Jaccard score by 1.0%. Additionally, on the AutoLaparo dataset, our method achieves a 1.5% improvement in the Jaccard score for phase recognition, while also exhibiting robust generalization to patient-specific variations. Our code and weight are available at https://github.com/kk42yy/CoStoDet-DDPM.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2503.10149.pdf' target='_blank'>https://arxiv.org/pdf/2503.10149.pdf</a></span>   <span><a href='https://github.com/peakpang/UGP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenxuan Zeng, Qiao Wu, Xiyu Zhang, Lin Yuanbo Wu, Pei An, Jiaqi Yang, Ji Wang, Peng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10149">Unlocking Generalization Power in LiDAR Point Cloud Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world environments, a LiDAR point cloud registration method with robust generalization capabilities (across varying distances and datasets) is crucial for ensuring safety in autonomous driving and other LiDAR-based applications. However, current methods fall short in achieving this level of generalization. To address these limitations, we propose UGP, a pruned framework designed to enhance generalization power for LiDAR point cloud registration. The core insight in UGP is the elimination of cross-attention mechanisms to improve generalization, allowing the network to concentrate on intra-frame feature extraction. Additionally, we introduce a progressive self-attention module to reduce ambiguity in large-scale scenes and integrate Bird's Eye View (BEV) features to incorporate semantic information about scene elements. Together, these enhancements significantly boost the network's generalization performance. We validated our approach through various generalization experiments in multiple outdoor scenes. In cross-distance generalization experiments on KITTI and nuScenes, UGP achieved state-of-the-art mean Registration Recall rates of 94.5% and 91.4%, respectively. In cross-dataset generalization from nuScenes to KITTI, UGP achieved a state-of-the-art mean Registration Recall of 90.9%. Code will be available at https://github.com/peakpang/UGP.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2503.08906.pdf' target='_blank'>https://arxiv.org/pdf/2503.08906.pdf</a></span>   <span><a href='https://github.com/ChongQingNoSubway/Prompt-OT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiwen Chen, Wenhui Zhu, Peijie Qiu, Hao Wang, Huayu Li, Haiyu Wu, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08906">Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT)-guided prompt learning framework that mitigates forgetting by preserving the structural consistency of feature distributions between pre-trained and fine-tuned models. Unlike conventional point-wise constraints, OT naturally captures cross-instance relationships and expands the feasible parameter space for prompt tuning, allowing a better trade-off between adaptation and generalization. Our approach enforces joint constraints on both vision and text representations, ensuring a holistic feature alignment. Extensive experiments on benchmark datasets demonstrate that our simple yet effective method can outperform existing prompt learning strategies in base-to-novel generalization, cross-dataset evaluation, and domain generalization without additional augmentation or ensemble techniques. The code is available at https://github.com/ChongQingNoSubway/Prompt-OT
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2503.06520.pdf' target='_blank'>https://arxiv.org/pdf/2503.06520.pdf</a></span>   <span><a href='https://github.com/dvlab-research/Seg-Zero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, Jiaya Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06520">Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2503.03417.pdf' target='_blank'>https://arxiv.org/pdf/2503.03417.pdf</a></span>   <span><a href='https://github.com/JabezNzomo99/claim-matching-robustness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jabez Magomere, Emanuele La Malfa, Manuel Tonneau, Ashkan Kazemi, Scott Hale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03417">When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online misinformation remains a critical challenge, and fact-checkers increasingly rely on claim matching systems that use sentence embedding models to retrieve relevant fact-checks. However, as users interact with claims online, they often introduce edits, and it remains unclear whether current embedding models used in retrieval are robust to such edits. To investigate this, we introduce a perturbation framework that generates valid and natural claim variations, enabling us to assess the robustness of a wide-range of sentence embedding models in a multi-stage retrieval pipeline and evaluate the effectiveness of various mitigation approaches. Our evaluation reveals that standard embedding models exhibit notable performance drops on edited claims, while LLM-distilled embedding models offer improved robustness at a higher computational cost. Although a strong reranker helps to reduce the performance drop, it cannot fully compensate for first-stage retrieval gaps. To address these retrieval gaps, we evaluate train- and inference-time mitigation approaches, demonstrating that they can improve in-domain robustness by up to 17 percentage points and boost out-of-domain generalization by 10 percentage points. Overall, our findings provide practical improvements to claim-matching systems, enabling more reliable fact-checking of evolving misinformation. Code and data are available at https://github.com/JabezNzomo99/claim-matching-robustness.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2503.02101.pdf' target='_blank'>https://arxiv.org/pdf/2503.02101.pdf</a></span>   <span><a href='https://github.com/heboyong/Generalized-Diffusion-Detector' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyong He, Yuxiang Ji, Qianwen Ye, Zhuoyue Tan, Liaoni Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02101">Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) for object detection aims to enhance detectors' performance in unseen scenarios. This task remains challenging due to complex variations in real-world applications. Recently, diffusion models have demonstrated remarkable capabilities in diverse scene generation, which inspires us to explore their potential for improving DG tasks. Instead of generating images, our method extracts multi-step intermediate features during the diffusion process to obtain domain-invariant features for generalized detection. Furthermore, we propose an efficient knowledge transfer framework that enables detectors to inherit the generalization capabilities of diffusion models through feature and object-level alignment, without increasing inference time. We conduct extensive experiments on six challenging DG benchmarks. The results demonstrate that our method achieves substantial improvements of 14.0% mAP over existing DG approaches across different domains and corruption types. Notably, our method even outperforms most domain adaptation methods without accessing any target domain data. Moreover, the diffusion-guided detectors show consistent improvements of 15.9% mAP on average compared to the baseline. Our work aims to present an effective approach for domain-generalized detection and provide potential insights for robust visual recognition in real-world scenarios. The code is available at https://github.com/heboyong/Generalized-Diffusion-Detector.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2503.00986.pdf' target='_blank'>https://arxiv.org/pdf/2503.00986.pdf</a></span>   <span><a href='https://github.com/OpenRobotLab/EgoHOD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baoqi Pei, Yifei Huang, Jilan Xu, Guo Chen, Yuping He, Lijin Yang, Yali Wang, Weidi Xie, Yu Qiao, Fei Wu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00986">Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In egocentric video understanding, the motion of hands and objects as well as their interactions play a significant role by nature. However, existing egocentric video representation learning methods mainly focus on aligning video representation with high-level narrations, overlooking the intricate dynamics between hands and objects. In this work, we aim to integrate the modeling of fine-grained hand-object dynamics into the video representation learning process. Since no suitable data is available, we introduce HOD, a novel pipeline employing a hand-object detector and a large language model to generate high-quality narrations with detailed descriptions of hand-object dynamics. To learn these fine-grained dynamics, we propose EgoVideo, a model with a new lightweight motion adapter to capture fine-grained hand-object motion information. Through our co-training strategy, EgoVideo effectively and efficiently leverages the fine-grained hand-object dynamics in the HOD data. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple egocentric downstream tasks, including improvements of 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and 16.3% in EGTEA classification in zero-shot settings. Furthermore, our model exhibits robust generalization capabilities in hand-object interaction and robot manipulation tasks. Code and data are available at https://github.com/OpenRobotLab/EgoHOD/.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2503.00429.pdf' target='_blank'>https://arxiv.org/pdf/2503.00429.pdf</a></span>   <span><a href='https://github.com/yjyddq/DADM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Yang, Xun Lin, Zitong Yu, Liepiao Zhang, Xin Liu, Hui Li, Xiaochen Yuan, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00429">DADM: Dual Alignment of Domain and Modality for Face Anti-spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the availability of diverse sensor modalities (i.e., RGB, Depth, Infrared) and the success of multi-modal learning, multi-modal face anti-spoofing (FAS) has emerged as a prominent research focus. The intuition behind it is that leveraging multiple modalities can uncover more intrinsic spoofing traces. However, this approach presents more risk of misalignment. We identify two main types of misalignment: (1) \textbf{Intra-domain modality misalignment}, where the importance of each modality varies across different attacks. For instance, certain modalities (e.g., Depth) may be non-defensive against specific attacks (e.g., 3D mask), indicating that each modality has unique strengths and weaknesses in countering particular attacks. Consequently, simple fusion strategies may fall short. (2) \textbf{Inter-domain modality misalignment}, where the introduction of additional modalities exacerbates domain shifts, potentially overshadowing the benefits of complementary fusion. To tackle (1), we propose a alignment module between modalities based on mutual information, which adaptively enhances favorable modalities while suppressing unfavorable ones. To address (2), we employ a dual alignment optimization method that aligns both sub-domain hyperplanes and modality angle margins, thereby mitigating domain gaps. Our method, dubbed \textbf{D}ual \textbf{A}lignment of \textbf{D}omain and \textbf{M}odality (DADM), achieves state-of-the-art performance in extensive experiments across four challenging protocols demonstrating its robustness in multi-modal domain generalization scenarios. The codes will be released soon.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2502.20619.pdf' target='_blank'>https://arxiv.org/pdf/2502.20619.pdf</a></span>   <span><a href='https://github.com/Senyh/StyCona' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane, Zhaolin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20619">Style Content Decomposition-based Data Augmentation for Domain Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to domain shifts across diverse medical imaging modalities, learned segmentation models often suffer significant performance degradation during deployment. These domain shifts, typically caused by variations in imaging systems, generally comprise two principal components: 1) \textbf{"style" shifts}, referring to global disparities in image properties such as illumination, contrast, and color; and 2) \textbf{"content" shifts}, which involve local discrepancies in anatomical structures. To address domain shifts in medical image segmentation, a core challenge arises: how can we decouple the factors within images that determine their "style" and "content" components? To this end, we first propose a linear style-content decomposition method that factorizes an image into style codes and content maps, explicitly modeling the "style" and "content" components. Building on this, we introduce a \textbf{Sty}le-\textbf{Con}tent decomposition-based data \textbf{a}ugmentation algorithm (StyCona), which leverages this decomposition strategy to guide augmentation of both the global style and local content of source-domain images, enabling the training of a well-generalized model for domain-generalizable medical image segmentation. StyCona is a simple yet effective plug-and-play module that substantially improves model generalization without requiring additional training parameters or modifications to segmentation model architectures. Experiments on cardiac magnetic resonance imaging and fundus photography segmentation tasks, with single and multiple target domains respectively, demonstrate the effectiveness of StyCona and its superiority over state-of-the-art domain generalization methods. The code will be released at https://github.com/Senyh/StyCona.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2502.20158.pdf' target='_blank'>https://arxiv.org/pdf/2502.20158.pdf</a></span>   <span><a href='https://github.com/Mia-YatingYu/Open-MeDe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Yu, Congqi Cao, Yifan Zhang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20158">Learning to Generalize without Bias for Open-Vocabulary Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging the effective visual-text alignment and static generalizability from CLIP, recent video learners adopt CLIP initialization with further regularization or recombination for generalization in open-vocabulary action recognition in-context. However, due to the static bias of CLIP, such video learners tend to overfit on shortcut static features, thereby compromising their generalizability, especially to novel out-of-context actions. To address this issue, we introduce Open-MeDe, a novel Meta-optimization framework with static Debiasing for Open-vocabulary action recognition. From a fresh perspective of generalization, Open-MeDe adopts a meta-learning approach to improve known-to-open generalizing and image-to-video debiasing in a cost-effective manner. Specifically, Open-MeDe introduces a cross-batch meta-optimization scheme that explicitly encourages video learners to quickly generalize to arbitrary subsequent data via virtual evaluation, steering a smoother optimization landscape. In effect, the free of CLIP regularization during optimization implicitly mitigates the inherent static bias of the video meta-learner. We further apply self-ensemble over the optimization trajectory to obtain generic optimal parameters that can achieve robust generalization to both in-context and out-of-context novel data. Extensive evaluations show that Open-MeDe not only surpasses state-of-the-art regularization methods tailored for in-context open-vocabulary action recognition but also substantially excels in out-of-context scenarios.Code is released at https://github.com/Mia-YatingYu/Open-MeDe.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2502.19167.pdf' target='_blank'>https://arxiv.org/pdf/2502.19167.pdf</a></span>   <span><a href='https://github.com/AI4HealthUOL/ppg-ood-generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Moulaeifard, Peter H. Charlton, Nils Strodthoff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19167">Generalizable deep learning for photoplethysmography-based blood pressure estimation -- A Benchmarking Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photoplethysmography (PPG)-based blood pressure (BP) estimation represents a promising alternative to cuff-based BP measurements. Recently, an increasing number of deep learning models have been proposed to infer BP from the raw PPG waveform. However, these models have been predominantly evaluated on in-distribution test sets, which immediately raises the question of the generalizability of these models to external datasets. To investigate this question, we trained five deep learning models on the recently released PulseDB dataset, provided in-distribution benchmarking results on this dataset, and then assessed out-of-distribution performance on several external datasets. The best model (XResNet1d101) achieved in-distribution MAEs of 9.4 and 6.0 mmHg for systolic and diastolic BP respectively on PulseDB (with subject-specific calibration), and 14.0 and 8.5 mmHg respectively without calibration. Equivalent MAEs on external test datasets without calibration ranged from 15.0 to 25.1 mmHg (SBP) and 7.0 to 10.4 mmHg (DBP). Our results indicate that the performance is strongly influenced by the differences in BP distributions between datasets. We investigated a simple way of improving performance through sample-based domain adaptation and put forward recommendations for training models with good generalization properties. With this work, we hope to educate more researchers for the importance and challenges of out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2502.18104.pdf' target='_blank'>https://arxiv.org/pdf/2502.18104.pdf</a></span>   <span><a href='https://github.com/HanNieWHU/PromptMID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Nie, Bin Luo, Jun Liu, Zhitao Fu, Huan Zhou, Shuo Zhang, Weixing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18104">PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ideal goal of image matching is to achieve stable and efficient performance in unseen domains. However, many existing learning-based optical-SAR image matching methods, despite their effectiveness in specific scenarios, exhibit limited generalization and struggle to adapt to practical applications. Repeatedly training or fine-tuning matching models to address domain differences is not only not elegant enough but also introduces additional computational overhead and data production costs. In recent years, general foundation models have shown great potential for enhancing generalization. However, the disparity in visual domains between natural and remote sensing images poses challenges for their direct application. Therefore, effectively leveraging foundation models to improve the generalization of optical-SAR image matching remains challenge. To address the above challenges, we propose PromptMID, a novel approach that constructs modality-invariant descriptors using text prompts based on land use classification as priors information for optical and SAR image matching. PromptMID extracts multi-scale modality-invariant features by leveraging pre-trained diffusion models and visual foundation models (VFMs), while specially designed feature aggregation modules effectively fuse features across different granularities. Extensive experiments on optical-SAR image datasets from four diverse regions demonstrate that PromptMID outperforms state-of-the-art matching methods, achieving superior results in both seen and unseen domains and exhibiting strong cross-domain generalization capabilities. The source code will be made publicly available https://github.com/HanNieWHU/PromptMID.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2502.13061.pdf' target='_blank'>https://arxiv.org/pdf/2502.13061.pdf</a></span>   <span><a href='https://github.com/JingbiaoMei/RGCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13061">Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While Large Multimodal Models (LMMs) have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both supervised fine-tuning (SFT) and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Analysis reveals that our approach achieves improved robustness under adversarial attacks compared to SFT models. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems. Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability. Code available at https://github.com/JingbiaoMei/RGCL
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2502.12413.pdf' target='_blank'>https://arxiv.org/pdf/2502.12413.pdf</a></span>   <span><a href='https://github.com/kokolerk/DivIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Wang, Yuhang Zhou, Zhixiong Zhang, Qiguang Chen, Yongqiang Chen, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12413">DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution generalization is a common problem that expects the model to perform well in the different distributions even far from the train data. A popular approach to addressing this issue is invariant learning (IL), in which the model is compiled to focus on invariant features instead of spurious features by adding strong constraints during training. However, there are some potential pitfalls of strong invariant constraints. Due to the limited number of diverse environments and over-regularization in the feature space, it may lead to a loss of important details in the invariant features while alleviating the spurious correlations, namely the over-invariance, which can also degrade the generalization performance. We theoretically define the over-invariance and observe that this issue occurs in various classic IL methods. To alleviate this issue, we propose a simple approach Diverse Invariant Learning (DivIL) by adding the unsupervised contrastive learning and the random masking mechanism compensatory for the invariant constraints, which can be applied to various IL methods. Furthermore, we conduct experiments across multiple modalities across 12 datasets and 6 classic models, verifying our over-invariance insight and the effectiveness of our DivIL framework. Our code is available at https://github.com/kokolerk/DivIL.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2502.07200.pdf' target='_blank'>https://arxiv.org/pdf/2502.07200.pdf</a></span>   <span><a href='https://github.com/RaviShah1/DCIN-CQG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Shah, Atsushi Fukuda, Quan Huu Cap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07200">Color-Quality Invariance for Robust Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization (SDG) in medical image segmentation remains a significant challenge, particularly for images with varying color distributions and qualities. Previous approaches often struggle when models trained on high-quality images fail to generalize to low-quality test images due to these color and quality shifts. In this work, we propose two novel techniques to enhance generalization: dynamic color image normalization (DCIN) module and color-quality generalization (CQG) loss. The DCIN dynamically normalizes the color of test images using two reference image selection strategies. Specifically, the DCIN utilizes a global reference image selection (GRIS), which finds a universal reference image, and a local reference image selection (LRIS), which selects a semantically similar reference image per test sample. Additionally, CQG loss enforces invariance to color and quality variations by ensuring consistent segmentation predictions across transformed image pairs. Experimental results show that our proposals significantly improve segmentation performance over the baseline on two target domain datasets, despite being trained solely on a single source domain. Notably, our model achieved up to a 32.3-point increase in Dice score compared to the baseline, consistently producing robust and usable results even under substantial domain shifts. Our work contributes to the development of more robust medical image segmentation models that generalize across unseen domains. The implementation code is available at https://github.com/RaviShah1/DCIN-CQG.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2502.04204.pdf' target='_blank'>https://arxiv.org/pdf/2502.04204.pdf</a></span>   <span><a href='https://github.com/fshp971/adv-icl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaopeng Fu, Liang Ding, Jingfeng Zhang, Di Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04204">Short-length Adversarial Training Helps LLMs Defend Long-length Jailbreak Attacks: Theoretical and Empirical Evidence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Jailbreak attacks against large language models (LLMs) aim to induce harmful behaviors in LLMs through carefully crafted adversarial prompts. To mitigate attacks, one way is to perform adversarial training (AT)-based alignment, i.e., training LLMs on some of the most adversarial prompts to help them learn how to behave safely under attacks. During AT, the length of adversarial prompts plays a critical role in the robustness of aligned LLMs. While long-length adversarial prompts during AT might lead to strong LLM robustness, their synthesis however is very resource-consuming, which may limit the application of LLM AT. This paper focuses on adversarial suffix jailbreak attacks and unveils that to defend against a jailbreak attack with an adversarial suffix of length $Î(M)$, it is enough to align LLMs on prompts with adversarial suffixes of length $Î(\sqrt{M})$. Theoretically, we analyze the adversarial in-context learning of linear transformers on linear regression tasks and prove a robust generalization bound for trained transformers. The bound depends on the term $Î(\sqrt{M_{\text{test}}}/M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the numbers of adversarially perturbed in-context samples during training and testing. Empirically, we conduct AT on popular open-source LLMs and evaluate their robustness against jailbreak attacks of different adversarial suffix lengths. Results confirm a positive correlation between the attack success rate and the ratio of the square root of the adversarial suffix length during jailbreaking to the length during AT. Our findings show that it is practical to defend against ``long-length'' jailbreak attacks via efficient ``short-length'' AT. The code is available at https://github.com/fshp971/adv-icl.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2502.02525.pdf' target='_blank'>https://arxiv.org/pdf/2502.02525.pdf</a></span>   <span><a href='https://github.com/CNJianLiu/Diff9D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Liu, Wei Sun, Hui Yang, Pengchao Deng, Chongpei Liu, Nicu Sebe, Hossein Rahmani, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02525">Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucial for enabling augmented reality and robotic manipulation. Category-level methods have received extensive research attention due to their potential for generalization to intra-class unknown objects. However, these methods require manual collection and labeling of large-scale real-world training data. To address this problem, we introduce a diffusion-based paradigm for domain-generalized category-level 9-DoF object pose estimation. Our motivation is to leverage the latent generalization ability of the diffusion model to address the domain generalization challenge in object pose estimation. This entails training the model exclusively on rendered synthetic data to achieve generalization to real-world scenes. We propose an effective diffusion model to redefine 9-DoF object pose estimation from a generative perspective. Our model does not require any 3D shape priors during training or inference. By employing the Denoising Diffusion Implicit Model, we demonstrate that the reverse diffusion process can be executed in as few as 3 steps, achieving near real-time performance. Finally, we design a robotic grasping system comprising both hardware and software components. Through comprehensive experiments on two benchmark datasets and the real-world robotic system, we show that our method achieves state-of-the-art domain generalization performance. Our code will be made public at https://github.com/CNJianLiu/Diff9D.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2501.18739.pdf' target='_blank'>https://arxiv.org/pdf/2501.18739.pdf</a></span>   <span><a href='https://github.com/Zehong-Wang/GPM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehong Wang, Zheyuan Zhang, Tianyi Ma, Nitesh V Chawla, Chuxu Zhang, Yanfang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18739">Beyond Message Passing: Neural Graph Pattern Machine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph learning tasks often hinge on identifying key substructure patterns -- such as triadic closures in social networks or benzene rings in molecular graphs -- that underpin downstream performance. However, most existing graph neural networks (GNNs) rely on message passing, which aggregates local neighborhood information iteratively and struggles to explicitly capture such fundamental motifs, like triangles, k-cliques, and rings. This limitation hinders both expressiveness and long-range dependency modeling. In this paper, we introduce the Neural Graph Pattern Machine (GPM), a novel framework that bypasses message passing by learning directly from graph substructures. GPM efficiently extracts, encodes, and prioritizes task-relevant graph patterns, offering greater expressivity and improved ability to capture long-range dependencies. Empirical evaluations across four standard tasks -- node classification, link prediction, graph classification, and graph regression -- demonstrate that GPM outperforms state-of-the-art baselines. Further analysis reveals that GPM exhibits strong out-of-distribution generalization, desirable scalability, and enhanced interpretability. Code and datasets are available at: https://github.com/Zehong-Wang/GPM.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2501.18592.pdf' target='_blank'>https://arxiv.org/pdf/2501.18592.pdf</a></span>   <span><a href='https://github.com/donghao51/Awesome-Multimodal-Adaptation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/donghao51/Awesome-Multimodal-Adaptation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18592">Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, achieving domain adaptation and generalization poses significant challenges, as models must adapt to or generalize across unknown target distributions. Extending these capabilities to unseen multimodal distributions, i.e., multimodal domain adaptation and generalization, is even more challenging due to the distinct characteristics of different modalities. Significant progress has been made over the years, with applications ranging from action recognition to semantic segmentation. Besides, the recent advent of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired works leveraging these models to enhance adaptation and generalization performances or adapting them to downstream tasks. This survey provides the first comprehensive review of recent advances from traditional approaches to foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal test-time adaptation; (3) Multimodal domain generalization; (4) Domain adaptation and generalization with the help of multimodal foundation models; and (5) Adaptation of multimodal foundation models. For each topic, we formally define the problem and thoroughly review existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We maintain an active repository that contains up-to-date literature at https://github.com/donghao51/Awesome-Multimodal-Adaptation.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2501.14693.pdf' target='_blank'>https://arxiv.org/pdf/2501.14693.pdf</a></span>   <span><a href='https://github.com/MichiganNLP/TAMA,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naihao Deng, Rada Mihalcea
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14693">Rethinking Table Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2501.10080.pdf' target='_blank'>https://arxiv.org/pdf/2501.10080.pdf</a></span>   <span><a href='https://github.com/AIT-Assistive-Autonomous-Systems/Hopomop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Schwingshackl, Fabio Francisco Oberweger, Markus Murschitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10080">Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel approach to few-shot semantic segmentation for machinery with multiple parts that exhibit spatial and hierarchical relationships. Our method integrates the foundation models CLIPSeg and Segment Anything Model (SAM) with the interest point detector SuperPoint and a graph convolutional network (GCN) to accurately segment machinery parts. By providing 1 to 25 annotated samples, our model, evaluated on a purely synthetic dataset depicting a truck-mounted loading crane, achieves effective segmentation across various levels of detail. Training times are kept under five minutes on consumer GPUs. The model demonstrates robust generalization to real data, achieving a qualitative synthetic-to-real generalization with a $J\&F$ score of 92.2 on real data using 10 synthetic support samples. When benchmarked on the DAVIS 2017 dataset, it achieves a $J\&F$ score of 71.5 in semi-supervised video segmentation with three support samples. This method's fast training times and effective generalization to real data make it a valuable tool for autonomous systems interacting with machinery and infrastructure, and illustrate the potential of combined and orchestrated foundation models for few-shot segmentation tasks.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2501.09877.pdf' target='_blank'>https://arxiv.org/pdf/2501.09877.pdf</a></span>   <span><a href='https://github.com/Jingchensun/clap-s' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingchen Sun, Shaobo Han, Wataru Kohno, Changyou Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09877">CLAP-S: Support Set Based Adaptation for Downstream Fiber-optic Acoustic Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive Language-Audio Pretraining (CLAP) models have demonstrated unprecedented performance in various acoustic signal recognition tasks. Fiber-optic-based acoustic recognition is one of the most important downstream tasks and plays a significant role in environmental sensing. Adapting CLAP for fiber-optic acoustic recognition has become an active research area. As a non-conventional acoustic sensor, fiber-optic acoustic recognition presents a challenging, domain-specific, low-shot deployment environment with significant domain shifts due to unique frequency response and noise characteristics. To address these challenges, we propose a support-based adaptation method, CLAP-S, which linearly interpolates a CLAP Adapter with the Support Set, leveraging both implicit knowledge through fine-tuning and explicit knowledge retrieved from memory for cross-domain generalization. Experimental results show that our method delivers competitive performance on both laboratory-recorded fiber-optic ESC-50 datasets and a real-world fiber-optic gunshot-firework dataset. Our research also provides valuable insights for other downstream acoustic recognition tasks. The code and gunshot-firework dataset are available at https://github.com/Jingchensun/clap-s.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2501.04958.pdf' target='_blank'>https://arxiv.org/pdf/2501.04958.pdf</a></span>   <span><a href='https://github.com/yinghemedical/imbalance-aware_domain_adaptation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Li, Xinglin Zhang, Jun Liang, Tao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04958">Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo Development Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models in medical imaging face dual challenges: domain shift, where models perform poorly when deployed in settings different from their training environment, and class imbalance, where certain disease conditions are naturally underrepresented. We present Imbalance-Aware Domain Adaptation (IADA), a novel framework that simultaneously tackles both challenges through three key components: (1) adaptive feature learning with class-specific attention mechanisms, (2) balanced domain alignment with dynamic weighting, and (3) adaptive threshold optimization. Our theoretical analysis establishes convergence guarantees and complexity bounds. Through extensive experiments on embryo development assessment across four imaging modalities, IADA demonstrates significant improvements over existing methods, achieving up to 25.19\% higher accuracy while maintaining balanced performance across classes. In challenging scenarios with low-quality imaging systems, IADA shows robust generalization with AUC improvements of up to 12.56\%. These results demonstrate IADA's potential for developing reliable and equitable medical imaging systems for diverse clinical settings. The code is made public available at \url{https://github.com/yinghemedical/imbalance-aware_domain_adaptation}
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2501.02012.pdf' target='_blank'>https://arxiv.org/pdf/2501.02012.pdf</a></span>   <span><a href='https://github.com/jh-liang/Information-Subtraction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keng Hou Leong, Yuxuan Xiu, Wai Kin, Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02012">Information Subtraction: Learning Representations for Conditional Entropy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The representations of conditional entropy and conditional mutual information are significant in explaining the unique effects among variables. While previous studies based on conditional contrastive sampling have effectively removed information regarding discrete sensitive variables, they have not yet extended their scope to continuous cases. This paper introduces Information Subtraction, a framework designed to generate representations that preserve desired information while eliminating the undesired. We implement a generative-based architecture that outputs these representations by simultaneously maximizing an information term and minimizing another. With its flexibility in disentangling information, we can iteratively apply Information Subtraction to represent arbitrary information components between continuous variables, thereby explaining the various relationships that exist between them. Our results highlight the representations' ability to provide semantic features of conditional entropy. By subtracting sensitive and domain-specific information, our framework demonstrates effective performance in fair learning and domain generalization. The code for this paper is available at https://github.com/jh-liang/Information-Subtraction
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2412.18342.pdf' target='_blank'>https://arxiv.org/pdf/2412.18342.pdf</a></span>   <span><a href='https://github.com/KPeng9510/HyProMeta' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/KPeng9510/HyProMeta' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Peng, Di Wen, Sarfraz M. Saquib, Yufan Chen, Junwei Zheng, David Schneider, Kailun Yang, Jiamin Wu, Alina Roitberg, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18342">Mitigating Label Noise using Prompt-Based Hyperbolic Meta-Learning in Open-Set Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Set Domain Generalization (OSDG) is a challenging task requiring models to accurately predict familiar categories while minimizing confidence for unknown categories to effectively reject them in unseen domains. While the OSDG field has seen considerable advancements, the impact of label noise--a common issue in real-world datasets--has been largely overlooked. Label noise can mislead model optimization, thereby exacerbating the challenges of open-set recognition in novel domains. In this study, we take the first step towards addressing Open-Set Domain Generalization under Noisy Labels (OSDG-NL) by constructing dedicated benchmarks derived from widely used OSDG datasets, including PACS and DigitsDG. We evaluate baseline approaches by integrating techniques from both label denoising and OSDG methodologies, highlighting the limitations of existing strategies in handling label noise effectively. To address these limitations, we propose HyProMeta, a novel framework that integrates hyperbolic category prototypes for label noise-aware meta-learning alongside a learnable new-category agnostic prompt designed to enhance generalization to unseen classes. Our extensive experiments demonstrate the superior performance of HyProMeta compared to state-of-the-art methods across the newly established benchmarks. The source code of this work is released at https://github.com/KPeng9510/HyProMeta.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2412.18303.pdf' target='_blank'>https://arxiv.org/pdf/2412.18303.pdf</a></span>   <span><a href='https://github.com/Yushu-Li/ECALP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushu Li, Yongyi Su, Adam Goodge, Kui Jia, Xun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18303">Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have revolutionized machine learning by leveraging large pre-trained models to tackle various downstream tasks. Although label, training, and data efficiency have improved, many state-of-the-art VLMs still require task-specific hyperparameter tuning and fail to fully exploit test samples. To overcome these challenges, we propose a graph-based approach for label-efficient adaptation and inference. Our method dynamically constructs a graph over text prompts, few-shot examples, and test samples, using label propagation for inference without task-specific tuning. Unlike existing zero-shot label propagation techniques, our approach requires no additional unlabeled support set and effectively leverages the test sample manifold through dynamic graph expansion. We further introduce a context-aware feature re-weighting mechanism to improve task adaptation accuracy. Additionally, our method supports efficient graph expansion, enabling real-time inductive inference. Extensive evaluations on downstream tasks, such as fine-grained categorization and out-of-distribution generalization, demonstrate the effectiveness of our approach. The source code is available at https://github.com/Yushu-Li/ECALP.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2412.15380.pdf' target='_blank'>https://arxiv.org/pdf/2412.15380.pdf</a></span>   <span><a href='https://github.com/Meghnak13/UG-CEMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meghana Karri, Amit Soni Arya, Koushik Biswas, Nicol`o Gennaro, Vedat Cicek, Gorkem Durak, Yuri S. Velichko, Ulas Bagci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15380">Uncertainty-Guided Cross Attention Ensemble Mean Teacher for Semi-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a novel framework, Uncertainty-Guided Cross Attention Ensemble Mean Teacher (UG-CEMT), for achieving state-of-the-art performance in semi-supervised medical image segmentation. UG-CEMT leverages the strengths of co-training and knowledge distillation by combining a Cross-attention Ensemble Mean Teacher framework (CEMT) inspired by Vision Transformers (ViT) with uncertainty-guided consistency regularization and Sharpness-Aware Minimization emphasizing uncertainty. UG-CEMT improves semi-supervised performance while maintaining a consistent network architecture and task setting by fostering high disparity between sub-networks. Experiments demonstrate significant advantages over existing methods like Mean Teacher and Cross-pseudo Supervision in terms of disparity, domain generalization, and medical image segmentation performance. UG-CEMT achieves state-of-the-art results on multi-center prostate MRI and cardiac MRI datasets, where object segmentation is particularly challenging. Our results show that using only 10\% labeled data, UG-CEMT approaches the performance of fully supervised methods, demonstrating its effectiveness in exploiting unlabeled data for robust medical image segmentation. The code is publicly available at \url{https://github.com/Meghnak13/UG-CEMT}
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2412.13742.pdf' target='_blank'>https://arxiv.org/pdf/2412.13742.pdf</a></span>   <span><a href='https://github.com/taozh2017/KnowSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiwen Huang, Tao Zhou, Huazhu Fu, Yizhe Zhang, Yi Zhou, Chen Gong, Dong Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13742">Learnable Prompting SAM-induced Knowledge Distillation for Semi-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The limited availability of labeled data has driven advancements in semi-supervised learning for medical image segmentation. Modern large-scale models tailored for general segmentation, such as the Segment Anything Model (SAM), have revealed robust generalization capabilities. However, applying these models directly to medical image segmentation still exposes performance degradation. In this paper, we propose a learnable prompting SAM-induced Knowledge distillation framework (KnowSAM) for semi-supervised medical image segmentation. Firstly, we propose a Multi-view Co-training (MC) strategy that employs two distinct sub-networks to employ a co-teaching paradigm, resulting in more robust outcomes. Secondly, we present a Learnable Prompt Strategy (LPS) to dynamically produce dense prompts and integrate an adapter to fine-tune SAM specifically for medical image segmentation tasks. Moreover, we propose SAM-induced Knowledge Distillation (SKD) to transfer useful knowledge from SAM to two sub-networks, enabling them to learn from SAM's predictions and alleviate the effects of incorrect pseudo-labels during training. Notably, the predictions generated by our subnets are used to produce mask prompts for SAM, facilitating effective inter-module information exchange. Extensive experimental results on various medical segmentation tasks demonstrate that our model outperforms the state-of-the-art semi-supervised segmentation approaches. Crucially, our SAM distillation framework can be seamlessly integrated into other semi-supervised segmentation methods to enhance performance. The code will be released upon acceptance of this manuscript at: https://github.com/taozh2017/KnowSAM
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2412.12456.pdf' target='_blank'>https://arxiv.org/pdf/2412.12456.pdf</a></span>   <span><a href='https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunkai Li, Zhengyu Wu, Jiayi Wu, Hanwen Cui, Jishuo Jia, Rong-Hua Li, Guoren Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12456">Graph Learning in the Era of LLMs: A Survey from the Perspective of Data, Models, and Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing prevalence of cross-domain Text-Attributed Graph (TAG) Data (e.g., citation networks, recommendation systems, social networks, and ai4science), the integration of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as collaborators, LLM as predictor) has emerged as a promising technological paradigm. The core of this new graph learning paradigm lies in the synergistic combination of GNNs' ability to capture complex structural relationships and LLMs' proficiency in understanding informative contexts from the rich textual descriptions of graphs. Therefore, we can leverage graph description texts with rich semantic context to fundamentally enhance Data quality, thereby improving the representational capacity of model-centric approaches in line with data-centric machine learning principles. By leveraging the strengths of these distinct neural network architectures, this integrated approach addresses a wide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph question answering), particularly in complex industrial scenarios (e.g., supervised, few-shot, and zero-shot settings). In other words, we can treat text as a medium to enable cross-domain generalization of graph learning Model, allowing a single graph model to effectively handle the diversity of downstream graph-based Task across different data domains. This work serves as a foundational reference for researchers and practitioners looking to advance graph learning methodologies in the rapidly evolving landscape of LLM. We consistently maintain the related open-source materials at \url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2412.10680.pdf' target='_blank'>https://arxiv.org/pdf/2412.10680.pdf</a></span>   <span><a href='https://github.com/fine68/UCDR2024' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Jiang, Zhi-Qi Cheng, Gabriel Moreira, Jiawen Zhu, Jingdong Sun, Bukun Ren, Jun-Yan He, Qi Dai, Xian-Sheng Hua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10680">UCDR-Adapter: Exploring Adaptation of Pre-Trained Vision-Language Models for Universal Cross-Domain Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Universal Cross-Domain Retrieval (UCDR) retrieves relevant images from unseen domains and classes without semantic labels, ensuring robust generalization. Existing methods commonly employ prompt tuning with pre-trained vision-language models but are inherently limited by static prompts, reducing adaptability. We propose UCDR-Adapter, which enhances pre-trained models with adapters and dynamic prompt generation through a two-phase training strategy. First, Source Adapter Learning integrates class semantics with domain-specific visual knowledge using a Learnable Textual Semantic Template and optimizes Class and Domain Prompts via momentum updates and dual loss functions for robust alignment. Second, Target Prompt Generation creates dynamic prompts by attending to masked source prompts, enabling seamless adaptation to unseen domains and classes. Unlike prior approaches, UCDR-Adapter dynamically adapts to evolving data distributions, enhancing both flexibility and generalization. During inference, only the image branch and generated prompts are used, eliminating reliance on textual inputs for highly efficient retrieval. Extensive benchmark experiments show that UCDR-Adapter consistently outperforms ProS in most cases and other state-of-the-art methods on UCDR, U(c)CDR, and U(d)CDR settings.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2412.09074.pdf' target='_blank'>https://arxiv.org/pdf/2412.09074.pdf</a></span>   <span><a href='https://github.com/jinsuby/DomCLP' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/jinsuby/DomCLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin-Seop Lee, Noo-ri Kim, Jee-Hyong Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09074">DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL) methods based on the instance discrimination tasks with InfoNCE have achieved remarkable success. Despite their success, SSL models often struggle to generate effective representations for unseen-domain data. To address this issue, research on unsupervised domain generalization (UDG), which aims to develop SSL models that can generate domain-irrelevant features, has been conducted. Most UDG approaches utilize contrastive learning with InfoNCE to generate representations, and perform feature alignment based on strong assumptions to generalize domain-irrelevant common features from multi-source domains. However, existing methods that rely on instance discrimination tasks are not effective at extracting domain-irrelevant common features. This leads to the suppression of domain-irrelevant common features and the amplification of domain-relevant features, thereby hindering domain generalization. Furthermore, strong assumptions underlying feature alignment can lead to biased feature learning, reducing the diversity of common features. In this paper, we propose a novel approach, DomCLP, Domain-wise Contrastive Learning with Prototype Mixup. We explore how InfoNCE suppresses domain-irrelevant common features and amplifies domain-relevant features. Based on this analysis, we propose Domain-wise Contrastive Learning (DCon) to enhance domain-irrelevant common features. We also propose Prototype Mixup Learning (PMix) to generalize domain-irrelevant common features across multiple domains without relying on strong assumptions. The proposed method consistently outperforms state-of-the-art methods on the PACS and DomainNet datasets across various label fractions, showing significant improvements. Our code will be released. Our project page is available at https://github.com/jinsuby/DomCLP.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2412.04245.pdf' target='_blank'>https://arxiv.org/pdf/2412.04245.pdf</a></span>   <span><a href='https://github.com/berndprach/IntriguingProperties' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernd Prach, Christoph H. Lampert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04245">Intriguing Properties of Robust Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite extensive research since the community learned about adversarial examples 10 years ago, we still do not know how to train high-accuracy classifiers that are guaranteed to be robust to small perturbations of their inputs. Previous works often argued that this might be because no classifier exists that is robust and accurate at the same time. However, in computer vision this assumption does not match reality where humans are usually accurate and robust on most tasks of interest. We offer an alternative explanation and show that in certain settings robust generalization is only possible with unrealistically large amounts of data. Specifically, we find a setting where a robust classifier exists, it is easy to learn an accurate classifier, yet it requires an exponential amount of data to learn a robust classifier. Based on this theoretical result, we evaluate the influence of the amount of training data on datasets such as CIFAR-10. Our findings indicate that the amount of training data is the main factor determining the robust performance. Furthermore we show that there are low magnitude directions in the data which are useful for non-robust generalization but are not available for robust classifiers. We provide code at https://github.com/berndprach/IntriguingProperties.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2412.02837.pdf' target='_blank'>https://arxiv.org/pdf/2412.02837.pdf</a></span>   <span><a href='https://github.com/sarthaxxxxx/BATCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarthak Kumar Maharana, Baoming Zhang, Leonid Karlinsky, Rogerio Feris, Yunhui Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02837">$\texttt{BATCLIP}$: Bimodal Online Test-Time Adaptation for CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although open-vocabulary classification models like Contrastive Language Image Pretraining (CLIP) have demonstrated strong zero-shot learning capabilities, their robustness to common image corruptions remains poorly understood. Through extensive experiments, we show that zero-shot CLIP lacks robustness to common image corruptions during test-time, necessitating the adaptation of CLIP to unlabeled corrupted images using test-time adaptation (TTA). However, we found that existing TTA methods have severe limitations in adapting CLIP due to their unimodal nature. To address these limitations, we propose $\texttt{BATCLIP}$, a bimodal $\textbf{online}$ TTA method designed to improve CLIP's robustness to common image corruptions. The key insight of our approach is not only to adapt the visual encoders for improving image features but also to strengthen the alignment between image and text features by promoting a stronger association between the image class prototype, computed using pseudo-labels, and the corresponding text feature. We evaluate our approach on benchmark image corruption datasets and achieve state-of-the-art results in online TTA for CLIP. Furthermore, we evaluate our proposed TTA approach on various domain generalization datasets to demonstrate its generalization capabilities. Our code is available at https://github.com/sarthaxxxxx/BATCLIP
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2411.13284.pdf' target='_blank'>https://arxiv.org/pdf/2411.13284.pdf</a></span>   <span><a href='https://github.com/StrohmayerJ/DATTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Strohmayer, Rafael Sterzinger, Matthias WÃ¶dlinger, Martin Kampel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13284">DATTA: Domain-Adversarial Test-Time Adaptation for Cross-Domain WiFi-Based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain generalization is an open problem in WiFi-based sensing due to variations in environments, devices, and subjects, causing domain shifts in channel state information. To address this, we propose Domain-Adversarial Test-Time Adaptation (DATTA), a novel framework combining domain-adversarial training (DAT), test-time adaptation (TTA), and weight resetting to facilitate adaptation to unseen target domains and to prevent catastrophic forgetting. DATTA is integrated into a lightweight, flexible architecture optimized for speed. We conduct a comprehensive evaluation of DATTA, including an ablation study on all key components using publicly available data, and verify its suitability for real-time applications such as human activity recognition. When combining a SotA video-based variant of TTA with WiFi-based DAT and comparing it to DATTA, our method achieves an 8.1% higher F1-Score. The PyTorch implementation of DATTA is publicly available at: https://github.com/StrohmayerJ/DATTA.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2411.06040.pdf' target='_blank'>https://arxiv.org/pdf/2411.06040.pdf</a></span>   <span><a href='https://github.com/hasanjawad001/CGLearn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jawad Chowdhury, Gabriel Terejanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06040">CGLearn: Consistent Gradient-Based Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving generalization and achieving highly predictive, robust machine learning models necessitates learning the underlying causal structure of the variables of interest. A prominent and effective method for this is learning invariant predictors across multiple environments. In this work, we introduce a simple yet powerful approach, CGLearn, which relies on the agreement of gradients across various environments. This agreement serves as a powerful indication of reliable features, while disagreement suggests less reliability due to potential differences in underlying causal mechanisms. Our proposed method demonstrates superior performance compared to state-of-the-art methods in both linear and nonlinear settings across various regression and classification tasks. CGLearn shows robust applicability even in the absence of separate environments by exploiting invariance across different subsamples of observational data. Comprehensive experiments on both synthetic and real-world datasets highlight its effectiveness in diverse scenarios. Our findings underscore the importance of leveraging gradient agreement for learning causal invariance, providing a significant step forward in the field of robust machine learning. The source code of the linear and nonlinear implementation of CGLearn is open-source and available at: https://github.com/hasanjawad001/CGLearn.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2411.05223.pdf' target='_blank'>https://arxiv.org/pdf/2411.05223.pdf</a></span>   <span><a href='https://github.com/ratschlab/ICMSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boqi Chen, Yuanzhi Zhu, Yunke Ao, Sebastiano Caprara, Reto Sutter, Gunnar RÃ¤tsch, Ender Konukoglu, Anna Susmelj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05223">Generalizable Single-Source Cross-modality Medical Image Segmentation via Invariant Causal Mechanisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization (SDG) aims to learn a model from a single source domain that can generalize well on unseen target domains. This is an important task in computer vision, particularly relevant to medical imaging where domain shifts are common. In this work, we consider a challenging yet practical setting: SDG for cross-modality medical image segmentation. We combine causality-inspired theoretical insights on learning domain-invariant representations with recent advancements in diffusion-based augmentation to improve generalization across diverse imaging modalities. Guided by the ``intervention-augmentation equivariant'' principle, we use controlled diffusion models (DMs) to simulate diverse imaging styles while preserving the content, leveraging rich generative priors in large-scale pretrained DMs to comprehensively perturb the multidimensional style variable. Extensive experiments on challenging cross-modality segmentation tasks demonstrate that our approach consistently outperforms state-of-the-art SDG methods across three distinct anatomies and imaging modalities. The source code is available at \href{https://github.com/ratschlab/ICMSeg}{https://github.com/ratschlab/ICMSeg}.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2411.04224.pdf' target='_blank'>https://arxiv.org/pdf/2411.04224.pdf</a></span>   <span><a href='https://github.com/StrohmayerJ/WiFlexFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Strohmayer, Matthias WÃ¶dlinger, Martin Kampel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04224">WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose WiFlexFormer, a highly efficient Transformer-based architecture designed for WiFi Channel State Information (CSI)-based person-centric sensing. We benchmark WiFlexFormer against state-of-the-art vision and specialized architectures for processing radio frequency data and demonstrate that it achieves comparable Human Activity Recognition (HAR) performance while offering a significantly lower parameter count and faster inference times. With an inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is optimized for real-time inference. Additionally, its low parameter count contributes to improved cross-domain generalization, where it often outperforms larger models. Our comprehensive evaluation shows that WiFlexFormer is a potential solution for efficient, scalable WiFi-based sensing applications. The PyTorch implementation of WiFlexFormer is publicly available at: https://github.com/StrohmayerJ/WiFlexFormer.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2411.04219.pdf' target='_blank'>https://arxiv.org/pdf/2411.04219.pdf</a></span>   <span><a href='https://github.com/divelab/AIRS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Xu, Haiyang Yu, Montgomery Bohde, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04219">Equivariant Graph Network Approximations of High-Degree Polynomials for Force Field Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in equivariant deep models have shown promise in accurately predicting atomic potentials and force fields in molecular dynamics simulations. Using spherical harmonics (SH) and tensor products (TP), these equivariant networks gain enhanced physical understanding, like symmetries and many-body interactions. Beyond encoding physical insights, SH and TP are also crucial to represent equivariant polynomial functions. In this work, we analyze the equivariant polynomial functions for the equivariant architecture, and introduce a novel equivariant network, named PACE. The proposed PACE utilizes edge booster and the Atomic Cluster Expansion (ACE) technique to approximate a greater number of $SE(3) \times S_n$ equivariant polynomial functions with enhanced degrees. As experimented in commonly used benchmarks, PACE demonstrates state-of-the-art performance in predicting atomic energy and force fields, with robust generalization capability across various geometric distributions under molecular dynamics (MD) across different temperature conditions. Our code is publicly available as part of the AIRS library https://github.com/divelab/AIRS/.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2411.03829.pdf' target='_blank'>https://arxiv.org/pdf/2411.03829.pdf</a></span>   <span><a href='https://github.com/gaozhitong/MultiShiftSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitong Gao, Bingnan Li, Mathieu Salzmann, Xuming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03829">Generalize or Detect? Towards Robust Semantic Segmentation Under Multiple Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In open-world scenarios, where both novel classes and domains may exist, an ideal segmentation model should detect anomaly classes for safety and generalize to new domains. However, existing methods often struggle to distinguish between domain-level and semantic-level distribution shifts, leading to poor out-of-distribution (OOD) detection or domain generalization performance. In this work, we aim to equip the model to generalize effectively to covariate-shift regions while precisely identifying semantic-shift regions. To achieve this, we design a novel generative augmentation method to produce coherent images that incorporate both anomaly (or novel) objects and various covariate shifts at both image and object levels. Furthermore, we introduce a training strategy that recalibrates uncertainty specifically for semantic shifts and enhances the feature extractor to align features associated with domain shifts. We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts. Our method achieves state-of-the-art performance across all benchmarks for both OOD detection and domain generalization. Code is available at https://github.com/gaozhitong/MultiShiftSeg.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2411.02614.pdf' target='_blank'>https://arxiv.org/pdf/2411.02614.pdf</a></span>   <span><a href='https://github.com/sharonchokuwa/dg-adr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharon Chokuwa, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02614">Divergent Domains, Convergent Grading: Enhancing Generalization in Diabetic Retinopathy Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic Retinopathy (DR) constitutes 5% of global blindness cases. While numerous deep learning approaches have sought to enhance traditional DR grading methods, they often falter when confronted with new out-of-distribution data thereby impeding their widespread application. In this study, we introduce a novel deep learning method for achieving domain generalization (DG) in DR grading and make the following contributions. First, we propose a new way of generating image-to-image diagnostically relevant fundus augmentations conditioned on the grade of the original fundus image. These augmentations are tailored to emulate the types of shifts in DR datasets thus increase the model's robustness. Second, we address the limitations of the standard classification loss in DG for DR fundus datasets by proposing a new DG-specific loss, domain alignment loss; which ensures that the feature vectors from all domains corresponding to the same class converge onto the same manifold for better domain generalization. Third, we tackle the coupled problem of data imbalance across DR domains and classes by proposing to employ Focal loss which seamlessly integrates with our new alignment loss. Fourth, due to inevitable observer variability in DR diagnosis that induces label noise, we propose leveraging self-supervised pretraining. This approach ensures that our DG model remains robust against early susceptibility to label noise, even when only a limited dataset of non-DR fundus images is available for pretraining. Our method demonstrates significant improvements over the strong Empirical Risk Minimization baseline and other recently proposed state-of-the-art DG methods for DR grading. Code is available at https://github.com/sharonchokuwa/dg-adr.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2410.22629.pdf' target='_blank'>https://arxiv.org/pdf/2410.22629.pdf</a></span>   <span><a href='https://github.com/Cuzyoung/CrossEarth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Gong, Zhixiang Wei, Di Wang, Xiaoxing Hu, Xianzheng Ma, Hongruixuan Chen, Yuru Jia, Yupeng Deng, Zhenming Ji, Xiangwei Zhu, Xue Yang, Naoto Yokoya, Jing Zhang, Bo Du, Junchi Yan, Liangpei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22629">CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. Despite the substantial domain gaps in RS images that are characterized by variabilities such as location, wavelength, and sensor type, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies targeting the RSDG issue, especially for semantic segmentation tasks, where existing models are developed for specific unknown domains, struggling with issues of underfitting on other unknown scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 cross-domain settings across various regions, spectral bands, platforms, and climates, providing a comprehensive framework for testing the generalizability of future RSDG models. Extensive experiments on this benchmark demonstrate the superiority of CrossEarth over existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2410.22622.pdf' target='_blank'>https://arxiv.org/pdf/2410.22622.pdf</a></span>   <span><a href='https://github.com/judydnguyen/PARDON-FedDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dung Thuy Nguyen, Taylor T. Johnson, Kevin Leach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22622">PARDON: Privacy-Aware and Robust Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) shows promise in preserving privacy and enabling collaborative learning. However, most current solutions focus on private data collected from a single domain. A significant challenge arises when client data comes from diverse domains (i.e., domain shift), leading to poor performance on unseen domains. Existing Federated Domain Generalization approaches address this problem but assume each client holds data for an entire domain, limiting their practicality in real-world scenarios with domain-based heterogeneity and client sampling. In addition, certain methods enable information sharing among clients, raising privacy concerns as this information could be used to reconstruct sensitive private data.
  To overcome this, we introduce FISC, a novel FedDG paradigm designed to robustly handle more complicated domain distributions between clients while ensuring security. FISC enables learning across domains by extracting an interpolative style from local styles and employing contrastive learning. This strategy gives clients multi-domain representations and unbiased convergent targets. Empirical results on multiple datasets, including PACS, Office-Home, and IWildCam, show FISC outperforms state-of-the-art (SOTA) methods. Our method achieves accuracy on unseen domains, with improvements ranging from 3.64% to 57.22% on unseen domains. Our code is available at https://github.com/judydnguyen/PARDON-FedDG.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2410.22228.pdf' target='_blank'>https://arxiv.org/pdf/2410.22228.pdf</a></span>   <span><a href='https://github.com/Nanolbw/SuGAr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Liu, Haoyang Li, Shuning Wang, Shuo Nie, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22228">Subgraph Aggregation for Out-of-Distribution Generalization on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization in Graph Neural Networks (GNNs) has gained significant attention due to its critical importance in graph-based predictions in real-world scenarios. Existing methods primarily focus on extracting a single causal subgraph from the input graph to achieve generalizable predictions. However, relying on a single subgraph can lead to susceptibility to spurious correlations and is insufficient for learning invariant patterns behind graph data. Moreover, in many real-world applications, such as molecular property prediction, multiple critical subgraphs may influence the target label property. To address these challenges, we propose a novel framework, SubGraph Aggregation (SuGAr), designed to learn a diverse set of subgraphs that are crucial for OOD generalization on graphs. Specifically, SuGAr employs a tailored subgraph sampler and diversity regularizer to extract a diverse set of invariant subgraphs. These invariant subgraphs are then aggregated by averaging their representations, which enriches the subgraph signals and enhances coverage of the underlying causal structures, thereby improving OOD generalization. Extensive experiments on both synthetic and real-world datasets demonstrate that \ours outperforms state-of-the-art methods, achieving up to a 24% improvement in OOD generalization on graphs. To the best of our knowledge, this is the first work to study graph OOD generalization by learning multiple invariant subgraphs. code: https://github.com/Nanolbw/SuGAr
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2410.21331.pdf' target='_blank'>https://arxiv.org/pdf/2410.21331.pdf</a></span>   <span><a href='https://github.com/PKU-ML/Beyond_Interpretability' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Zhang, Yifei Wang, Jingyi Cui, Xiang Pan, Qi Lei, Stefanie Jegelka, Yisen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21331">Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models often suffer from a lack of interpretability due to polysemanticity, where individual neurons are activated by multiple unrelated semantics, resulting in unclear attributions of model behavior. Recent advances in monosemanticity, where neurons correspond to consistent and distinct semantics, have significantly improved interpretability but are commonly believed to compromise accuracy. In this work, we challenge the prevailing belief of the accuracy-interpretability tradeoff, showing that monosemantic features not only enhance interpretability but also bring concrete gains in model performance. Across multiple robust learning scenarios-including input and label noise, few-shot learning, and out-of-domain generalization-our results show that models leveraging monosemantic features significantly outperform those relying on polysemantic features. Furthermore, we provide empirical and theoretical understandings on the robustness gains of feature monosemanticity. Our preliminary analysis suggests that monosemanticity, by promoting better separation of feature representations, leads to more robust decision boundaries. This diverse evidence highlights the generality of monosemanticity in improving model robustness. As a first step in this new direction, we embark on exploring the learning benefits of monosemanticity beyond interpretability, supporting the long-standing hypothesis of linking interpretability and robustness. Code is available at \url{https://github.com/PKU-ML/Beyond_Interpretability}.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2410.20542.pdf' target='_blank'>https://arxiv.org/pdf/2410.20542.pdf</a></span>   <span><a href='https://github.com/nokia-bell-labs/papagei-foundation-model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvind Pillai, Dimitris Spathis, Fahim Kawsar, Mohammad Malekzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20542">PaPaGei: Open Foundation Models for Optical Physiological Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photoplethysmography (PPG) is the leading non-invasive technique for monitoring biosignals and cardiovascular health, with widespread adoption in both clinical settings and consumer wearable devices. While machine learning models trained on PPG signals have shown promise, they tend to be task-specific and struggle with generalization. Current research is limited by the use of single-device datasets, insufficient exploration of out-of-domain generalization, and a lack of publicly available models, which hampers reproducibility. To address these limitations, we present PaPaGei, the first open foundation model for PPG signals. The model is pre-trained on over 57,000 hours of data, comprising 20 million unlabeled PPG segments from publicly available datasets. We introduce a novel representation learning approach that leverages domain knowledge of PPG signal morphology across individuals, enabling the capture of richer representations compared to traditional contrastive learning methods. We evaluate PaPaGei against state-of-the-art time-series foundation models and self-supervised learning benchmarks across 20 tasks from 10 diverse datasets, spanning cardiovascular health, sleep disorders, pregnancy monitoring, and wellbeing assessment. Our model demonstrates superior performance, improving classification and regression metrics by 6.3% and 2.9% respectively in at least 14 tasks. Notably, PaPaGei achieves these results while being more data- and parameter-efficient, outperforming models that are 70x larger. Beyond accuracy, we examine model robustness across different skin tones, establishing a benchmark for bias evaluation in future models. PaPaGei can serve as both a feature extractor and an encoder for multimodal models, opening up new opportunities for multimodal health monitoring.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2410.20406.pdf' target='_blank'>https://arxiv.org/pdf/2410.20406.pdf</a></span>   <span><a href='https://github.com/auniquesun/Point-PRC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Sun, Qiuhong Ke, Yongcai Wang, Wang Chen, Kang Yang, Deying Li, Jianfei Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20406">Point-PRC: A Prompt Learning Based Regulation Framework for Generalizable Point Cloud Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the 3D domain generalization (3DDG) ability of large 3D models based on prevalent prompt learning. Recent works demonstrate the performances of 3D point cloud recognition can be boosted remarkably by parameter-efficient prompt tuning. However, we observe that the improvement on downstream tasks comes at the expense of a severe drop in 3D domain generalization. To resolve this challenge, we present a comprehensive regulation framework that allows the learnable prompts to actively interact with the well-learned general knowledge in large 3D models to maintain good generalization. Specifically, the proposed framework imposes multiple explicit constraints on the prompt learning trajectory by maximizing the mutual agreement between task-specific predictions and task-agnostic knowledge. We design the regulation framework as a plug-and-play module to embed into existing representative large 3D models. Surprisingly, our method not only realizes consistently increasing generalization ability but also enhances task-specific 3D recognition performances across various 3DDG benchmarks by a clear margin. Considering the lack of study and evaluation on 3DDG, we also create three new benchmarks, namely base-to-new, cross-dataset and few-shot generalization benchmarks, to enrich the field and inspire future research. Code and benchmarks are available at \url{https://github.com/auniquesun/Point-PRC}.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2410.19265.pdf' target='_blank'>https://arxiv.org/pdf/2410.19265.pdf</a></span>   <span><a href='https://github.com/kaize0409/Awesome-Graph-OOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexin Zhang, Shuhan Liu, Song Wang, Weili Shi, Chen Chen, Pan Li, Sheng Li, Jundong Li, Kaize Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19265">A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution Generalization to Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distribution shifts on graphs -- the discrepancies in data distribution between training and employing a graph machine learning model -- are ubiquitous and often unavoidable in real-world scenarios. These shifts may severely deteriorate model performance, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph machine learning under distribution shifts, aiming to train models to achieve satisfactory performance on out-of-distribution (OOD) test data. In our survey, we provide an up-to-date and forward-looking review of deep graph learning under distribution shifts. Specifically, we cover three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation. We begin by formally formulating the problems and discussing various types of distribution shifts that can affect graph learning, such as covariate shifts and concept shifts. To provide a better understanding of the literature, we introduce a systematic taxonomy that classifies existing methods into model-centric and data-centric approaches, investigating the techniques used in each category. We also summarize commonly used datasets in this research area to facilitate further investigation. Finally, we point out promising research directions and the corresponding challenges to encourage further study in this vital domain. We also provide a continuously updated reading list at https://github.com/kaize0409/Awesome-Graph-OOD.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2410.18122.pdf' target='_blank'>https://arxiv.org/pdf/2410.18122.pdf</a></span>   <span><a href='https://github.com/ioverho/misinfo-general' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivo Verhoeven, Pushkar Mishra, Ekaterina Shutova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18122">Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article introduces misinfo-general, a benchmark dataset for evaluating misinformation models' ability to perform out-of-distribution generalization. Misinformation changes rapidly, much more quickly than moderators can annotate at scale, resulting in a shift between the training and inference data distributions. As a result, misinformation detectors need to be able to perform out-of-distribution generalization, an attribute they currently lack. Our benchmark uses distant labelling to enable simulating covariate shifts in misinformation content. We identify time, event, topic, publisher, political bias, misinformation type as important axes for generalization, and we evaluate a common class of baseline models on each. Using article metadata, we show how this model fails desiderata, which is not necessarily obvious from classification metrics. Finally, we analyze properties of the data to ensure limited presence of modelling shortcuts. We make the dataset and accompanying code publicly available: https://github.com/ioverho/misinfo-general
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2410.17146.pdf' target='_blank'>https://arxiv.org/pdf/2410.17146.pdf</a></span>   <span><a href='https://github.com/wang-kee/LiNeS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Wang, Nikolaos Dimitriadis, Alessandro Favero, Guillermo Ortiz-Jimenez, Francois Fleuret, Pascal Frossard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17146">LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning pre-trained models has become the standard approach to endow them with specialized knowledge, but it poses fundamental challenges. In particular, \textit{(i)} fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks, and \textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to significant performance loss. To address these challenges, we introduce LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. In multi-task model merging scenarios, layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Our method is simple to implement, computationally efficient and complementary to many existing techniques. Our source code is available at https://github.com/wang-kee/LiNeS
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2410.17020.pdf' target='_blank'>https://arxiv.org/pdf/2410.17020.pdf</a></span>   <span><a href='https://github.com/liangchen527/LFME' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Chen, Yong Zhang, Yibing Song, Zhiqiang Shen, Lingqiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17020">LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) methods aim to maintain good performance in an unseen target domain by using training data from multiple source domains. While success on certain occasions are observed, enhancing the baseline across most scenarios remains challenging. This work introduces a simple yet effective framework, dubbed learning from multiple experts (LFME), that aims to make the target model an expert in all source domains to improve DG. Specifically, besides learning the target model used in inference, LFME will also train multiple experts specialized in different domains, whose output probabilities provide professional guidance by simply regularizing the logit of the target model. Delving deep into the framework, we reveal that the introduced logit regularization term implicitly provides effects of enabling the target model to harness more information, and mining hard samples from the experts during training. Extensive experiments on benchmarks from different DG tasks demonstrate that LFME is consistently beneficial to the baseline and can achieve comparable performance to existing arts. Code is available at~\url{https://github.com/liangchen527/LFME}.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2410.16845.pdf' target='_blank'>https://arxiv.org/pdf/2410.16845.pdf</a></span>   <span><a href='https://github.com/draym28/FGSAM_NeurIPS24' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Luo, Yuhan Chen, Siya Qiu, Yiwei Wang, Chen Zhang, Yan Zhou, Xiaochun Cao, Jing Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16845">Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have shown superior performance in node classification. However, GNNs perform poorly in the Few-Shot Node Classification (FSNC) task that requires robust generalization to make accurate predictions for unseen classes with limited labels. To tackle the challenge, we propose the integration of Sharpness-Aware Minimization (SAM)--a technique designed to enhance model generalization by finding a flat minimum of the loss landscape--into GNN training. The standard SAM approach, however, consists of two forward-backward steps in each training iteration, doubling the computational cost compared to the base optimizer (e.g., Adam). To mitigate this drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM), that integrates the rapid training of Multi-Layer Perceptrons (MLPs) with the superior performance of GNNs. Specifically, we utilize GNNs for parameter perturbation while employing MLPs to minimize the perturbed loss so that we can find a flat minimum with good generalization more efficiently. Moreover, our method reutilizes the gradient from the perturbation phase to incorporate graph topology into the minimization process at almost zero additional cost. To further enhance training efficiency, we develop FGSAM+ that executes exact perturbations periodically. Extensive experiments demonstrate that our proposed algorithm outperforms the standard SAM with lower computational costs in FSNC tasks. In particular, our FGSAM+ as a SAM variant offers a faster optimization than the base optimizer in most cases. In addition to FSNC, our proposed methods also demonstrate competitive performance in the standard node classification task for heterophilic graphs, highlighting the broad applicability. The code is available at https://github.com/draym28/FGSAM_NeurIPS24.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2410.16146.pdf' target='_blank'>https://arxiv.org/pdf/2410.16146.pdf</a></span>   <span><a href='https://github.com/C0notSilly/AdvFrequency' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilin He, Jingyu Hu, Qinliang Lin, Cheng Luo, Weicheng Xie, Siyang Song, Muhammad Haris Khan, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16146">Towards Combating Frequency Simplicity-biased Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization methods aim to learn transferable knowledge from source domains that can generalize well to unseen target domains. Recent studies show that neural networks frequently suffer from a simplicity-biased learning behavior which leads to over-reliance on specific frequency sets, namely as frequency shortcuts, instead of semantic information, resulting in poor generalization performance. Despite previous data augmentation techniques successfully enhancing generalization performances, they intend to apply more frequency shortcuts, thereby causing hallucinations of generalization improvement. In this paper, we aim to prevent such learning behavior of applying frequency shortcuts from a data-driven perspective. Given the theoretical justification of models' biased learning behavior on different spatial frequency components, which is based on the dataset frequency properties, we argue that the learning behavior on various frequency components could be manipulated by changing the dataset statistical structure in the Fourier domain. Intuitively, as frequency shortcuts are hidden in the dominant and highly dependent frequencies of dataset structure, dynamically perturbating the over-reliance frequency components could prevent the application of frequency shortcuts. To this end, we propose two effective data augmentation modules designed to collaboratively and adaptively adjust the frequency characteristic of the dataset, aiming to dynamically influence the learning behavior of the model and ultimately serving as a strategy to mitigate shortcut learning. Code is available at AdvFrequency (https://github.com/C0notSilly/AdvFrequency).
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2410.16020.pdf' target='_blank'>https://arxiv.org/pdf/2410.16020.pdf</a></span>   <span><a href='https://github.com/lingeringlight/START' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lingeringlight/START' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Guo, Lei Qi, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16020">START: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to enable models to generalize to unseen target domains by learning from multiple source domains. Existing DG methods primarily rely on convolutional neural networks (CNNs), which inherently learn texture biases due to their limited receptive fields, making them prone to overfitting source domains. While some works have introduced transformer-based methods (ViTs) for DG to leverage the global receptive field, these methods incur high computational costs due to the quadratic complexity of self-attention. Recently, advanced state space models (SSMs), represented by Mamba, have shown promising results in supervised learning tasks by achieving linear complexity in sequence length during training and fast RNN-like computation during inference. Inspired by this, we investigate the generalization ability of the Mamba model under domain shifts and find that input-dependent matrices within SSMs could accumulate and amplify domain-specific features, thus hindering model generalization. To address this issue, we propose a novel SSM-based architecture with saliency-based token-aware transformation (namely START), which achieves state-of-the-art (SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our START can selectively perturb and suppress domain-specific features in salient tokens within the input-dependent matrices of SSMs, thus effectively reducing the discrepancy between different domains. Extensive experiments on five benchmarks demonstrate that START outperforms existing SOTA DG methods with efficient linear complexity. Our code is available at https://github.com/lingeringlight/START.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2410.14817.pdf' target='_blank'>https://arxiv.org/pdf/2410.14817.pdf</a></span>   <span><a href='https://github.com/EricElmoznino/complexity_compositionality' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, Guillaume Lajoie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14817">A Complexity-Based Theory of Compositionality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, compositional representations can enable a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, we lack satisfying formal definitions for it that are measurable and mathematical. Here, we propose such a definition, which we call representational compositionality, that accounts for and extends our intuitions about compositionality. The definition is conceptually simple, quantitative, grounded in algorithmic information theory, and applicable to any representation. Intuitively, representational compositionality states that a compositional representation satisfies three properties. First, it must be expressive. Second, it must be possible to re-describe the representation as a function of discrete symbolic sequences with re-combinable parts, analogous to sentences in natural language. Third, the function that relates these symbolic sequences to the representation, analogous to semantics in natural language, must be simple. Through experiments on both synthetic and real world data, we validate our definition of compositionality and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools. We hope that our definition can inspire the design of novel, theoretically-driven models that better capture the mechanisms of compositional thought. We make our code available at https://github.com/EricElmoznino/complexity_compositionality.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2410.11397.pdf' target='_blank'>https://arxiv.org/pdf/2410.11397.pdf</a></span>   <span><a href='https://github.com/XeniaLLL/FOOGD-main.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinting Liao, Weiming Liu, Pengyang Zhou, Fengyuan Yu, Jiahe Xu, Jun Wang, Wenjie Wang, Chaochao Chen, Xiaolin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11397">FOOGD: Federated Collaboration for Both Out-of-distribution Generalization and Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) is a promising machine learning paradigm that collaborates with client models to capture global knowledge. However, deploying FL models in real-world scenarios remains unreliable due to the coexistence of in-distribution data and unexpected out-of-distribution (OOD) data, such as covariate-shift and semantic-shift data. Current FL researches typically address either covariate-shift data through OOD generalization or semantic-shift data via OOD detection, overlooking the simultaneous occurrence of various OOD shifts. In this work, we propose FOOGD, a method that estimates the probability density of each client and obtains reliable global distribution as guidance for the subsequent FL process. Firstly, SM3D in FOOGD estimates score model for arbitrary distributions without prior constraints, and detects semantic-shift data powerfully. Then SAG in FOOGD provides invariant yet diverse knowledge for both local covariate-shift generalization and client performance generalization. In empirical validations, FOOGD significantly enjoys three main advantages: (1) reliably estimating non-normalized decentralized distributions, (2) detecting semantic shift data via score values, and (3) generalizing to covariate-shift data by regularizing feature extractor. The prejoct is open in https://github.com/XeniaLLL/FOOGD-main.git.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2410.11267.pdf' target='_blank'>https://arxiv.org/pdf/2410.11267.pdf</a></span>   <span><a href='https://github.com/sanphouwang/fedccrl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Wang, Yongxin Guo, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11267">FedCCRL: Federated Domain Generalization with Cross-Client Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to train models that can effectively generalize to unseen domains. However, in the context of Federated Learning (FL), where clients collaboratively train a model without directly sharing their data, most existing DG algorithms are not directly applicable to the FL setting due to privacy constraints, as well as the limited data quantity and domain diversity at each client. To tackle these challenges, we propose FedCCRL, a lightweight federated domain generalization method that significantly improves the model's generalization ability while preserving privacy and ensuring computational and communication efficiency. Specifically, FedCCRL comprises two principal modules: the first is a cross-client feature extension module, which increases local domain diversity via cross-client domain transfer and domain-invariant feature perturbation; the second is a representation and prediction dual-stage alignment module, which enables the model to effectively capture domain-invariant features. Extensive experimental results demonstrate that FedCCRL achieves the state-of-the-art performance on the PACS, OfficeHome and miniDomainNet datasets across FL settings of varying numbers of clients. Code is available at https://github.com/sanphouwang/fedccrl
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2410.10105.pdf' target='_blank'>https://arxiv.org/pdf/2410.10105.pdf</a></span>   <span><a href='https://github.com/qianyu-dlut/DiffDIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Yu, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Lihe Zhang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10105">High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of high-resolution (HR), fine-grained image segmentation, the primary challenge is balancing broad contextual awareness with the precision required for detailed object delineation, capturing intricate details and the finest edges of objects. Diffusion models, trained on vast datasets comprising billions of image-text pairs, such as SD V2.1, have revolutionized text-to-image synthesis by delivering exceptional quality, fine detail resolution, and strong contextual awareness, making them an attractive solution for high-resolution image segmentation. To this end, we propose DiffDIS, a diffusion-driven segmentation model that taps into the potential of the pre-trained U-Net within diffusion models, specifically designed for high-resolution, fine-grained object segmentation. By leveraging the robust generalization capabilities and rich, versatile image representation prior of the SD models, coupled with a task-specific stable one-step denoising approach, we significantly reduce the inference time while preserving high-fidelity, detailed generation. Additionally, we introduce an auxiliary edge generation task to not only enhance the preservation of fine details of the object boundaries, but reconcile the probabilistic nature of diffusion with the deterministic demands of segmentation. With these refined strategies in place, DiffDIS serves as a rapid object mask generation model, specifically optimized for generating detailed binary maps at high resolutions, while demonstrating impressive accuracy and swift processing. Experiments on the DIS5K dataset demonstrate the superiority of DiffDIS, achieving state-of-the-art results through a streamlined inference process. The source code will be publicly available at https://github.com/qianyu-dlut/DiffDIS.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2410.08000.pdf' target='_blank'>https://arxiv.org/pdf/2410.08000.pdf</a></span>   <span><a href='https://github.com/HaoyueBaiZJU/aha' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyue Bai, Jifan Zhang, Robert Nowak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08000">AHA: Human-Assisted Out-of-Distribution Generalization and Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern machine learning models deployed often encounter distribution shifts in real-world applications, manifesting as covariate or semantic out-of-distribution (OOD) shifts. These shifts give rise to challenges in OOD generalization and OOD detection. This paper introduces a novel, integrated approach AHA (Adaptive Human-Assisted OOD learning) to simultaneously address both OOD generalization and detection through a human-assisted framework by labeling data in the wild. Our approach strategically labels examples within a novel maximum disambiguation region, where the number of semantic and covariate OOD data roughly equalizes. By labeling within this region, we can maximally disambiguate the two types of OOD data, thereby maximizing the utility of the fixed labeling budget. Our algorithm first utilizes a noisy binary search algorithm that identifies the maximal disambiguation region with high probability. The algorithm then continues with annotating inside the identified labeling region, reaping the full benefit of human feedback. Extensive experiments validate the efficacy of our framework. We observed that with only a few hundred human annotations, our method significantly outperforms existing state-of-the-art methods that do not involve human assistance, in both OOD generalization and OOD detection. Code is publicly available at \url{https://github.com/HaoyueBaiZJU/aha}.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2410.05270.pdf' target='_blank'>https://arxiv.org/pdf/2410.05270.pdf</a></span>   <span><a href='https://github.com/astra-vision/ProLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick PÃ©rez, Raoul de Charette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05270">CLIP's Visual Embedding Projector is a Few-shot Cornucopia</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of adapting a contrastively pretrained vision-language model like CLIP (Radford et al., 2021) for few-shot classification. The literature addresses this problem by learning a linear classifier of the frozen visual features, optimizing word embeddings, or learning external feature adapters. We introduce an alternative way for few-shot CLIP adaptation without adding ''external'' parameters to optimize. We find that simply fine-tuning the embedding projection matrix of the vision encoder leads to better performance than all baselines. Furthermore, we show that regularizing training with the distance between the fine-tuned and pretrained matrices adds reliability for adapting CLIP, making the results stable across different learning rates in the ''validation-free'' setting. This simple approach, coined ProLIP, yields state-of-the-art performance on 11 few-shot classification benchmarks, few-shot cross-dataset transfer, domain generalization, and base-to-new class generalization. We also show that ProLIP significantly outperforms prompt tuning when extended to another task of test-time adaptation, while being one order of magnitude faster to train. Code will be made available at: https://github.com/astra-vision/ProLIP .
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2410.04671.pdf' target='_blank'>https://arxiv.org/pdf/2410.04671.pdf</a></span>   <span><a href='https://github.com/MiracleDance/CAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Yao, Jialin Li, Yifeng Zhou, Yong Liu, Xi Jiang, Chengjie Wang, Feng Zheng, Yuexian Zou, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04671">CAR: Controllable Autoregressive Modeling for Visual Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable generation, which enables fine-grained control over generated outputs, has emerged as a critical focus in visual generative models. Currently, there are two primary technical approaches in visual generation: diffusion models and autoregressive models. Diffusion models, as exemplified by ControlNet and T2I-Adapter, offer advanced control mechanisms, whereas autoregressive models, despite showcasing impressive generative quality and scalability, remain underexplored in terms of controllability and flexibility. In this study, we introduce Controllable AutoRegressive Modeling (CAR), a novel, plug-and-play framework that integrates conditional control into multi-scale latent variable modeling, enabling efficient control generation within a pre-trained visual autoregressive model. CAR progressively refines and captures control representations, which are injected into each autoregressive step of the pre-trained model to guide the generation process. Our approach demonstrates excellent controllability across various types of conditions and delivers higher image quality compared to previous methods. Additionally, CAR achieves robust generalization with significantly fewer training resources compared to those required for pre-training the model. To the best of our knowledge, we are the first to propose a control framework for pre-trained autoregressive visual generation models.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2410.04372.pdf' target='_blank'>https://arxiv.org/pdf/2410.04372.pdf</a></span>   <span><a href='https://github.com/skJack/DiffusionFake.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Sun, Shen Chen, Taiping Yao, Hong Liu, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04372">DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of Deepfake technology has made face swapping highly realistic, raising concerns about the malicious use of fabricated facial content. Existing methods often struggle to generalize to unseen domains due to the diverse nature of facial manipulations. In this paper, we revisit the generation process and identify a universal principle: Deepfake images inherently contain information from both source and target identities, while genuine faces maintain a consistent identity. Building upon this insight, we introduce DiffusionFake, a novel plug-and-play framework that reverses the generative process of face forgeries to enhance the generalization of detection models. DiffusionFake achieves this by injecting the features extracted by the detection model into a frozen pre-trained Stable Diffusion model, compelling it to reconstruct the corresponding target and source images. This guided reconstruction process constrains the detection network to capture the source and target related features to facilitate the reconstruction, thereby learning rich and disentangled representations that are more resilient to unseen forgeries. Extensive experiments demonstrate that DiffusionFake significantly improves cross-domain generalization of various detector architectures without introducing additional parameters during inference. Our Codes are available in https://github.com/skJack/DiffusionFake.git.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2410.03499.pdf' target='_blank'>https://arxiv.org/pdf/2410.03499.pdf</a></span>   <span><a href='https://github.com/sunnyinAI/FedStein' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Gupta, Nikita Jangid, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03499">FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) facilitates data privacy by enabling collaborative in-situ training across decentralized clients. Despite its inherent advantages, FL faces significant challenges of performance and convergence when dealing with data that is not independently and identically distributed (non-i.i.d.). While previous research has primarily addressed the issue of skewed label distribution across clients, this study focuses on the less explored challenge of multi-domain FL, where client data originates from distinct domains with varying feature distributions. We introduce a novel method designed to address these challenges FedStein: Enhancing Multi-Domain Federated Learning Through the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS) estimates of batch normalization (BN) statistics across clients, while maintaining local BN parameters. The non-BN layer parameters are exchanged via standard FL techniques. Extensive experiments conducted across three datasets and multiple models demonstrate that FedStein surpasses existing methods such as FedAvg and FedBN, with accuracy improvements exceeding 14% in certain domains leading to enhanced domain generalization. The code is available at https://github.com/sunnyinAI/FedStein
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2410.02505.pdf' target='_blank'>https://arxiv.org/pdf/2410.02505.pdf</a></span>   <span><a href='https://github.com/Kai-Liu001/Dog-IQA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Kai-Liu001/Dog-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Liu, Ziqing Zhang, Wenbo Li, Renjing Pei, Fenglong Song, Xiaohong Liu, Linghe Kong, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02505">Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) serves as the golden standard for all models' performance in nearly all computer vision fields. However, it still suffers from poor out-of-distribution generalization ability and expensive training costs. To address these problems, we propose Dog-IQA, a standard-guided zero-shot mix-grained IQA method, which is training-free and utilizes the exceptional prior knowledge of multimodal large language models (MLLMs). To obtain accurate IQA scores, namely scores consistent with humans, we design an MLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA applies two techniques. First, Dog-IQA objectively scores with specific standards that utilize MLLM's behavior pattern and minimize the influence of subjective factors. Second, Dog-IQA comprehensively takes local semantic objects and the whole image as input and aggregates their scores, leveraging local and global information. Our proposed Dog-IQA achieves state-of-the-art (SOTA) performance compared with training-free methods, and competitive performance compared with training-based methods in cross-dataset scenarios. Our code will be available at https://github.com/Kai-Liu001/Dog-IQA.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2410.02396.pdf' target='_blank'>https://arxiv.org/pdf/2410.02396.pdf</a></span>   <span><a href='https://github.com/duguodong7/pcb-merging' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02396">Parameter Competition Balancing for Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \url{https://github.com/duguodong7/pcb-merging}.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2409.19774.pdf' target='_blank'>https://arxiv.org/pdf/2409.19774.pdf</a></span>   <span><a href='https://github.com/NikosEfth/crafting-shifts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Efthymiadis, Giorgos Tolias, OndÅej Chum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19774">Crafting Distribution Shifts for Validation and Training in Single Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization attempts to learn a model on a source domain and deploy it to unseen target domains. Limiting access only to source domain data imposes two key challenges - how to train a model that can generalize and how to verify that it does. The standard practice of validation on the training distribution does not accurately reflect the model's generalization ability, while validation on the test distribution is a malpractice to avoid. In this work, we construct an independent validation set by transforming source domain images with a comprehensive list of augmentations, covering a broad spectrum of potential distribution shifts in target domains. We demonstrate a high correlation between validation and test performance for multiple methods and across various datasets. The proposed validation achieves a relative accuracy improvement over the standard validation equal to 15.4% or 1.6% when used for method selection or learning rate tuning, respectively. Furthermore, we introduce a novel family of methods that increase the shape bias through enhanced edge maps. To benefit from the augmentations during training and preserve the independence of the validation set, a k-fold validation process is designed to separate the augmentation types used in training and validation. The method that achieves the best performance on the augmented validation is selected from the proposed family. It achieves state-of-the-art performance on various standard benchmarks. Code at: https://github.com/NikosEfth/crafting-shifts
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2409.19663.pdf' target='_blank'>https://arxiv.org/pdf/2409.19663.pdf</a></span>   <span><a href='https://github.com/xpq-tech/KETI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaopeng Li, Shasha Li, Shangwen Wang, Shezheng Song, Bin Ji, Huijun Liu, Jun Ma, Jie Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19663">Identifying Knowledge Editing Types in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge editing has emerged as an efficient technique for updating the knowledge of large language models (LLMs), attracting increasing attention in recent years. However, there is a lack of effective measures to prevent the malicious misuse of this technique, which could lead to harmful edits in LLMs. These malicious modifications could cause LLMs to generate toxic content, misleading users into inappropriate actions. In front of this risk, we introduce a new task, $\textbf{K}$nowledge $\textbf{E}$diting $\textbf{T}$ype $\textbf{I}$dentification (KETI), aimed at identifying different types of edits in LLMs, thereby providing timely alerts to users when encountering illicit edits. As part of this task, we propose KETIBench, which includes five types of harmful edits covering the most popular toxic types, as well as one benign factual edit. We develop five classical classification models and three BERT-based models as baseline identifiers for both open-source and closed-source LLMs. Our experimental results, across 92 trials involving four models and three knowledge editing methods, demonstrate that all eight baseline identifiers achieve decent identification performance, highlighting the feasibility of identifying malicious edits in LLMs. Additional analyses reveal that the performance of the identifiers is independent of the reliability of the knowledge editing methods and exhibits cross-domain generalization, enabling the identification of edits from unknown sources. All data and code are available in https://github.com/xpq-tech/KETI.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2409.17555.pdf' target='_blank'>https://arxiv.org/pdf/2409.17555.pdf</a></span>   <span><a href='https://github.com/KPeng9510/EBiL-HaDS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/KPeng9510/EBiL-HaDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Peng, Di Wen, Kailun Yang, Ao Luo, Yufan Chen, Jia Fu, M. Saquib Sarfraz, Alina Roitberg, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17555">Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Open-Set Domain Generalization (OSDG), the model is exposed to both new variations of data appearance (domains) and open-set conditions, where both known and novel categories are present at test time. The challenges of this task arise from the dual need to generalize across diverse domains and accurately quantify category novelty, which is critical for applications in dynamic environments. Recently, meta-learning techniques have demonstrated superior results in OSDG, effectively orchestrating the meta-train and -test tasks by employing varied random categories and predefined domain partition strategies. These approaches prioritize a well-designed training schedule over traditional methods that focus primarily on data augmentation and the enhancement of discriminative feature learning. The prevailing meta-learning models in OSDG typically utilize a predefined sequential domain scheduler to structure data partitions. However, a crucial aspect that remains inadequately explored is the influence brought by strategies of domain schedulers during training. In this paper, we observe that an adaptive domain scheduler benefits more in OSDG compared with prefixed sequential and random domain schedulers. We propose the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) to achieve an adaptive domain scheduler. This method strategically sequences domains by assessing their reliabilities in utilizing a follower network, trained with confidence scores learned in an evidential manner, regularized by max rebiasing discrepancy, and optimized in a bi-level manner. The results show that our method substantially improves OSDG performance and achieves more discriminative embeddings for both the seen and unseen categories. The source code is publicly available at https://github.com/KPeng9510/EBiL-HaDS.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2409.15730.pdf' target='_blank'>https://arxiv.org/pdf/2409.15730.pdf</a></span>   <span><a href='https://github.com/Sephirex-X/LatentDriver' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyu Xiao, Jiang-Jiang Liu, Sen Yang, Xiaofan Li, Xiaoqing Ye, Wankou Yang, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15730">Learning Multiple Probabilistic Decisions from Latent World Model in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The autoregressive world model exhibits robust generalization capabilities in vectorized scene understanding but encounters difficulties in deriving actions due to insufficient uncertainty modeling and self-delusion. In this paper, we explore the feasibility of deriving decisions from an autoregressive world model by addressing these challenges through the formulation of multiple probabilistic hypotheses. We propose LatentDriver, a framework models the environment's next states and the ego vehicle's possible actions as a mixture distribution, from which a deterministic control signal is then derived. By incorporating mixture modeling, the stochastic nature of decisionmaking is captured. Additionally, the self-delusion problem is mitigated by providing intermediate actions sampled from a distribution to the world model. Experimental results on the recently released close-loop benchmark Waymax demonstrate that LatentDriver surpasses state-of-the-art reinforcement learning and imitation learning methods, achieving expert-level performance. The code and models will be made available at https://github.com/Sephirex-X/LatentDriver.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2409.14396.pdf' target='_blank'>https://arxiv.org/pdf/2409.14396.pdf</a></span>   <span><a href='https://github.com/nblt/Flat-LoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Li, Zhengbao He, Yujun Li, Yasheng Wang, Lifeng Shang, Xiaolin Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14396">Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning large-scale pre-trained models is prohibitively expensive in terms of computation and memory costs. Low-Rank Adaptation (LoRA), a popular Parameter-Efficient Fine-Tuning (PEFT) method, offers an efficient solution by optimizing only low-rank matrices. Despite recent progress in improving LoRA's performance, the relationship between the LoRA optimization space and the full parameter space is often overlooked. A solution that appears flat in the loss landscape of the LoRA space may still exhibit sharp directions in the full parameter space, potentially compromising generalization. We introduce Flat-LoRA, which aims to identify a low-rank adaptation situated in a flat region of the full parameter space. Instead of adopting the well-established sharpness-aware minimization approach, which incurs significant computation and memory overheads, we employ a Bayesian expectation loss objective to preserve training efficiency. Further, we design a refined random perturbation generation strategy for improved performance and carefully manage memory overhead using random seeds. Experiments across diverse tasks-including mathematical reasoning, coding abilities, dialogue generation, instruction following, and text-to-image generation-demonstrate that Flat-LoRA improves both in-domain and out-of-domain generalization. Code is available at https://github.com/nblt/Flat-LoRA.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2409.14163.pdf' target='_blank'>https://arxiv.org/pdf/2409.14163.pdf</a></span>   <span><a href='https://github.com/zhanghr2001/PromptTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Jingwen Fu, Badong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14163">PromptTA: Prompt-driven Text Adapter for Source-free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Source-free domain generalization (SFDG) tackles the challenge of adapting models to unseen target domains without access to source domain data. To deal with this challenging task, recent advances in SFDG have primarily focused on leveraging the text modality of vision-language models such as CLIP. These methods involve developing a transferable linear classifier based on diverse style features extracted from the text and learned prompts or deriving domain-unified text representations from domain banks. However, both style features and domain banks have limitations in capturing comprehensive domain knowledge. In this work, we propose Prompt-Driven Text Adapter (PromptTA) method, which is designed to better capture the distribution of style features and employ resampling to ensure thorough coverage of domain knowledge. To further leverage this rich domain information, we introduce a text adapter that learns from these style features for efficient domain information storage. Extensive experiments conducted on four benchmark datasets demonstrate that PromptTA achieves state-of-the-art performance. The code is available at https://github.com/zhanghr2001/PromptTA.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2409.12522.pdf' target='_blank'>https://arxiv.org/pdf/2409.12522.pdf</a></span>   <span><a href='https://github.com/wkklavis/DAPSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhikai Wei, Wenhui Dong, Peilin Zhou, Yuliang Gu, Zhou Zhao, Yongchao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12522">Prompting Segment Anything Model with Domain-Adaptive Prototype for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning based methods often suffer from performance degradation caused by domain shift. In recent years, many sophisticated network structures have been designed to tackle this problem. However, the advent of large model trained on massive data, with its exceptional segmentation capability, introduces a new perspective for solving medical segmentation problems. In this paper, we propose a novel Domain-Adaptive Prompt framework for fine-tuning the Segment Anything Model (termed as DAPSAM) to address single-source domain generalization (SDG) in segmenting medical images. DAPSAM not only utilizes a more generalization-friendly adapter to fine-tune the large model, but also introduces a self-learning prototype-based prompt generator to enhance model's generalization ability. Specifically, we first merge the important low-level features into intermediate features before feeding to each adapter, followed by an attention filter to remove redundant information. This yields more robust image embeddings. Then, we propose using a learnable memory bank to construct domain-adaptive prototypes for prompt generation, helping to achieve generalizable medical image segmentation. Extensive experimental results demonstrate that our DAPSAM achieves state-of-the-art performance on two SDG medical image segmentation tasks with different modalities. The code is available at https://github.com/wkklavis/DAPSAM.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2409.12293.pdf' target='_blank'>https://arxiv.org/pdf/2409.12293.pdf</a></span>   <span><a href='https://github.com/LuGroupUMN/ICL_Linear_Systems' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank Cole, Yulong Lu, Wuzhe Xu, Tianhao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12293">In-Context Learning of Linear Systems: Generalization Theory and Applications to Operator Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study theoretical guarantees for solving linear systems in-context using a linear transformer architecture. For in-domain generalization, we provide neural scaling laws that bound the generalization error in terms of the number of tasks and sizes of samples used in training and inference. For out-of-domain generalization, we find that the behavior of trained transformers under task distribution shifts depends crucially on the distribution of the tasks seen during training. We introduce a novel notion of task diversity and show that it defines a necessary and sufficient condition for pre-trained transformers generalize under task distribution shifts. We also explore applications of learning linear systems in-context, such as to in-context operator learning for PDEs. Finally, we provide some numerical experiments to validate the established theory.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2409.12040.pdf' target='_blank'>https://arxiv.org/pdf/2409.12040.pdf</a></span>   <span><a href='https://github.com/XieYiping66/SFDA-rPPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiping Xie, Zitong Yu, Bingjie Wu, Weicheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12040">SFDA-rPPG: Source-Free Domain Adaptive Remote Physiological Measurement with Spatio-Temporal Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote Photoplethysmography (rPPG) is a non-contact method that uses facial video to predict changes in blood volume, enabling physiological metrics measurement. Traditional rPPG models often struggle with poor generalization capacity in unseen domains. Current solutions to this problem is to improve its generalization in the target domain through Domain Generalization (DG) or Domain Adaptation (DA). However, both traditional methods require access to both source domain data and target domain data, which cannot be implemented in scenarios with limited access to source data, and another issue is the privacy of accessing source domain data. In this paper, we propose the first Source-free Domain Adaptation benchmark for rPPG measurement (SFDA-rPPG), which overcomes these limitations by enabling effective domain adaptation without access to source domain data. Our framework incorporates a Three-Branch Spatio-Temporal Consistency Network (TSTC-Net) to enhance feature consistency across domains. Furthermore, we propose a new rPPG distribution alignment loss based on the Frequency-domain Wasserstein Distance (FWD), which leverages optimal transport to align power spectrum distributions across domains effectively and further enforces the alignment of the three branches. Extensive cross-domain experiments and ablation studies demonstrate the effectiveness of our proposed method in source-free domain adaptation settings. Our findings highlight the significant contribution of the proposed FWD loss for distributional alignment, providing a valuable reference for future research and applications. The source code is available at https://github.com/XieYiping66/SFDA-rPPG
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2409.11570.pdf' target='_blank'>https://arxiv.org/pdf/2409.11570.pdf</a></span>   <span><a href='https://github.com/mhnazeri/VertiCoder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Nazeri, Aniket Datar, Anuj Pokhrel, Chenhui Pan, Garrett Warnell, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11570">VertiCoder: Self-Supervised Kinodynamic Representation Learning on Vertically Challenging Terrain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present VertiCoder, a self-supervised representation learning approach for robot mobility on vertically challenging terrain. Using the same pre-training process, VertiCoder can handle four different downstream tasks, including forward kinodynamics learning, inverse kinodynamics learning, behavior cloning, and patch reconstruction with a single representation. VertiCoder uses a TransformerEncoder to learn the local context of its surroundings by random masking and next patch reconstruction. We show that VertiCoder achieves better performance across all four different tasks compared to specialized End-to-End models with 77% fewer parameters. We also show VertiCoder's comparable performance against state-of-the-art kinodynamic modeling and planning approaches in real-world robot deployment. These results underscore the efficacy of VertiCoder in mitigating overfitting and fostering more robust generalization across diverse environmental contexts and downstream vehicle kinodynamic tasks.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2409.11206.pdf' target='_blank'>https://arxiv.org/pdf/2409.11206.pdf</a></span>   <span><a href='https://github.com/Addy-1998/High_Order_Graphs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Humnabadkar, Arindam Sikdar, Benjamin Cave, Huaizhong Zhang, Paul Bakaki, Ardhendu Behera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11206">High-Order Evolving Graphs for Enhanced Representation of Traffic Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an innovative framework for traffic dynamics analysis using High-Order Evolving Graphs, designed to improve spatio-temporal representations in autonomous driving contexts. Our approach constructs temporal bidirectional bipartite graphs that effectively model the complex interactions within traffic scenes in real-time. By integrating Graph Neural Networks (GNNs) with high-order multi-aggregation strategies, we significantly enhance the modeling of traffic scene dynamics, providing a more accurate and detailed analysis of these interactions. Additionally, we incorporate inductive learning techniques inspired by the GraphSAGE framework, enabling our model to adapt to new and unseen traffic scenarios without the need for retraining, thus ensuring robust generalization. Through extensive experiments on the ROAD and ROAD Waymo datasets, we establish a comprehensive baseline for further developments, demonstrating the potential of our method in accurately capturing traffic behavior. Our results emphasize the value of high-order statistical moments and feature-gated attention mechanisms in improving traffic behavior analysis, laying the groundwork for advancing autonomous driving technologies. Our source code is available at: https://github.com/Addy-1998/High_Order_Graphs
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2409.07960.pdf' target='_blank'>https://arxiv.org/pdf/2409.07960.pdf</a></span>   <span><a href='https://github.com/kerem-cekmeceli/Foundation-Models-for-Medical-Imagery' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kerem Cekmeceli, Meva Himmetoglu, Guney I. Tombak, Anna Susmelj, Ertunc Erdil, Ender Konukoglu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07960">Do Vision Foundation Models Enhance Domain Generalization in Medical Image Segmentation?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks achieve state-of-the-art performance in many supervised learning tasks when the training data distribution matches the test data distribution. However, their performance drops significantly under domain (covariate) shift, a prevalent issue in medical image segmentation due to varying acquisition settings across different scanner models and protocols. Recently, foundational models (FMs) trained on large datasets have gained attention for their ability to be adapted for downstream tasks and achieve state-of-the-art performance with excellent generalization capabilities on natural images. However, their effectiveness in medical image segmentation remains underexplored. In this paper, we investigate the domain generalization performance of various FMs, including DinoV2, SAM, MedSAM, and MAE, when fine-tuned using various parameter-efficient fine-tuning (PEFT) techniques such as Ladder and Rein (+LoRA) and decoder heads. We introduce a novel decode head architecture, HQHSAM, which simply integrates elements from two state-of-the-art decoder heads, HSAM and HQSAM, to enhance segmentation performance. Our extensive experiments on multiple datasets, encompassing various anatomies and modalities, reveal that FMs, particularly with the HQHSAM decode head, improve domain generalization for medical image segmentation. Moreover, we found that the effectiveness of PEFT techniques varies across different FMs. These findings underscore the potential of FMs to enhance the domain generalization performance of neural networks in medical image segmentation across diverse clinical settings, providing a solid foundation for future research. Code and models are available for research purposes at \url{https://github.com/kerem-cekmeceli/Foundation-Models-for-Medical-Imagery}.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2409.04699.pdf' target='_blank'>https://arxiv.org/pdf/2409.04699.pdf</a></span>   <span><a href='https://github.com/alusi123/DFA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Wang, ALuSi, Xun Yang, Ke Xu, Huibin Tan, Xingyi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04699">Dual-stream Feature Augmentation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) task aims to learn a robust model from source domains that could handle the out-of-distribution (OOD) issue. In order to improve the generalization ability of the model in unseen domains, increasing the diversity of training samples is an effective solution. However, existing augmentation approaches always have some limitations. On the one hand, the augmentation manner in most DG methods is not enough as the model may not see the perturbed features in approximate the worst case due to the randomness, thus the transferability in features could not be fully explored. On the other hand, the causality in discriminative features is not involved in these methods, which harms the generalization ability of model due to the spurious correlations. To address these issues, we propose a Dual-stream Feature Augmentation~(DFA) method by constructing some hard features from two perspectives. Firstly, to improve the transferability, we construct some targeted features with domain related augmentation manner. Through the guidance of uncertainty, some hard cross-domain fictitious features are generated to simulate domain shift. Secondly, to take the causality into consideration, the spurious correlated non-causal information is disentangled by an adversarial mask, then the more discriminative features can be extracted through these hard causal related information. Different from previous fixed synthesizing strategy, the two augmentations are integrated into a unified learnable feature disentangle model. Based on these hard features, contrastive learning is employed to keep the semantic consistency and improve the robustness of the model. Extensive experiments on several datasets demonstrated that our approach could achieve state-of-the-art performance for domain generalization. Our code is available at: https://github.com/alusi123/DFA.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2409.03501.pdf' target='_blank'>https://arxiv.org/pdf/2409.03501.pdf</a></span>   <span><a href='https://github.com/RizhaoCai/FAS_Aug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rizhao Cai, Cecelia Soh, Zitong Yu, Haoliang Li, Wenhan Yang, Alex Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03501">Towards Data-Centric Face Anti-Spoofing: Improving Cross-domain Generalization via Physics-based Data Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Anti-Spoofing (FAS) research is challenged by the cross-domain problem, where there is a domain gap between the training and testing data. While recent FAS works are mainly model-centric, focusing on developing domain generalization algorithms for improving cross-domain performance, data-centric research for face anti-spoofing, improving generalization from data quality and quantity, is largely ignored. Therefore, our work starts with data-centric FAS by conducting a comprehensive investigation from the data perspective for improving cross-domain generalization of FAS models. More specifically, at first, based on physical procedures of capturing and recapturing, we propose task-specific FAS data augmentation (FAS-Aug), which increases data diversity by synthesizing data of artifacts, such as printing noise, color distortion, moirÃ© pattern, \textit{etc}. Our experiments show that using our FAS augmentation can surpass traditional image augmentation in training FAS models to achieve better cross-domain performance. Nevertheless, we observe that models may rely on the augmented artifacts, which are not environment-invariant, and using FAS-Aug may have a negative effect. As such, we propose Spoofing Attack Risk Equalization (SARE) to prevent models from relying on certain types of artifacts and improve the generalization performance. Last but not least, our proposed FAS-Aug and SARE with recent Vision Transformer backbones can achieve state-of-the-art performance on the FAS cross-domain generalization protocols. The implementation is available at https://github.com/RizhaoCai/FAS_Aug.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2409.01128.pdf' target='_blank'>https://arxiv.org/pdf/2409.01128.pdf</a></span>   <span><a href='https://github.com/jinglin-liang/DDDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinglin Liang, Jin Zhong, Hanlin Gu, Zhongqi Lu, Xingxing Tang, Gang Dai, Shuangping Huang, Lixin Fan, Qiang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01128">Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Class Continual Learning (FCCL) merges the challenges of distributed client learning with the need for seamless adaptation to new classes without forgetting old ones. The key challenge in FCCL is catastrophic forgetting, an issue that has been explored to some extent in Continual Learning (CL). However, due to privacy preservation requirements, some conventional methods, such as experience replay, are not directly applicable to FCCL. Existing FCCL methods mitigate forgetting by generating historical data through federated training of GANs or data-free knowledge distillation. However, these approaches often suffer from unstable training of generators or low-quality generated data, limiting their guidance for the model. To address this challenge, we propose a novel method of data replay based on diffusion models. Instead of training a diffusion model, we employ a pre-trained conditional diffusion model to reverse-engineer each class, searching the corresponding input conditions for each class within the model's input space, significantly reducing computational resources and time consumption while ensuring effective generation. Furthermore, we enhance the classifier's domain generalization ability on generated and real data through contrastive learning, indirectly improving the representational capability of generated data for real data. Comprehensive experiments demonstrate that our method significantly outperforms existing baselines. Code is available at https://github.com/jinglin-liang/DDDR.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2408.16768.pdf' target='_blank'>https://arxiv.org/pdf/2408.16768.pdf</a></span>   <span><a href='https://github.com/ZiyuGuo99/SAM2Point' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16768">SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SAM2Point, a preliminary exploration adapting Segment Anything Model 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point interprets any 3D data as a series of multi-directional videos, and leverages SAM 2 for 3D-space segmentation, without further training or 2D-3D projection. Our framework supports various prompt types, including 3D points, boxes, and masks, and can generalize across diverse scenarios, such as 3D objects, indoor scenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple 3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight the robust generalization capabilities of SAM2Point. To our best knowledge, we present the most faithful implementation of SAM in 3D, which may serve as a starting point for future research in promptable 3D segmentation. Online Demo: https://huggingface.co/spaces/ZiyuG/SAM2Point . Code: https://github.com/ZiyuGuo99/SAM2Point .
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2408.11787.pdf' target='_blank'>https://arxiv.org/pdf/2408.11787.pdf</a></span>   <span><a href='https://github.com/xq141839/NuSegDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenye Lou, Qing Xu, Zekun Jiang, Xiangjian He, Zhen Chen, Yi Wang, Chenxin Li, Maggie M. He, Wenting Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11787">NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain-generalized nuclei segmentation refers to the generalizability of models to unseen domains based on knowledge learned from source domains and is challenged by various image conditions, cell types, and stain strategies. Recently, the Segment Anything Model (SAM) has made great success in universal image segmentation by interactive prompt modes (e.g., point and box). Despite its strengths, the original SAM presents limited adaptation to medical images. Moreover, SAM requires providing manual bounding box prompts for each object to produce satisfactory segmentation masks, so it is laborious in nuclei segmentation scenarios. To address these limitations, we propose a domain-generalizable framework for nuclei image segmentation, abbreviated to NuSegDG. Specifically, we first devise a Heterogeneous Space Adapter (HS-Adapter) to learn multi-dimensional feature representations of different nuclei domains by injecting a small number of trainable parameters into the image encoder of SAM. To alleviate the labor-intensive requirement of manual prompts, we introduce a Gaussian-Kernel Prompt Encoder (GKP-Encoder) to generate density maps driven by a single point, which guides segmentation predictions by mixing position prompts and semantic prompts. Furthermore, we present a Two-Stage Mask Decoder (TSM-Decoder) to effectively convert semantic masks to instance maps without the manual demand for morphological shape refinement. Based on our experimental evaluations, the proposed NuSegDG demonstrates state-of-the-art performance in nuclei instance segmentation, exhibiting superior domain generalization capabilities. The source code is available at https://github.com/xq141839/NuSegDG.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2408.10613.pdf' target='_blank'>https://arxiv.org/pdf/2408.10613.pdf</a></span>   <span><a href='https://github.com/ma787639046/tdro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyuan Ma, Yongliang Ma, Xing Wu, Zhenpeng Su, Ming Zhou, Songlin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10613">Task-level Distributionally Robust Optimization for Large Language Model-based Dense Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous heterogeneous fine-tuning collections from different domains. However, the discussion about its training data distribution is still minimal. Previous studies rely on empirically assigned dataset choices or sampling ratios, which inevitably lead to sub-optimal retrieval performances. In this paper, we propose a new task-level Distributionally Robust Optimization (tDRO) algorithm for LLM-DR fine-tuning, targeted at improving the universal domain generalization ability by end-to-end reweighting the data distribution of each task. The tDRO parameterizes the domain weights and updates them with scaled domain gradients. The optimized weights are then transferred to the LLM-DR fine-tuning to train more robust retrievers. Experiments show optimal improvements in large-scale retrieval benchmarks and reduce up to 30% dataset usage after applying our optimization algorithm with a series of different-sized LLM-DR models.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2408.09278.pdf' target='_blank'>https://arxiv.org/pdf/2408.09278.pdf</a></span>   <span><a href='https://github.com/hrlblab/layer_segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchao Zhu, Mengmeng Yin, Ruining Deng, Yitian Long, Yu Wang, Yaohong Wang, Shilin Zhao, Haichun Yang, Yuankai Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09278">Cross-Species Data Integration for Enhanced Layer Segmentation in Kidney Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate delineation of the boundaries between the renal cortex and medulla is crucial for subsequent functional structural analysis and disease diagnosis. Training high-quality deep-learning models for layer segmentation relies on the availability of large amounts of annotated data. However, due to the patient's privacy of medical data and scarce clinical cases, constructing pathological datasets from clinical sources is relatively difficult and expensive. Moreover, using external natural image datasets introduces noise during the domain generalization process. Cross-species homologous data, such as mouse kidney data, which exhibits high structural and feature similarity to human kidneys, has the potential to enhance model performance on human datasets. In this study, we incorporated the collected private Periodic Acid-Schiff (PAS) stained mouse kidney dataset into the human kidney dataset for joint training. The results showed that after introducing cross-species homologous data, the semantic segmentation models based on CNN and Transformer architectures achieved an average increase of 1.77% and 1.24% in mIoU, and 1.76% and 0.89% in Dice score for the human renal cortex and medulla datasets, respectively. This approach is also capable of enhancing the model's generalization ability. This indicates that cross-species homologous data, as a low-noise trainable data source, can help improve model performance under conditions of limited clinical samples. Code is available at https://github.com/hrlblab/layer_segmentation.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2408.04949.pdf' target='_blank'>https://arxiv.org/pdf/2408.04949.pdf</a></span>   <span><a href='https://github.com/gianlucarloni/crocodile' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Carloni, Sotirios A Tsaftaris, Sara Colantonio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04949">CROCODILE: Causality aids RObustness via COntrastive DIsentangled LEarning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to domain shift, deep learning image classifiers perform poorly when applied to a domain different from the training one. For instance, a classifier trained on chest X-ray (CXR) images from one hospital may not generalize to images from another hospital due to variations in scanner settings or patient characteristics. In this paper, we introduce our CROCODILE framework, showing how tools from causality can foster a model's robustness to domain shift via feature disentanglement, contrastive learning losses, and the injection of prior knowledge. This way, the model relies less on spurious correlations, learns the mechanism bringing from images to prediction better, and outperforms baselines on out-of-distribution (OOD) data. We apply our method to multi-label lung disease classification from CXRs, utilizing over 750000 images from four datasets. Our bias-mitigation method improves domain generalization and fairness, broadening the applicability and reliability of deep learning models for a safer medical image analysis. Find our code at: https://github.com/gianlucarloni/crocodile.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2408.02792.pdf' target='_blank'>https://arxiv.org/pdf/2408.02792.pdf</a></span>   <span><a href='https://github.com/sfu-mial/LesionElevation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Abhishek, Ghassan Hamarneh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02792">Lesion Elevation Prediction from Skin Images Improves Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep learning-based computer-aided diagnosis for skin lesion image analysis is approaching dermatologists' performance levels, there are several works showing that incorporating additional features such as shape priors, texture, color constancy, and illumination further improves the lesion diagnosis performance. In this work, we look at another clinically useful feature, skin lesion elevation, and investigate the feasibility of predicting and leveraging skin lesion elevation labels. Specifically, we use a deep learning model to predict image-level lesion elevation labels from 2D skin lesion images. We test the elevation prediction accuracy on the derm7pt dataset, and use the elevation prediction model to estimate elevation labels for images from five other datasets: ISIC 2016, 2017, and 2018 Challenge datasets, MSK, and DermoFit. We evaluate cross-domain generalization by using these estimated elevation labels as auxiliary inputs to diagnosis models, and show that these improve the classification performance, with AUROC improvements of up to 6.29% and 2.69% for dermoscopic and clinical images, respectively. The code is publicly available at https://github.com/sfu-mial/LesionElevation.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2408.01697.pdf' target='_blank'>https://arxiv.org/pdf/2408.01697.pdf</a></span>   <span><a href='https://github.com/maowenyu-11/InfoIGL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyu Mao, Jiancan Wu, Haoyang Liu, Yongduo Sui, Xiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01697">Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph out-of-distribution (OOD) generalization remains a major challenge in graph learning since graph neural networks (GNNs) often suffer from severe performance degradation under distribution shifts. Invariant learning, aiming to extract invariant features across varied distributions, has recently emerged as a promising approach for OOD generation. Despite the great success of invariant learning in OOD problems for Euclidean data (i.e., images), the exploration within graph data remains constrained by the complex nature of graphs. Existing studies, such as data augmentation or causal intervention, either suffer from disruptions to invariance during the graph manipulation process or face reliability issues due to a lack of supervised signals for causal parts. In this work, we propose a novel framework, called Invariant Graph Learning based on Information bottleneck theory (InfoIGL), to extract the invariant features of graphs and enhance models' generalization ability to unseen distributions. Specifically, InfoIGL introduces a redundancy filter to compress task-irrelevant information related to environmental factors. Cooperating with our designed multi-level contrastive learning, we maximize the mutual information among graphs of the same class in the downstream classification tasks, preserving invariant features for prediction to a great extent. An appealing feature of InfoIGL is its strong generalization ability without depending on supervised signal of invariance. Experiments on both synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance under OOD generalization for graph classification tasks. The source code is available at https://github.com/maowenyu-11/InfoIGL.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2408.01181.pdf' target='_blank'>https://arxiv.org/pdf/2408.01181.pdf</a></span>   <span><a href='https://github.com/daixiangzi/VAR-CLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, Xingyu Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01181">VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VAR is a new generation paradigm that employs 'next-scale prediction' as opposed to 'next-token prediction'. This innovative transformation enables auto-regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However, the original VAR model is constrained to class-conditioned synthesis, relying solely on textual captions for guidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model that integrates Visual Auto-Regressive techniques with the capabilities of CLIP. The VAR-CLIP framework encodes captions into text embeddings, which are then utilized as textual conditions for image generation. To facilitate training on extensive datasets, such as ImageNet, we have constructed a substantial image-text dataset leveraging BLIP2. Furthermore, we delve into the significance of word positioning within CLIP for the purpose of caption guidance. Extensive experiments confirm VAR-CLIP's proficiency in generating fantasy images with high fidelity, textual congruence, and aesthetic excellence. Our project page are https://github.com/daixiangzi/VAR-CLIP
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2408.00640.pdf' target='_blank'>https://arxiv.org/pdf/2408.00640.pdf</a></span>   <span><a href='https://github.com/asbjrnmunk/amaes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>AsbjÃ¸rn Munk, Jakob Ambsdorf, Sebastian Llambias, Mads Nielsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00640">AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data for 3D-Native Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the impact of self-supervised pretraining of 3D semantic segmentation models on a large-scale, domain-specific dataset. We introduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public sources, the largest public dataset available, and revisit a number of design choices for pretraining modern segmentation architectures by simplifying and optimizing state-of-the-art methods, and combining them with a novel augmentation strategy. The resulting AMAES framework is based on masked-image-modeling and intensity-based augmentation reversal and balances memory usage, runtime, and finetuning performance. Using the popular U-Net and the recent MedNeXt architecture as backbones, we evaluate the effect of pretraining on three challenging downstream tasks, covering single-sequence, low-resource settings, and out-of-domain generalization. The results highlight that pretraining on the proposed dataset with AMAES significantly improves segmentation performance in the majority of evaluated cases, and that it is beneficial to pretrain the model with augmentations, despite pretraing on a large-scale dataset. Code and model checkpoints for reproducing results, as well as the BRAINS-45K dataset are available at \url{https://github.com/asbjrnmunk/amaes}.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2407.21600.pdf' target='_blank'>https://arxiv.org/pdf/2407.21600.pdf</a></span>   <span><a href='https://github.com/Solor-pikachu/ROGER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shoujin Huang, Guanxiong Luo, Yunlin Zhao, Yilong Liu, Yuwan Wang, Kexin Yang, Jingzhe Liu, Hua Guo, Min Wang, Lingyan Zhang, Mengye Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21600">Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at https://github.com/Solor-pikachu/ROGER.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2407.21534.pdf' target='_blank'>https://arxiv.org/pdf/2407.21534.pdf</a></span>   <span><a href='https://github.com/mrwu-mac/ControlMLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo, Hao Fei, Guannan Jiang, Xiaoshuai Sun, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21534">ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a training-free method to inject visual prompts into Multimodal Large Language Models (MLLMs) through test-time optimization of a learnable latent variable. We observe that attention, as the core module of MLLMs, connects text prompt tokens and visual tokens, ultimately determining the final results. Our approach involves adjusting visual tokens from the MLP output at test time, controlling the attention response to ensure text prompt tokens attend to visual tokens in referring regions. We optimize a learnable latent variable based on an energy function, enhancing the strength of referring regions in the attention map. This enables detailed region description and reasoning without the need for substantial training costs or model retraining. Our method offers a promising direction for integrating referring abilities into MLLMs, and supports referring with box, mask, scribble and point. The results demonstrate that our method exhibits out-of-domain generalization and interpretability.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2407.19708.pdf' target='_blank'>https://arxiv.org/pdf/2407.19708.pdf</a></span>   <span><a href='https://github.com/xingyumex/ALEN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ezequiel Perez-Zarate, Oscar Ramos-Soto, Chunxiao Liu, Diego Oliva, Marco Perez-Cisneros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19708">ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-light image enhancement is an important task in computer vision, essential for improving the visibility and quality of images captured in non-optimal lighting conditions. Inadequate illumination can lead to significant information loss and poor image quality, impacting various applications such as surveillance. photography, or even autonomous driving. In this regard, automated methods have been developed to automatically adjust illumination in the image for a better visual perception. Current enhancement techniques often use specific datasets to enhance low-light images, but still present challenges when adapting to diverse real-world conditions, where illumination degradation may be localized to specific regions. To address this challenge, the Adaptive Light Enhancement Network (ALEN) is introduced, whose main approach is the use of a classification mechanism to determine whether local or global illumination enhancement is required. Subsequently, estimator networks adjust illumination based on this classification and simultaneously enhance color fidelity. ALEN integrates the Light Classification Network (LCNet) for illuminance categorization, complemented by the Single-Channel Network (SCNet), and Multi-Channel Network (MCNet) for precise estimation of illumination and color, respectively. Extensive experiments on publicly available datasets for low-light conditions were carried out to underscore ALEN's robust generalization capabilities, demonstrating superior performance in both quantitative metrics and qualitative assessments when compared to recent state-of-the-art methods. The ALEN not only enhances image quality in terms of visual perception but also represents an advancement in high-level vision tasks, such as semantic segmentation, as presented in this work. The code of this method is available at https://github.com/xingyumex/ALEN
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2407.18534.pdf' target='_blank'>https://arxiv.org/pdf/2407.18534.pdf</a></span>   <span><a href='https://github.com/zou-longkun/RPD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Longkun Zou, Wanru Zhu, Ke Chen, Lihua Guo, Kailing Guo, Kui Jia, Yaowei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18534">Boosting Cross-Domain Point Classification via Distilling Relational Priors from 2D Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic pattern of an object point cloud is determined by its topological configuration of local geometries. Learning discriminative representations can be challenging due to large shape variations of point sets in local regions and incomplete surface in a global perspective, which can be made even more severe in the context of unsupervised domain adaptation (UDA). In specific, traditional 3D networks mainly focus on local geometric details and ignore the topological structure between local geometries, which greatly limits their cross-domain generalization. Recently, the transformer-based models have achieved impressive performance gain in a range of image-based tasks, benefiting from its strong generalization capability and scalability stemming from capturing long range correlation across local patches. Inspired by such successes of visual transformers, we propose a novel Relational Priors Distillation (RPD) method to extract relational priors from the well-trained transformers on massive images, which can significantly empower cross-domain representations with consistent topological priors of objects. To this end, we establish a parameter-frozen pre-trained transformer module shared between 2D teacher and 3D student models, complemented by an online knowledge distillation strategy for semantically regularizing the 3D student model. Furthermore, we introduce a novel self-supervised task centered on reconstructing masked point cloud patches using corresponding masked multi-view image features, thereby empowering the model with incorporating 3D geometric information. Experiments on the PointDA-10 and the Sim-to-Real datasets verify that the proposed method consistently achieves the state-of-the-art performance of UDA for point cloud classification. The source code of this work is available at https://github.com/zou-longkun/RPD.git.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2407.15524.pdf' target='_blank'>https://arxiv.org/pdf/2407.15524.pdf</a></span>   <span><a href='https://github.com/azrealwang/MSPD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanrui Wang, Ching-Chun Chang, Chun-Shien Lu, Ching-Chia Kao, Isao Echizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15524">Minimal Cascade Gradient Smoothing for Fast Transferable Preemptive Adversarial Defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial attacks persist as a major challenge in deep learning. While training- and test-time defenses are well-studied, they often reduce clean accuracy, incur high cost, or fail under adaptive threats. In contrast, preemptive defenses, which perturb media before release, offer a practical alternative but remain slow, model-coupled, and brittle. We propose the Minimal Sufficient Preemptive Defense (MSPD), a fast, transferable framework that defends against future attacks without access to the target model or gradients. MSPD is driven by Minimal Cascade Gradient Smoothing (MCGS), a two-epoch optimization paradigm executed on a surrogate backbone. This defines a minimal yet effective regime for robust generalization across unseen models and attacks. MSPD runs at 0.02s/image (CIFAR-10) and 0.26s/image (ImageNet), 28--1696x faster than prior preemptive methods, while improving robust accuracy by +5% and clean accuracy by +3.7% across 11 models and 7 attacks. To evaluate adaptive robustness, we introduce Preemptive Reversion, the first white-box diagnostic attack that cancels preemptive perturbations under full gradient access. Even in this setting, MSPD retains a +2.2% robustness margin over the baseline. In practice, when gradients are unavailable, MSPD remains reliable and efficient. MSPD, MCGS, and Preemptive Reversion are each supported by formal theoretical proofs. The implementation is available at https://github.com/azrealwang/MSPD.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2407.11356.pdf' target='_blank'>https://arxiv.org/pdf/2407.11356.pdf</a></span>   <span><a href='https://github.com/qiumuyang/SIAB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyang Qiu, Jian Zhang, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11356">The Devil is in the Statistics: Mitigating and Exploiting Statistics Difference for Generalizable Semi-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent success of domain generalization in medical image segmentation, voxel-wise annotation for all source domains remains a huge burden. Semi-supervised domain generalization has been proposed very recently to combat this challenge by leveraging limited labeled data along with abundant unlabeled data collected from multiple medical institutions, depending on precisely harnessing unlabeled data while improving generalization simultaneously. In this work, we observe that domain shifts between medical institutions cause disparate feature statistics, which significantly deteriorates pseudo-label quality due to an unexpected normalization process. Nevertheless, this phenomenon could be exploited to facilitate unseen domain generalization. Therefore, we propose 1) multiple statistics-individual branches to mitigate the impact of domain shifts for reliable pseudo-labels and 2) one statistics-aggregated branch for domain-invariant feature learning. Furthermore, to simulate unseen domains with statistics difference, we approach this from two aspects, i.e., a perturbation with histogram matching at image level and a random batch normalization selection strategy at feature level, producing diverse statistics to expand the training distribution. Evaluation results on three medical image datasets demonstrate the effectiveness of our method compared with recent SOTA methods. The code is available at https://github.com/qiumuyang/SIAB.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2407.10681.pdf' target='_blank'>https://arxiv.org/pdf/2407.10681.pdf</a></span>   <span><a href='https://github.com/WtaoZhao/geomix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Zhao, Qitian Wu, Chenxiao Yang, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10681">GeoMix: Towards Geometry-Aware Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixup has shown considerable success in mitigating the challenges posed by limited labeled data in image classification. By synthesizing samples through the interpolation of features and labels, Mixup effectively addresses the issue of data scarcity. However, it has rarely been explored in graph learning tasks due to the irregularity and connectivity of graph data. Specifically, in node classification tasks, Mixup presents a challenge in creating connections for synthetic data. In this paper, we propose Geometric Mixup (GeoMix), a simple and interpretable Mixup approach leveraging in-place graph editing. It effectively utilizes geometry information to interpolate features and labels with those from the nearby neighborhood, generating synthetic nodes and establishing connections for them. We conduct theoretical analysis to elucidate the rationale behind employing geometry information for node Mixup, emphasizing the significance of locality enhancement-a critical aspect of our method's design. Extensive experiments demonstrate that our lightweight Geometric Mixup achieves state-of-the-art results on a wide variety of standard datasets with limited labeled data. Furthermore, it significantly improves the generalization capability of underlying GNNs across various challenging out-of-distribution generalization tasks. Our code is available at https://github.com/WtaoZhao/geomix.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2407.10204.pdf' target='_blank'>https://arxiv.org/pdf/2407.10204.pdf</a></span>   <span><a href='https://github.com/LEOXC1571/DEROG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Xu, Yao Cheng, Jianxiang Yu, Haosen Wang, Jingsong Lv, Yao Liu, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10204">Improving Graph Out-of-distribution Generalization Beyond Causality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods for graph out-of-distribution (OOD) generalization primarily rely on empirical studies on synthetic datasets. Such approaches tend to overemphasize the causal relationships between invariant sub-graphs and labels, thereby neglecting the non-negligible role of environment in real-world scenarios. In contrast to previous studies that impose rigid independence assumptions on environments and invariant sub-graphs, this paper presents the theorems of environment-label dependency and mutable rationale invariance, where the former characterizes the usefulness of environments in determining graph labels while the latter refers to the mutable importance of graph rationales. Based on analytic investigations, a novel variational inference based method named ``Probability Dependency on Environments and Rationales for OOD Graphs on Real-world Data'' (DEROG) is introduced. To alleviate the adverse effect of unknown prior knowledge on environments and rationales, DEROG utilizes generalized Bayesian inference. Further, DEROG employs an EM-based algorithm for optimization. Finally, extensive experiments on real-world datasets under different distribution shifts are conducted to show the superiority of DEROG. Our code is publicly available at https://github.com/LEOXC1571/DEROG.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2407.05736.pdf' target='_blank'>https://arxiv.org/pdf/2407.05736.pdf</a></span>   <span><a href='https://github.com/wklix/TransMA/tree/master' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Wu, Zixu Wang, Xiulong Yang, Yangyang Chen, Zhenqi Han, Jialu Zhang, Lizhuang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05736">TransMA: an explainable multi-modal deep learning model for predicting properties of ionizable lipid nanoparticles in mRNA delivery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the primary mRNA delivery vehicles, ionizable lipid nanoparticles (LNPs) exhibit excellent safety, high transfection efficiency, and strong immune response induction. However, the screening process for LNPs is time-consuming and costly. To expedite the identification of high-transfection-efficiency mRNA drug delivery systems, we propose an explainable LNPs transfection efficiency prediction model, called TransMA. TransMA employs a multi-modal molecular structure fusion architecture, wherein the fine-grained atomic spatial relationship extractor named molecule 3D Transformer captures three-dimensional spatial features of the molecule, and the coarse-grained atomic sequence extractor named molecule Mamba captures one-dimensional molecular features. We design the mol-attention mechanism block, enabling it to align coarse and fine-grained atomic features and captures relationships between atomic spatial and sequential structures. TransMA achieves state-of-the-art performance in predicting transfection efficiency using the scaffold and cliff data splitting methods on the current largest LNPs dataset, including Hela and RAW cell lines. Moreover, we find that TransMA captures the relationship between subtle structural changes and significant transfection efficiency variations, providing valuable insights for LNPs design. Additionally, TransMA's predictions on external transfection efficiency data maintain a consistent order with actual transfection efficiencies, demonstrating its robust generalization capability. The code, model and data are made publicly available at https://github.com/wklix/TransMA/tree/master. We hope that high-accuracy transfection prediction models in the future can aid in LNPs design and initial screening, thereby assisting in accelerating the mRNA design process.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2407.05576.pdf' target='_blank'>https://arxiv.org/pdf/2407.05576.pdf</a></span>   <span><a href='https://github.com/yuggiehk/CaRe-Ego/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuejiao Su, Yi Wang, Lap-Pui Chau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05576">CaRe-Ego: Contact-aware Relationship Modeling for Egocentric Interactive Hand-object Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric Interactive hand-object segmentation (EgoIHOS) requires the segmentation of hands and interacting objects in egocentric images, which is crucial for understanding human behavior in assistive systems. Previous methods typically recognize hands and interacting objects as distinct semantic categories based solely on visual features, or simply use hand predictions as auxiliary cues for object segmentation. Despite the promising progress achieved by these methods, they fail to adequately model the interactive relationships between hands and objects while ignoring the coupled physical relationships among object categories, ultimately constraining their segmentation performance. To make up for the shortcomings of existing methods, we propose a novel method called CaRe-Ego that achieves state-of-the-art performance by emphasizing the contact between hands and objects from two aspects. First, we introduce a Hand-guided Object Feature Enhancer (HOFE) to establish the hand-object interactive relationships to extract more contact-relevant and discriminative object features. Second, we design the Contact-centric Object Decoupling Strategy (CODS) to explicitly model and disentangle coupling relationships among object categories, thereby emphasizing contact-aware feature learning. Experiments on various in-domain and out-of-domain test sets show that Care-Ego significantly outperforms existing methods with robust generalization capability. Codes are publicly available at https://github.com/yuggiehk/CaRe-Ego/.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2407.03588.pdf' target='_blank'>https://arxiv.org/pdf/2407.03588.pdf</a></span>   <span><a href='https://github.com/Mehrdad-Noori/FDS.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo Adolfo Vargas Hakim, David Osowiechi, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03588">FDS: Feedback-guided Domain Synthesis with Multi-Source Conditional Diffusion Models for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization techniques aim to enhance model robustness by simulating novel data distributions during training, typically through various augmentation or stylization strategies. However, these methods frequently suffer from limited control over the diversity of generated images and lack assurance that these images span distinct distributions. To address these challenges, we propose FDS, Feedback-guided Domain Synthesis, a novel strategy that employs diffusion models to synthesize novel, pseudo-domains by training a single model on all source domains and performing domain mixing based on learned features. By incorporating images that pose classification challenges to models trained on original samples, alongside the original dataset, we ensure the generation of a training set that spans a broad distribution spectrum. Our comprehensive evaluations demonstrate that this methodology sets new benchmarks in domain generalization performance across a range of challenging datasets, effectively managing diverse types of domain shifts. The code can be found at: \url{https://github.com/Mehrdad-Noori/FDS.git}.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2407.03056.pdf' target='_blank'>https://arxiv.org/pdf/2407.03056.pdf</a></span>   <span><a href='https://github.com/miccunifi/KDPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Mistretta, Alberto Baldrati, Marco Bertini, Andrew D. Bagdanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03056">Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a parameter-efficient method for adapting VLMs, but state-of-the-art approaches require annotated samples. In this paper we propose a novel approach to prompt learning based on unsupervised knowledge distillation from more powerful models. Our approach, which we call Knowledge Distillation Prompt Learning (KDPL), can be integrated into existing prompt learning techniques and eliminates the need for labeled examples during adaptation. Our experiments on more than ten standard benchmark datasets demonstrate that KDPL is very effective at improving generalization of learned prompts for zero-shot domain generalization, zero-shot cross-dataset generalization, and zero-shot base-to-novel class generalization problems. KDPL requires no ground-truth labels for adaptation, and moreover we show that even in the absence of any knowledge of training class names it can be used to effectively transfer knowledge. The code is publicly available at https://github.com/miccunifi/KDPL.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2407.02900.pdf' target='_blank'>https://arxiv.org/pdf/2407.02900.pdf</a></span>   <span><a href='https://github.com/sdoerrich97/vits-are-generative-models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Doerrich, Francesco Di Salvo, Christian Ledig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02900">Self-supervised Vision Transformer are Scalable Generative Models for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite notable advancements, the integration of deep learning (DL) techniques into impactful clinical applications, particularly in the realm of digital histopathology, has been hindered by challenges associated with achieving robust generalization across diverse imaging domains and characteristics. Traditional mitigation strategies in this field such as data augmentation and stain color normalization have proven insufficient in addressing this limitation, necessitating the exploration of alternative methodologies. To this end, we propose a novel generative method for domain generalization in histopathology images. Our method employs a generative, self-supervised Vision Transformer to dynamically extract characteristics of image patches and seamlessly infuse them into the original images, thereby creating novel, synthetic images with diverse attributes. By enriching the dataset with such synthesized images, we aim to enhance its holistic nature, facilitating improved generalization of DL models to unseen domains. Extensive experiments conducted on two distinct histopathology datasets demonstrate the effectiveness of our proposed approach, outperforming the state of the art substantially, on the Camelyon17-wilds challenge dataset (+2%) and on a second epithelium-stroma dataset (+26%). Furthermore, we emphasize our method's ability to readily scale with increasingly available unlabeled data samples and more complex, higher parametric architectures. Source code is available at https://github.com/sdoerrich97/vits-are-generative-models .
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2407.02893.pdf' target='_blank'>https://arxiv.org/pdf/2407.02893.pdf</a></span>   <span><a href='https://github.com/HiLab-git/UGTST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Luo, Xiangde Luo, Zijun Gao, Guotai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02893">An Uncertainty-guided Tiered Self-training Framework for Active Source-free Domain Adaptation in Prostate Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have exhibited remarkable efficacy in accurately delineating the prostate for diagnosis and treatment of prostate diseases, but challenges persist in achieving robust generalization across different medical centers. Source-free Domain Adaptation (SFDA) is a promising technique to adapt deep segmentation models to address privacy and security concerns while reducing domain shifts between source and target domains. However, recent literature indicates that the performance of SFDA remains far from satisfactory due to unpredictable domain gaps. Annotating a few target domain samples is acceptable, as it can lead to significant performance improvement with a low annotation cost. Nevertheless, due to extremely limited annotation budgets, careful consideration is needed in selecting samples for annotation. Inspired by this, our goal is to develop Active Source-free Domain Adaptation (ASFDA) for medical image segmentation. Specifically, we propose a novel Uncertainty-guided Tiered Self-training (UGTST) framework, consisting of efficient active sample selection via entropy-based primary local peak filtering to aggregate global uncertainty and diversity-aware redundancy filter, coupled with a tiered self-learning strategy, achieves stable domain adaptation. Experimental results on cross-center prostate MRI segmentation datasets revealed that our method yielded marked advancements, with a mere 5% annotation, exhibiting an average Dice score enhancement of 9.78% and 7.58% in two target domains compared with state-of-the-art methods, on par with fully supervised learning. Code is available at:https://github.com/HiLab-git/UGTST
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2407.01749.pdf' target='_blank'>https://arxiv.org/pdf/2407.01749.pdf</a></span>   <span><a href='https://github.com/Alexkael/ICorr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaojie Jin, Ronghui Mu, Xinping Yi, Xiaowei Huang, Lijun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01749">Invariant Correlation of Representation with Label: Enhancing Domain Generalization in Noisy Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Invariant Risk Minimization (IRM) approach aims to address the challenge of domain generalization by training a feature representation that remains invariant across multiple environments. However, in noisy environments, IRM-related techniques such as IRMv1 and VREx may be unable to achieve the optimal IRM solution, primarily due to erroneous optimization directions. To address this issue, we introduce ICorr (an abbreviation for Invariant Correlation), a novel approach designed to surmount the above challenge in noisy settings. Additionally, we dig into a case study to analyze why previous methods may lose ground while ICorr can succeed. Through a theoretical lens, particularly from a causality perspective, we illustrate that the invariant correlation of representation with label is a necessary condition for the optimal invariant predictor in noisy environments, whereas the optimization motivations for other methods may not be. Furthermore, we empirically demonstrate the effectiveness of ICorr by comparing it with other domain generalization methods on various noisy datasets. The code is available at https://github.com/Alexkael/ICorr.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2407.01518.pdf' target='_blank'>https://arxiv.org/pdf/2407.01518.pdf</a></span>   <span><a href='https://github.com/donghao51/MOOSA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/donghao51/MOOSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Dong, Eleni Chatzi, Olga Fink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01518">Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of open-set domain generalization (OSDG) involves recognizing novel classes within unseen domains, which becomes more challenging with multiple modalities as input. Existing works have only addressed unimodal OSDG within the meta-learning framework, without considering multimodal scenarios. In this work, we introduce a novel approach to address Multimodal Open-Set Domain Generalization (MM-OSDG) for the first time, utilizing self-supervision. To this end, we introduce two innovative multimodal self-supervised pretext tasks: Masked Cross-modal Translation and Multimodal Jigsaw Puzzles. These tasks facilitate the learning of multimodal representative features, thereby enhancing generalization and open-class detection capabilities. Additionally, we propose a novel entropy weighting mechanism to balance the loss across different modalities. Furthermore, we extend our approach to tackle also the Multimodal Open-Set Domain Adaptation (MM-OSDA) problem, especially in scenarios where unlabeled data from the target domain is available. Extensive experiments conducted under MM-OSDG, MM-OSDA, and Multimodal Closed-Set DG settings on the EPIC-Kitchens and HAC datasets demonstrate the efficacy and versatility of the proposed approach. Our source code is available at https://github.com/donghao51/MOOSA.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2407.01400.pdf' target='_blank'>https://arxiv.org/pdf/2407.01400.pdf</a></span>   <span><a href='https://github.com/MarcLafon/gallop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Lafon, Elias Ramzi, ClÃ©ment Rambour, Nicolas Audebert, Nicolas Thome
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01400">GalLoP: Learning Global and Local Prompts for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has been widely adopted to efficiently adapt vision-language models (VLMs), e.g. CLIP, for few-shot image classification. Despite their success, most prompt learning methods trade-off between classification accuracy and robustness, e.g. in domain generalization or out-of-distribution (OOD) detection. In this work, we introduce Global-Local Prompts (GalLoP), a new prompt learning method that learns multiple diverse prompts leveraging both global and local visual features. The training of the local prompts relies on local features with an enhanced vision-text alignment. To focus only on pertinent features, this local alignment is coupled with a sparsity strategy in the selection of the local features. We enforce diversity on the set of prompts using a new ``prompt dropout'' technique and a multiscale strategy on the local prompts. GalLoP outperforms previous prompt learning methods on accuracy on eleven datasets in different few shots settings and with various backbones. Furthermore, GalLoP shows strong robustness performances in both domain generalization and OOD detection, even outperforming dedicated OOD detection methods. Code and instructions to reproduce our results: https://github.com/MarcLafon/gallop.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2407.00718.pdf' target='_blank'>https://arxiv.org/pdf/2407.00718.pdf</a></span>   <span><a href='https://github.com/HuiqianLi/ASPS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiqian Li, Dingwen Zhang, Jieru Yao, Longfei Han, Zhongyu Li, Junwei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00718">ASPS: Augmented Segment Anything Model for Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Polyp segmentation plays a pivotal role in colorectal cancer diagnosis. Recently, the emergence of the Segment Anything Model (SAM) has introduced unprecedented potential for polyp segmentation, leveraging its powerful pre-training capability on large-scale datasets. However, due to the domain gap between natural and endoscopy images, SAM encounters two limitations in achieving effective performance in polyp segmentation. Firstly, its Transformer-based structure prioritizes global and low-frequency information, potentially overlooking local details, and introducing bias into the learned features. Secondly, when applied to endoscopy images, its poor out-of-distribution (OOD) performance results in substandard predictions and biased confidence output. To tackle these challenges, we introduce a novel approach named Augmented SAM for Polyp Segmentation (ASPS), equipped with two modules: Cross-branch Feature Augmentation (CFA) and Uncertainty-guided Prediction Regularization (UPR). CFA integrates a trainable CNN encoder branch with a frozen ViT encoder, enabling the integration of domain-specific knowledge while enhancing local features and high-frequency details. Moreover, UPR ingeniously leverages SAM's IoU score to mitigate uncertainty during the training procedure, thereby improving OOD performance and domain generalization. Extensive experimental results demonstrate the effectiveness and utility of the proposed method in improving SAM's performance in polyp segmentation. Our code is available at https://github.com/HuiqianLi/ASPS.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2406.18037.pdf' target='_blank'>https://arxiv.org/pdf/2406.18037.pdf</a></span>   <span><a href='https://github.com/dyxu-cuhkcse/SMG-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dunyuan Xu, Xi Wang, Jingyang Zhang, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18037">Towards Synchronous Memorizability and Generalizability with Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to learn sequentially from different data sites is crucial for a deep network in solving practical medical image diagnosis problems due to privacy restrictions and storage limitations. However, adapting on incoming site leads to catastrophic forgetting on past sites and decreases generalizablity on unseen sites. Existing Continual Learning (CL) and Domain Generalization (DG) methods have been proposed to solve these two challenges respectively, but none of them can address both simultaneously. Recognizing this limitation, this paper proposes a novel training paradigm, learning towards Synchronous Memorizability and Generalizability (SMG-Learning). To achieve this, we create the orientational gradient alignment to ensure memorizability on previous sites, and arbitrary gradient alignment to enhance generalizability on unseen sites. This approach is named as Parallel Gradient Alignment (PGA). Furthermore, we approximate the PGA as dual meta-objectives using the first-order Taylor expansion to reduce computational cost of aligning gradients. Considering that performing gradient alignments, especially for previous sites, is not feasible due to the privacy constraints, we design a Site-Modulated Diffusion (SMD) model to generate images with site-specific learnable prompts, replaying images have similar data distributions as previous sites. We evaluate our method on two medical image segmentation tasks, where data from different sites arrive sequentially. Experimental results show that our method efficiently enhances both memorizability and generalizablity better than other state-of-the-art methods, delivering satisfactory performance across all sites. Our code will be available at: https://github.com/dyxu-cuhkcse/SMG-Learning.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2406.17536.pdf' target='_blank'>https://arxiv.org/pdf/2406.17536.pdf</a></span>   <span><a href='https://github.com/francescodisalvo05/medmnistc-api' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Di Salvo, Sebastian Doerrich, Christian Ledig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17536">MedMNIST-C: Comprehensive benchmark and improved classifier robustness by simulating realistic image corruptions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of neural-network-based systems into clinical practice is limited by challenges related to domain generalization and robustness. The computer vision community established benchmarks such as ImageNet-C as a fundamental prerequisite to measure progress towards those challenges. Similar datasets are largely absent in the medical imaging community which lacks a comprehensive benchmark that spans across imaging modalities and applications. To address this gap, we create and open-source MedMNIST-C, a benchmark dataset based on the MedMNIST+ collection covering 12 datasets and 9 imaging modalities. We simulate task and modality-specific image corruptions of varying severity to comprehensively evaluate the robustness of established algorithms against real-world artifacts and distribution shifts. We further provide quantitative evidence that our simple-to-use artificial corruptions allow for highly performant, lightweight data augmentation to enhance model robustness. Unlike traditional, generic augmentation strategies, our approach leverages domain knowledge, exhibiting significantly higher robustness when compared to widely adopted methods. By introducing MedMNIST-C and open-sourcing the corresponding library allowing for targeted data augmentations, we contribute to the development of increasingly robust methods tailored to the challenges of medical imaging. The code is available at https://github.com/francescodisalvo05/medmnistc-api .
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2406.15685.pdf' target='_blank'>https://arxiv.org/pdf/2406.15685.pdf</a></span>   <span><a href='https://github.com/ParastooSotoudeh/PathoWAve' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Parastoo Sotoudeh Sharifi, M. Omair Ahmad, M. N. S. Swamy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15685">PathoWAve: A Deep Learning-based Weight Averaging Method for Improving Domain Generalization in Histopathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in deep learning (DL) have significantly advanced medical image analysis. In the field of medical image processing, particularly in histopathology image analysis, the variation in staining protocols and differences in scanners present significant domain shift challenges, undermine the generalization capabilities of models to the data from unseen domains, prompting the need for effective domain generalization (DG) strategies to improve the consistency and reliability of automated cancer detection tools in diagnostic decision-making. In this paper, we introduce Pathology Weight Averaging (PathoWAve), a multi-source DG strategy for addressing domain shift phenomenon of DL models in histopathology image analysis. Integrating specific weight averaging technique with parallel training trajectories and a strategically combination of regular augmentations with histopathology-specific data augmentation methods, PathoWAve enables a comprehensive exploration and precise convergence within the loss landscape. This method significantly enhanced generalization capabilities of DL models across new, unseen histopathology domains. To the best of our knowledge, PathoWAve is the first proposed weight averaging method for DG in histopathology image analysis. Our quantitative results on Camelyon17 WILDS dataset demonstrate PathoWAve's superiority over previous proposed methods to tackle the domain shift phenomenon in histopathology image processing. Our code is available at \url{https://github.com/ParastooSotoudeh/PathoWAve}.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2406.15669.pdf' target='_blank'>https://arxiv.org/pdf/2406.15669.pdf</a></span>   <span><a href='https://github.com/jsunn-y/CARE/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Yang, Ariane Mora, Shengchao Liu, Bruce J. Wittmann, Anima Anandkumar, Frances H. Arnold, Yisong Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15669">CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzymes are important proteins that catalyze chemical reactions. In recent years, machine learning methods have emerged to predict enzyme function from sequence; however, there are no standardized benchmarks to evaluate these methods. We introduce CARE, a benchmark and dataset suite for the Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1) classification of a protein sequence by its enzyme commission (EC) number and (2) retrieval of an EC number given a chemical reaction. For each task, we design train-test splits to evaluate different kinds of out-of-distribution generalization that are relevant to real use cases. For the classification task, we provide baselines for state-of-the-art methods. Because the retrieval task has not been previously formalized, we propose a method called Contrastive Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task and compare it to the recent method, CLIPZyme. CARE is available at https://github.com/jsunn-y/CARE/.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2406.12452.pdf' target='_blank'>https://arxiv.org/pdf/2406.12452.pdf</a></span>   <span><a href='https://github.com/RolnickLab/ami-dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Jain, Fagner Cunha, Michael James Bunsen, Juan SebastiÃ¡n CaÃ±as, LÃ©onard Pasi, Nathan Pinoy, Flemming Helsing, JoAnne Russo, Marc Botham, Michael Sabourin, Jonathan FrÃ©chette, Alexandre Anctil, Yacksecari Lopez, Eduardo Navarro, Filonila Perez Pimentel, Ana Cecilia Zamora, JosÃ© Alejandro Ramirez Silva, Jonathan Gagnon, Tom August, Kim Bjerge, Alba Gomez Segura, Marc BÃ©lisle, Yves Basset, Kent P. McFarland, David Roy, Toke Thomas HÃ¸ye, Maxim LarrivÃ©e, David Rolnick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12452">Insect Identification in the Wild: The AMI Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Insects represent half of all global biodiversity, yet many of the world's insects are disappearing, with severe implications for ecosystems and agriculture. Despite this crisis, data on insect diversity and abundance remain woefully inadequate, due to the scarcity of human experts and the lack of scalable tools for monitoring. Ecologists have started to adopt camera traps to record and study insects, and have proposed computer vision algorithms as an answer for scalable data processing. However, insect monitoring in the wild poses unique challenges that have not yet been addressed within computer vision, including the combination of long-tailed data, extremely similar classes, and significant distribution shifts. We provide the first large-scale machine learning benchmarks for fine-grained insect recognition, designed to match real-world tasks faced by ecologists. Our contributions include a curated dataset of images from citizen science platforms and museums, and an expert-annotated dataset drawn from automated camera traps across multiple continents, designed to test out-of-distribution generalization under field conditions. We train and evaluate a variety of baseline algorithms and introduce a combination of data augmentation techniques that enhance generalization across geographies and hardware setups.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2406.04963.pdf' target='_blank'>https://arxiv.org/pdf/2406.04963.pdf</a></span>   <span><a href='https://github.com/fannie1208/GLIND' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qitian Wu, Fan Nie, Chenxiao Yang, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04963">Learning Divergence Fields for Shift-Robust Graph Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world data generation often involves certain geometries (e.g., graphs) that induce instance-level interdependence. This characteristic makes the generalization of learning models more difficult due to the intricate interdependent patterns that impact data-generative distributions and can vary from training to testing. In this work, we propose a geometric diffusion model with learnable divergence fields for the challenging generalization problem with interdependent data. We generalize the diffusion equation with stochastic diffusivity at each time step, which aims to capture the multi-faceted information flows among interdependent data. Furthermore, we derive a new learning objective through causal inference, which can guide the model to learn generalizable patterns of interdependence that are insensitive across domains. Regarding practical implementation, we introduce three model instantiations that can be considered as the generalized versions of GCN, GAT, and Transformers, respectively, which possess advanced robustness against distribution shifts. We demonstrate their promising efficacy for out-of-distribution generalization on diverse real-world datasets.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2406.04221.pdf' target='_blank'>https://arxiv.org/pdf/2406.04221.pdf</a></span>   <span><a href='https://github.com/siyuanliii/masa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04221">Matching Anything by Segmenting Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT). Current methods predominantly rely on labeled domain-specific video datasets, which limits the cross-domain generalization of learned similarity embeddings. We propose MASA, a novel method for robust instance association learning, capable of matching any objects within videos across diverse domains without tracking labels. Leveraging the rich object segmentation from the Segment Anything Model (SAM), MASA learns instance-level correspondence through exhaustive data transformations. We treat the SAM outputs as dense object region proposals and learn to match those regions from a vast image collection. We further design a universal MASA adapter which can work in tandem with foundational segmentation or detection models and enable them to track any detected objects. Those combinations present strong zero-shot tracking ability in complex domains. Extensive tests on multiple challenging MOT and MOTS benchmarks indicate that the proposed method, using only unlabeled static images, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain video sequences, in zero-shot association. Project Page: https://matchinganything.github.io/
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2406.00777.pdf' target='_blank'>https://arxiv.org/pdf/2406.00777.pdf</a></span>   <span><a href='https://github.com/Yux1angJi/DIFF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Ji, Boyong He, Chenyuan Qu, Zhuoyue Tan, Chuan Qin, Liaoni Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00777">Diffusion Features to Bridge Domain Gap for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained diffusion models have demonstrated remarkable proficiency in synthesizing images across a wide range of scenarios with customizable prompts, indicating their effective capacity to capture universal features. Motivated by this, our study delves into the utilization of the implicit knowledge embedded within diffusion models to address challenges in cross-domain semantic segmentation. This paper investigates the approach that leverages the sampling and fusion techniques to harness the features of diffusion models efficiently. We propose DIffusion Feature Fusion (DIFF) as a backbone use for extracting and integrating effective semantic representations through the diffusion process. By leveraging the strength of text-to-image generation capability, we introduce a new training framework designed to implicitly learn posterior knowledge from it. Through rigorous evaluation in the contexts of domain generalization semantic segmentation, we establish that our methodology surpasses preceding approaches in mitigating discrepancies across distinct domains and attains the state-of-the-art (SOTA) benchmark.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2406.00429.pdf' target='_blank'>https://arxiv.org/pdf/2406.00429.pdf</a></span>   <span><a href='https://github.com/qinzheng2000/GeneralTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Le Wang, Sanping Zhou, Panpan Fu, Gang Hua, Wei Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00429">Towards Generalizable Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking MOT encompasses various tracking scenarios, each characterized by unique traits. Effective trackers should demonstrate a high degree of generalizability across diverse scenarios. However, existing trackers struggle to accommodate all aspects or necessitate hypothesis and experimentation to customize the association information motion and or appearance for a given scenario, leading to narrowly tailored solutions with limited generalizability. In this paper, we investigate the factors that influence trackers generalization to different scenarios and concretize them into a set of tracking scenario attributes to guide the design of more generalizable trackers. Furthermore, we propose a point-wise to instance-wise relation framework for MOT, i.e., GeneralTrack, which can generalize across diverse scenarios while eliminating the need to balance motion and appearance. Thanks to its superior generalizability, our proposed GeneralTrack achieves state-of-the-art performance on multiple benchmarks and demonstrates the potential for domain generalization. https://github.com/qinzheng2000/GeneralTrack.git
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2406.00275.pdf' target='_blank'>https://arxiv.org/pdf/2406.00275.pdf</a></span>   <span><a href='https://github.com/Huage001/StyDeSty' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songhua Liu, Xin Jin, Xingyi Yang, Jingwen Ye, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00275">StyDeSty: Min-Max Stylization and Destylization for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization (single DG) aims at learning a robust model generalizable to unseen domains from only one training domain, making it a highly ambitious and challenging task. State-of-the-art approaches have mostly relied on data augmentations, such as adversarial perturbation and style enhancement, to synthesize new data and thus increase robustness. Nevertheless, they have largely overlooked the underlying coherence between the augmented domains, which in turn leads to inferior results in real-world scenarios. In this paper, we propose a simple yet effective scheme, termed as \emph{StyDeSty}, to explicitly account for the alignment of the source and pseudo domains in the process of data augmentation, enabling them to interact with each other in a self-consistent manner and further giving rise to a latent domain with strong generalization power. The heart of StyDeSty lies in the interaction between a \emph{stylization} module for generating novel stylized samples using the source domain, and a \emph{destylization} module for transferring stylized and source samples to a latent domain to learn content-invariant features. The stylization and destylization modules work adversarially and reinforce each other. During inference, the destylization module transforms the input sample with an arbitrary style shift to the latent domain, in which the downstream tasks are carried out. Specifically, the location of the destylization layer within the backbone network is determined by a dedicated neural architecture search (NAS) strategy. We evaluate StyDeSty on multiple benchmarks and demonstrate that it yields encouraging results, outperforming the state of the art by up to {13.44%} on classification accuracy. Codes are available here: https://github.com/Huage001/StyDeSty.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2405.18861.pdf' target='_blank'>https://arxiv.org/pdf/2405.18861.pdf</a></span>   <span><a href='https://github.com/MediaBrain-SJTU/DISAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruipeng Zhang, Ziqing Fan, Jiangchao Yao, Ya Zhang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18861">Domain-Inspired Sharpness-Aware Minimization Under Domain Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a Domain-Inspired Sharpness-Aware Minimization (DISAM) algorithm for optimization under domain shifts. It is motivated by the inconsistent convergence degree of SAM across different domains, which induces optimization bias towards certain domains and thus impairs the overall convergence. To address this issue, we consider the domain-level convergence consistency in the sharpness estimation to prevent the overwhelming (deficient) perturbations for less (well) optimized domains. Specifically, DISAM introduces the constraint of minimizing variance in the domain loss, which allows the elastic gradient calibration in perturbation generation: when one domain is optimized above the averaging level \textit{w.r.t.} loss, the gradient perturbation towards that domain will be weakened automatically, and vice versa. Under this mechanism, we theoretically show that DISAM can achieve faster overall convergence and improved generalization in principle when inconsistent convergence emerges. Extensive experiments on various domain generalization benchmarks show the superiority of DISAM over a range of state-of-the-art methods. Furthermore, we show the superior efficiency of DISAM in parameter-efficient fine-tuning combined with the pretraining models. The source code is released at https://github.com/MediaBrain-SJTU/DISAM.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2405.16591.pdf' target='_blank'>https://arxiv.org/pdf/2405.16591.pdf</a></span>   <span><a href='https://github.com/WLuLi/CapS-Adapter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijie Wang, Guandu Liu, Bin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16591">CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language foundational models, such as CLIP, have demonstrated significant strides in zero-shot classification. However, the extensive parameterization of models like CLIP necessitates a resource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have introduced training-free methods aimed at bolstering the efficacy of downstream tasks. While these approaches incorporate support sets to maintain data distribution consistency between knowledge cache and test sets, they often fall short in terms of generalization on the test set, particularly when faced with test data exhibiting substantial distributional variations. In this work, we present CapS-Adapter, an innovative method that employs a caption-based support set, effectively harnessing both image and caption features to exceed existing state-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly constructs support sets that closely mirror target distributions, utilizing instance-level distribution features extracted from multimodal large models. By leveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances predictive accuracy through the use of multimodal support sets. Our method achieves outstanding zero-shot classification results across 19 benchmark datasets, improving accuracy by 2.19\% over the previous leading method. Our contributions are substantiated through extensive validation on multiple benchmark datasets, demonstrating superior performance and robust generalization capabilities. Our code is made publicly available at https://github.com/WLuLi/CapS-Adapter.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2405.16304.pdf' target='_blank'>https://arxiv.org/pdf/2405.16304.pdf</a></span>   <span><a href='https://github.com/MahdiyarMM/FedGaLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhad Pourpanah, Mahdiyar Molahasani, Milad Soltany, Michael Greenspan, Ali Etemad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16304">Federated Unsupervised Domain Generalization using Global and Local Alignment of Gradients</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of federated domain generalization in an unsupervised setting for the first time. We first theoretically establish a connection between domain shift and alignment of gradients in unsupervised federated learning and show that aligning the gradients at both client and server levels can facilitate the generalization of the model to new (target) domains. Building on this insight, we propose a novel method named FedGaLA, which performs gradient alignment at the client level to encourage clients to learn domain-invariant features, as well as global gradient alignment at the server to obtain a more generalized aggregated model. To empirically evaluate our method, we perform various experiments on four commonly used multi-domain datasets, PACS, OfficeHome, DomainNet, and TerraInc. The results demonstrate the effectiveness of our method which outperforms comparable baselines. Ablation and sensitivity studies demonstrate the impact of different components and parameters in our approach. The source code is available at: https://github.com/MahdiyarMM/FedGaLA.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2405.16075.pdf' target='_blank'>https://arxiv.org/pdf/2405.16075.pdf</a></span>   <span><a href='https://github.com/Zekun-Cai/Koodos' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Cai, Guangji Bai, Renhe Jiang, Xuan Song, Liang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16075">Continuous Temporal Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal Domain Generalization (TDG) addresses the challenge of training predictive models under temporally varying data distributions. Traditional TDG approaches typically focus on domain data collected at fixed, discrete time intervals, which limits their capability to capture the inherent dynamics within continuous-evolving and irregularly-observed temporal domains. To overcome this, this work formalizes the concept of Continuous Temporal Domain Generalization (CTDG), where domain data are derived from continuous times and are collected at arbitrary times. CTDG tackles critical challenges including: 1) Characterizing the continuous dynamics of both data and models, 2) Learning complex high-dimensional nonlinear dynamics, and 3) Optimizing and controlling the generalization across continuous temporal domains. To address them, we propose a Koopman operator-driven continuous temporal domain generalization (Koodos) framework. We formulate the problem within a continuous dynamic system and leverage the Koopman theory to learn the underlying dynamics; the framework is further enhanced with a comprehensive optimization strategy equipped with analysis and control driven by prior knowledge of the dynamics patterns. Extensive experiments demonstrate the effectiveness and efficiency of our approach. The code can be found at: https://github.com/Zekun-Cai/Koodos.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2405.15961.pdf' target='_blank'>https://arxiv.org/pdf/2405.15961.pdf</a></span>   <span><a href='https://github.com/fpsluozi/SMD-SMOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiran Luo, Joshua Feinglass, Tejas Gokhale, Kuan-Cheng Lee, Chitta Baral, Yezhou Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15961">Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measures and Synthetic Scene Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) is a challenging task in machine learning that requires a coherent ability to comprehend shifts across various domains through extraction of domain-invariant features. DG performance is typically evaluated by performing image classification in domains of various image styles. However, current methodology lacks quantitative understanding about shifts in stylistic domain, and relies on a vast amount of pre-training data, such as ImageNet1K, which are predominantly in photo-realistic style with weakly supervised class labels. Such a data-driven practice could potentially result in spurious correlation and inflated performance on DG benchmarks. In this paper, we introduce a new DG paradigm to address these risks. We first introduce two new quantitative measures ICV and IDD to describe domain shifts in terms of consistency of classes within one domain and similarity between two stylistic domains. We then present SuperMarioDomains (SMD), a novel synthetic multi-domain dataset sampled from video game scenes with more consistent classes and sufficient dissimilarity compared to ImageNet1K. We demonstrate our DG method SMOS. SMOS first uses SMD to train a precursor model, which is then used to ground the training on a DG benchmark. We observe that SMOS contributes to state-of-the-art performance across five DG benchmarks, gaining large improvements to performances on abstract domains along with on-par or slight improvements to those on photo-realistic domains. Our qualitative analysis suggests that these improvements can be attributed to reduced distributional divergence between originally distant domains. Our data are available at https://github.com/fpsluozi/SMD-SMOS .
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2405.15317.pdf' target='_blank'>https://arxiv.org/pdf/2405.15317.pdf</a></span>   <span><a href='https://github.com/Chengyui/NuwaTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinguo Cheng, Chunwei Yang, Wanlin Cai, Yuxuan Liang, Qingsong Wen, Yuankai Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15317">NuwaTS: a Foundation Model Mending Every Incomplete Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series imputation is critical for many real-world applications and has been widely studied. However, existing models often require specialized designs tailored to specific missing patterns, variables, or domains which limits their generalizability. In addition, current evaluation frameworks primarily focus on domain-specific tasks and often rely on time-wise train/validation/test data splits, which fail to rigorously assess a model's ability to generalize across unseen variables or domains. In this paper, we present \textbf{NuwaTS}, a novel framework that repurposes Pre-trained Language Models (PLMs) for general time series imputation. Once trained, NuwaTS can be applied to impute missing data across any domain. We introduce specialized embeddings for each sub-series patch, capturing information about the patch, its missing data patterns, and its statistical characteristics. By combining contrastive learning with the imputation task, we train PLMs to create a versatile, one-for-all imputation model. Additionally, we employ a plug-and-play fine-tuning approach, enabling efficient adaptation to domain-specific tasks with minimal adjustments. To evaluate cross-variable and cross-domain generalization, we propose a new benchmarking protocol that partitions the datasets along the variable dimension. Experimental results on over seventeen million time series samples from diverse domains demonstrate that NuwaTS outperforms state-of-the-art domain-specific models across various datasets under the proposed benchmarking protocol. Furthermore, we show that NuwaTS generalizes to other time series tasks, such as forecasting. Our codes are available at https://github.com/Chengyui/NuwaTS.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2405.14497.pdf' target='_blank'>https://arxiv.org/pdf/2405.14497.pdf</a></span>   <span><a href='https://github.com/msohaildanish/DivAlign' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Sohail Danish, Muhammad Haris Khan, Muhammad Akhtar Munir, M. Saquib Sarfraz, Mohsen Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14497">Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we tackle the problem of domain generalization for object detection, specifically focusing on the scenario where only a single source domain is available. We propose an effective approach that involves two key steps: diversifying the source domain and aligning detections based on class prediction confidence and localization. Firstly, we demonstrate that by carefully selecting a set of augmentations, a base detector can outperform existing methods for single domain generalization by a good margin. This highlights the importance of domain diversification in improving the performance of object detectors. Secondly, we introduce a method to align detections from multiple views, considering both classification and localization outputs. This alignment procedure leads to better generalized and well-calibrated object detector models, which are crucial for accurate decision-making in safety-critical applications. Our approach is detector-agnostic and can be seamlessly applied to both single-stage and two-stage detectors. To validate the effectiveness of our proposed methods, we conduct extensive experiments and ablations on challenging domain-shift scenarios. The results consistently demonstrate the superiority of our approach compared to existing methods. Our code and models are available at: https://github.com/msohaildanish/DivAlign
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2405.09131.pdf' target='_blank'>https://arxiv.org/pdf/2405.09131.pdf</a></span>   <span><a href='https://github.com/ToughStoneX/MVS_Evaluation_Benchmark' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ToughStoneX/Robust-MVS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbin Xu, Weitao Chen, Baigui Sun, Xuansong Xie, Wenxiong Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09131">RobustMVS: Single Domain Generalized Deep Multi-view Stereo</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the impressive performance of Multi-view Stereo (MVS) approaches given plenty of training samples, the performance degradation when generalizing to unseen domains has not been clearly explored yet. In this work, we focus on the domain generalization problem in MVS. To evaluate the generalization results, we build a novel MVS domain generalization benchmark including synthetic and real-world datasets. In contrast to conventional domain generalization benchmarks, we consider a more realistic but challenging scenario, where only one source domain is available for training. The MVS problem can be analogized back to the feature matching task, and maintaining robust feature consistency among views is an important factor for improving generalization performance. To address the domain generalization problem in MVS, we propose a novel MVS framework, namely RobustMVS. A DepthClustering-guided Whitening (DCW) loss is further introduced to preserve the feature consistency among different views, which decorrelates multi-view features from viewpoint-specific style information based on geometric priors from depth maps. The experimental results further show that our method achieves superior performance on the domain generalization benchmark.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2405.08586.pdf' target='_blank'>https://arxiv.org/pdf/2405.08586.pdf</a></span>   <span><a href='https://github.com/NancyQuris/XDomainMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingnan Liu, Yingtian Zou, Rui Qiao, Fusheng Liu, Mong Li Lee, Wynne Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08586">Cross-Domain Feature Augmentation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to develop models that are robust to distribution shifts. Existing methods focus on learning invariance across domains to enhance model robustness, and data augmentation has been widely used to learn invariant predictors, with most methods performing augmentation in the input space. However, augmentation in the input space has limited diversity whereas in the feature space is more versatile and has shown promising results. Nonetheless, feature semantics is seldom considered and existing feature augmentation methods suffer from a limited variety of augmented features. We decompose features into class-generic, class-specific, domain-generic, and domain-specific components. We propose a cross-domain feature augmentation method named XDomainMix that enables us to increase sample diversity while emphasizing the learning of invariant representations to achieve domain generalization. Experiments on widely used benchmark datasets demonstrate that our proposed method is able to achieve state-of-the-art performance. Quantitative analysis indicates that our feature augmentation approach facilitates the learning of effective models that are invariant across different domains.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2405.04286.pdf' target='_blank'>https://arxiv.org/pdf/2405.04286.pdf</a></span>   <span><a href='https://github.com/NLP2CT/GECScore' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xuebo Liu, Lidia S. Chao, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04286">Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The efficacy of detectors for texts generated by large language models (LLMs) substantially depends on the availability of large-scale training data. However, white-box zero-shot detectors, which require no such data, are limited by the accessibility of the source model of the LLM-generated text. In this paper, we propose a simple yet effective black-box zero-shot detection approach based on the observation that, from the perspective of LLMs, human-written texts typically contain more grammatical errors than LLM-generated texts. This approach involves calculating the Grammar Error Correction Score (GECScore) for the given text to differentiate between human-written and LLM-generated text. Experimental results show that our method outperforms current state-of-the-art (SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.62% across XSum and Writing Prompts dataset. Additionally, our approach demonstrates strong reliability in the wild, exhibiting robust generalization and resistance to paraphrasing attacks. Data and code are available at: https://github.com/NLP2CT/GECScore.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2405.03728.pdf' target='_blank'>https://arxiv.org/pdf/2405.03728.pdf</a></span>   <span><a href='https://github.com/ninja-wm/POM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobin Li, Kai Wu, Yujian Betterest Li, Xiaoyu Zhang, Handing Wang, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03728">Pretrained Optimization Model for Zero-Shot Black Box Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. It is crucial to ensure reliable and robust performance in various applications. Current optimizers often struggle with zero-shot optimization and require intricate hyperparameter tuning to adapt to new tasks. To address this, we propose a Pretrained Optimization Model (POM) that leverages knowledge gained from optimizing diverse tasks, offering efficient solutions to zero-shot optimization through direct application or fine-tuning with few-shot samples. Evaluation on the BBOB benchmark and two robot control tasks demonstrates that POM outperforms state-of-the-art black-box optimization methods, especially for high-dimensional tasks. Fine-tuning POM with a small number of samples and budget yields significant performance improvements. Moreover, POM demonstrates robust generalization across diverse task distributions, dimensions, population sizes, and optimization horizons. For code implementation, see https://github.com/ninja-wm/POM/.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2405.02797.pdf' target='_blank'>https://arxiv.org/pdf/2405.02797.pdf</a></span>   <span><a href='https://github.com/Guliisgreat/VDPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixiang Chi, Li Gu, Tao Zhong, Huan Liu, Yuanhao Yu, Konstantinos N Plataniotis, Yang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02797">Adapting to Distribution Shift by Visual Domain Prompt Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2405.01524.pdf' target='_blank'>https://arxiv.org/pdf/2405.01524.pdf</a></span>   <span><a href='https://github.com/dyballa/generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luciano Dyballa, Evan Gerritz, Steven W. Zucker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01524">A separability-based approach to quantifying generalization: which layer is best?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization to unseen data remains poorly understood for deep learning classification and foundation models, especially in the open set scenario. How can one assess the ability of networks to adapt to new or extended versions of their input space in the spirit of few-shot learning, out-of-distribution generalization, domain adaptation, and category discovery? Which layers of a network are likely to generalize best? We provide a new method for evaluating the capacity of networks to represent a sampled domain, regardless of whether the network has been trained on all classes in that domain. Our approach is the following: after fine-tuning state-of-the-art pre-trained models for visual classification on a particular domain, we assess their performance on data from related but distinct variations in that domain. Generalization power is quantified as a function of the latent embeddings of unseen data from intermediate layers for both unsupervised and supervised settings. Working throughout all stages of the network, we find that (i) high classification accuracy does not imply high generalizability; and (ii) deeper layers in a model do not always generalize the best, which has implications for pruning. Since the trends observed across datasets are largely consistent, we conclude that our approach reveals (a function of) the intrinsic capacity of the different layers of a model to generalize. Our code is available at https://github.com/dyballa/generalization
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2405.01228.pdf' target='_blank'>https://arxiv.org/pdf/2405.01228.pdf</a></span>   <span><a href='https://github.com/liamheng/Non-IID_Medical_Image_Segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Li, Haojin Li, Jianyu Chen, Zhongxi Qiu, Huazhu Fu, Lidai Wang, Yan Hu, Jiang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01228">RaffeSDG: Random Frequency Filtering enabled Single-source Domain Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models often encounter challenges in making accurate inferences when there are domain shifts between the source and target data. This issue is particularly pronounced in clinical settings due to the scarcity of annotated data resulting from the professional and private nature of medical data. Despite the existence of decent solutions, many of them are hindered in clinical settings due to limitations in data collection and computational complexity. To tackle domain shifts in data-scarce medical scenarios, we propose a Random frequency filtering enabled Single-source Domain Generalization algorithm (RaffeSDG), which promises robust out-of-domain inference with segmentation models trained on a single-source domain. A filter-based data augmentation strategy is first proposed to promote domain variability within a single-source domain by introducing variations in frequency space and blending homologous samples. Then Gaussian filter-based structural saliency is also leveraged to learn robust representations across augmented samples, further facilitating the training of generalizable segmentation models. To validate the effectiveness of RaffeSDG, we conducted extensive experiments involving out-of-domain inference on segmentation tasks for three human tissues imaged by four diverse modalities. Through thorough investigations and comparisons, compelling evidence was observed in these experiments, demonstrating the potential and generalizability of RaffeSDG. The code is available at https://github.com/liamheng/Non-IID_Medical_Image_Segmentation.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2404.19286.pdf' target='_blank'>https://arxiv.org/pdf/2404.19286.pdf</a></span>   <span><a href='https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuanghao Bai, Yuedi Zhang, Wanqi Zhou, Zhirong Luan, Badong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19286">Soft Prompt Generation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2404.16804.pdf' target='_blank'>https://arxiv.org/pdf/2404.16804.pdf</a></span>   <span><a href='https://github.com/Gahyeonkim09/AAPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gahyeon Kim, Sohee Kim, Seokju Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16804">AAPL: Adding Attributes to Prompt Learning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large pre-trained vision-language models have demonstrated remarkable performance on zero-shot downstream tasks. Building upon this, recent studies, such as CoOp and CoCoOp, have proposed the use of prompt learning, where context within a prompt is replaced with learnable vectors, leading to significant improvements over manually crafted prompts. However, the performance improvement for unseen classes is still marginal, and to tackle this problem, data augmentation has been frequently used in traditional zero-shot learning techniques. Through our experiments, we have identified important issues in CoOp and CoCoOp: the context learned through traditional image augmentation is biased toward seen classes, negatively impacting generalization to unseen classes. To address this problem, we propose adversarial token embedding to disentangle low-level visual augmentation features from high-level class information when inducing bias in learnable prompts. Through our novel mechanism called "Adding Attributes to Prompt Learning", AAPL, we guide the learnable context to effectively extract text features by focusing on high-level features for unseen classes. We have conducted experiments across 11 datasets, and overall, AAPL shows favorable performances compared to the existing methods in few-shot learning, zero-shot learning, cross-dataset, and domain generalization tasks.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2404.13992.pdf' target='_blank'>https://arxiv.org/pdf/2404.13992.pdf</a></span>   <span><a href='https://github.com/zhangda1018/DPD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Gao, Da Zhang, Qiyu Wang, Zhiyuan Zhao, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13992">Dynamic Proxy Domain Generalizes the Crowd Localization by Better Binary Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowd localization targets on predicting each instance precise location within an image. Current advanced methods propose the pixel-wise binary classification to tackle the congested prediction, in which the pixel-level thresholds binarize the prediction confidence of being the pedestrian head. Since the crowd scenes suffer from extremely varying contents, counts and scales, the confidence-threshold learner is fragile and under-generalized encountering domain knowledge shift. Moreover, at the most time, the target domain is agnostic in training. Hence, it is imperative to exploit how to enhance the generalization of confidence-threshold locator to the latent target domain. In this paper, we propose a Dynamic Proxy Domain (DPD) method to generalize the learner under domain shift. Concretely, based on the theoretical analysis to the generalization error risk upper bound on the latent target domain to a binary classifier, we propose to introduce a generated proxy domain to facilitate generalization. Then, based on the theory, we design a DPD algorithm which is composed by a training paradigm and proxy domain generator to enhance the domain generalization of the confidence-threshold learner. Besides, we conduct our method on five kinds of domain shift scenarios, demonstrating the effectiveness on generalizing the crowd localization. Our code will be available at https://github.com/zhangda1018/DPD.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2404.13671.pdf' target='_blank'>https://arxiv.org/pdf/2404.13671.pdf</a></span>   <span><a href='https://github.com/CASIA-IVA-Lab/FiLo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Hao Li, Ming Tang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13671">FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot anomaly detection (ZSAD) methods entail detecting anomalies directly without access to any known normal or abnormal samples within the target item categories. Existing approaches typically rely on the robust generalization capabilities of multimodal pretrained models, computing similarities between manually crafted textual features representing "normal" or "abnormal" semantics and image features to detect anomalies and localize anomalous patches. However, the generic descriptions of "abnormal" often fail to precisely match diverse types of anomalies across different object categories. Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales. To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc). FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection. HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes. Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-the-art performance with an image-level AUC of 83.9% and a pixel-level AUC of 95.9% on the VisA dataset. Code is available at https://github.com/CASIA-IVA-Lab/FiLo.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2404.09521.pdf' target='_blank'>https://arxiv.org/pdf/2404.09521.pdf</a></span>   <span><a href='https://github.com/tidiane-camaret/contextual_rl_zero_shot' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tidiane-camaret/contextual_rl_zero_shot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tidiane Camaret Ndir, AndrÃ© Biedenkapp, Noor Awad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09521">Inferring Behavior-Specific Context Improves Zero-Shot Generalization in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we address the challenge of zero-shot generalization (ZSG) in Reinforcement Learning (RL), where agents must adapt to entirely novel environments without additional training. We argue that understanding and utilizing contextual cues, such as the gravity level of the environment, is critical for robust generalization, and we propose to integrate the learning of context representations directly with policy learning. Our algorithm demonstrates improved generalization on various simulated domains, outperforming prior context-learning techniques in zero-shot settings. By jointly learning policy and context, our method acquires behavior-specific context representations, enabling adaptation to unseen environments and marks progress towards reinforcement learning systems that generalize across diverse real-world tasks. Our code and experiments are available at https://github.com/tidiane-camaret/contextual_rl_zero_shot.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2404.07840.pdf' target='_blank'>https://arxiv.org/pdf/2404.07840.pdf</a></span>   <span><a href='https://github.com/ernie-research/gptfluence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yekun Chai, Qingyi Liu, Shuohuan Wang, Yu Sun, Qiwei Peng, Hua Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07840">On Training Data Influence of GPT Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We make our code and data publicly available at https://github.com/ernie-research/gptfluence.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2404.07794.pdf' target='_blank'>https://arxiv.org/pdf/2404.07794.pdf</a></span>   <span><a href='https://github.com/longshaocong/DGMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Long, Qianyu Zhou, Xiangtai Li, Xuequan Lu, Chenhao Ying, Yuan Luo, Lizhuang Ma, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07794">DGMamba: Domain Generalization via Generalized State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization~(DG) aims at solving distribution shift problems in various scenes. Existing approaches are based on Convolution Neural Networks (CNNs) or Vision Transformers (ViTs), which suffer from limited receptive fields or quadratic complexities issues. Mamba, as an emerging state space model (SSM), possesses superior linear complexity and global receptive fields. Despite this, it can hardly be applied to DG to address distribution shifts, due to the hidden state issues and inappropriate scan mechanisms. In this paper, we propose a novel framework for DG, named DGMamba, that excels in strong generalizability toward unseen domains and meanwhile has the advantages of global receptive fields, and efficient linear complexity. Our DGMamba compromises two core components: Hidden State Suppressing~(HSS) and Semantic-aware Patch refining~(SPR). In particular, HSS is introduced to mitigate the influence of hidden states associated with domain-specific features during output prediction. SPR strives to encourage the model to concentrate more on objects rather than context, consisting of two designs: Prior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely, PFS aims to shuffle the non-semantic patches within images, creating more flexible and effective sequences from images, and DCI is designed to regularize Mamba with the combination of mismatched non-semantic and semantic information by fusing patches among domains. Extensive experiments on five commonly used DG benchmarks demonstrate that the proposed DGMamba achieves remarkably superior results to state-of-the-art models. The code will be made publicly available at https://github.com/longshaocong/DGMamba.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2404.05600.pdf' target='_blank'>https://arxiv.org/pdf/2404.05600.pdf</a></span>   <span><a href='https://github.com/0nutation/SpeechGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Zhang, Zhaowei Li, Shimin Li, Xin Zhang, Pengyu Wang, Yaqian Zhou, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05600">SpeechAlign: Aligning Speech Generation to Human Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2404.01509.pdf' target='_blank'>https://arxiv.org/pdf/2404.01509.pdf</a></span>   <span><a href='https://github.com/paulgavrikov/biases_vs_generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Gavrikov, Janis Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01509">Can Biases in ImageNet Models Explain Generalization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different training methods to understand how and if these biases - including shape bias, spectral biases, and critical bands - interact with generalization. Our extensive study results reveal that contrary to previous findings, these biases are insufficient to accurately predict the generalization of a model holistically. We provide access to all checkpoints and evaluation code at https://github.com/paulgavrikov/biases_vs_generalization
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2404.01272.pdf' target='_blank'>https://arxiv.org/pdf/2404.01272.pdf</a></span>   <span><a href='https://github.com/ShahinaKK/LG_SDG.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahina Kunhimon, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01272">Language Guided Domain Generalized Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single source domain generalization (SDG) holds promise for more reliable and consistent image segmentation across real-world clinical settings particularly in the medical domain, where data privacy and acquisition cost constraints often limit the availability of diverse datasets. Depending solely on visual features hampers the model's capacity to adapt effectively to various domains, primarily because of the presence of spurious correlations and domain-specific characteristics embedded within the image features. Incorporating text features alongside visual features is a potential solution to enhance the model's understanding of the data, as it goes beyond pixel-level information to provide valuable context. Textual cues describing the anatomical structures, their appearances, and variations across various imaging modalities can guide the model in domain adaptation, ultimately contributing to more robust and consistent segmentation. In this paper, we propose an approach that explicitly leverages textual information by incorporating a contrastive learning mechanism guided by the text encoder features to learn a more robust feature representation. We assess the effectiveness of our text-guided contrastive feature alignment technique in various scenarios, including cross-modality, cross-sequence, and cross-site settings for different segmentation tasks. Our approach achieves favorable performance against existing methods in literature. Our code and model weights are available at https://github.com/ShahinaKK/LG_SDG.git.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2404.00921.pdf' target='_blank'>https://arxiv.org/pdf/2404.00921.pdf</a></span>   <span><a href='https://github.com/clovaai/WSSHM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomyoung Kim, Myeong Yeon Yi, Joonsang Yu, Young Joon Yoo, Sung Ju Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00921">Towards Label-Efficient Human Matting: A Simple Baseline for Weakly Semi-Supervised Trimap-Free Human Matting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a new practical training method for human matting, which demands delicate pixel-level human region identification and significantly laborious annotations. To reduce the annotation cost, most existing matting approaches often rely on image synthesis to augment the dataset. However, the unnaturalness of synthesized training images brings in a new domain generalization challenge for natural images. To address this challenge, we introduce a new learning paradigm, weakly semi-supervised human matting (WSSHM), which leverages a small amount of expensive matte labels and a large amount of budget-friendly segmentation labels, to save the annotation cost and resolve the domain generalization problem. To achieve the goal of WSSHM, we propose a simple and effective training method, named Matte Label Blending (MLB), that selectively guides only the beneficial knowledge of the segmentation and matte data to the matting model. Extensive experiments with our detailed analysis demonstrate our method can substantially improve the robustness of the matting model using a few matte data and numerous segmentation data. Our training method is also easily applicable to real-time models, achieving competitive accuracy with breakneck inference speed (328 FPS on NVIDIA V100 GPU). The implementation code is available at \url{https://github.com/clovaai/WSSHM}.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2404.00851.pdf' target='_blank'>https://arxiv.org/pdf/2404.00851.pdf</a></span>   <span><a href='https://github.com/mlvlab/ProMetaR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyoung Park, Juyeon Ko, Hyunwoo J. Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00851">Prompt Learning via Meta-Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained vision-language models have shown impressive success on various computer vision tasks with their zero-shot generalizability. Recently, prompt learning approaches have been explored to efficiently and effectively adapt the vision-language models to a variety of downstream tasks. However, most existing prompt learning methods suffer from task overfitting since the general knowledge of the pre-trained vision language models is forgotten while the prompts are finetuned on a small data set from a specific target task. To address this issue, we propose a Prompt Meta-Regularization (ProMetaR) to improve the generalizability of prompt learning for vision-language models. Specifically, ProMetaR meta-learns both the regularizer and the soft prompts to harness the task-specific knowledge from the downstream tasks and task-agnostic general knowledge from the vision-language models. Further, ProMetaR augments the task to generate multiple virtual tasks to alleviate the meta-overfitting. In addition, we provide the analysis to comprehend how ProMetaR improves the generalizability of prompt tuning in the perspective of the gradient alignment. Our extensive experiments demonstrate that our ProMetaR improves the generalizability of conventional prompt learning methods under base-to-base/base-to-new and domain generalization settings. The code of ProMetaR is available at https://github.com/mlvlab/ProMetaR.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2404.00710.pdf' target='_blank'>https://arxiv.org/pdf/2404.00710.pdf</a></span>   <span><a href='https://github.com/mainaksingha01/ODG-CLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mainak Singha, Ankit Jha, Shirsha Bose, Ashwin Nair, Moloud Abdar, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00710">Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We delve into Open Domain Generalization (ODG), marked by domain and category shifts between training's labeled source and testing's unlabeled target domains. Existing solutions to ODG face limitations due to constrained generalizations of traditional CNN backbones and errors in detecting target open samples in the absence of prior knowledge. Addressing these pitfalls, we introduce ODG-CLIP, harnessing the semantic prowess of the vision-language model, CLIP. Our framework brings forth three primary innovations: Firstly, distinct from prevailing paradigms, we conceptualize ODG as a multi-class classification challenge encompassing both known and novel categories. Central to our approach is modeling a unique prompt tailored for detecting unknown class samples, and to train this, we employ a readily accessible stable diffusion model, elegantly generating proxy images for the open class. Secondly, aiming for domain-tailored classification (prompt) weights while ensuring a balance of precision and simplicity, we devise a novel visual stylecentric prompt learning mechanism. Finally, we infuse images with class-discriminative knowledge derived from the prompt space to augment the fidelity of CLIP's visual embeddings. We introduce a novel objective to safeguard the continuity of this infused semantic intel across domains, especially for the shared classes. Through rigorous testing on diverse datasets, covering closed and open-set DG contexts, ODG-CLIP demonstrates clear supremacy, consistently outpacing peers with performance boosts between 8%-16%. Code will be available at https://github.com/mainaksingha01/ODG-CLIP.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2403.15901.pdf' target='_blank'>https://arxiv.org/pdf/2403.15901.pdf</a></span>   <span><a href='https://github.com/keeplearning-again/MatchSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Huo, Ruiqiang Xiao, Haotian Zheng, Yang Liu, Sebastien Ourselin, Rachel Sparks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15901">MatchSeg: Towards Better Segmentation via Reference Image Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets. We validated our method across four public datasets. Experimental results demonstrate superior segmentation performance and powerful domain generalization ability of MatchSeg against existing methods for domain-specific and cross-domain segmentation tasks. Our code is made available at https://github.com/keeplearning-again/MatchSeg
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2403.15647.pdf' target='_blank'>https://arxiv.org/pdf/2403.15647.pdf</a></span>   <span><a href='https://github.com/zgy600/RetiGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ze Chen, Gongyu Zhang, Jiayu Huo, Joan Nunez do Rio, Charalampos Komninos, Yang Liu, Rachel Sparks, Sebastien Ourselin, Christos Bergeles, Timothy Jackson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15647">RetiGen: A Framework for Generalized Retinal Diagnosis Using Multi-View Fundus Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a novel framework for enhancing domain generalization in medical imaging, specifically focusing on utilizing unlabelled multi-view colour fundus photographs. Unlike traditional approaches that rely on single-view imaging data and face challenges in generalizing across diverse clinical settings, our method leverages the rich information in the unlabelled multi-view imaging data to improve model robustness and accuracy. By incorporating a class balancing method, a test-time adaptation technique and a multi-view optimization strategy, we address the critical issue of domain shift that often hampers the performance of machine learning models in real-world applications. Experiments comparing various state-of-the-art domain generalization and test-time optimization methodologies show that our approach consistently outperforms when combined with existing baseline and state-of-the-art methods. We also show our online method improves all existing techniques. Our framework demonstrates improvements in domain generalization capabilities and offers a practical solution for real-world deployment by facilitating online adaptation to new, unseen datasets. Our code is available at https://github.com/zgy600/RetiGen .
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2403.15268.pdf' target='_blank'>https://arxiv.org/pdf/2403.15268.pdf</a></span>   <span><a href='https://github.com/Xnhyacinth/IAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15268">Awakening Augmented Generation: Learning to Awaken Internal Knowledge of Large Language Models for Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-Augmented-Generation and Generation-Augmented-Generation have been proposed to enhance the knowledge required for question answering with Large Language Models (LLMs) by leveraging richer context. However, the former relies on external resources, and both require incorporating explicit documents into the context, which increases execution costs and susceptibility to noise data during inference. Recent works indicate that LLMs model rich knowledge, but it is often not effectively activated and awakened. Inspired by this, we propose a novel knowledge-augmented framework, $\textbf{Awakening-Augmented-Generation}$ (AAG), which mimics the human ability to answer questions using only thinking and recalling to compensate for knowledge gaps, thereby awaking relevant knowledge in LLMs without relying on external resources. AAG consists of two key components for awakening richer context. Explicit awakening fine-tunes a context generator to create a synthetic, compressed document that functions as symbolic context. Implicit awakening utilizes a hypernetwork to generate adapters based on the question and synthetic document, which are inserted into LLMs to serve as parameter context. Experimental results on three datasets demonstrate that AAG exhibits significant advantages in both open-domain and closed-book settings, as well as in out-of-distribution generalization. Our code will be available at \url{https://github.com/Xnhyacinth/IAG}.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2403.14356.pdf' target='_blank'>https://arxiv.org/pdf/2403.14356.pdf</a></span>   <span><a href='https://github.com/marrlab/DomainLab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Sun, Carla Feistner, Alexej Gossmann, George Schwarz, Rao Muhammad Umer, Lisa Beer, Patrick Rockenschaub, Rahul Babu Shrestha, Armin Gruber, Nutan Chen, Sayedali Shetab Boushehri, Florian Buettner, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14356">DomainLab: A modular Python package for domain generalization in deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Poor generalization performance caused by distribution shifts in unseen domains often hinders the trustworthy deployment of deep neural networks. Many domain generalization techniques address this problem by adding a domain invariant regularization loss terms during training. However, there is a lack of modular software that allows users to combine the advantages of different methods with minimal effort for reproducibility. DomainLab is a modular Python package for training user specified neural networks with composable regularization loss terms. Its decoupled design allows the separation of neural networks from regularization loss construction. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. In addition, DomainLab offers powerful benchmarking functionality to evaluate the generalization performance of neural networks in out-of-distribution data. The package supports running the specified benchmark on an HPC cluster or on a standalone machine. The package is well tested with over 95 percent coverage and well documented. From the user perspective, it is closed to modification but open to extension. The package is under the MIT license, and its source code, tutorial and documentation can be found at https://github.com/marrlab/DomainLab.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2403.12964.pdf' target='_blank'>https://arxiv.org/pdf/2403.12964.pdf</a></span>   <span><a href='https://github.com/zhangce01/SimNL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zhangce01/SimNL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12964">Enhancing Vision-Language Few-Shot Adaptation with Negative Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale pre-trained Vision-Language Models (VLMs) have exhibited impressive zero-shot performance and transferability, allowing them to adapt to downstream tasks in a data-efficient manner. However, when only a few labeled samples are available, adapting VLMs to distinguish subtle differences between similar classes in specific downstream tasks remains challenging. In this work, we propose a Simple yet effective Negative Learning approach, SimNL, to more efficiently exploit the task-specific knowledge from few-shot labeled samples. Unlike previous methods that focus on identifying a set of representative positive features defining "what is a {CLASS}", SimNL discovers a complementary set of negative features that define "what is not a {CLASS}", providing additional insights that supplement the positive features to enhance task-specific recognition capability. Further, we identify that current adaptation approaches are particularly vulnerable to potential noise in the few-shot sample set. To mitigate this issue, we introduce a plug-and-play few-shot instance reweighting technique to suppress noisy outliers and amplify clean samples for more stable adaptation. Our extensive experimental results across 15 datasets validate that the proposed SimNL outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency. Code is available at https://github.com/zhangce01/SimNL.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2403.11792.pdf' target='_blank'>https://arxiv.org/pdf/2403.11792.pdf</a></span>   <span><a href='https://github.com/lingeringlight/SETA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lingeringlight/SETA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Guo, Lei Qi, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11792">SETA: Semantic-Aware Token Augmentation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to enhance the model robustness against domain shifts without accessing target domains. A prevalent category of methods for DG is data augmentation, which focuses on generating virtual samples to simulate domain shifts. However, existing augmentation techniques in DG are mainly tailored for convolutional neural networks (CNNs), with limited exploration in token-based architectures, i.e., vision transformer (ViT) and multi-layer perceptrons (MLP) models. In this paper, we study the impact of prior CNN-based augmentation methods on token-based models, revealing their performance is suboptimal due to the lack of incentivizing the model to learn holistic shape information. To tackle the issue, we propose the SEmantic-aware Token Augmentation (SETA) method. SETA transforms token features by perturbing local edge cues while preserving global shape features, thereby enhancing the model learning of shape information. To further enhance the generalization ability of the model, we introduce two stylized variants of our method combined with two state-of-the-art style augmentation methods in DG. We provide a theoretical insight into our method, demonstrating its effectiveness in reducing the generalization risk bound. Comprehensive experiments on five benchmarks prove that our method achieves SOTA performances across various ViT and MLP architectures. Our code is available at https://github.com/lingeringlight/SETA.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2403.11689.pdf' target='_blank'>https://arxiv.org/pdf/2403.11689.pdf</a></span>   <span><a href='https://github.com/zhaohaoyu376/morestyle' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhao, Wenhui Dong, Rui Yu, Zhou Zhao, Du Bo, Yongchao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11689">MoreStyle: Relax Low-frequency Constraint of Fourier-based Image Reconstruction in Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of single-source domain generalization (SDG) in medical image segmentation is crucial due to frequent domain shifts in clinical image datasets. To address the challenge of poor generalization across different domains, we introduce a Plug-and-Play module for data augmentation called MoreStyle. MoreStyle diversifies image styles by relaxing low-frequency constraints in Fourier space, guiding the image reconstruction network. With the help of adversarial learning, MoreStyle further expands the style range and pinpoints the most intricate style combinations within latent features. To handle significant style variations, we introduce an uncertainty-weighted loss. This loss emphasizes hard-to-classify pixels resulting only from style shifts while mitigating true hard-to-classify pixels in both MoreStyle-generated and original images. Extensive experiments on two widely used benchmarks demonstrate that the proposed MoreStyle effectively helps to achieve good domain generalization ability, and has the potential to further boost the performance of some state-of-the-art SDG methods. Source code is available at https://github.com/zhaohaoyu376/morestyle.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2403.11371.pdf' target='_blank'>https://arxiv.org/pdf/2403.11371.pdf</a></span>   <span><a href='https://github.com/Baolu1998/V2X-DGW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baolu Li, Jinlong Li, Xinyu Liu, Runsheng Xu, Zhengzhong Tu, Jiacheng Guo, Xiaopeng Li, Hongkai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11371">V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perception systems have shown the significant success on 3D object detection. While these models perform well in the trained clean weather, they struggle in unseen adverse weather conditions with the domain gap. In this paper, we propose a Domain Generalization based approach, named \textit{V2X-DGW}, for LiDAR-based 3D object detection on multi-agent perception system under adverse weather conditions. Our research aims to not only maintain favorable multi-agent performance in the clean weather but also promote the performance in the unseen adverse weather conditions by learning only on the clean weather data. To realize the Domain Generalization, we first introduce the Adaptive Weather Augmentation (AWA) to mimic the unseen adverse weather conditions, and then propose two alignments for generalizable representation learning: Trust-region Weather-invariant Alignment (TWA) and Agent-aware Contrastive Alignment (ACA). To evaluate this research, we add Fog, Rain, Snow conditions on two publicized multi-agent datasets based on physics-based models, resulting in two new datasets: OPV2V-w and V2XSet-w. Extensive experiments demonstrate that our V2X-DGW achieved significant improvements in the unseen adverse weathers. The code is available at https://github.com/Baolu1998/V2X-DGW.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2403.11310.pdf' target='_blank'>https://arxiv.org/pdf/2403.11310.pdf</a></span>   <span><a href='https://github.com/davidpengucf/DAF-DG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qucheng Peng, Ce Zheng, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11310">A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D human pose data collected in controlled laboratory settings present challenges for pose estimators that generalize across diverse scenarios. To address this, domain generalization is employed. Current methodologies in domain generalization for 3D human pose estimation typically utilize adversarial training to generate synthetic poses for training. Nonetheless, these approaches exhibit several limitations. First, the lack of prior information about the target domain complicates the application of suitable augmentation through a single pose augmentor, affecting generalization on target domains. Moreover, adversarial training's discriminator tends to enforce similarity between source and synthesized poses, impeding the exploration of out-of-source distributions. Furthermore, the pose estimator's optimization is not exposed to domain shifts, limiting its overall generalization ability.
  To address these limitations, we propose a novel framework featuring two pose augmentors: the weak and the strong augmentors. Our framework employs differential strategies for generation and discrimination processes, facilitating the preservation of knowledge related to source poses and the exploration of out-of-source distributions without prior information about target poses. Besides, we leverage meta-optimization to simulate domain shifts in the optimization process of the pose estimator, thereby improving its generalization ability. Our proposed approach significantly outperforms existing methods, as demonstrated through comprehensive experiments on various benchmark datasets.Our code will be released at \url{https://github.com/davidpengucf/DAF-DG}.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2403.11193.pdf' target='_blank'>https://arxiv.org/pdf/2403.11193.pdf</a></span>   <span><a href='https://github.com/aeolusguan/NMRF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongfan Guan, Chen Wang, Yun-Hui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11193">Neural Markov Random Field for Stereo Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stereo matching is a core task for many computer vision and robotics applications. Despite their dominance in traditional stereo methods, the hand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy compared to end-to-end deep models. While deep learning representations have greatly improved the unary terms of the MRF models, the overall accuracy is still severely limited by the hand-crafted pairwise terms and message passing. To address these issues, we propose a neural MRF model, where both potential functions and message passing are designed using data-driven neural networks. Our fully data-driven model is built on the foundation of variational inference theory, to prevent convergence issues and retain stereo MRF's graph inductive bias. To make the inference tractable and scale well to high-resolution images, we also propose a Disparity Proposal Network (DPN) to adaptively prune the search space of disparity. The proposed approach ranks $1^{st}$ on both KITTI 2012 and 2015 leaderboards among all published methods while running faster than 100 ms. This approach significantly outperforms prior global methods, e.g., lowering D1 metric by more than 50% on KITTI 2015. In addition, our method exhibits strong cross-domain generalization and can recover sharp edges. The codes at https://github.com/aeolusguan/NMRF
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2403.10087.pdf' target='_blank'>https://arxiv.org/pdf/2403.10087.pdf</a></span>   <span><a href='https://github.com/jzc777/SE-inceptionV3-L2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junzhuo Chen, Zonghan Lu, Shitong Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10087">Monkeypox disease recognition model based on improved SE-InceptionV3</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the wake of the global spread of monkeypox, accurate disease recognition has become crucial. This study introduces an improved SE-InceptionV3 model, embedding the SENet module and incorporating L2 regularization into the InceptionV3 framework to enhance monkeypox disease detection. Utilizing the Kaggle monkeypox dataset, which includes images of monkeypox and similar skin conditions, our model demonstrates a noteworthy accuracy of 96.71% on the test set, outperforming conventional methods and deep learning models. The SENet modules channel attention mechanism significantly elevates feature representation, while L2 regularization ensures robust generalization. Extensive experiments validate the models superiority in precision, recall, and F1 score, highlighting its effectiveness in differentiating monkeypox lesions in diverse and complex cases. The study not only provides insights into the application of advanced CNN architectures in medical diagnostics but also opens avenues for further research in model optimization and hyperparameter tuning for enhanced disease recognition. https://github.com/jzc777/SE-inceptionV3-L2
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2403.09400.pdf' target='_blank'>https://arxiv.org/pdf/2403.09400.pdf</a></span>   <span><a href='https://github.com/BioMedIA-MBZUAI/ConDiSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandr Matsun, Numan Saeed, Fadillah Adamsyah Maani, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09400">ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical data often exhibits distribution shifts, which cause test-time performance degradation for deep learning models trained using standard supervised learning pipelines. This challenge is addressed in the field of Domain Generalization (DG) with the sub-field of Single Domain Generalization (SDG) being specifically interesting due to the privacy- or logistics-related issues often associated with medical data. Existing disentanglement-based SDG methods heavily rely on structural information embedded in segmentation masks, however classification labels do not provide such dense information. This work introduces a novel SDG method aimed at medical image classification that leverages channel-wise contrastive disentanglement. It is further enhanced with reconstruction-based style regularization to ensure extraction of distinct style and structure feature representations. We evaluate our method on the complex task of multicenter histopathology image classification, comparing it against state-of-the-art (SOTA) SDG baselines. Results demonstrate that our method surpasses the SOTA by a margin of 1% in average accuracy while also showing more stable performance. This study highlights the importance and challenges of exploring SDG frameworks in the context of the classification task. The code is publicly available at https://github.com/BioMedIA-MBZUAI/ConDiSR
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2403.09124.pdf' target='_blank'>https://arxiv.org/pdf/2403.09124.pdf</a></span>   <span><a href='https://github.com/Shimmer93/MPCount' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoxuan Peng, S. -H. Gary Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09124">Single Domain Generalization for Crowd Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to its promising results, density map regression has been widely employed for image-based crowd counting. The approach, however, often suffers from severe performance degradation when tested on data from unseen scenarios, the so-called "domain shift" problem. To address the problem, we investigate in this work single domain generalization (SDG) for crowd counting. The existing SDG approaches are mainly for image classification and segmentation, and can hardly be extended to our case due to its regression nature and label ambiguity (i.e., ambiguous pixel-level ground truths). We propose MPCount, a novel effective SDG approach even for narrow source distribution. MPCount stores diverse density values for density map regression and reconstructs domain-invariant features by means of only one memory bank, a content error mask and attention consistency loss. By partitioning the image into grids, it employs patch-wise classification as an auxiliary task to mitigate label ambiguity. Through extensive experiments on different datasets, MPCount is shown to significantly improve counting accuracy compared to the state of the art under diverse scenarios unobserved in the training data characterized by narrow source distribution. Code is available at https://github.com/Shimmer93/MPCount.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2403.08649.pdf' target='_blank'>https://arxiv.org/pdf/2403.08649.pdf</a></span>   <span><a href='https://github.com/liangchen527/CausEB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Chen, Yong Zhang, Yibing Song, Zhen Zhang, Lingqiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08649">A Causal Inspired Early-Branching Structure for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning domain-invariant semantic representations is crucial for achieving domain generalization (DG), where a model is required to perform well on unseen target domains. One critical challenge is that standard training often results in entangled semantic and domain-specific features. Previous works suggest formulating the problem from a causal perspective and solving the entanglement problem by enforcing marginal independence between the causal (\ie semantic) and non-causal (\ie domain-specific) features. Despite its simplicity, the basic marginal independent-based idea alone may be insufficient to identify the causal feature. By d-separation, we observe that the causal feature can be further characterized by being independent of the domain conditioned on the object, and we propose the following two strategies as complements for the basic framework.
  First, the observation implicitly implies that for the same object, the causal feature should not be associated with the non-causal feature, revealing that the common practice of obtaining the two features with a shared base feature extractor and two lightweight prediction heads might be inappropriate. To meet the constraint, we propose a simple early-branching structure, where the causal and non-causal feature obtaining branches share the first few blocks while diverging thereafter, for better structure design; Second, the observation implies that the causal feature remains invariant across different domains for the same object. To this end, we suggest that augmentation should be incorporated into the framework to better characterize the causal feature, and we further suggest an effective random domain sampling scheme to fulfill the task. Theoretical and experimental results show that the two strategies are beneficial for the basic marginal independent-based framework. Code is available at \url{https://github.com/liangchen527/CausEB}.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2403.07329.pdf' target='_blank'>https://arxiv.org/pdf/2403.07329.pdf</a></span>   <span><a href='https://github.com/SJShin-AI/UDIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjae Shin, HeeSun Bae, Byeonghu Na, Yoon-Yeong Kim, Il-Chul Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07329">Unknown Domain Inconsistency Minimization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain's loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there's still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscape of perturbed domains, we expect to achieve generalization grounded on these flat minima for the unknown domains. Theoretically, we validate that merging SAM optimization with the UDIM objective establishes an upper bound for the true objective of the DG task. In an empirical aspect, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets. Notably, UDIM shows statistically significant improvements in scenarios with more restrictive domain information, underscoring UDIM's generalization capability in unseen domains. Our code is available at \url{https://github.com/SJShin-AI/UDIM}.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2403.06947.pdf' target='_blank'>https://arxiv.org/pdf/2403.06947.pdf</a></span>   <span><a href='https://github.com/keke-nice/Greip' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuting Zhang, Hao Lu, Xin Liu, Yingcong Chen, Kaishun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06947">Advancing Generalizable Remote Physiological Measurement through the Integration of Explicit and Implicit Prior Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote photoplethysmography (rPPG) is a promising technology that captures physiological signals from face videos, with potential applications in medical health, emotional computing, and biosecurity recognition. The demand for rPPG tasks has expanded from demonstrating good performance on intra-dataset testing to cross-dataset testing (i.e., domain generalization). However, most existing methods have overlooked the prior knowledge of rPPG, resulting in poor generalization ability. In this paper, we propose a novel framework that simultaneously utilizes explicit and implicit prior knowledge in the rPPG task. Specifically, we systematically analyze the causes of noise sources (e.g., different camera, lighting, skin types, and movement) across different domains and incorporate these prior knowledge into the network. Additionally, we leverage a two-branch network to disentangle the physiological feature distribution from noises through implicit label correlation. Our extensive experiments demonstrate that the proposed method not only outperforms state-of-the-art methods on RGB cross-dataset evaluation but also generalizes well from RGB datasets to NIR datasets. The code is available at https://github.com/keke-nice/Greip.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2403.05912.pdf' target='_blank'>https://arxiv.org/pdf/2403.05912.pdf</a></span>   <span><a href='https://github.com/nanase1025/M-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hairong Shi, Songhao Han, Shaofei Huang, Yue Liao, Guanbin Li, Xiangxing Kong, Hua Zhu, Xiaomu Wang, Si Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05912">Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tumor lesion segmentation on CT or MRI images plays a critical role in cancer diagnosis and treatment planning. Considering the inherent differences in tumor lesion segmentation data across various medical imaging modalities and equipment, integrating medical knowledge into the Segment Anything Model (SAM) presents promising capability due to its versatility and generalization potential. Recent studies have attempted to enhance SAM with medical expertise by pre-training on large-scale medical segmentation datasets. However, challenges still exist in 3D tumor lesion segmentation owing to tumor complexity and the imbalance in foreground and background regions. Therefore, we introduce Mask-Enhanced SAM (M-SAM), an innovative architecture tailored for 3D tumor lesion segmentation. We propose a novel Mask-Enhanced Adapter (MEA) within M-SAM that enriches the semantic information of medical images with positional data from coarse segmentation masks, facilitating the generation of more precise segmentation masks. Furthermore, an iterative refinement scheme is implemented in M-SAM to refine the segmentation masks progressively, leading to improved performance. Extensive experiments on seven tumor lesion segmentation datasets indicate that our M-SAM not only achieves high segmentation accuracy but also exhibits robust generalization. The code is available at https://github.com/nanase1025/M-SAM.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2403.05265.pdf' target='_blank'>https://arxiv.org/pdf/2403.05265.pdf</a></span>   <span><a href='https://github.com/zzqbjt/Spoiler-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zinan Zeng, Sen Ye, Zijian Cai, Heng Wang, Yuhan Liu, Haokai Zhang, Minnan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05265">MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user's information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively. To handle genre-specific spoilers, we then adopt Mixture-of-Experts architecture to process information in three modalities to promote robustness. Finally, we use an expert fusion layer to integrate the features from different perspectives and make predictions based on the fused embedding. Experiments demonstrate that MMoE achieves state-of-the-art performance on two widely-used spoiler detection datasets, surpassing previous SOTA methods by 2.56% and 8.41% in terms of accuracy and F1-score. Further experiments also demonstrate MMoE's superiority in robustness and generalization. Our code is available at https://github.com/zzqbjt/Spoiler-Detection.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2403.04697.pdf' target='_blank'>https://arxiv.org/pdf/2403.04697.pdf</a></span>   <span><a href='https://github.com/yuankaishen2001/AUFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaishen Yuan, Zitong Yu, Xin Liu, Weicheng Xie, Huanjing Yue, Jingyu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04697">AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic. Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data. Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics. Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge. Then the MoKE collaborates with other MoKEs in the expert group to obtain aggregated information and inject it into the frozen Vision Transformer (ViT) to achieve parameter-efficient AU detection. Additionally, we design a Margin-truncated Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the model to focus more on activated AUs, differentiate the difficulty of unactivated AUs, and discard potential mislabeled samples. Extensive experiments from various perspectives, including within-domain, cross-domain, data efficiency, and micro-expression domain, demonstrate AUFormer's state-of-the-art performance and robust generalization abilities without relying on additional relevant data. The code for AUFormer is available at https://github.com/yuankaishen2001/AUFormer.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2403.03662.pdf' target='_blank'>https://arxiv.org/pdf/2403.03662.pdf</a></span>   <span><a href='http://github.com/MKashifAli/MetaVideoStab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Kashif Ali, Eun Woo Im, Dongjin Kim, Tae Hyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03662">Harnessing Meta-Learning for Improving Full-Frame Video Stabilization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video stabilization is a longstanding computer vision problem, particularly pixel-level synthesis solutions for video stabilization which synthesize full frames add to the complexity of this task. These techniques aim to stabilize videos by synthesizing full frames while enhancing the stability of the considered video. This intensifies the complexity of the task due to the distinct mix of unique motion profiles and visual content present in each video sequence, making robust generalization with fixed parameters difficult. In our study, we introduce a novel approach to enhance the performance of pixel-level synthesis solutions for video stabilization by adapting these models to individual input video sequences. The proposed adaptation exploits low-level visual cues accessible during test-time to improve both the stability and quality of resulting videos. We highlight the efficacy of our methodology of "test-time adaptation" through simple fine-tuning of one of these models, followed by significant stability gain via the integration of meta-learning techniques. Notably, significant improvement is achieved with only a single adaptation step. The versatility of the proposed algorithm is demonstrated by consistently improving the performance of various pixel-level synthesis models for video stabilization in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2402.19298.pdf' target='_blank'>https://arxiv.org/pdf/2402.19298.pdf</a></span>   <span><a href='https://github.com/OMGGGGG/mmdg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xun Lin, Shuai Wang, Rizhao Cai, Yizhong Liu, Ying Fu, Zitong Yu, Wenzhong Tang, Alex Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19298">Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Anti-Spoofing (FAS) is crucial for securing face recognition systems against presentation attacks. With advancements in sensor manufacture and multi-modal learning techniques, many multi-modal FAS approaches have emerged. However, they face challenges in generalizing to unseen attacks and deployment conditions. These challenges arise from (1) modality unreliability, where some modality sensors like depth and infrared undergo significant domain shifts in varying environments, leading to the spread of unreliable information during cross-modal feature fusion, and (2) modality imbalance, where training overly relies on a dominant modality hinders the convergence of others, reducing effectiveness against attack types that are indistinguishable sorely using the dominant modality. To address modality unreliability, we propose the Uncertainty-Guided Cross-Adapter (U-Adapter) to recognize unreliably detected regions within each modality and suppress the impact of unreliable regions on other modalities. For modality imbalance, we propose a Rebalanced Modality Gradient Modulation (ReGrad) strategy to rebalance the convergence speed of all modalities by adaptively adjusting their gradients. Besides, we provide the first large-scale benchmark for evaluating multi-modal FAS performance under domain generalization scenarios. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. Source code and protocols will be released on https://github.com/OMGGGGG/mmdg.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2402.18817.pdf' target='_blank'>https://arxiv.org/pdf/2402.18817.pdf</a></span>   <span><a href='https://github.com/leminhbinh0209/CVPR24-FAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binh M. Le, Simon S. Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18817">Gradient Alignment for Cross-Domain Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in domain generalization (DG) for face anti-spoofing (FAS) have garnered considerable attention. Traditional methods have focused on designing learning objectives and additional modules to isolate domain-specific features while retaining domain-invariant characteristics in their representations. However, such approaches often lack guarantees of consistent maintenance of domain-invariant features or the complete removal of domain-specific features. Furthermore, most prior works of DG for FAS do not ensure convergence to a local flat minimum, which has been shown to be advantageous for DG. In this paper, we introduce GAC-FAS, a novel learning objective that encourages the model to converge towards an optimal flat minimum without necessitating additional learning modules. Unlike conventional sharpness-aware minimizers, GAC-FAS identifies ascending points for each domain and regulates the generalization gradient updates at these points to align coherently with empirical risk minimization (ERM) gradient updates. This unique approach specifically guides the model to be robust against domain shifts. We demonstrate the efficacy of GAC-FAS through rigorous testing on challenging cross-domain FAS datasets, where it establishes state-of-the-art performance. The code is available at https://github.com/leminhbinh0209/CVPR24-FAS.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2402.16092.pdf' target='_blank'>https://arxiv.org/pdf/2402.16092.pdf</a></span>   <span><a href='https://github.com/daintlab/stochastic_cross_attention' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungwon Seo, Suho Lee, Sangheum Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16092">StochCA: A Novel Approach for Exploiting Pretrained Models with Cross-Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Utilizing large-scale pretrained models is a well-known strategy to enhance performance on various target tasks. It is typically achieved through fine-tuning pretrained models on target tasks. However, na\"Ä±ve fine-tuning may not fully leverage knowledge embedded in pretrained models. In this study, we introduce a novel fine-tuning method, called stochastic cross-attention (StochCA), specific to Transformer architectures. This method modifies the Transformer's self-attention mechanism to selectively utilize knowledge from pretrained models during fine-tuning. Specifically, in each block, instead of self-attention, cross-attention is performed stochastically according to the predefined probability, where keys and values are extracted from the corresponding block of a pretrained model. By doing so, queries and channel-mixing multi-layer perceptron layers of a target model are fine-tuned to target tasks to learn how to effectively exploit rich representations of pretrained models. To verify the effectiveness of StochCA, extensive experiments are conducted on benchmarks in the areas of transfer learning and domain generalization, where the exploitation of pretrained models is critical. Our experimental results show the superiority of StochCA over state-of-the-art approaches in both areas. Furthermore, we demonstrate that StochCA is complementary to existing approaches, i.e., it can be combined with them to further improve performance. Our code is available at https://github.com/daintlab/stochastic_cross_attention
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2402.14810.pdf' target='_blank'>https://arxiv.org/pdf/2402.14810.pdf</a></span>   <span><a href='https://github.com/Meowuu7/GeneOH-Diffusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyi Liu, Li Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14810">GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a "denoising via diffusion" strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications. Project website: https://meowuu7.github.io/GeneOH-Diffusion/.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2402.11494.pdf' target='_blank'>https://arxiv.org/pdf/2402.11494.pdf</a></span>   <span><a href='https://github.com/fannie1208/CaNet' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/fannie1208/CaNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11494">Graph Out-of-Distribution Generalization via Causal Intervention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4\% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks. Source codes are available at https://github.com/fannie1208/CaNet.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2402.09178.pdf' target='_blank'>https://arxiv.org/pdf/2402.09178.pdf</a></span>   <span><a href='https://github.com/DXOMARK-Research/PIQ2023' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Chahine, Sira Ferradans, Javier Vazquez-Corral, Jean Ponce
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09178">Generalized Portrait Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated and robust portrait quality assessment (PQA) is of paramount importance in high-impact applications such as smartphone photography. This paper presents FHIQA, a learning-based approach to PQA that introduces a simple but effective quality score rescaling method based on image semantics, to enhance the precision of fine-grained image quality metrics while ensuring robust generalization to various scene settings beyond the training dataset. The proposed approach is validated by extensive experiments on the PIQ23 benchmark and comparisons with the current state of the art. The source code of FHIQA will be made publicly available on the PIQ23 GitHub repository at https://github.com/DXOMARK-Research/PIQ2023.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2402.07785.pdf' target='_blank'>https://arxiv.org/pdf/2402.07785.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/hypo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyue Bai, Yifei Ming, Julian Katz-Samuels, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07785">HYPO: Hyperspherical Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines and achieves superior performance. Code is available at https://github.com/deeplearning-wisc/hypo.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2402.05035.pdf' target='_blank'>https://arxiv.org/pdf/2402.05035.pdf</a></span>   <span><a href='https://github.com/Ziwei-Niu/DG_for_MedIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwei Niu, Shuyi Ouyang, Shiao Xie, Yen-wei Chen, Lanfen Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05035">A Survey on Domain Generalization for Medical Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical Image Analysis (MedIA) has emerged as a crucial tool in computer-aided diagnosis systems, particularly with the advancement of deep learning (DL) in recent years. However, well-trained deep models often experience significant performance degradation when deployed in different medical sites, modalities, and sequences, known as a domain shift issue. In light of this, Domain Generalization (DG) for MedIA aims to address the domain shift challenge by generalizing effectively and performing robustly across unknown data distributions. This paper presents the a comprehensive review of substantial developments in this area. First, we provide a formal definition of domain shift and domain generalization in medical field, and discuss several related settings. Subsequently, we summarize the recent methods from three viewpoints: data manipulation level, feature representation level, and model training level, and present some algorithms in detail for each viewpoints. Furthermore, we introduce the commonly used datasets. Finally, we summarize existing literature and present some potential research topics for the future. For this survey, we also created a GitHub project by collecting the supporting resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2402.04672.pdf' target='_blank'>https://arxiv.org/pdf/2402.04672.pdf</a></span>   <span><a href='https://github.com/wufan-cse/G-NAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Wu, Jinling Gao, Lanqing Hong, Xinbing Wang, Chenghu Zhou, Nanyang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04672">G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we focus on a realistic yet challenging task, Single Domain Generalization Object Detection (S-DGOD), where only one source domain's data can be used for training object detectors, but have to generalize multiple distinct target domains. In S-DGOD, both high-capacity fitting and generalization abilities are needed due to the task's complexity. Differentiable Neural Architecture Search (NAS) is known for its high capacity for complex data fitting and we propose to leverage Differentiable NAS to solve S-DGOD. However, it may confront severe over-fitting issues due to the feature imbalance phenomenon, where parameters optimized by gradient descent are biased to learn from the easy-to-learn features, which are usually non-causal and spuriously correlated to ground truth labels, such as the features of background in object detection data. Consequently, this leads to serious performance degradation, especially in generalizing to unseen target domains with huge domain gaps between the source domain and target domains. To address this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware objective, preventing NAS from over-fitting by using gradient descent to optimize parameters not only on a subset of easy-to-learn features but also the remaining predictive features for generalization, and the overall framework is named G-NAS. Experimental results on the S-DGOD urban-scene datasets demonstrate that the proposed G-NAS achieves SOTA performance compared to baseline methods. Codes are available at https://github.com/wufan-cse/G-NAS.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2402.04416.pdf' target='_blank'>https://arxiv.org/pdf/2402.04416.pdf</a></span>   <span><a href='https://github.com/Chris210634/mudg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Liao, Christian So, Theodoros Tsiligkaridis, Brian Kulis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04416">Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) is an important problem that learns a model which generalizes to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (MUDG) problem, which uses a large task-agnostic unlabeled source dataset during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be accurately and efficiently searched in a joint vision-language space. We make three contributions in the MUDG setting. Firstly, we show theoretically that cross-modal approximate nearest neighbor search suffers from low recall due to the large distance between text queries and the image centroids used for coarse quantization. Accordingly, we propose paired k-means, a simple clustering algorithm that improves nearest neighbor recall by storing centroids in query space instead of image space. Secondly, we propose an adaptive text augmentation scheme for target labels designed to improve zero-shot accuracy and diversify retrieved image data. Lastly, we present two simple but effective components to further improve downstream target accuracy. We compare against state-of-the-art name-only transfer, source-free DG and zero-shot (ZS) methods on their respective benchmarks and show consistent improvement in accuracy on 20 diverse datasets. Code is available: https://github.com/Chris210634/mudg
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2402.04087.pdf' target='_blank'>https://arxiv.org/pdf/2402.04087.pdf</a></span>   <span><a href='https://github.com/mrflogs/ICLR24' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, Tieniu Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04087">A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches. Our code is publicly available at \url{https://github.com/mrflogs/ICLR24}.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2401.14846.pdf' target='_blank'>https://arxiv.org/pdf/2401.14846.pdf</a></span>   <span><a href='https://github.com/qiaoruiyt/NoiseRobustDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Qiao, Bryan Kian Hsiang Low
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14846">Understanding Domain Generalization: A Noise Robustness Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We conjecture that the failure mode of ERM arising from spurious correlations may be less pronounced in practice.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2401.13965.pdf' target='_blank'>https://arxiv.org/pdf/2401.13965.pdf</a></span>   <span><a href='https://github.com/Adnan-Khan7/UPLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adnan Khan, Mai A. Shaaban, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13965">Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Beyond attaining domain generalization (DG), visual recognition models should also be data-efficient during learning by leveraging limited labels. We study the problem of Semi-Supervised Domain Generalization (SSDG) which is crucial for real-world applications like automated healthcare. SSDG requires learning a cross-domain generalizable model when the given training data is only partially labelled. Empirical investigations reveal that the DG methods tend to underperform in SSDG settings, likely because they are unable to exploit the unlabelled data. Semi-supervised learning (SSL) shows improved but still inferior results compared to fully-supervised learning. A key challenge, faced by the best-performing SSL-based SSDG methods, is selecting accurate pseudo-labels under multiple domain shifts and reducing overfitting to source domains under limited labels. In this work, we propose new SSDG approach, which utilizes a novel uncertainty-guided pseudo-labelling with model averaging (UPLM). Our uncertainty-guided pseudo-labelling (UPL) uses model uncertainty to improve pseudo-labelling selection, addressing poor model calibration under multi-source unlabelled data. The UPL technique, enhanced by our novel model averaging (MA) strategy, mitigates overfitting to source domains with limited labels. Extensive experiments on key representative DG datasets suggest that our method demonstrates effectiveness against existing methods. Our code and chosen labelled data seeds are available on GitHub: https://github.com/Adnan-Khan7/UPLM
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2401.08815.pdf' target='_blank'>https://arxiv.org/pdf/2401.08815.pdf</a></span>   <span><a href='https://github.com/boschresearch/ALDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08815">Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent advances in large-scale diffusion models, little progress has been made on the layout-to-image (L2I) synthesis task. Current L2I models either suffer from poor editability via text or weak alignment between the generated image and the input layout. This limits their usability in practice. To mitigate this, we propose to integrate adversarial supervision into the conventional training pipeline of L2I diffusion models (ALDM). Specifically, we employ a segmentation-based discriminator which provides explicit feedback to the diffusion generator on the pixel-level alignment between the denoised image and the input layout. To encourage consistent adherence to the input layout over the sampling steps, we further introduce the multistep unrolling strategy. Instead of looking at a single timestep, we unroll a few steps recursively to imitate the inference process, and ask the discriminator to assess the alignment of denoised images with the layout over a certain time window. Our experiments show that ALDM enables layout faithfulness of the generated images, while allowing broad editability via text prompts. Moreover, we showcase its usefulness for practical applications: by synthesizing target distribution samples via text control, we improve domain generalization of semantic segmentation models by a large margin (~12 mIoU points).
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2401.03459.pdf' target='_blank'>https://arxiv.org/pdf/2401.03459.pdf</a></span>   <span><a href='https://github.com/guobaoxiao/BCLNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyang Miao, Guobao Xiao, Shiping Wang, Jun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03459">BCLNet: Bilateral Consensus Learning for Two-View Correspondence Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Correspondence pruning aims to establish reliable correspondences between two related images and recover relative camera motion. Existing approaches often employ a progressive strategy to handle the local and global contexts, with a prominent emphasis on transitioning from local to global, resulting in the neglect of interactions between different contexts. To tackle this issue, we propose a parallel context learning strategy that involves acquiring bilateral consensus for the two-view correspondence pruning task. In our approach, we design a distinctive self-attention block to capture global context and parallel process it with the established local context learning module, which enables us to simultaneously capture both local and global consensuses. By combining these local and global consensuses, we derive the required bilateral consensus. We also design a recalibration block, reducing the influence of erroneous consensus information and enhancing the robustness of the model. The culmination of our efforts is the Bilateral Consensus Learning Network (BCLNet), which efficiently estimates camera pose and identifies inliers (true correspondences). Extensive experiments results demonstrate that our network not only surpasses state-of-the-art methods on benchmark datasets but also showcases robust generalization abilities across various feature extraction techniques. Noteworthily, BCLNet obtains 3.98\% mAP5$^{\circ}$ gains over the second best method on unknown outdoor dataset, and obviously accelerates model training speed. The source code will be available at: https://github.com/guobaoxiao/BCLNet.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2401.02731.pdf' target='_blank'>https://arxiv.org/pdf/2401.02731.pdf</a></span>   <span><a href='https://github.com/wuhy68/Parameter-Efficient-MoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyuan Wu, Haisheng Zheng, Zhuolun He, Bei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02731">Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5. Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2401.02076.pdf' target='_blank'>https://arxiv.org/pdf/2401.02076.pdf</a></span>   <span><a href='https://github.com/SARIHUST/SAMMed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanhui Wang, Huaize Ye, Yi Xia, Xueyan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02076">Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to reduce domain shifts between domains to achieve promising performance on the unseen target domain, which has been widely practiced in medical image segmentation. Single-source domain generalization (SDG) is the most challenging setting that trains on only one source domain. Although existing methods have made considerable progress on SDG of medical image segmentation, the performances are still far from the applicable standards when faced with a relatively large domain shift. In this paper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve the ability of generalization. Specifically, we introduce a parallel framework, the source images are sent into the SAM module and normal segmentation module respectively. To reduce the calculation resources, we apply a merging strategy before sending images to the SAM module. We extract the bounding boxes from the segmentation module and send the refined version as prompts to the SAM module. We evaluate our model on a classic DG dataset and achieve competitive results compared to other state-of-the-art DG methods. Furthermore, We conducted a series of ablation experiments to prove the effectiveness of the proposed method. The code is publicly available at https://github.com/SARIHUST/SAMMed.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2312.16451.pdf' target='_blank'>https://arxiv.org/pdf/2312.16451.pdf</a></span>   <span><a href='https://github.com/excitedkid/vipaug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ingyun Lee, Wooju Lee, Hyun Myung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16451">Domain Generalization with Vital Phase Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks have shown remarkable performance in image classification. However, their performance significantly deteriorates with corrupted input data. Domain generalization methods have been proposed to train robust models against out-of-distribution data. Data augmentation in the frequency domain is one of such approaches that enable a model to learn phase features to establish domain-invariant representations. This approach changes the amplitudes of the input data while preserving the phases. However, using fixed phases leads to susceptibility to phase fluctuations because amplitudes and phase fluctuations commonly occur in out-of-distribution. In this study, to address this problem, we introduce an approach using finite variation of the phases of input data rather than maintaining fixed phases. Based on the assumption that the degree of domain-invariant features varies for each phase, we propose a method to distinguish phases based on this degree. In addition, we propose a method called vital phase augmentation (VIPAug) that applies the variation to the phases differently according to the degree of domain-invariant features of given phases. The model depends more on the vital phases that contain more domain-invariant features for attaining robustness to amplitude and phase fluctuations. We present experimental evaluations of our proposed approach, which exhibited improved performance for both clean and corrupted data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100 datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet datasets. Our code is available at https://github.com/excitedkid/vipaug.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2312.15612.pdf' target='_blank'>https://arxiv.org/pdf/2312.15612.pdf</a></span>   <span><a href='https://github.com/ViTAE-Transformer/APTv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Yang, Yingqi Deng, Yufei Xu, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15612">APTv2: Benchmarking Animal Pose Estimation and Tracking with a Large-scale Dataset and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Animal Pose Estimation and Tracking (APT) is a critical task in detecting and monitoring the keypoints of animals across a series of video frames, which is essential for understanding animal behavior. Past works relating to animals have primarily focused on either animal tracking or single-frame animal pose estimation only, neglecting the integration of both aspects. The absence of comprehensive APT datasets inhibits the progression and evaluation of animal pose estimation and tracking methods based on videos, thereby constraining their real-world applications. To fill this gap, we introduce APTv2, the pioneering large-scale benchmark for animal pose estimation and tracking. APTv2 comprises 2,749 video clips filtered and collected from 30 distinct animal species. Each video clip includes 15 frames, culminating in a total of 41,235 frames. Following meticulous manual annotation and stringent verification, we provide high-quality keypoint and tracking annotations for a total of 84,611 animal instances, split into easy and hard subsets based on the number of instances that exists in the frame. With APTv2 as the foundation, we establish a simple baseline method named \posetrackmethodname and provide benchmarks for representative models across three tracks: (1) single-frame animal pose estimation track to evaluate both intra- and inter-domain transfer learning performance, (2) low-data transfer and generalization track to evaluate the inter-species domain generalization performance, and (3) animal pose tracking track. Our experimental results deliver key empirical insights, demonstrating that APTv2 serves as a valuable benchmark for animal pose estimation and tracking. It also presents new challenges and opportunities for future research. The code and dataset are released at \href{https://github.com/ViTAE-Transformer/APTv2}{https://github.com/ViTAE-Transformer/APTv2}.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2312.15275.pdf' target='_blank'>https://arxiv.org/pdf/2312.15275.pdf</a></span>   <span><a href='https://github.com/LyesSaadSaoud/MARS-Object-Detection/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lyes Saad Saoud, Lakmal Seneviratne, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15275">MARS: Multi-Scale Adaptive Robotics Vision for Underwater Object Detection and Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater robotic vision encounters significant challenges, necessitating advanced solutions to enhance performance and adaptability. This paper presents MARS (Multi-Scale Adaptive Robotics Vision), a novel approach to underwater object detection tailored for diverse underwater scenarios. MARS integrates Residual Attention YOLOv3 with Domain-Adaptive Multi-Scale Attention (DAMSA) to enhance detection accuracy and adapt to different domains. During training, DAMSA introduces domain class-based attention, enabling the model to emphasize domain-specific features. Our comprehensive evaluation across various underwater datasets demonstrates MARS's performance. On the original dataset, MARS achieves a mean Average Precision (mAP) of 58.57\%, showcasing its proficiency in detecting critical underwater objects like echinus, starfish, holothurian, scallop, and waterweeds. This capability holds promise for applications in marine robotics, marine biology research, and environmental monitoring. Furthermore, MARS excels at mitigating domain shifts. On the augmented dataset, which incorporates all enhancements (+Domain +Residual+Channel Attention+Multi-Scale Attention), MARS achieves an mAP of 36.16\%. This result underscores its robustness and adaptability in recognizing objects and performing well across a range of underwater conditions. The source code for MARS is publicly available on GitHub at https://github.com/LyesSaadSaoud/MARS-Object-Detection/
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2312.12720.pdf' target='_blank'>https://arxiv.org/pdf/2312.12720.pdf</a></span>   <span><a href='https://github.com/gtzheng/AdvST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangtao Zheng, Mengdi Huai, Aidong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12720">AdvST: Revisiting Data Augmentations for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization (SDG) aims to train a robust model against unknown target domain shifts using data from a single source domain. Data augmentation has been proven an effective approach to SDG. However, the utility of standard augmentations, such as translate, or invert, has not been fully exploited in SDG; practically, these augmentations are used as a part of a data preprocessing procedure. Although it is intuitive to use many such augmentations to boost the robustness of a model to out-of-distribution domain shifts, we lack a principled approach to harvest the benefit brought from multiple these augmentations. Here, we conceptualize standard data augmentations with learnable parameters as semantics transformations that can manipulate certain semantics of a sample, such as the geometry or color of an image. Then, we propose Adversarial learning with Semantics Transformations (AdvST) that augments the source domain data with semantics transformations and learns a robust model with the augmented data. We theoretically show that AdvST essentially optimizes a distributionally robust optimization objective defined on a set of semantics distributions induced by the parameters of semantics transformations. We demonstrate that AdvST can produce samples that expand the coverage on target domain data. Compared with the state-of-the-art methods, AdvST, despite being a simple method, is surprisingly competitive and achieves the best average SDG performance on the Digits, PACS, and DomainNet datasets. Our code is available at https://github.com/gtzheng/AdvST.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2312.12133.pdf' target='_blank'>https://arxiv.org/pdf/2312.12133.pdf</a></span>   <span><a href='https://github.com/WoojuLee24/OA-DG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wooju Lee, Dasol Hong, Hyungtae Lim, Hyun Myung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12133">Object-Aware Domain Generalization for Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-domain generalization (S-DG) aims to generalize a model to unseen environments with a single-source domain. However, most S-DG approaches have been conducted in the field of classification. When these approaches are applied to object detection, the semantic features of some objects can be damaged, which can lead to imprecise object localization and misclassification. To address these problems, we propose an object-aware domain generalization (OA-DG) method for single-domain generalization in object detection. Our method consists of data augmentation and training strategy, which are called OA-Mix and OA-Loss, respectively. OA-Mix generates multi-domain data with multi-level transformation and object-aware mixing strategy. OA-Loss enables models to learn domain-invariant representations for objects and backgrounds from the original and OA-Mixed images. Our proposed method outperforms state-of-the-art works on standard benchmarks. Our code is available at https://github.com/WoojuLee24/OA-DG.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2312.12098.pdf' target='_blank'>https://arxiv.org/pdf/2312.12098.pdf</a></span>   <span><a href='https://github.com/dgist-cvlab/MultiDensityDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeyeul Kim, Jungwan Woo, Jeonghoon Kim, Sunghoon Im
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12098">Rethinking LiDAR Domain Generalization: Single Source as Multiple Density Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of LiDAR-based perception, significant strides have been made, yet domain generalization remains a substantial challenge. The performance often deteriorates when models are applied to unfamiliar datasets with different LiDAR sensors or deployed in new environments, primarily due to variations in point cloud density distributions. To tackle this challenge, we propose a Density Discriminative Feature Embedding (DDFE) module, capitalizing on the observation that a single source LiDAR point cloud encompasses a spectrum of densities. The DDFE module is meticulously designed to extract density-specific features within a single source domain, facilitating the recognition of objects sharing similar density characteristics across different LiDAR sensors. In addition, we introduce a simple yet effective density augmentation technique aimed at expanding the spectrum of density in source data, thereby enhancing the capabilities of the DDFE. Our DDFE stands out as a versatile and lightweight domain generalization module. It can be seamlessly integrated into various 3D backbone networks, where it has demonstrated superior performance over current state-of-the-art domain generalization methods. Code is available at https://github.com/dgist-cvlab/MultiDensityDG.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2312.09254.pdf' target='_blank'>https://arxiv.org/pdf/2312.09254.pdf</a></span>   <span><a href='https://github.com/bartn8/vppdc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Bartolomei, Matteo Poggi, Andrea Conti, Fabio Tosi, Stefano Mattoccia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09254">Revisiting Depth Completion from a Stereo Matching Perspective for Cross-domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a new framework for depth completion robust against domain-shifting issues. It exploits the generalization capability of modern stereo networks to face depth completion, by processing fictitious stereo pairs obtained through a virtual pattern projection paradigm. Any stereo network or traditional stereo matcher can be seamlessly plugged into our framework, allowing for the deployment of a virtual stereo setup that is future-proof against advancement in the stereo field. Exhaustive experiments on cross-domain generalization support our claims. Hence, we argue that our framework can help depth completion to reach new deployment scenarios.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2312.07577.pdf' target='_blank'>https://arxiv.org/pdf/2312.07577.pdf</a></span>   <span><a href='https://github.com/mlfoundations/tableshift' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Josh Gardner, Zoran Popovic, Ludwig Schmidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07577">Benchmarking Distribution Shift in Tabular Data with TableShift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robustness to distribution shift has become a growing concern for text and image models as they transition from research subjects to deployment in the real world. However, high-quality benchmarks for distribution shift in tabular machine learning tasks are still lacking despite the widespread real-world use of tabular data and differences in the models used for tabular data in comparison to text and images. As a consequence, the robustness of tabular models to distribution shift is poorly understood. To address this issue, we introduce TableShift, a distribution shift benchmark for tabular data. TableShift contains 15 binary classification tasks in total, each with an associated shift, and includes a diverse set of data sources, prediction targets, and distribution shifts. The benchmark covers domains including finance, education, public policy, healthcare, and civic participation, and is accessible using only a few lines of Python code via the TableShift API. We conduct a large-scale study comparing several state-of-the-art tabular data models alongside robust learning and domain generalization methods on the benchmark tasks. Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) domain robustness methods can reduce shift gaps but at the cost of reduced ID accuracy; (3) a strong relationship between shift gap (difference between ID and OOD performance) and shifts in the label distribution.
  The benchmark data, Python package, model implementations, and more information about TableShift are available at https://github.com/mlfoundations/tableshift and https://tableshift.org .
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2312.06401.pdf' target='_blank'>https://arxiv.org/pdf/2312.06401.pdf</a></span>   <span><a href='https://github.com/EricTan7/TGP-T' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Tan, Jun Li, Yizhuang Zhou, Jun Wan, Zhen Lei, Xiangyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06401">Compound Text-Guided Prompt Tuning via Image-Adaptive Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2312.06275.pdf' target='_blank'>https://arxiv.org/pdf/2312.06275.pdf</a></span>   <span><a href='https://github.com/multimodallearning/DG-TTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Weihsbach, Christian N. Kruse, Alexander Bigalke, Mattias P. Heinrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06275">DG-TTA: Out-of-domain Medical Image Segmentation through Augmentation and Descriptor-driven Domain Generalization and Test-Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: Applying pre-trained medical deep learning segmentation models on out-of-domain images often yields predictions of insufficient quality. In this study, we propose to use a powerful generalizing descriptor along with augmentation to enable domain-generalized pre-training and test-time adaptation, achieving high-quality segmentation in unseen domains.
  Materials and Methods: In this retrospective study five different publicly available datasets (2012 to 2022) including 3D CT and MRI images are used to evaluate segmentation performance in out-of-domain scenarios. The settings include abdominal, spine, and cardiac imaging. The data is randomly split into training and test samples. Domain-generalized pre-training on source data is used to obtain the best initial performance in the target domain. We introduce the combination of the generalizing SSC descriptor and GIN intensity augmentation for optimal generalization. Segmentation results are subsequently optimized at test time, where we propose to adapt the pre-trained models for every unseen scan with a consistency scheme using the same augmentation-descriptor combination. The segmentation is evaluated using Dice similarity and Hausdorff distance and the significance of improvements is tested with the Wilcoxon signed-rank test.
  Results: The proposed generalized pre-training and subsequent test-time adaptation improves model performance significantly in CT to MRI cross-domain prediction for abdominal (+46.2% and +28.2% Dice), spine (+72.9%), and cardiac (+14.2% and +55.7% Dice) scenarios (p<0.001).
  Conclusion: Our method enables optimal, independent usage of medical image source and target data and bridges domain gaps successfully with a compact and efficient methodology. Open-source code available at: https://github.com/multimodallearning/DG-TTA
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2312.05525.pdf' target='_blank'>https://arxiv.org/pdf/2312.05525.pdf</a></span>   <span><a href='https://github.com/lishuhuai527/COCO-UniHuman' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Jin, Shuhuai Li, Tong Li, Wentao Liu, Chen Qian, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05525">You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-centric perception (e.g. detection, segmentation, pose estimation, and attribute analysis) is a long-standing problem for computer vision. This paper introduces a unified and versatile framework (HQNet) for single-stage multi-person multi-task human-centric perception (HCP). Our approach centers on learning a unified human query representation, denoted as Human Query, which captures intricate instance-level features for individual persons and disentangles complex multi-person scenarios. Although different HCP tasks have been well-studied individually, single-stage multi-task learning of HCP tasks has not been fully exploited in the literature due to the absence of a comprehensive benchmark dataset. To address this gap, we propose COCO-UniHuman benchmark to enable model development and comprehensive evaluation. Experimental results demonstrate the proposed method's state-of-the-art performance among multi-task HCP models and its competitive performance compared to task-specific HCP models. Moreover, our experiments underscore Human Query's adaptability to new HCP tasks, thus demonstrating its robust generalization capability. Codes and data are available at https://github.com/lishuhuai527/COCO-UniHuman.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2312.01850.pdf' target='_blank'>https://arxiv.org/pdf/2312.01850.pdf</a></span>   <span><a href='https://github.com/JNiemeijer/DIDEX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Niemeijer, Manuel Schwonberg, Jan-Aike TermÃ¶hlen, Nico M. Schmidt, Tim Fingscheidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01850">Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When models, e.g., for semantic segmentation, are applied to images that are vastly different from training data, the performance will drop significantly. Domain adaptation methods try to overcome this issue, but need samples from the target domain. However, this might not always be feasible for various reasons and therefore domain generalization methods are useful as they do not require any target data. We present a new diffusion-based domain extension (DIDEX) method and employ a diffusion model to generate a pseudo-target domain with diverse text prompts. In contrast to existing methods, this allows to control the style and content of the generated images and to introduce a high diversity. In a second step, we train a generalizing model by adapting towards this pseudo-target domain. We outperform previous approaches by a large margin across various datasets and architectures without using any real data. For the generalization from GTA5, we improve state-of-the-art mIoU performance by 3.8% absolute on average and for SYNTHIA by 11.8% absolute, marking a big step for the generalization performance on these benchmarks. Code is available at https://github.com/JNiemeijer/DIDEX
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2311.14494.pdf' target='_blank'>https://arxiv.org/pdf/2311.14494.pdf</a></span>   <span><a href='https://github.com/WU-CVGL/MVControl/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14494">MVControl: Adding Conditional Control to Multi-view Diffusion for Controllable Text-to-3D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MVControl, a novel neural network architecture that enhances existing pre-trained multi-view 2D diffusion models by incorporating additional input conditions, e.g. edge maps. Our approach enables the generation of controllable multi-view images and view-consistent 3D content. To achieve controllable multi-view image generation, we leverage MVDream as our base model, and train a new neural network module as additional plugin for end-to-end task-specific condition learning. To precisely control the shapes and views of generated images, we innovatively propose a new conditioning mechanism that predicts an embedding encapsulating the input spatial and view conditions, which is then injected to the network globally. Once MVControl is trained, score-distillation (SDS) loss based optimization can be performed to generate 3D content, in which process we propose to use a hybrid diffusion prior. The hybrid prior relies on a pre-trained Stable-Diffusion network and our trained MVControl for additional guidance. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content. Code available at https://github.com/WU-CVGL/MVControl/.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2311.14315.pdf' target='_blank'>https://arxiv.org/pdf/2311.14315.pdf</a></span>   <span><a href='https://github.com/less-and-less-bugs/RDCM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Liu, Wenya Wang, Hao Sun, Anderson Rocha, Haoliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14315">Robust Domain Misinformation Detection via Multi-modal Feature Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social media misinformation harms individuals and societies and is potentialized by fast-growing multi-modal content (i.e., texts and images), which accounts for higher "credibility" than text-only news pieces. Although existing supervised misinformation detection methods have obtained acceptable performances in key setups, they may require large amounts of labeled data from various events, which can be time-consuming and tedious. In turn, directly training a model by leveraging a publicly available dataset may fail to generalize due to domain shifts between the training data (a.k.a. source domains) and the data from target domains. Most prior work on domain shift focuses on a single modality (e.g., text modality) and ignores the scenario where sufficient unlabeled target domain data may not be readily available in an early stage. The lack of data often happens due to the dynamic propagation trend (i.e., the number of posts related to fake news increases slowly before catching the public attention). We propose a novel robust domain and cross-modal approach (\textbf{RDCM}) for multi-modal misinformation detection. It reduces the domain shift by aligning the joint distribution of textual and visual modalities through an inter-domain alignment module and bridges the semantic gap between both modalities through a cross-modality alignment module. We also propose a framework that simultaneously considers application scenarios of domain generalization (in which the target domain data is unavailable) and domain adaptation (in which unlabeled target domain data is available). Evaluation results on two public multi-modal misinformation detection datasets (Pheme and Twitter Datasets) evince the superiority of the proposed model. The formal implementation of this paper can be found in this link: https://github.com/less-and-less-bugs/RDCM
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2311.13928.pdf' target='_blank'>https://arxiv.org/pdf/2311.13928.pdf</a></span>   <span><a href='https://github.com/MetaVisionLab/PE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luojun Lin, Zhifeng Shen, Zhishu Sun, Yuanlong Yu, Lei Zhang, Weijie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13928">Parameter Exchange for Robust Dynamic Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agnostic domain shift is the main reason of model degradation on the unknown target domains, which brings an urgent need to develop Domain Generalization (DG). Recent advances at DG use dynamic networks to achieve training-free adaptation on the unknown target domains, termed Dynamic Domain Generalization (DDG), which compensates for the lack of self-adaptability in static models with fixed weights. The parameters of dynamic networks can be decoupled into a static and a dynamic component, which are designed to learn domain-invariant and domain-specific features, respectively. Based on the existing arts, in this work, we try to push the limits of DDG by disentangling the static and dynamic components more thoroughly from an optimization perspective. Our main consideration is that we can enable the static component to learn domain-invariant features more comprehensively by augmenting the domain-specific information. As a result, the more comprehensive domain-invariant features learned by the static component can then enforce the dynamic component to focus more on learning adaptive domain-specific features. To this end, we propose a simple yet effective Parameter Exchange (PE) method to perturb the combination between the static and dynamic components. We optimize the model using the gradients from both the perturbed and non-perturbed feed-forward jointly to implicitly achieve the aforementioned disentanglement. In this way, the two components can be optimized in a mutually-beneficial manner, which can resist the agnostic domain shifts and improve the self-adaptability on the unknown target domain. Extensive experiments show that PE can be easily plugged into existing dynamic networks to improve their generalization ability without bells and whistles.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2311.12327.pdf' target='_blank'>https://arxiv.org/pdf/2311.12327.pdf</a></span>   <span><a href='https://github.com/AnonymGiant/ViLaM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Yang, Lijian Xu, Hao Sun, Hongsheng Li, Shaoting Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12327">Enhancing Visual Grounding and Generalization: A Multi-Task Cycle Training Approach for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual grounding (VG) occupies a pivotal position in multi-modality vision-language models. In this study, we propose ViLaM, a large multi-modality model, that supports multi-tasks of VG using the cycle training strategy, with abundant interaction instructions. The cycle training between referring expression generation (REG) and referring expression comprehension (REC) is introduced. It enhances the consistency between visual location and referring expressions, and addresses the need for high-quality, multi-tasks VG datasets. Moreover, multi-tasks of VG are promoted in our model, contributed by the cycle training strategy. The multi-tasks in REC encompass a range of granularities, from region-level to pixel-level, which include referring bbox detection, referring keypoints detection, and referring image segmentation. In REG, referring region classification determines the fine-grained category of the target, while referring region captioning generates a comprehensive description. Meanwhile, all tasks participate in the joint training, synergistically enhancing one another and collectively improving the overall performance of the model. Furthermore, leveraging the capabilities of large language models, ViLaM extends a wide range of instructions, thereby significantly enhancing its generalization and interaction potentials. Extensive public datasets corroborate the superior capabilities of our model in VG with muti-tasks. Additionally, validating its robust generalization, ViLaM is validated under open-set and few-shot scenarios. Especially in the medical field, our model demonstrates cross-domain robust generalization capabilities. Furthermore, we contribute a VG dataset, especially with multi-tasks. To support and encourage the community focused on VG, we have made both the dataset and our code public: https://github.com/AnonymGiant/ViLaM.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2311.10339.pdf' target='_blank'>https://arxiv.org/pdf/2311.10339.pdf</a></span>   <span><a href='https://github.com/AIRLABkhu/A2XP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Geunhyeok Yu, Hyoseok Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10339">A2XP: Towards Private Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Neural Networks (DNNs) have become pivotal in various fields, especially in computer vision, outperforming previous methodologies. A critical challenge in their deployment is the bias inherent in data across different domains, such as image style and environmental conditions, leading to domain gaps. This necessitates techniques for learning general representations from biased training data, known as domain generalization. This paper presents Attend to eXpert Prompts (A2XP), a novel approach for domain generalization that preserves the privacy and integrity of the network architecture. A2XP consists of two phases: Expert Adaptation and Domain Generalization. In the first phase, prompts for each source domain are optimized to guide the model towards the optimal direction. In the second phase, two embedder networks are trained to effectively amalgamate these expert prompts, aiming for an optimal output. Our extensive experiments demonstrate that A2XP achieves state-of-the-art results over existing non-private domain generalization methods. The experimental results validate that the proposed approach not only tackles the domain generalization challenge in DNNs but also offers a privacy-preserving, efficient solution to the broader field of computer vision.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2311.08400.pdf' target='_blank'>https://arxiv.org/pdf/2311.08400.pdf</a></span>   <span><a href='https://github.com/bytedance/OmniScient-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihang Yu, Xiaohui Shen, Liang-Chieh Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08400">Towards Open-Ended Visual Recognition with Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localizing and recognizing objects in the open-ended physical world poses a long-standing challenge within the domain of machine perception. Recent methods have endeavored to address the issue by employing a class-agnostic mask (or box) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP) using pre-extracted text embeddings. However, it is worth noting that these open-vocabulary recognition models still exhibit limitations in practical applications. On one hand, they rely on the provision of class names during testing, where the recognition performance heavily depends on this predefined set of semantic classes by users. On the other hand, when training with multiple datasets, human intervention is required to alleviate the label definition conflict between them. In this paper, we introduce the OmniScient Model (OSM), a novel Large Language Model (LLM) based mask classifier, as a straightforward and effective solution to the aforementioned challenges. Specifically, OSM predicts class labels in a generative manner, thus removing the supply of class names during both training and testing. It also enables cross-dataset training without any human interference, exhibiting robust generalization capabilities due to the world knowledge acquired from the LLM. By combining OSM with an off-the-shelf mask proposal model, we present promising results on various benchmarks, and demonstrate its effectiveness in handling novel concepts. Code/model are available at https://github.com/bytedance/OmniScient-Model.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2311.02583.pdf' target='_blank'>https://arxiv.org/pdf/2311.02583.pdf</a></span>   <span><a href='https://github.com/yezanting/FSDA-DG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zanting Ye, Ke Wang, Wenbing Lv, Qianjin Feng, Lijun Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02583">FSDA-DG: Improving Cross-Domain Generalizability of Medical Image Segmentation with Few Source Domain Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based medical image segmentation faces significant challenges arising from limited labeled data and domain shifts. While prior approaches have primarily addressed these issues independently, their simultaneous occurrence is common in medical imaging. A method that generalizes to unseen domains using only minimal annotations offers significant practical value due to reduced data annotation and development costs. In pursuit of this goal, we propose FSDA-DG, a novel solution to improve cross-domain generalizability of medical image segmentation with few single-source domain annotations. Specifically, our approach introduces semantics-guided semi-supervised data augmentation. This method divides images into global broad regions and semantics-guided local regions, and applies distinct augmentation strategies to enrich data distribution. Within this framework, both labeled and unlabeled data are transformed into extensive domain knowledge while preserving domain-invariant semantic information. Additionally, FSDA-DG employs a multi-decoder U-Net pipeline semi-supervised learning (SSL) network to improve domain-invariant representation learning through consistent prior assumption across multiple perturbations. By integrating data-level and model-level designs, FSDA-DG achieves superior performance compared to state-of-the-art methods in two challenging single domain generalization (SDG) tasks with limited annotations. The code is publicly available at https://github.com/yezanting/FSDA-DG.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2311.02236.pdf' target='_blank'>https://arxiv.org/pdf/2311.02236.pdf</a></span>   <span><a href='https://github.com/mit-ll/robust-vision-language-finetuning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Vogt-Lowell, Noah Lee, Theodoros Tsiligkaridis, Marc Vaillant
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02236">Robust Fine-Tuning of Vision-Language Models for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning enables the sharing of common knowledge among models for a variety of downstream tasks, but traditional methods suffer in limited training data settings and produce narrow models incapable of effectively generalizing under distribution shifts. Foundation models have recently demonstrated impressive zero-shot inference capabilities and robustness under distribution shifts. However, zero-shot evaluation for these models has been predominantly confined to benchmarks with simple distribution shifts, limiting our understanding of their effectiveness under the more realistic shifts found in practice. Moreover, common fine-tuning methods for these models have yet to be evaluated against vision models in few-shot scenarios where training data is limited. To address these gaps, we present a new recipe for few-shot fine-tuning of the popular vision-language foundation model CLIP and evaluate its performance on challenging benchmark datasets with realistic distribution shifts from the WILDS collection. Our experimentation demonstrates that, while zero-shot CLIP fails to match performance of trained vision models on more complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only counterparts in terms of in-distribution and out-of-distribution accuracy at all levels of training data availability. This provides a strong incentive for adoption of foundation models within few-shot learning applications operating with real-world data. Code is available at https://github.com/mit-ll/robust-vision-language-finetuning
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2310.20638.pdf' target='_blank'>https://arxiv.org/pdf/2310.20638.pdf</a></span>   <span><a href='https://github.com/Vaibhav-Khamankar/FuseStyle' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaibhav Khamankar, Sutanu Bera, Saumik Bhattacharya, Debashis Sen, Prabir Kumar Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20638">Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Histopathological images are essential for medical diagnosis and treatment planning, but interpreting them accurately using machine learning can be challenging due to variations in tissue preparation, staining and imaging protocols. Domain generalization aims to address such limitations by enabling the learning models to generalize to new datasets or populations. Style transfer-based data augmentation is an emerging technique that can be used to improve the generalizability of machine learning models for histopathological images. However, existing style transfer-based methods can be computationally expensive, and they rely on artistic styles, which can negatively impact model accuracy. In this study, we propose a feature domain style mixing technique that uses adaptive instance normalization to generate style-augmented versions of images. We compare our proposed method with existing style transfer-based data augmentation methods and found that it performs similarly or better, despite requiring less computation and time. Our results demonstrate the potential of feature domain statistics mixing in the generalization of learning models for histopathological image analysis.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2310.20271.pdf' target='_blank'>https://arxiv.org/pdf/2310.20271.pdf</a></span>   <span><a href='https://github.com/WenRuxue/DeTTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruxue Wen, Hangjie Yuan, Dong Ni, Wenbo Xiao, Yaoyao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.20271">From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In medical image segmentation, domain generalization poses a significant challenge due to domain shifts caused by variations in data acquisition devices and other factors. These shifts are particularly pronounced in the most common scenario, which involves only single-source domain data due to privacy concerns. To address this, we draw inspiration from the self-supervised learning paradigm that effectively discourages overfitting to the source domain. We propose the Denoising Y-Net (DeY-Net), a novel approach incorporating an auxiliary denoising decoder into the basic U-Net architecture. The auxiliary decoder aims to perform denoising training, augmenting the domain-invariant representation that facilitates domain generalization. Furthermore, this paradigm provides the potential to utilize unlabeled data. Building upon denoising training, we propose Denoising Test Time Adaptation (DeTTA) that further: (i) adapts the model to the target domain in a sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive experiments conducted on widely-adopted liver segmentation benchmarks demonstrate significant domain generalization improvements over our baseline and state-of-the-art results compared to other methods. Code is available at https://github.com/WenRuxue/DeTTA.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2310.19795.pdf' target='_blank'>https://arxiv.org/pdf/2310.19795.pdf</a></span>   <span><a href='https://github.com/donghao51/SimMMDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Dong, Ismail Nejjar, Han Sun, Eleni Chatzi, Olga Fink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19795">SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, achieving domain generalization (DG) presents significant challenges as models are required to generalize to unknown target distributions. Generalizing to unseen multi-modal distributions poses even greater difficulties due to the distinct properties exhibited by different modalities. To overcome the challenges of achieving domain generalization in multi-modal scenarios, we propose SimMMDG, a simple yet effective multi-modal DG framework. We argue that mapping features from different modalities into the same embedding space impedes model generalization. To address this, we propose splitting the features within each modality into modality-specific and modality-shared components. We employ supervised contrastive learning on the modality-shared features to ensure they possess joint properties and impose distance constraints on modality-specific features to promote diversity. In addition, we introduce a cross-modal translation module to regularize the learned features, which can also be used for missing-modality generalization. We demonstrate that our framework is theoretically well-supported and achieves strong performance in multi-modal DG on the EPIC-Kitchens dataset and the novel Human-Animal-Cartoon (HAC) dataset introduced in this paper. Our source code and HAC dataset are available at https://github.com/donghao51/SimMMDG.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2310.18893.pdf' target='_blank'>https://arxiv.org/pdf/2310.18893.pdf</a></span>   <span><a href='https://github.com/google-research/google-research/tree/master/ev3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Ding, Masrour Zoghi, Guy Tennenholtz, Maryam Karimzadehgan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18893">Ever Evolving Evaluator (EV3): Towards Flexible and Reliable Meta-Optimization for Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce EV3, a novel meta-optimization framework designed to efficiently train scalable machine learning models through an intuitive explore-assess-adapt protocol. In each iteration of EV3, we explore various model parameter updates, assess them using pertinent evaluation methods, and then adapt the model based on the optimal updates and previous progress history. EV3 offers substantial flexibility without imposing stringent constraints like differentiability on the key objectives relevant to the tasks of interest, allowing for exploratory updates with intentionally-biased gradients and through a diversity of losses and optimizers. Additionally, the assessment phase provides reliable safety controls to ensure robust generalization, and can dynamically prioritize tasks in scenarios with multiple objectives. With inspiration drawn from evolutionary algorithms, meta-learning, and neural architecture search, we investigate an application of EV3 to knowledge distillation. Our experimental results illustrate EV3's capability to safely explore the modeling landscape, while hinting at its potential applicability across numerous domains due to its inherent flexibility and adaptability. Finally, we provide a JAX implementation of EV3, along with source code for experiments, available at: https://github.com/google-research/google-research/tree/master/ev3.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2310.17942.pdf' target='_blank'>https://arxiv.org/pdf/2310.17942.pdf</a></span>   <span><a href='https://github.com/KunyuLin/STDN/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun-Yu Lin, Jia-Run Du, Yipeng Gao, Jiaming Zhou, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17942">Diversifying Spatial-Temporal Perception for Video Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video domain generalization aims to learn generalizable video classification models for unseen target domains by training in a source domain. A critical challenge of video domain generalization is to defend against the heavy reliance on domain-specific cues extracted from the source domain when recognizing target videos. To this end, we propose to perceive diverse spatial-temporal cues in videos, aiming to discover potential domain-invariant cues in addition to domain-specific cues. We contribute a novel model named Spatial-Temporal Diversification Network (STDN), which improves the diversity from both space and time dimensions of video data. First, our STDN proposes to discover various types of spatial cues within individual frames by spatial grouping. Then, our STDN proposes to explicitly model spatial-temporal dependencies between video contents at multiple space-time scales by spatial-temporal relation modeling. Extensive experiments on three benchmarks of different types demonstrate the effectiveness and versatility of our approach.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2310.12541.pdf' target='_blank'>https://arxiv.org/pdf/2310.12541.pdf</a></span>   <span><a href='https://github.com/FeiLiu36/LLM4MOEA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12541">Large Language Model for Multi-objective Evolutionary Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the search operators need a carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well on new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose a new version of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on different test benchmarks show that our proposed method can achieve competitive performance with widely used MOEAs. It is also promising to see the operator only learned from a few instances can have robust generalization performance on unseen problems with quite different patterns and settings. The results reveal the potential benefits of using pre-trained LLMs in the design of MOEAs.To foster reproducibility and accessibility, the source code is https://github.com/FeiLiu36/LLM4MOEA.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2310.11346.pdf' target='_blank'>https://arxiv.org/pdf/2310.11346.pdf</a></span>   <span><a href='https://github.com/EnVision-Research/Generalizable-BEV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Lu, Yunpeng Zhang, Qing Lian, Dalong Du, Yingcong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11346">Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting objects in 3D space using multiple cameras, known as Multi-Camera 3D Object Detection (MC3D-Det), has gained prominence with the advent of bird's-eye view (BEV) approaches. However, these methods often struggle when faced with unfamiliar testing environments due to the lack of diverse training data encompassing various viewpoints and environments. To address this, we propose a novel method that aligns 3D detection with 2D camera plane results, ensuring consistent and accurate detections. Our framework, anchored in perspective debiasing, helps the learning of features resilient to domain shifts. In our approach, we render diverse view maps from BEV features and rectify the perspective bias of these maps, leveraging implicit foreground volumes to bridge the camera and BEV planes. This two-step process promotes the learning of perspective- and context-independent features, crucial for accurate object detection across varying viewpoints, camera parameters, and environmental conditions. Notably, our model-agnostic approach preserves the original network structure without incurring additional inference costs, facilitating seamless integration across various models and simplifying deployment. Furthermore, we also show our approach achieves satisfactory results in real data when trained only with virtual datasets, eliminating the need for real scene annotations. Experimental results on both Domain Generalization (DG) and Unsupervised Domain Adaptation (UDA) clearly demonstrate its effectiveness. The codes are available at https://github.com/EnVision-Research/Generalizable-BEV.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2310.11320.pdf' target='_blank'>https://arxiv.org/pdf/2310.11320.pdf</a></span>   <span><a href='https://github.com/xmed-lab/GenericSSL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Wang, Xiaomeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11320">Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Volume-wise labeling in 3D medical images is a time-consuming task that requires expertise. As a result, there is growing interest in using semi-supervised learning (SSL) techniques to train models with limited labeled data. However, the challenges and practical applications extend beyond SSL to settings such as unsupervised domain adaptation (UDA) and semi-supervised domain generalization (SemiDG). This work aims to develop a generic SSL framework that can handle all three settings. We identify two main obstacles to achieving this goal in the existing SSL framework: 1) the weakness of capturing distribution-invariant features; and 2) the tendency for unlabeled data to be overwhelmed by labeled data, leading to over-fitting to the labeled data during training. To address these issues, we propose an Aggregating & Decoupling framework. The aggregating part consists of a Diffusion encoder that constructs a common knowledge set by extracting distribution-invariant features from aggregated information from multiple distributions/domains. The decoupling part consists of three decoders that decouple the training process with labeled and unlabeled data, thus avoiding over-fitting to labeled data, specific domains and classes. We evaluate our proposed framework on four benchmark datasets for SSL, Class-imbalanced SSL, UDA and SemiDG. The results showcase notable improvements compared to state-of-the-art methods across all four settings, indicating the potential of our framework to tackle more challenging SSL scenarios. Code and models are available at: https://github.com/xmed-lab/GenericSSL.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2310.10908.pdf' target='_blank'>https://arxiv.org/pdf/2310.10908.pdf</a></span>   <span><a href='https://github.com/qiuzh20/EMoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Qiu, Zeyu Huang, Jie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10908">Unlocking Emergent Modularity in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modular Neural Networks (MNNs) demonstrate various advantages over monolithic models. Existing MNNs are generally $\textit{explicit}$: their modular architectures are pre-defined, with individual modules expected to implement distinct functions. Recent works reveal that there exists $\textit{implicit}$ modularity in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures spontaneously exhibit during the early pre-training phase. Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized. In this work, focusing on unlocking the emergent modularity in LMs, we showcase that standard LMs could be fine-tuned as their Mixture-of-Expert (MoEs) counterparts without introducing any extra parameters. Such MoEs are derived from emergent modularity and are referred to as Emergent MoEs (EMoE). Our experiments demonstrate that fine-tuning EMoE effectively improves downstream in-domain and out-of-domain generalization compared with vanilla fine-tuning. Our analysis and ablation studies further illustrate that it is robust to various configurations and can scale up to Large Language Models (i.e., Llama2-7B and Llama-30B). Code is available at https://github.com/qiuzh20/EMoE.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2310.10402.pdf' target='_blank'>https://arxiv.org/pdf/2310.10402.pdf</a></span>   <span><a href='https://github.com/BAAI-DCAI/Training-Data-Synthesis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhao Yuan, Jie Zhang, Shuyang Sun, Philip Torr, Bo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10402">Real-Fake: Effective Training Data Synthesis Through Distribution Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evaluation, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by current methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, we analyze the principles underlying training data synthesis for supervised learning and elucidate a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy. Through extensive experiments, we demonstrate the effectiveness of our synthetic data across diverse image classification tasks, both as a replacement for and augmentation to real datasets, while also benefits such as out-of-distribution generalization, privacy preservation, and scalability. Specifically, we achieve 70.9% top1 classification accuracy on ImageNet1K when training solely with synthetic data equivalent to 1 X the original real data size, which increases to 76.0% when scaling up to 10 X synthetic data.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2310.10008.pdf' target='_blank'>https://arxiv.org/pdf/2310.10008.pdf</a></span>   <span><a href='https://github.com/invictus717/UniDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyuan Zhang, Kaixiong Gong, Xiaohan Ding, Kaipeng Zhang, Fangrui Lv, Kurt Keutzer, Xiangyu Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10008">Towards Unified and Effective Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose $\textbf{UniDG}$, a novel and $\textbf{Uni}$fied framework for $\textbf{D}$omain $\textbf{G}$eneralization that is capable of significantly enhancing the out-of-distribution generalization performance of foundation models regardless of their architectures. The core idea of UniDG is to finetune models during the inference stage, which saves the cost of iterative training. Specifically, we encourage models to learn the distribution of test data in an unsupervised manner and impose a penalty regarding the updating step of model parameters. The penalty term can effectively reduce the catastrophic forgetting issue as we would like to maximally preserve the valuable knowledge in the original model. Empirically, across 12 visual backbones, including CNN-, MLP-, and Transformer-based models, ranging from 1.89M to 303M parameters, UniDG shows an average accuracy improvement of +5.4% on DomainBed. These performance results demonstrate the superiority and versatility of UniDG. The code is publicly available at https://github.com/invictus717/UniDG
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2310.06670.pdf' target='_blank'>https://arxiv.org/pdf/2310.06670.pdf</a></span>   <span><a href='https://github.com/Masseeh/DCAug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Masih Aminbeidokhti, Fidel A. Guerrero PeÃ±a, Heitor Rapela Medeiros, Thomas Dubail, Eric Granger, Marco Pedersoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06670">Domain Generalization by Rejecting Extreme Augmentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is one of the most effective techniques for regularizing deep learning models and improving their recognition performance in a variety of tasks and domains. However, this holds for standard in-domain settings, in which the training and test data follow the same distribution. For the out-of-domain case, where the test data follow a different and unknown distribution, the best recipe for data augmentation is unclear. In this paper, we show that for out-of-domain and domain generalization settings, data augmentation can provide a conspicuous and robust improvement in performance. To do that, we propose a simple training procedure: (i) use uniform sampling on standard data augmentation transformations; (ii) increase the strength transformations to account for the higher data variance expected when working out-of-domain, and (iii) devise a new reward function to reject extreme transformations that can harm the training. With this procedure, our data augmentation scheme achieves a level of accuracy that is comparable to or better than state-of-the-art methods on benchmark domain generalization datasets. Code: \url{https://github.com/Masseeh/DCAug}
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2310.06670.pdf' target='_blank'>https://arxiv.org/pdf/2310.06670.pdf</a></span>   <span><a href='https://github.com/Masseeh/DCAug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Masih Aminbeidokhti, Fidel A. Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Eric Granger, Marco Pedersoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06670">Domain Generalization by Rejecting Extreme Augmentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is one of the most effective techniques for regularizing deep learning models and improving their recognition performance in a variety of tasks and domains. However, this holds for standard in-domain settings, in which the training and test data follow the same distribution. For the out-of-domain case, where the test data follow a different and unknown distribution, the best recipe for data augmentation is unclear. In this paper, we show that for out-of-domain and domain generalization settings, data augmentation can provide a conspicuous and robust improvement in performance. To do that, we propose a simple training procedure: (i) use uniform sampling on standard data augmentation transformations; (ii) increase the strength transformations to account for the higher data variance expected when working out-of-domain, and (iii) devise a new reward function to reject extreme transformations that can harm the training. With this procedure, our data augmentation scheme achieves a level of accuracy that is comparable to or better than state-of-the-art methods on benchmark domain generalization datasets. Code: https://github.com/Masseeh/DCAug
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2310.04706.pdf' target='_blank'>https://arxiv.org/pdf/2310.04706.pdf</a></span>   <span><a href='https://github.com/ZexuSun/OILCA-NeurIPS23' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowei He, Zexu Sun, Jinxin Liu, Shuai Zhang, Xu Chen, Chen Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04706">Offline Imitation Learning with Variational Counterfactual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In offline imitation learning (IL), an agent aims to learn an optimal expert behavior policy without additional online environment interactions. However, in many real-world scenarios, such as robotics manipulation, the offline dataset is collected from suboptimal behaviors without rewards. Due to the scarce expert data, the agents usually suffer from simply memorizing poor trajectories and are vulnerable to variations in the environments, lacking the capability of generalizing to new environments. To automatically generate high-quality expert data and improve the generalization ability of the agent, we propose a framework named \underline{O}ffline \underline{I}mitation \underline{L}earning with \underline{C}ounterfactual data \underline{A}ugmentation (OILCA) by doing counterfactual inference. In particular, we leverage identifiable variational autoencoder to generate \textit{counterfactual} samples for expert data augmentation. We theoretically analyze the influence of the generated expert data and the improvement of generalization. Moreover, we conduct extensive experiments to demonstrate that our approach significantly outperforms various baselines on both \textsc{DeepMind Control Suite} benchmark for in-distribution performance and \textsc{CausalWorld} benchmark for out-of-distribution generalization. Our code is available at \url{https://github.com/ZexuSun/OILCA-NeurIPS23}.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2310.04539.pdf' target='_blank'>https://arxiv.org/pdf/2310.04539.pdf</a></span>   <span><a href='https://github.com/TrustMLRG/AdvCertainty' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minxing Zhang, Michael Backes, Xiao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04539">Generating Less Certain Adversarial Examples Improves Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper revisits the robust overfitting phenomenon of adversarial training. Observing that models with better robust generalization performance are less certain in predicting adversarially generated training inputs, we argue that overconfidence in predicting adversarial examples is a potential cause. Therefore, we hypothesize that generating less certain adversarial examples improves robust generalization, and propose a formal definition of adversarial certainty that captures the variance of the model's predicted logits on adversarial examples. Our theoretical analysis of synthetic distributions characterizes the connection between adversarial certainty and robust generalization. Accordingly, built upon the notion of adversarial certainty, we develop a general method to search for models that can generate training-time adversarial inputs with reduced certainty, while maintaining the model's capability in distinguishing adversarial examples. Extensive experiments on image benchmarks demonstrate that our method effectively learns models with consistently improved robustness and mitigates robust overfitting, confirming the importance of generating less certain adversarial examples for robust generalization. Our implementations are available as open-source code at: https://github.com/TrustMLRG/AdvCertainty.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2310.03738.pdf' target='_blank'>https://arxiv.org/pdf/2310.03738.pdf</a></span>   <span><a href='https://github.com/bit-ml/Stylist' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03738">Robust Novelty Detection through Style-Conscious Feature Ranking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Novelty detection seeks to identify samples deviating from a known distribution, yet data shifts in a multitude of ways, and only a few consist of relevant changes. Aligned with out-of-distribution generalization literature, we advocate for a formal distinction between task-relevant semantic or content changes and irrelevant style changes. This distinction forms the basis for robust novelty detection, emphasizing the identification of semantic changes resilient to style distributional shifts. To this end, we introduce Stylist, a method that utilizes pretrained large-scale model representations to selectively discard environment-biased features. By computing per-feature scores based on feature distribution distances between environments, Stylist effectively eliminates features responsible for spurious correlations, enhancing novelty detection performance. Evaluations on adapted domain generalization datasets and a synthetic dataset demonstrate Stylist's efficacy in improving novelty detection across diverse datasets with stylistic and content shifts. The code is available at https://github.com/bit-ml/Stylist.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2310.02692.pdf' target='_blank'>https://arxiv.org/pdf/2310.02692.pdf</a></span>   <span><a href='https://github.com/noparkee/Graph-Clustering-based-DG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nokyung Park, Daewon Chae, Jeongyong Shim, Sangpil Kim, Eun-Sol Kim, Jinkyu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02692">Clustering-based Image-Text Graph Matching for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning domain-invariant visual representations is important to train a model that can generalize well to unseen target task domains. Recent works demonstrate that text descriptions contain high-level class-discriminative information and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. However, they use pivot embedding in a global manner (i.e., aligning an image embedding with sentence-level text embedding), which does not fully utilize the semantic cues of given text description. In this work, we advocate for the use of local alignment between image regions and corresponding textual descriptions to get domain-invariant features. To this end, we first represent image and text inputs as graphs. We then cluster nodes within these graphs and match the graph-based image node features to the nodes of textual graphs. This matching process is conducted both globally and locally, tightly aligning visual and textual semantic sub-structures. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. The code is available at: https://github.com/noparkee/Graph-Clustering-based-DG
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2310.02579.pdf' target='_blank'>https://arxiv.org/pdf/2310.02579.pdf</a></span>   <span><a href='https://github.com/Graph-COM/SPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, Pan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02579">On the Stability of Expressive Positional Encodings for Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding. Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a ``hard partition'' of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to ``softly partition'' eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods. Our code is available at \url{https://github.com/Graph-COM/SPE}.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2309.16748.pdf' target='_blank'>https://arxiv.org/pdf/2309.16748.pdf</a></span>   <span><a href='https://github.com/facebookresearch/XRM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Pezeshki, Diane Bouchacourt, Mark Ibrahim, Nicolas Ballas, Pascal Vincent, David Lopez-Paz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16748">Discovering environments with XRM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Environment annotations are essential for the success of many out-of-distribution (OOD) generalization methods. Unfortunately, these are costly to obtain and often limited by human annotators' biases. To achieve robust generalization, it is essential to develop algorithms for automatic environment discovery within datasets. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods introduce hyper-parameters and early-stopping criteria, which require a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data. Algorithms built on top of XRM environments achieve oracle worst-group-accuracy, addressing a long-standing challenge in OOD generalization. Code available at \url{https://github.com/facebookresearch/XRM}.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2309.09670.pdf' target='_blank'>https://arxiv.org/pdf/2309.09670.pdf</a></span>   <span><a href='https://github.com/BioMedIA-MBZUAI/DGM-DR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandr Matsun, Dana O. Mohamed, Sharon Chokuwa, Muhammad Ridzuan, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09670">DGM-DR: Domain Generalization with Mutual Information Regularized Diabetic Retinopathy Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The domain shift between training and testing data presents a significant challenge for training generalizable deep learning models. As a consequence, the performance of models trained with the independent and identically distributed (i.i.d) assumption deteriorates when deployed in the real world. This problem is exacerbated in the medical imaging context due to variations in data acquisition across clinical centers, medical apparatus, and patients. Domain generalization (DG) aims to address this problem by learning a model that generalizes well to any unseen target domain. Many domain generalization techniques were unsuccessful in learning domain-invariant representations due to the large domain shift. Furthermore, multiple tasks in medical imaging are not yet extensively studied in existing literature when it comes to DG point of view. In this paper, we introduce a DG method that re-establishes the model objective function as a maximization of mutual information with a large pretrained model to the medical imaging field. We re-visit the problem of DG in Diabetic Retinopathy (DR) classification to establish a clear benchmark with a correct model selection strategy and to achieve robust domain-invariant representation for an improved generalization. Moreover, we conduct extensive experiments on public datasets to show that our proposed method consistently outperforms the previous state-of-the-art by a margin of 5.25% in average accuracy and a lower standard deviation. Source code available at https://github.com/BioMedIA-MBZUAI/DGM-DR
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2309.08513.pdf' target='_blank'>https://arxiv.org/pdf/2309.08513.pdf</a></span>   <span><a href='https://github.com/showlab/SCT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Henry Hengyuan Zhao, Pichao Wang, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08513">SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained vision transformers have strong representation benefits to various downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT) methods have been proposed, and their experiments demonstrate that tuning only 1\% extra parameters could surpass full fine-tuning in low-data resource scenarios. However, these methods overlook the task-specific information when fine-tuning diverse downstream tasks. In this paper, we propose a simple yet effective method called "Salient Channel Tuning" (SCT) to leverage the task-specific information by forwarding the model with the task images to select partial channels in a feature map that enables us to tune only 1/8 channels leading to significantly lower parameter costs. Experiments on 19 visual transfer learning downstream tasks demonstrate that our SCT outperforms full fine-tuning on 18 out of 19 tasks by adding only 0.11M parameters of the ViT-B, which is 780$\times$ fewer than its full fine-tuning counterpart. Furthermore, experiments on domain generalization and few-shot classification further demonstrate the effectiveness and generic of our approach. The code is available at https://github.com/showlab/SCT.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2309.06337.pdf' target='_blank'>https://arxiv.org/pdf/2309.06337.pdf</a></span>   <span><a href='https://github.com/koncle/DG-with-Large-LR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06337">Exploring Flat Minima for Domain Generalization with Large Learning Rates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to generalize to arbitrary unseen domains. A promising approach to improve model generalization in DG is the identification of flat minima. One typical method for this task is SWAD, which involves averaging weights along the training trajectory. However, the success of weight averaging depends on the diversity of weights, which is limited when training with a small learning rate. Instead, we observe that leveraging a large learning rate can simultaneously promote weight diversity and facilitate the identification of flat regions in the loss landscape. However, employing a large learning rate suffers from the convergence problem, which cannot be resolved by simply averaging the training weights. To address this issue, we introduce a training strategy called Lookahead which involves the weight interpolation, instead of average, between fast and slow weights. The fast weight explores the weight space with a large learning rate, which is not converged while the slow weight interpolates with it to ensure the convergence. Besides, weight interpolation also helps identify flat minima by implicitly optimizing the local entropy loss that measures flatness. To further prevent overfitting during training, we propose two variants to regularize the training weight with weighted averaged weight or with accumulated history weight. Taking advantage of this new perspective, our methods achieve state-of-the-art performance on both classification and semantic segmentation domain generalization benchmarks. The code is available at https://github.com/koncle/DG-with-Large-LR.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2309.05551.pdf' target='_blank'>https://arxiv.org/pdf/2309.05551.pdf</a></span>   <span><a href='https://github.com/aimagelab/open-fashion-clip' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Giuseppe Cartella, Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05551">OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The inexorable growth of online shopping and e-commerce demands scalable and robust machine learning-based solutions to accommodate customer requirements. In the context of automatic tagging classification and multimodal retrieval, prior works either defined a low generalizable supervised learning approach or more reusable CLIP-based techniques while, however, training on closed source data. In this work, we propose OpenFashionCLIP, a vision-and-language contrastive learning method that only adopts open-source fashion data stemming from diverse domains, and characterized by varying degrees of specificity. Our approach is extensively validated across several tasks and benchmarks, and experimental results highlight a significant out-of-domain generalization capability and consistent improvements over state-of-the-art methods both in terms of accuracy and recall. Source code and trained models are publicly available at: https://github.com/aimagelab/open-fashion-clip.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2309.05527.pdf' target='_blank'>https://arxiv.org/pdf/2309.05527.pdf</a></span>   <span><a href='https://github.com/PJLab-ADG/3DTrans#resimad' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Zhang, Xinyu Cai, Jiakang Yuan, Donglin Yang, Jianfei Guo, Xiangchao Yan, Renqiu Xia, Botian Shi, Min Dou, Tao Chen, Si Liu, Junchi Yan, Yu Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05527">ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shifts such as sensor type changes and geographical situation variations are prevalent in Autonomous Driving (AD), which poses a challenge since AD model relying on the previous domain knowledge can be hardly directly deployed to a new domain without additional costs. In this paper, we provide a new perspective and approach of alleviating the domain shifts, by proposing a Reconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, the implicit reconstruction process is based on the knowledge from the previous old domain, aiming to convert the domain-related knowledge into domain-invariant representations, e.g., 3D scene-level meshes. Besides, the point clouds simulation process of multiple new domains is conditioned on the above reconstructed 3D meshes, where the target-domain-like simulation samples can be obtained, thus reducing the cost of collecting and annotating new-domain data for the subsequent perception process. For experiments, we consider different cross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes, Waymo-to-ONCE, etc, to verify the zero-shot target-domain perception using ReSimAD. Results demonstrate that our method is beneficial to boost the domain generalization ability, even promising for 3D pre-training.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2309.03563.pdf' target='_blank'>https://arxiv.org/pdf/2309.03563.pdf</a></span>   <span><a href='https://github.com/jiangshdd/AllLablesTogether' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangshu Du, Congying Xia, Wenpeng Yin, Tingting Liang, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03563">All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic Encoding Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In intent detection tasks, leveraging meaningful semantic information from intent labels can be particularly beneficial for few-shot scenarios. However, existing few-shot intent detection methods either ignore the intent labels, (e.g. treating intents as indices) or do not fully utilize this information (e.g. only using part of the intent labels). In this work, we present an end-to-end One-to-All system that enables the comparison of an input utterance with all label candidates. The system can then fully utilize label semantics in this way. Experiments on three few-shot intent detection tasks demonstrate that One-to-All is especially effective when the training resource is extremely scarce, achieving state-of-the-art performance in 1-, 3- and 5-shot settings. Moreover, we present a novel pretraining strategy for our model that utilizes indirect supervision from paraphrasing, enabling zero-shot cross-domain generalization on intent detection tasks. Our code is at https://github.com/jiangshdd/AllLablesTogether.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2309.03493.pdf' target='_blank'>https://arxiv.org/pdf/2309.03493.pdf</a></span>   <span><a href='https://github.com/UARK-AICV/SAM3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat-Tan Bui, Dinh-Hieu Hoang, Minh-Triet Tran, Gianfranco Doretto, Donald Adjeroh, Brijesh Patel, Arabinda Choudhary, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03493">SAM3D: Segment Anything Model in Volumetric Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image segmentation remains a pivotal component in medical image analysis, aiding in the extraction of critical information for precise diagnostic practices. With the advent of deep learning, automated image segmentation methods have risen to prominence, showcasing exceptional proficiency in processing medical imagery. Motivated by the Segment Anything Model (SAM)-a foundational model renowned for its remarkable precision and robust generalization capabilities in segmenting 2D natural images-we introduce SAM3D, an innovative adaptation tailored for 3D volumetric medical image analysis. Unlike current SAM-based methods that segment volumetric data by converting the volume into separate 2D slices for individual analysis, our SAM3D model processes the entire 3D volume image in a unified approach. Extensive experiments are conducted on multiple medical image datasets to demonstrate that our network attains competitive results compared with other state-of-the-art methods in 3D medical segmentation tasks while being significantly efficient in terms of parameters. Code and checkpoints are available at https://github.com/UARK-AICV/SAM3D.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2309.01286.pdf' target='_blank'>https://arxiv.org/pdf/2309.01286.pdf</a></span>   <span><a href='https://github.com/DeweiHu/MAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dewei Hu, Hao Li, Han Liu, Xing Yao, Jiacheng Wang, Ipek Oguz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01286">MAP: Domain Generalization via Meta-Learning on Anatomy-Consistent Pseudo-Modalities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep models suffer from limited generalization capability to unseen domains, which has severely hindered their clinical applicability. Specifically for the retinal vessel segmentation task, although the model is supposed to learn the anatomy of the target, it can be distracted by confounding factors like intensity and contrast. We propose Meta learning on Anatomy-consistent Pseudo-modalities (MAP), a method that improves model generalizability by learning structural features. We first leverage a feature extraction network to generate three distinct pseudo-modalities that share the vessel structure of the original image. Next, we use the episodic learning paradigm by selecting one of the pseudo-modalities as the meta-train dataset, and perform meta-testing on a continuous augmented image space generated through Dirichlet mixup of the remaining pseudo-modalities. Further, we introduce two loss functions that facilitate the model's focus on shape information by clustering the latent vectors obtained from images featuring identical vasculature. We evaluate our model on seven public datasets of various retinal imaging modalities and we conclude that MAP has substantially better generalizability. Our code is publically available at https://github.com/DeweiHu/MAP.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2308.14960.pdf' target='_blank'>https://arxiv.org/pdf/2308.14960.pdf</a></span>   <span><a href='https://github.com/mlvlab/RPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyung Choi, Sanghyeok Lee, Hyunwoo J. Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14960">Read-only Prompt Optimization for Vision-Language Few-shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, prompt tuning has proven effective in adapting pre-trained vision-language models to downstream tasks. These methods aim to adapt the pre-trained models by introducing learnable prompts while keeping pre-trained weights frozen. However, learnable prompts can affect the internal representation within the self-attention module, which may negatively impact performance variance and generalization, especially in data-deficient settings. To address these issues, we propose a novel approach, Read-only Prompt Optimization (RPO). RPO leverages masked attention to prevent the internal representation shift in the pre-trained model. Further, to facilitate the optimization of RPO, the read-only prompts are initialized based on special tokens of the pre-trained model. Our extensive experiments demonstrate that RPO outperforms CLIP and CoCoOp in base-to-new generalization and domain generalization while displaying better robustness. Also, the proposed method achieves better generalization on extremely data-deficient settings, while improving parameter efficiency and computational overhead. Code is available at https://github.com/mlvlab/RPO.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2308.14596.pdf' target='_blank'>https://arxiv.org/pdf/2308.14596.pdf</a></span>   <span><a href='https://github.com/nerdslab/LatentDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Liu, Sahil Khose, Jingyun Xiao, Lakshmi Sathidevi, Keerthan Ramnath, Zsolt Kira, Eva L. Dyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14596">LatentDR: Improving Model Generalization Through Sample-Aware Latent Degradation and Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant advances in deep learning, models often struggle to generalize well to new, unseen domains, especially when training data is limited. To address this challenge, we propose a novel approach for distribution-aware latent augmentation that leverages the relationships across samples to guide the augmentation procedure. Our approach first degrades the samples stochastically in the latent space, mapping them to augmented labels, and then restores the samples from their corrupted versions during training. This process confuses the classifier in the degradation step and restores the overall class distribution of the original samples, promoting diverse intra-class/cross-domain variability. We extensively evaluate our approach on a diverse set of datasets and tasks, including domain generalization benchmarks and medical imaging datasets with strong domain shift, where we show our approach achieves significant improvements over existing methods for latent space augmentation. We further show that our method can be flexibly adapted to long-tail recognition tasks, demonstrating its versatility in building more generalizable models. Code is available at https://github.com/nerdslab/LatentDR.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2308.14212.pdf' target='_blank'>https://arxiv.org/pdf/2308.14212.pdf</a></span>   <span><a href='https://github.com/Sanoojan/CLIP-DRDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanoojan Baliah, Fadillah A. Maani, Santosh Sanjeev, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14212">Exploring the Transfer Learning Capabilities of CLIP in Domain Generalization for Diabetic Retinopathy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic Retinopathy (DR), a leading cause of vision impairment, requires early detection and treatment. Developing robust AI models for DR classification holds substantial potential, but a key challenge is ensuring their generalization in unfamiliar domains with varying data distributions. To address this, our paper investigates cross-domain generalization, also known as domain generalization (DG), within the context of DR classification. DG, a challenging problem in the medical domain, is complicated by the difficulty of gathering labeled data across different domains, such as patient demographics and disease stages. Some recent studies have shown the effectiveness of using CLIP to handle the DG problem in natural images. In this study, we investigate CLIP's transfer learning capabilities and its potential for cross-domain generalization in diabetic retinopathy (DR) classification. We carry out comprehensive experiments to assess the efficacy and potential of CLIP in addressing DG for DR classification. Further, we introduce a multi-modal fine-tuning strategy named Context Optimization with Learnable Visual Tokens (CoOpLVT), which enhances context optimization by conditioning on visual features. Our findings demonstrate that the proposed method increases the F1-score by 1.8% over the baseline, thus underlining its promise for effective DG in DR classification. Our code is publicly available at https://github.com/Sanoojan/CLIP-DRDG.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2308.14030.pdf' target='_blank'>https://arxiv.org/pdf/2308.14030.pdf</a></span>   <span><a href='https://github.com/ladderlab-xjtu/forensic_pathology' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Shen, Jun Zhang, Xinggong Liang, Zeyi Hao, Kehan Li, Fan Wang, Zhenyuan Wang, Chunfeng Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14030">Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-Supervised Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Forensic pathology is critical in analyzing death manner and time from the microscopic aspect to assist in the establishment of reliable factual bases for criminal investigation. In practice, even the manual differentiation between different postmortem organ tissues is challenging and relies on expertise, considering that changes like putrefaction and autolysis could significantly change typical histopathological appearance. Developing AI-based computational pathology techniques to assist forensic pathologists is practically meaningful, which requires reliable discriminative representation learning to capture tissues' fine-grained postmortem patterns. To this end, we propose a framework called FPath, in which a dedicated self-supervised contrastive learning strategy and a context-aware multiple-instance learning (MIL) block are designed to learn discriminative representations from postmortem histopathological images acquired at varying magnification scales. Our self-supervised learning step leverages multiple complementary contrastive losses and regularization terms to train a double-tier backbone for fine-grained and informative patch/instance embedding. Thereafter, the context-aware MIL adaptively distills from the local instances a holistic bag/image-level representation for the recognition task. On a large-scale database of $19,607$ experimental rat postmortem images and $3,378$ real-world human decedent images, our FPath led to state-of-the-art accuracy and promising cross-domain generalization in recognizing seven different postmortem tissues. The source code will be released on \href{https://github.com/ladderlab-xjtu/forensic_pathology}{https://github.com/ladderlab-xjtu/forensic\_pathology}.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2308.11778.pdf' target='_blank'>https://arxiv.org/pdf/2308.11778.pdf</a></span>   <span><a href='https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sobhan Hemati, Guojun Zhang, Amir Estiri, Xi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11778">Understanding Hessian Alignment for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis explains the success of many regularizers like CORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier's head Hessian and/or gradient. Finally, we propose two simple yet effective methods to match the classifier's head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and without directly calculating Hessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transferability, severe correlation shift, label shift and diversity shift. Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2308.11605.pdf' target='_blank'>https://arxiv.org/pdf/2308.11605.pdf</a></span>   <span><a href='https://github.com/mainaksingha01/GOPro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mainak Singha, Ankit Jha, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11605">GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale foundation models, such as CLIP, have demonstrated remarkable success in visual recognition tasks by embedding images in a semantically rich space. Self-supervised learning (SSL) has also shown promise in improving visual recognition by learning invariant features. However, the combination of CLIP with SSL is found to face challenges due to the multi-task framework that blends CLIP's contrastive loss and SSL's loss, including difficulties with loss weighting and inconsistency among different views of images in CLIP's output space. To overcome these challenges, we propose a prompt learning-based model called GOPro, which is a unified framework that ensures similarity between various augmented views of input images in a shared image-text embedding space, using a pair of learnable image and text projectors atop CLIP, to promote invariance and generalizability. To automatically learn such prompts, we leverage the visual content and style primitives extracted from pre-trained CLIP and adapt them to the target task. In addition to CLIP's cross-domain contrastive loss, we introduce a visual contrastive loss and a novel prompt consistency loss, considering the different views of the images. GOPro is trained end-to-end on all three loss objectives, combining the strengths of CLIP and SSL in a principled manner. Empirical evaluations demonstrate that GOPro outperforms the state-of-the-art prompting techniques on three challenging domain generalization tasks across multiple benchmarks by a significant margin. Our code is available at https://github.com/mainaksingha01/GOPro.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2308.11158.pdf' target='_blank'>https://arxiv.org/pdf/2308.11158.pdf</a></span>   <span><a href='https://github.com/liangchen527/RIDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Chen, Yong Zhang, Yibing Song, Anton van den Hengel, Lingqiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11158">Domain Generalization via Rationale Invariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper offers a new perspective to ease the challenge of domain generalization, which involves maintaining robust results even in unseen environments. Our design focuses on the decision-making process in the final classifier layer. Specifically, we propose treating the element-wise contributions to the final results as the rationale for making a decision and representing the rationale for each sample as a matrix. For a well-generalized model, we suggest the rationale matrices for samples belonging to the same category should be similar, indicating the model relies on domain-invariant clues to make decisions, thereby ensuring robust results. To implement this idea, we introduce a rationale invariance loss as a simple regularization technique, requiring only a few lines of code. Our experiments demonstrate that the proposed approach achieves competitive results across various datasets, despite its simplicity. Code is available at \url{https://github.com/liangchen527/RIDG}.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2308.10601.pdf' target='_blank'>https://arxiv.org/pdf/2308.10601.pdf</a></span>   <span><a href='https://github.com/Zhijin-Ge/STM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Ge, Fanhua Shang, Hongying Liu, Yuanyuan Liu, Liang Wan, Wei Feng, Xiaosen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10601">Improving the Transferability of Adversarial Examples with Arbitrary Style Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on clean inputs. Although many attack methods can achieve high success rates in the white-box setting, they also exhibit weak transferability in the black-box setting. Recently, various methods have been proposed to improve adversarial transferability, in which the input transformation is one of the most effective methods. In this work, we notice that existing input transformation-based works mainly adopt the transformed data in the same domain for augmentation. Inspired by domain generalization, we aim to further improve the transferability using the data augmented from different domains. Specifically, a style transfer network can alter the distribution of low-level visual features in an image while preserving semantic content for humans. Hence, we propose a novel attack method named Style Transfer Method (STM) that utilizes a proposed arbitrary style transfer network to transform the images into different domains. To avoid inconsistent semantic information of stylized images for the classification network, we fine-tune the style transfer network and mix up the generated images added by random noise with the original images to maintain semantic consistency and boost input diversity. Extensive experimental results on the ImageNet-compatible dataset show that our proposed method can significantly improve the adversarial transferability on either normally trained models or adversarially trained models than state-of-the-art input transformation-based attacks. Code is available at: https://github.com/Zhijin-Ge/STM.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2308.10285.pdf' target='_blank'>https://arxiv.org/pdf/2308.10285.pdf</a></span>   <span><a href='https://github.com/lingeringlight/DomainDrop' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lingeringlight/DomainDrop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Guo, Lei Qi, Yinghuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10285">DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Neural Networks have exhibited considerable success in various visual tasks. However, when applied to unseen test datasets, state-of-the-art models often suffer performance degradation due to domain shifts. In this paper, we introduce a novel approach for domain generalization from a novel perspective of enhancing the robustness of channels in feature maps to domain shifts. We observe that models trained on source domains contain a substantial number of channels that exhibit unstable activations across different domains, which are inclined to capture domain-specific features and behave abnormally when exposed to unseen target domains. To address the issue, we propose a DomainDrop framework to continuously enhance the channel robustness to domain shifts, where a domain discriminator is used to identify and drop unstable channels in feature maps of each network layer during forward propagation. We theoretically prove that our framework could effectively lower the generalization bound. Extensive experiments on several benchmarks indicate that our framework achieves state-of-the-art performance compared to other competing methods. Our code is available at https://github.com/lingeringlight/DomainDrop.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2308.10236.pdf' target='_blank'>https://arxiv.org/pdf/2308.10236.pdf</a></span>   <span><a href='https://github.com/Naiftt/FedSIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naif Alkhunaizi, Koushik Srivatsan, Faris Almalik, Ibrahim Almakky, Karthik Nandakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10236">FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lack of generalization to unseen domains/attacks is the Achilles heel of most face presentation attack detection (FacePAD) algorithms. Existing attempts to enhance the generalizability of FacePAD solutions assume that data from multiple source domains are available with a single entity to enable centralized training. In practice, data from different source domains may be collected by diverse entities, who are often unable to share their data due to legal and privacy constraints. While collaborative learning paradigms such as federated learning (FL) can overcome this problem, standard FL methods are ill-suited for domain generalization because they struggle to surmount the twin challenges of handling non-iid client data distributions during training and generalizing to unseen domains during inference. In this work, a novel framework called Federated Split learning with Intermediate representation Sampling (FedSIS) is introduced for privacy-preserving domain generalization. In FedSIS, a hybrid Vision Transformer (ViT) architecture is learned using a combination of FL and split learning to achieve robustness against statistical heterogeneity in the client data distributions without any sharing of raw data (thereby preserving privacy). To further improve generalization to unseen domains, a novel feature augmentation strategy called intermediate representation sampling is employed, and discriminative information from intermediate blocks of a ViT is distilled using a shared adapter network. The FedSIS approach has been evaluated on two well-known benchmarks for cross-domain FacePAD to demonstrate that it is possible to achieve state-of-the-art generalization performance without data sharing. Code: https://github.com/Naiftt/FedSIS
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2308.09391.pdf' target='_blank'>https://arxiv.org/pdf/2308.09391.pdf</a></span>   <span><a href='https://github.com/zzwdx/MEDIC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiran Wang, Jian Zhang, Lei Qi, Yinghuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09391">Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) is proposed to deal with the issue of domain shift, which occurs when statistical differences exist between source and target domains. However, most current methods do not account for a common realistic scenario where the source and target domains have different classes. To overcome this deficiency, open set domain generalization (OSDG) then emerges as a more practical setting to recognize unseen classes in unseen domains. An intuitive approach is to use multiple one-vs-all classifiers to define decision boundaries for each class and reject the outliers as unknown. However, the significant class imbalance between positive and negative samples often causes the boundaries biased towards positive ones, resulting in misclassification for known samples in the unseen target domain. In this paper, we propose a novel meta-learning-based framework called dualistic MEta-learning with joint DomaIn-Class matching (MEDIC), which considers gradient matching towards inter-domain and inter-class splits simultaneously to find a generalizable boundary balanced for all tasks. Experimental results demonstrate that MEDIC not only outperforms previous methods in open set scenarios, but also maintains competitive close set generalization ability at the same time. Our code is available at https://github.com/zzwdx/MEDIC.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2308.06160.pdf' target='_blank'>https://arxiv.org/pdf/2308.06160.pdf</a></span>   <span><a href='https://github.com/showlab/DatasetDM,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06160">DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current deep networks are very data-hungry and benefit from training on largescale datasets, which are often time-consuming to collect and annotate. By contrast, synthetic data can be generated infinitely using generative models such as DALL-E and diffusion models, with minimal effort and cost. In this paper, we present DatasetDM, a generic dataset generation model that can produce diverse synthetic images and the corresponding high-quality perception annotations (e.g., segmentation masks, and depth). Our method builds upon the pre-trained diffusion model and extends text-guided image synthesis to perception data generation. We show that the rich latent code of the diffusion model can be effectively decoded as accurate perception annotations using a decoder module. Training the decoder only needs less than 1% (around 100 images) manually labeled images, enabling the generation of an infinitely large annotated dataset. Then these synthetic data can be used for training various perception models for downstream tasks. To showcase the power of the proposed approach, we generate datasets with rich dense pixel-wise labels for a wide range of downstream tasks, including semantic segmentation, instance segmentation, and depth estimation. Notably, it achieves 1) state-of-the-art results on semantic segmentation and instance segmentation; 2) significantly more robust on domain generalization than using the real data alone; and state-of-the-art results in zero-shot segmentation setting; and 3) flexibility for efficient application and novel task composition (e.g., image editing). The project website and code can be found at https://weijiawu.github.io/DatasetDM_page/ and https://github.com/showlab/DatasetDM, respectively
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2308.05988.pdf' target='_blank'>https://arxiv.org/pdf/2308.05988.pdf</a></span>   <span><a href='https://github.com/darrenjkt/MS3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Darren Tsai, Julie Stephany Berrio, Mao Shan, Eduardo Nebot, Stewart Worrall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05988">MS3D++: Ensemble of Experts for Multi-Source Unsupervised Domain Adaptation in 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying 3D detectors in unfamiliar domains has been demonstrated to result in a significant 70-90% drop in detection rate due to variations in lidar, geography, or weather from their training dataset. This domain gap leads to missing detections for densely observed objects, misaligned confidence scores, and increased high-confidence false positives, rendering the detector highly unreliable. To address this, we introduce MS3D++, a self-training framework for multi-source unsupervised domain adaptation in 3D object detection. MS3D++ generates high-quality pseudo-labels, allowing 3D detectors to achieve high performance on a range of lidar types, regardless of their density. Our approach effectively fuses predictions of an ensemble of multi-frame pre-trained detectors from different source domains to improve domain generalization. We subsequently refine predictions temporally to ensure temporal consistency in box localization and object classification. Furthermore, we present an in-depth study into the performance and idiosyncrasies of various 3D detector components in a cross-domain context, providing valuable insights for improved cross-domain detector ensembling. Experimental results on Waymo, nuScenes and Lyft demonstrate that detectors trained with MS3D++ pseudo-labels achieve state-of-the-art performance, comparable to training with human-annotated labels in Bird's Eye View (BEV) evaluation for both low and high density lidar. Code is available at https://github.com/darrenjkt/MS3D
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2308.03322.pdf' target='_blank'>https://arxiv.org/pdf/2308.03322.pdf</a></span>   <span><a href='https://github.com/liyuke65535/Part-Aware-Transformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ni, Yuke Li, Lianli Gao, Heng Tao Shen, Jingkuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03322">Part-Aware Transformer for Generalizable Person Re-identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization person re-identification (DG-ReID) aims to train a model on source domains and generalize well on unseen domains. Vision Transformer usually yields better generalization ability than common CNN networks under distribution shifts. However, Transformer-based ReID models inevitably over-fit to domain-specific biases due to the supervised learning strategy on the source domain. We observe that while the global images of different IDs should have different features, their similar local parts (e.g., black backpack) are not bounded by this constraint. Motivated by this, we propose a pure Transformer model (termed Part-aware Transformer) for DG-ReID by designing a proxy task, named Cross-ID Similarity Learning (CSL), to mine local visual information shared by different IDs. This proxy task allows the model to learn generic features because it only cares about the visual similarity of the parts regardless of the ID labels, thus alleviating the side effect of domain-specific biases. Based on the local similarity obtained in CSL, a Part-guided Self-Distillation (PSD) is proposed to further improve the generalization of global features. Our method achieves state-of-the-art performance under most DG ReID settings. Under the Market$\to$Duke setting, our method exceeds state-of-the-art by 10.9% and 12.8% in Rank1 and mAP, respectively. The code is available at https://github.com/liyuke65535/Part-Aware-Transformer.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2308.00728.pdf' target='_blank'>https://arxiv.org/pdf/2308.00728.pdf</a></span>   <span><a href='https://github.com/jimmy19991222/ELFNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jieming Lou, Weide Liu, Zhuo Chen, Fayao Liu, Jun Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00728">ELFNet: Evidential Local-global Fusion for Stereo Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although existing stereo matching models have achieved continuous improvement, they often face issues related to trustworthiness due to the absence of uncertainty estimation. Additionally, effectively leveraging multi-scale and multi-view knowledge of stereo pairs remains unexplored. In this paper, we introduce the \textbf{E}vidential \textbf{L}ocal-global \textbf{F}usion (ELF) framework for stereo matching, which endows both uncertainty estimation and confidence-aware fusion with trustworthy heads. Instead of predicting the disparity map alone, our model estimates an evidential-based disparity considering both aleatoric and epistemic uncertainties. With the normal inverse-Gamma distribution as a bridge, the proposed framework realizes intra evidential fusion of multi-level predictions and inter evidential fusion between cost-volume-based and transformer-based stereo matching. Extensive experimental results show that the proposed framework exploits multi-view information effectively and achieves state-of-the-art overall performance both on accuracy and cross-domain generalization.
  The codes are available at https://github.com/jimmy19991222/ELFNet.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2308.00400.pdf' target='_blank'>https://arxiv.org/pdf/2308.00400.pdf</a></span>   <span><a href='https://github.com/zhangbo-nlp/ZRIGF' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zhangbo-nlp/ZRIGF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Zhang, Jian Wang, Hui Ma, Bo Xu, Hongfei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00400">ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses based on harmonized multimodal representations. Comprehensive experiments conducted on both text-based and image-grounded dialogue datasets demonstrate ZRIGF's efficacy in generating contextually pertinent and informative responses. Furthermore, we adopt a fully zero-resource scenario in the image-grounded dialogue dataset to demonstrate our framework's robust generalization capabilities in novel domains. The code is available at https://github.com/zhangbo-nlp/ZRIGF.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2307.16184.pdf' target='_blank'>https://arxiv.org/pdf/2307.16184.pdf</a></span>   <span><a href='https://github.com/mshukor/UnIVAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16184">UnIVAL: Unified Model for Image, Video, Audio and Language Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities, allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are released here: https://github.com/mshukor/UnIVAL.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2307.13943.pdf' target='_blank'>https://arxiv.org/pdf/2307.13943.pdf</a></span>   <span><a href='https://github.com/joffery/TRO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengchun Qiao, Xi Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13943">Topology-aware Robust Optimization for Out-of-distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2307.10507.pdf' target='_blank'>https://arxiv.org/pdf/2307.10507.pdf</a></span>   <span><a href='https://github.com/ubc-tea/FedSoup' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghui Chen, Meirui Jiang, Qi Dou, Zehua Wang, Xiaoxiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10507">FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve the model's generalization performance. We evaluate our method on retinal and pathological image classification tasks, and our proposed method achieves significant improvements for out-of-distribution generalization. Our code is available at https://github.com/ubc-tea/FedSoup.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2307.09005.pdf' target='_blank'>https://arxiv.org/pdf/2307.09005.pdf</a></span>   <span><a href='https://github.com/liamheng/Non-IID_Medical_Image_Segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Li, Haojin Li, Wei Zhao, Huazhu Fu, Xiuyun Su, Yan Hu, Jiang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09005">Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The annotation scarcity of medical image segmentation poses challenges in collecting sufficient training data for deep learning models. Specifically, models trained on limited data may not generalize well to other unseen data domains, resulting in a domain shift issue. Consequently, domain generalization (DG) is developed to boost the performance of segmentation models on unseen domains. However, the DG setup requires multiple source domains, which impedes the efficient deployment of segmentation algorithms in clinical scenarios. To address this challenge and improve the segmentation model's generalizability, we propose a novel approach called the Frequency-mixed Single-source Domain Generalization method (FreeSDG). By analyzing the frequency's effect on domain discrepancy, FreeSDG leverages a mixed frequency spectrum to augment the single-source domain. Additionally, self-supervision is constructed in the domain augmentation to learn robust context-aware representations for the segmentation task. Experimental results on five datasets of three modalities demonstrate the effectiveness of the proposed algorithm. FreeSDG outperforms state-of-the-art methods and significantly improves the segmentation model's generalizability. Therefore, FreeSDG provides a promising solution for enhancing the generalization of medical image segmentation models, especially when annotated data is scarce. The code is available at https://github.com/liamheng/Non-IID_Medical_Image_Segmentation.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2307.04942.pdf' target='_blank'>https://arxiv.org/pdf/2307.04942.pdf</a></span>   <span><a href='https://github.com/inouye-lab/FedDG_Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruqi Bai, Saurabh Bagchi, David I. Inouye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04942">Benchmarking Algorithms for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges. Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario. Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity. To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty. We then apply our methodology to evaluate 14 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG. Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client heterogeneity, or more realistic datasets. Please check our extendable benchmark code here: https://github.com/inouye-lab/FedDG_Benchmark.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2307.02138.pdf' target='_blank'>https://arxiv.org/pdf/2307.02138.pdf</a></span>   <span><a href='https://github.com/ETHRuiGong/PTDiffSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Gong, Martin Danelljan, Han Sun, Julio Delgado Mangas, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02138">Prompting Diffusion Representations for Cross-Domain Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While originally designed for image generation, diffusion models have recently shown to provide excellent pretrained feature representations for semantic segmentation. Intrigued by this result, we set out to explore how well diffusion-pretrained representations generalize to new domains, a crucial ability for any representation. We find that diffusion-pretraining achieves extraordinary domain generalization results for semantic segmentation, outperforming both supervised and self-supervised backbone networks. Motivated by this, we investigate how to utilize the model's unique ability of taking an input prompt, in order to further enhance its cross-domain performance. We introduce a scene prompt and a prompt randomization strategy to help further disentangle the domain-invariant information when training the segmentation head. Moreover, we propose a simple but highly effective approach for test-time domain adaptation, based on learning a scene prompt on the target domain in an unsupervised manner. Extensive experiments conducted on four synthetic-to-real and clear-to-adverse weather benchmarks demonstrate the effectiveness of our approaches. Without resorting to any complex techniques, such as image translation, augmentation, or rare-class sampling, we set a new state-of-the-art on all benchmarks. Our implementation will be publicly available at \url{https://github.com/ETHRuiGong/PTDiffSeg}.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2307.00648.pdf' target='_blank'>https://arxiv.org/pdf/2307.00648.pdf</a></span>   <span><a href='https://github.com/boschresearch/ISSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Li, Dan Zhang, Margret Keuper, Anna Khoreva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00648">Intra- & Extra-Source Exemplar-Based Style Synthesis for Improved Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an exemplar-based style synthesis pipeline to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image, preserving its semantic layout through noise prediction. Using the proposed masked noise encoder to randomize style and content combinations in the training set, i.e., intra-source style augmentation (ISSA) effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by $3\%$ mIoU in Cityscapes to Dark ZÃ¼rich. In addition, we demonstrate the strong plug-n-play ability of the proposed style synthesis pipeline, which is readily usable for extra-source exemplars e.g., web-crawled images, without any retraining or fine-tuning. Moreover, we study a new use case to indicate neural network's generalization capability by building a stylized proxy validation set. This application has significant practical sense for selecting models to be deployed in the open-world environment. Our code is available at \url{https://github.com/boschresearch/ISSA}.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2306.17820.pdf' target='_blank'>https://arxiv.org/pdf/2306.17820.pdf</a></span>   <span><a href='https://github.com/Alsace08/Meta-Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Wang, Zhuosheng Zhang, Pei Zhang, Baosong Yang, Rui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17820">Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural-symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits. To broaden symbolic methods' applicability and adaptability in the real world, we propose the Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge. We conduct extensive experiments on more than ten datasets encompassing conventional reasoning tasks like arithmetic, symbolic, and logical reasoning, and the more complex interactive reasoning tasks like theory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning significantly enhances in-context reasoning accuracy, learning efficiency, out-of-domain generalization, and output stability compared to the Chain-of-Thought technique. Code and data are publicly available at \url{https://github.com/Alsace08/Meta-Reasoning}.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2306.15902.pdf' target='_blank'>https://arxiv.org/pdf/2306.15902.pdf</a></span>   <span><a href='https://github.com/YangLing0818/GraphOOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling Yang, Jiayi Zheng, Heyuan Wang, Zhongyi Liu, Zhilin Huang, Shenda Hong, Wentao Zhang, Bin Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15902">Individual and Structural Graph Information Bottlenecks for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) graph generalization are critical for many real-world applications. Existing methods neglect to discard spurious or noisy features of inputs, which are irrelevant to the label. Besides, they mainly conduct instance-level class-invariant graph learning and fail to utilize the structural class relationships between graph instances. In this work, we endeavor to address these issues in a unified framework, dubbed Individual and Structural Graph Information Bottlenecks (IS-GIB). To remove class spurious feature caused by distribution shifts, we propose Individual Graph Information Bottleneck (I-GIB) which discards irrelevant information by minimizing the mutual information between the input graph and its embeddings. To leverage the structural intra- and inter-domain correlations, we propose Structural Graph Information Bottleneck (S-GIB). Specifically for a batch of graphs with multiple domains, S-GIB first computes the pair-wise input-input, embedding-embedding, and label-label correlations. Then it minimizes the mutual information between input graph and embedding pairs while maximizing the mutual information between embedding and label pairs. The critical insight of S-GIB is to simultaneously discard spurious features and learn invariant features from a high-order perspective by maintaining class relationships under multiple distributional shifts. Notably, we unify the proposed I-GIB and S-GIB to form our complementary framework IS-GIB. Extensive experiments conducted on both node- and graph-level tasks consistently demonstrate the superior generalization ability of IS-GIB. The code is available at https://github.com/YangLing0818/GraphOOD.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2306.11768.pdf' target='_blank'>https://arxiv.org/pdf/2306.11768.pdf</a></span>   <span><a href='https://github.com/zaixizhang/Awesome-SBDD},' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaixi Zhang, Jiaxian Yan, Yining Huang, Qi Liu, Enhong Chen, Mengdi Wang, Marinka Zitnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11768">Geometric Deep Learning for Structure-Based Drug Design: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structure-based drug design (SBDD) leverages the three-dimensional geometry of proteins to identify potential drug candidates. Traditional approaches, rooted in physicochemical modeling and domain expertise, are often resource-intensive. Recent advancements in geometric deep learning, which effectively integrate and process 3D geometric data, alongside breakthroughs in accurate protein structure predictions from tools like AlphaFold, have significantly propelled the field forward. This paper systematically reviews the state-of-the-art in geometric deep learning for SBDD. We begin by outlining foundational tasks in SBDD, discussing prevalent 3D protein representations, and highlighting representative predictive and generative models. Next, we provide an in-depth review of key tasks, including binding site prediction, binding pose generation, de novo molecule generation, linker design, protein pocket generation, and binding affinity prediction. For each task, we present formal problem definitions, key methods, datasets, evaluation metrics, and performance benchmarks. Lastly, we explore current challenges and future opportunities in SBDD. Challenges include oversimplified problem formulations, limited out-of-distribution generalization, biosecurity concerns related to the misuse of structural data, insufficient evaluation metrics and large-scale benchmarks, and the need for experimental validation and enhanced model interpretability. Opportunities lie in leveraging multimodal datasets, integrating domain knowledge, developing comprehensive benchmarks, establishing criteria aligned with clinical outcomes, and designing foundation models to expand the scope of design tasks. We also curate \url{https://github.com/zaixizhang/Awesome-SBDD}, reflecting ongoing contributions and new datasets in SBDD.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2306.11400.pdf' target='_blank'>https://arxiv.org/pdf/2306.11400.pdf</a></span>   <span><a href='https://github.com/Mechrev0/MuDPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongzhu Miao, Shasha Li, Jintao Tang, Ting Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11400">MuDPT: Multi-modal Deep-symphysis Prompt Tuning for Large Pre-trained Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt tuning, like CoOp, has recently shown promising vision recognizing and transfer learning ability on various downstream tasks with the emergence of large pre-trained vision-language models like CLIP. However, we identify that existing uni-modal prompt tuning approaches may result in sub-optimal performance since this uni-modal design breaks the original alignment of textual and visual representations in the pre-trained model. Inspired by the nature of pre-trained vision-language models, we aim to achieve completeness in prompt tuning and propose a novel approach called Multi-modal Deep-symphysis Prompt Tuning, dubbed as MuDPT, which extends independent multi-modal prompt tuning by additionally learning a model-agnostic transformative network to allow deep hierarchical bi-directional prompt fusion. We evaluate the effectiveness of MuDPT on few-shot vision recognition and out-of-domain generalization tasks. Compared with the state-of-the-art methods, MuDPT achieves better recognition and generalization ability with an apparent margin thanks to synergistic alignment of textual and visual representations. Our code is available at: https://github.com/Mechrev0/MuDPT.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2306.11078.pdf' target='_blank'>https://arxiv.org/pdf/2306.11078.pdf</a></span>   <span><a href='https://github.com/cbg-ethz/bmi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>PaweÅ CzyÅ¼, Frederic Grabowski, Julia E. Vogt, Niko Beerenwinkel, Alexander Marx
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11078">Beyond Normal: On the Evaluation of Mutual Information Estimators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2306.11074.pdf' target='_blank'>https://arxiv.org/pdf/2306.11074.pdf</a></span>   <span><a href='https://github.com/AndPotap/afr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shikai Qiu, Andres Potapczynski, Pavel Izmailov, Andrew Gordon Wilson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11074">Simple and Fast Group Robustness by Automatic Feature Reweighting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major challenge to out-of-distribution generalization is reliance on spurious features -- patterns that are predictive of the class label in the training data distribution, but not causally related to the target. Standard methods for reducing the reliance on spurious features typically assume that we know what the spurious feature is, which is rarely true in the real world. Methods that attempt to alleviate this limitation are complex, hard to tune, and lead to a significant computational overhead compared to standard training. In this paper, we propose Automatic Feature Reweighting (AFR), an extremely simple and fast method for updating the model to reduce the reliance on spurious features. AFR retrains the last layer of a standard ERM-trained base model with a weighted loss that emphasizes the examples where the ERM model predicts poorly, automatically upweighting the minority group without group labels. With this simple procedure, we improve upon the best reported results among competing methods trained without spurious attributes on several vision and natural language classification benchmarks, using only a fraction of their compute.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2306.09158.pdf' target='_blank'>https://arxiv.org/pdf/2306.09158.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/scone' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyue Bai, Gregory Canal, Xuefeng Du, Jeongyeol Kwon, Robert Nowak, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09158">Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern machine learning models deployed in the wild can encounter both covariate and semantic shifts, giving rise to the problems of out-of-distribution (OOD) generalization and OOD detection respectively. While both problems have received significant research attention lately, they have been pursued independently. This may not be surprising, since the two tasks have seemingly conflicting goals. This paper provides a new unified approach that is capable of simultaneously generalizing to covariate shifts while robustly detecting semantic shifts. We propose a margin-based learning framework that exploits freely available unlabeled data in the wild that captures the environmental test-time OOD distributions under both covariate and semantic shifts. We show both empirically and theoretically that the proposed margin constraint is the key to achieving both OOD generalization and detection. Extensive experiments show the superiority of our framework, outperforming competitive baselines that specialize in either OOD generalization or OOD detection. Code is publicly available at https://github.com/deeplearning-wisc/scone.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2306.07967.pdf' target='_blank'>https://arxiv.org/pdf/2306.07967.pdf</a></span>   <span><a href='https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, Zhiqiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07967">One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2306.07197.pdf' target='_blank'>https://arxiv.org/pdf/2306.07197.pdf</a></span>   <span><a href='https://github.com/TreeLLi/AROID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Li, Jianing Qiu, Michael Spratling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07197">AROID: Improving Adversarial Robustness Through Online Instance-Wise Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks are vulnerable to adversarial examples. Adversarial training (AT) is an effective defense against adversarial examples. However, AT is prone to overfitting which degrades robustness substantially. Recently, data augmentation (DA) was shown to be effective in mitigating robust overfitting if appropriately designed and optimized for AT. This work proposes a new method to automatically learn online, instance-wise, DA policies to improve robust generalization for AT. This is the first automated DA method specific for robustness. A novel policy learning objective, consisting of Vulnerability, Affinity and Diversity, is proposed and shown to be sufficiently effective and efficient to be practical for automatic DA generation during AT. Importantly, our method dramatically reduces the cost of policy search from the 5000 hours of AutoAugment and the 412 hours of IDBH to 9 hours, making automated DA more practical to use for adversarial robustness. This allows our method to efficiently explore a large search space for a more effective DA policy and evolve the policy as training progresses. Empirically, our method is shown to outperform all competitive DA methods across various model architectures and datasets. Our DA policy reinforced vanilla AT to surpass several state-of-the-art AT methods regarding both accuracy and robustness. It can also be combined with those advanced AT methods to further boost robustness. Code and pre-trained models are available at https://github.com/TreeLLi/AROID.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2306.05254.pdf' target='_blank'>https://arxiv.org/pdf/2306.05254.pdf</a></span>   <span><a href='https://github.com/ShishuaiHu/CCSDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shishuai Hu, Zehui Liao, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05254">Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based medical image segmentation models suffer from performance degradation when deployed to a new healthcare center. To address this issue, unsupervised domain adaptation and multi-source domain generalization methods have been proposed, which, however, are less favorable for clinical practice due to the cost of acquiring target-domain data and the privacy concerns associated with redistributing the data from multiple source domains. In this paper, we propose a \textbf{C}hannel-level \textbf{C}ontrastive \textbf{S}ingle \textbf{D}omain \textbf{G}eneralization (\textbf{C$^2$SDG}) model for medical image segmentation. In C$^2$SDG, the shallower features of each image and its style-augmented counterpart are extracted and used for contrastive training, resulting in the disentangled style representations and structure representations. The segmentation is performed based solely on the structure representations. Our method is novel in the contrastive perspective that enables channel-wise feature disentanglement using a single source domain. We evaluated C$^2$SDG against six SDG methods on a multi-domain joint optic cup and optic disc segmentation benchmark. Our results suggest the effectiveness of each module in C$^2$SDG and also indicate that C$^2$SDG outperforms the baseline and all competing methods with a large margin. The code will be available at \url{https://github.com/ShishuaiHu/CCSDG}.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2306.03932.pdf' target='_blank'>https://arxiv.org/pdf/2306.03932.pdf</a></span>   <span><a href='https://github.com/codezakh/SelTDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaid Khan, Vijay Kumar BG, Samuel Schulter, Xiang Yu, Yun Fu, Manmohan Chandraker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03932">Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finetuning a large vision language model (VLM) on a target dataset after large scale pretraining is a dominant paradigm in visual question answering (VQA). Datasets for specialized tasks such as knowledge-based VQA or VQA in non natural-image domains are orders of magnitude smaller than those for general-purpose VQA. While collecting additional labels for specialized tasks or domains can be challenging, unlabeled images are often available. We introduce SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA uses the VLM and target dataset to build a teacher model that can generate question-answer pseudolabels directly conditioned on an image alone, allowing us to pseudolabel unlabeled images. SelTDA then finetunes the initial VLM on the original dataset augmented with freshly pseudolabeled images. We describe a series of experiments showing that our self-taught data augmentation increases robustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills. The proposed strategy requires no additional annotations or architectural modifications, and is compatible with any modern encoder-decoder multimodal transformer. Code available at https://github.com/codezakh/SelTDA.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2306.02105.pdf' target='_blank'>https://arxiv.org/pdf/2306.02105.pdf</a></span>   <span><a href='https://github.com/bonaventuredossou/active_learning_african_asr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bonaventure F. P. Dossou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02105">Advancing African-Accented Speech Recognition: Epistemic Uncertainty-Driven Data Selection for Generalizable ASR Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accents play a pivotal role in shaping human communication, enhancing our ability to convey and comprehend messages with clarity and cultural nuance. While there has been significant progress in Automatic Speech Recognition (ASR), African-accented English ASR has been understudied due to a lack of training datasets, which are often expensive to create and demand colossal human labor. Combining several active learning paradigms and the core-set approach, we propose a new multi-rounds adaptation process that uses epistemic uncertainty to automate the annotation process, significantly reducing the associated costs and human labor. This novel method streamlines data annotation and strategically selects data samples contributing most to model uncertainty, enhancing training efficiency. We define a new U-WER metric to track model adaptation to hard accents. We evaluate our approach across several domains, datasets, and high-performing speech models. Our results show that our approach leads to a 27\% WER relative average improvement while requiring on average 45\% less data than established baselines. Our approach also improves out-of-distribution generalization for very low-resource accents, demonstrating its viability for building generalizable ASR models in the context of accented African ASR. We open-source the code here: https://github.com/bonaventuredossou/active_learning_african_asr.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2306.01195.pdf' target='_blank'>https://arxiv.org/pdf/2306.01195.pdf</a></span>   <span><a href='https://github.com/ShuvenduRoy/CoPrompt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuvendu Roy, Ali Etemad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01195">Consistency-guided Prompt Learning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and output spaces. This facilitates more effective adaptation to downstream tasks in a few-shot learning setting. Experiments show that CoPrompt outperforms existing methods on a range of evaluation suites, including base-to-novel generalization, domain generalization, and cross-dataset evaluation. On generalization, CoPrompt improves the state-of-the-art on zero-shot tasks and the overall harmonic mean over 11 datasets. Detailed ablation studies show the effectiveness of each of the components in CoPrompt. We make our code available at https://github.com/ShuvenduRoy/CoPrompt.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2306.01103.pdf' target='_blank'>https://arxiv.org/pdf/2306.01103.pdf</a></span>   <span><a href='https://github.com/divelab/LECI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shurui Gui, Meng Liu, Xiner Li, Youzhi Luo, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01103">Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for causal subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
  Our code is available at https://github.com/divelab/LECI.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2306.00499.pdf' target='_blank'>https://arxiv.org/pdf/2306.00499.pdf</a></span>   <span><a href='https://github.com/yifangao112/DeSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Gao, Wei Xia, Dingdu Hu, Wenkui Wang, Xin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00499">DeSAM: Decoupled Segment Anything Model for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based medical image segmentation models often suffer from domain shift, where the models trained on a source domain do not generalize well to other unseen domains. As a prompt-driven foundation model with powerful generalization capabilities, the Segment Anything Model (SAM) shows potential for improving the cross-domain robustness of medical image segmentation. However, SAM performs significantly worse in automatic segmentation scenarios than when manually prompted, hindering its direct application to domain generalization. Upon further investigation, we discovered that the degradation in performance was related to the coupling effect of inevitable poor prompts and mask generation. To address the coupling effect, we propose the Decoupled SAM (DeSAM). DeSAM modifies SAM's mask decoder by introducing two new modules: a prompt-relevant IoU module (PRIM) and a prompt-decoupled mask module (PDMM). PRIM predicts the IoU score and generates mask embeddings, while PDMM extracts multi-scale features from the intermediate layers of the image encoder and fuses them with the mask embeddings from PRIM to generate the final segmentation mask. This decoupled design allows DeSAM to leverage the pre-trained weights while minimizing the performance degradation caused by poor prompts. We conducted experiments on publicly available cross-site prostate and cross-modality abdominal image segmentation datasets. The results show that our DeSAM leads to a substantial performance improvement over previous state-of-theart domain generalization methods. The code is publicly available at https://github.com/yifangao112/DeSAM.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2306.00451.pdf' target='_blank'>https://arxiv.org/pdf/2306.00451.pdf</a></span>   <span><a href='https://github.com/lofrienger/S2ME' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>An Wang, Mengya Xu, Yang Zhang, Mobarakol Islam, Hongliang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00451">S$^2$ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-supervised Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully-supervised polyp segmentation has accomplished significant triumphs over the years in advancing the early diagnosis of colorectal cancer. However, label-efficient solutions from weak supervision like scribbles are rarely explored yet primarily meaningful and demanding in medical practice due to the expensiveness and scarcity of densely-annotated polyp data. Besides, various deployment issues, including data shifts and corruption, put forward further requests for model generalization and robustness. To address these concerns, we design a framework of Spatial-Spectral Dual-branch Mutual Teaching and Entropy-guided Pseudo Label Ensemble Learning (S$^2$ME). Concretely, for the first time in weakly-supervised medical image segmentation, we promote the dual-branch co-teaching framework by leveraging the intrinsic complementarity of features extracted from the spatial and spectral domains and encouraging cross-space consistency through collaborative optimization. Furthermore, to produce reliable mixed pseudo labels, which enhance the effectiveness of ensemble learning, we introduce a novel adaptive pixel-wise fusion technique based on the entropy guidance from the spatial and spectral branches. Our strategy efficiently mitigates the deleterious effects of uncertainty and noise present in pseudo labels and surpasses previous alternatives in terms of efficacy. Ultimately, we formulate a holistic optimization objective to learn from the hybrid supervision of scribbles and pseudo labels. Extensive experiments and evaluation on four public datasets demonstrate the superiority of our method regarding in-distribution accuracy, out-of-distribution generalization, and robustness, highlighting its promising clinical significance. Our code is available at https://github.com/lofrienger/S2ME.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2305.19949.pdf' target='_blank'>https://arxiv.org/pdf/2305.19949.pdf</a></span>   <span><a href='https://github.com/Chen-Ziyang/TriD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Chen, Yongsheng Pan, Yiwen Ye, Hengfei Cui, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19949">Treasure in Distribution: A Domain Randomization based Multi-Source Domain Generalization for 2D Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although recent years have witnessed the great success of convolutional neural networks (CNNs) in medical image segmentation, the domain shift issue caused by the highly variable image quality of medical images hinders the deployment of CNNs in real-world clinical applications. Domain generalization (DG) methods aim to address this issue by training a robust model on the source domain, which has a strong generalization ability. Previously, many DG methods based on feature-space domain randomization have been proposed, which, however, suffer from the limited and unordered search space of feature styles. In this paper, we propose a multi-source DG method called Treasure in Distribution (TriD), which constructs an unprecedented search space to obtain the model with strong robustness by randomly sampling from a uniform distribution. To learn the domain-invariant representations explicitly, we further devise a style-mixing strategy in our TriD, which mixes the feature styles by randomly mixing the augmented and original statistics along the channel wise and can be extended to other DG methods. Extensive experiments on two medical segmentation tasks with different modalities demonstrate that our TriD achieves superior generalization performance on unseen target-domain data. Code is available at https://github.com/Chen-Ziyang/TriD.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2305.16289.pdf' target='_blank'>https://arxiv.org/pdf/2305.16289.pdf</a></span>   <span><a href='https://github.com/lisadunlap/ALIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16289">Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many fine-grained classification tasks, like rare animal identification, have limited training data and consequently classifiers trained on these datasets often fail to generalize to variations in the domain like changes in weather or location. As such, we explore how natural language descriptions of the domains seen in training data can be used with large vision models trained on diverse pretraining datasets to generate useful variations of the training data. We introduce ALIA (Automated Language-guided Image Augmentation), a method which utilizes large vision and language models to automatically generate natural language descriptions of a dataset's domains and augment the training data via language-guided image editing. To maintain data integrity, a model trained on the original dataset filters out minimal image edits and those which corrupt class-relevant information. The resulting dataset is visually consistent with the original training data and offers significantly enhanced diversity. We show that ALIA is able to surpasses traditional data augmentation and text-to-image generated data on fine-grained classification tasks, including cases of domain generalization and contextual bias. Code is available at https://github.com/lisadunlap/ALIA.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2305.14463.pdf' target='_blank'>https://arxiv.org/pdf/2305.14463.pdf</a></span>   <span><a href='https://github.com/tareknaous/readme' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tarek Naous, Michael J. Ryan, Anton Lavrouk, Mohit Chandra, Wei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14463">ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a comprehensive evaluation of large language models for multilingual readability assessment. Existing evaluation resources lack domain and language diversity, limiting the ability for cross-domain and cross-lingual analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset with human annotations of 9757 sentences in Arabic, English, French, Hindi, and Russian, collected from 112 different data sources. This benchmark will encourage research on developing robust multilingual readability assessment methods. Using ReadMe++, we benchmark multilingual and monolingual language models in the supervised, unsupervised, and few-shot prompting settings. The domain and language diversity in ReadMe++ enable us to test more effective few-shot prompting, and identify shortcomings in state-of-the-art unsupervised methods. Our experiments also reveal exciting results of superior domain generalization and enhanced cross-lingual transfer capabilities by models trained on ReadMe++. We will make our data publicly available and release a python package tool for multilingual sentence readability prediction using our trained models at: https://github.com/tareknaous/readme
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2305.13031.pdf' target='_blank'>https://arxiv.org/pdf/2305.13031.pdf</a></span>   <span><a href='https://github.com/dingjiansw101/HGFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Ding, Nan Xue, Gui-Song Xia, Bernt Schiele, Dengxin Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13031">HGFormer: Hierarchical Grouping Transformer for Domain Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current semantic segmentation models have achieved great success under the independent and identically distributed (i.i.d.) condition. However, in real-world applications, test data might come from a different domain than training data. Therefore, it is important to improve model robustness against domain differences. This work studies semantic segmentation under the domain generalization setting, where a model is trained only on the source domain and tested on the unseen target domain. Existing works show that Vision Transformers are more robust than CNNs and show that this is related to the visual grouping property of self-attention. In this work, we propose a novel hierarchical grouping transformer (HGFormer) to explicitly group pixels to form part-level masks and then whole-level masks. The masks at different scales aim to segment out both parts and a whole of classes. HGFormer combines mask classification results at both scales for class label prediction. We assemble multiple interesting cross-domain settings by using seven public semantic segmentation datasets. Experiments show that HGFormer yields more robust semantic segmentation results than per-pixel classification methods and flat grouping transformers, and outperforms previous methods significantly. Code will be available at https://github.com/dingjiansw101/HGFormer.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2305.12704.pdf' target='_blank'>https://arxiv.org/pdf/2305.12704.pdf</a></span>   <span><a href='https://github.com/ut-vision/Rot-MVGaze' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ut-vision/Rot-MVGaze' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoichiro Hisadome, Tianyi Wu, Jiawei Qin, Yusuke Sugano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12704">Rotation-Constrained Cross-View Feature Fusion for Multi-View Appearance-based Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Appearance-based gaze estimation has been actively studied in recent years. However, its generalization performance for unseen head poses is still a significant limitation for existing methods. This work proposes a generalizable multi-view gaze estimation task and a cross-view feature fusion method to address this issue. In addition to paired images, our method takes the relative rotation matrix between two cameras as additional input. The proposed network learns to extract rotatable feature representation by using relative rotation as a constraint and adaptively fuses the rotatable features via stacked fusion modules. This simple yet efficient approach significantly improves generalization performance under unseen head poses without significantly increasing computational cost. The model can be trained with random combinations of cameras without fixing the positioning and can generalize to unseen camera pairs during inference. Through experiments using multiple datasets, we demonstrate the advantage of the proposed method over baseline methods, including state-of-the-art domain generalization approaches. The code will be available at https://github.com/ut-vision/Rot-MVGaze.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2305.09160.pdf' target='_blank'>https://arxiv.org/pdf/2305.09160.pdf</a></span>   <span><a href='https://github.com/SiyuanHuang95/SUG' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SiyuanHuang95/SUG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Huang, Bo Zhang, Botian Shi, Peng Gao, Yikang Li, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09160">SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although Domain Generalization (DG) problem has been fast-growing in the 2D image tasks, its exploration on 3D point cloud data is still insufficient and challenged by more complex and uncertain cross-domain variances with uneven inter-class modality distribution. In this paper, different from previous 2D DG works, we focus on the 3D DG problem and propose a Single-dataset Unified Generalization (SUG) framework that only leverages a single source dataset to alleviate the unforeseen domain differences faced by a well-trained source model. Specifically, we first design a Multi-grained Sub-domain Alignment (MSA) method, which can constrain the learned representations to be domain-agnostic and discriminative, by performing a multi-grained feature alignment process between the splitted sub-domains from the single source dataset. Then, a Sample-level Domain-aware Attention (SDA) strategy is presented, which can selectively enhance easy-to-adapt samples from different sub-domains according to the sample-level inter-domain distance to avoid the negative transfer. Experiments demonstrate that our SUG can boost the generalization ability for unseen target domains, even outperforming the existing unsupervised domain adaptation methods that have to access extensive target domain data. Our code is available at https://github.com/SiyuanHuang95/SUG.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2305.08386.pdf' target='_blank'>https://arxiv.org/pdf/2305.08386.pdf</a></span>   <span><a href='https://github.com/Zplusdragon/PLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Zuo, Jiahao Hong, Feng Zhang, Changqian Yu, Hanyu Zhou, Changxin Gao, Nong Sang, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08386">PLIP: Language-Image Pre-training for Person Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-image pre-training is an effective technique for learning powerful representations in general domains. However, when directly turning to person representation learning, these general pre-training methods suffer from unsatisfactory performance. The reason is that they neglect critical person-related characteristics, i.e., fine-grained attributes and identities. To address this issue, we propose a novel language-image pre-training framework for person representation learning, termed PLIP. Specifically, we elaborately design three pretext tasks: 1) Text-guided Image Colorization, aims to establish the correspondence between the person-related image regions and the fine-grained color-part textual phrases. 2) Image-guided Attributes Prediction, aims to mine fine-grained attribute information of the person body in the image; and 3) Identity-based Vision-Language Contrast, aims to correlate the cross-modal representations at the identity level rather than the instance level. Moreover, to implement our pre-train framework, we construct a large-scale person dataset with image-text pairs named SYNTH-PEDES by automatically generating textual annotations. We pre-train PLIP on SYNTH-PEDES and evaluate our models by spanning downstream person-centric tasks. PLIP not only significantly improves existing methods on all these tasks, but also shows great ability in the zero-shot and domain generalization settings. The code, dataset and weights will be released at~\url{https://github.com/Zplusdragon/PLIP}
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2305.07888.pdf' target='_blank'>https://arxiv.org/pdf/2305.07888.pdf</a></span>   <span><a href='https://github.com/Gaohan123/LAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Gao, Kaican Li, Weiyan Xie, Zhi Lin, Yongxiang Huang, Luning Wang, Caleb Chen Cao, Nevin L. Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.07888">Consistency Regularization for Domain Generalization with Logit Attribution Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) is about training models that generalize well under domain shift. Previous research on DG has been conducted mostly in single-source or multi-source settings. In this paper, we consider a third, lesser-known setting where a training domain is endowed with a collection of pairs of examples that share the same semantic information. Such semantic sharing (SS) pairs can be created via data augmentation and then utilized for consistency regularization (CR). We present a theory showing CR is conducive to DG and propose a novel CR method called Logit Attribution Matching (LAM). We conduct experiments on five DG benchmarks and four pretrained models with SS pairs created by both generic and targeted data augmentation methods. LAM outperforms representative single/multi-source DG methods and various CR methods that leverage SS pairs. The code and data of this project are available at https://github.com/Gaohan123/LAM
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2305.01134.pdf' target='_blank'>https://arxiv.org/pdf/2305.01134.pdf</a></span>   <span><a href='https://github.com/QData/PGrad' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Wang, Jake Grigsby, Yanjun Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01134">PGrad: Learning Principal Gradients For Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models fail to perform when facing out-of-distribution (OOD) domains, a challenging task known as domain generalization (DG). In this work, we develop a novel DG training strategy, we call PGrad, to learn a robust gradient direction, improving models' generalization ability on unseen domains. The proposed gradient aggregates the principal directions of a sampled roll-out optimization trajectory that measures the training dynamics across all training domains. PGrad's gradient design forces the DG training to ignore domain-dependent noise signals and updates all training domains with a robust direction covering main components of parameter dynamics. We further improve PGrad via bijection-based computational refinement and directional plus length-based calibrations. Our theoretical proof connects PGrad to the spectral analysis of Hessian in training neural networks. Experiments on DomainBed and WILDS benchmarks demonstrate that our approach effectively enables robust DG optimization and leads to smoothly decreased loss curves. Empirically, PGrad achieves competitive results across seven datasets, demonstrating its efficacy across both synthetic and real-world distributional shifts. Code is available at https://github.com/QData/PGrad.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2305.00447.pdf' target='_blank'>https://arxiv.org/pdf/2305.00447.pdf</a></span>   <span><a href='https://github.com/SAI990323/TALLRec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00447">TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendation, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https://github.com/SAI990323/TALLRec.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2304.13615.pdf' target='_blank'>https://arxiv.org/pdf/2304.13615.pdf</a></span>   <span><a href='https://github.com/lhoyer/HRDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Hoyer, Dengxin Dai, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13615">Domain Adaptive and Generalizable Network Architectures and Training Strategies for Semantic Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptation (UDA) and domain generalization (DG) enable machine learning models trained on a source domain to perform well on unlabeled or even unseen target domains. As previous UDA&DG semantic segmentation methods are mostly based on outdated networks, we benchmark more recent architectures, reveal the potential of Transformers, and design the DAFormer network tailored for UDA&DG. It is enabled by three training strategies to avoid overfitting to the source domain: While (1) Rare Class Sampling mitigates the bias toward common source domain classes, (2) a Thing-Class ImageNet Feature Distance and (3) a learning rate warmup promote feature transfer from ImageNet pretraining. As UDA&DG are usually GPU memory intensive, most previous methods downscale or crop images. However, low-resolution predictions often fail to preserve fine details while models trained with cropped images fall short in capturing long-range, domain-robust context information. Therefore, we propose HRDA, a multi-resolution framework for UDA&DG, that combines the strengths of small high-resolution crops to preserve fine segmentation details and large low-resolution crops to capture long-range context dependencies with a learned scale attention. DAFormer and HRDA significantly improve the state-of-the-art UDA&DG by more than 10 mIoU on 5 different benchmarks. The implementation is available at https://github.com/lhoyer/HRDA.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2304.12566.pdf' target='_blank'>https://arxiv.org/pdf/2304.12566.pdf</a></span>   <span><a href='https://github.com/yfzhang114/AdaNPC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Fan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12566">AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many recent machine learning tasks focus to develop models that can generalize to unseen distributions. Domain generalization (DG) has become one of the key topics in various fields. Several literatures show that DG can be arbitrarily hard without exploiting target domain information. To address this issue, test-time adaptive (TTA) methods are proposed. Existing TTA methods require offline target data or extra sophisticated optimization procedures during the inference stage. In this work, we adopt Non-Parametric Classifier to perform the test-time Adaptation (AdaNPC). In particular, we construct a memory that contains the feature and label pairs from training domains. During inference, given a test instance, AdaNPC first recalls K closed samples from the memory to vote for the prediction, and then the test feature and predicted label are added to the memory. In this way, the sample distribution in the memory can be gradually changed from the training distribution towards the test distribution with very little extra computation cost. We theoretically justify the rationality behind the proposed method. Besides, we test our model on extensive numerical experiments. AdaNPC significantly outperforms competitive baselines on various DG benchmarks. In particular, when the adaptation target is a series of domains, the adaptation accuracy of AdaNPC is 50% higher than advanced TTA methods. The code is available at https://github.com/yfzhang114/AdaNPC.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2304.05995.pdf' target='_blank'>https://arxiv.org/pdf/2304.05995.pdf</a></span>   <span><a href='https://github.com/mainaksingha01/APPLeNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mainak Singha, Ankit Jha, Bhupendra Solanki, Shirsha Bose, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05995">APPLeNet: Visual Attention Parameterized Prompt Learning for Few-Shot Remote Sensing Image Generalization using CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the success of large-scale vision-language models (VLMs) such as CLIP has led to their increased usage in various computer vision tasks. These models enable zero-shot inference through carefully crafted instructional text prompts without task-specific supervision. However, the potential of VLMs for generalization tasks in remote sensing (RS) has not been fully realized. To address this research gap, we propose a novel image-conditioned prompt learning strategy called the Visual Attention Parameterized Prompts Learning Network (APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning in RS scene classification and disentangles visual style and content primitives for domain generalization tasks. To achieve this, APPLeNet combines visual content features obtained from different layers of the vision encoder and style properties obtained from feature statistics of domain-specific batches. An attention-driven injection module is further introduced to generate visual tokens from this information. We also introduce an anti-correlation regularizer to ensure discrimination among the token embeddings, as this visual information is combined with the textual tokens. To validate APPLeNet, we curated four available RS benchmarks and introduced experimental protocols and datasets for three domain generalization tasks. Our results consistently outperform the relevant literature and code is available at https://github.com/mainaksingha01/APPLeNet
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2304.05640.pdf' target='_blank'>https://arxiv.org/pdf/2304.05640.pdf</a></span>   <span><a href='https://github.com/qianyuzqy/IADG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Xuequan Lu, Ran Yi, Shouhong Ding, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05640">Instance-Aware Domain Generalization for Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face anti-spoofing (FAS) based on domain generalization (DG) has been recently studied to improve the generalization on unseen scenarios. Previous methods typically rely on domain labels to align the distribution of each domain for learning domain-invariant representations. However, artificial domain labels are coarse-grained and subjective, which cannot reflect real domain distributions accurately. Besides, such domain-aware methods focus on domain-level alignment, which is not fine-grained enough to ensure that learned representations are insensitive to domain styles. To address these issues, we propose a novel perspective for DG FAS that aligns features on the instance level without the need for domain labels. Specifically, Instance-Aware Domain Generalization framework is proposed to learn the generalizable feature by weakening the features' sensitivity to instance-specific styles. Concretely, we propose Asymmetric Instance Adaptive Whitening to adaptively eliminate the style-sensitive feature correlation, boosting the generalization. Moreover, Dynamic Kernel Generator and Categorical Style Assembly are proposed to first extract the instance-specific features and then generate the style-diversified features with large style shifts, respectively, further facilitating the learning of style-insensitive features. Extensive experiments and analysis demonstrate the superiority of our method over state-of-the-art competitors. Code will be publicly available at https://github.com/qianyuzqy/IADG.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2304.04494.pdf' target='_blank'>https://arxiv.org/pdf/2304.04494.pdf</a></span>   <span><a href='https://github.com/liangchen527/ITTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Chen, Yong Zhang, Yibing Song, Ying Shan, Lingqiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04494">Improved Test-Time Adaptation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Generally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for updating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments indicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly considered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically defining an auxiliary objective, we propose a learnable consistency loss for the TTT task, which contains learnable parameters that can be adjusted toward better alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adaptive parameters during the test phase. Through extensive experiments, we show that the proposed two strategies are beneficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2304.02950.pdf' target='_blank'>https://arxiv.org/pdf/2304.02950.pdf</a></span>   <span><a href='https://github.com/K2OKOH/MAD"' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjun Xu, Lingyun Qin, Weijie Chen, Shiliang Pu, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02950">Multi-view Adversarial Discriminator: Mine the Non-causal Factors for Object Detection in Unseen Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift degrades the performance of object detection models in practical applications. To alleviate the influence of domain shift, plenty of previous work try to decouple and learn the domain-invariant (common) features from source domains via domain adversarial learning (DAL). However, inspired by causal mechanisms, we find that previous methods ignore the implicit insignificant non-causal factors hidden in the common features. This is mainly due to the single-view nature of DAL. In this work, we present an idea to remove non-causal factors from common features by multi-view adversarial training on source domains, because we observe that such insignificant non-causal factors may still be significant in other latent spaces (views) due to the multi-mode structure of data. To summarize, we propose a Multi-view Adversarial Discriminator (MAD) based domain generalization model, consisting of a Spurious Correlations Generator (SCG) that increases the diversity of source domain by random augmentation and a Multi-View Domain Classifier (MVDC) that maps features to multiple latent spaces, such that the non-causal factors are removed and the domain-invariant features are purified. Extensive experiments on six benchmarks show our MAD obtains state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2304.01973.pdf' target='_blank'>https://arxiv.org/pdf/2304.01973.pdf</a></span>   <span><a href='https://github.com/piotr-teterwak/erm_plusplus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Kate Saenko, Bryan A. Plummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01973">ERM++: An Improved Baseline for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to develop classifiers that can generalize to new, unseen data distributions, a critical capability when collecting new domain-specific data is impractical. A common DG baseline minimizes the empirical risk on the source domains. Recent studies have shown that this approach, known as Empirical Risk Minimization (ERM), can outperform most more complex DG methods when properly tuned. However, these studies have primarily focused on a narrow set of hyperparameters, neglecting other factors that can enhance robustness and prevent overfitting and catastrophic forgetting, properties which are critical for strong DG performance. In our investigation of training data utilization (i.e., duration and setting validation splits), initialization, and additional regularizers, we find that tuning these previously overlooked factors significantly improves model generalization across diverse datasets without adding much complexity. We call this improved, yet simple baseline ERM++. Despite its ease of implementation, ERM++ improves DG performance by over 5\% compared to prior ERM baselines on a standard benchmark of 5 datasets with a ResNet-50 and over 15\% with a ViT-B/16. It also outperforms all state-of-the-art methods on DomainBed datasets with both architectures. Importantly, ERM++ is easy to integrate into existing frameworks like DomainBed, making it a practical and powerful tool for researchers and practitioners. Overall, ERM++ challenges the need for more complex DG methods by providing a stronger, more reliable baseline that maintains simplicity and ease of use. Code is available at \url{https://github.com/piotr-teterwak/erm_plusplus}
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2304.01508.pdf' target='_blank'>https://arxiv.org/pdf/2304.01508.pdf</a></span>   <span><a href='https://github.com/SiyuanYan1/EPVT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Yan, Chi Liu, Zhen Yu, Lie Ju, Dwarikanath Mahapatrainst, Victoria Mar, Monika Janda, Peter Soyer, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01508">EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e., dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A domain mixup strategy is additionally devised to reduce the co-occurring artifacts in each domain, which allows for more flexible decision margins and mitigates the issue of incorrectly assigned domain labels. Experiments on four out-of-distribution datasets and six different biased ISIC datasets demonstrate the superior generalization ability of EPVT in skin lesion recognition across various environments. Code is avaliable at https://github.com/SiyuanYan1/EPVT.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2304.01434.pdf' target='_blank'>https://arxiv.org/pdf/2304.01434.pdf</a></span>   <span><a href='https://github.com/jaeill/CVPR23-VNE' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/jaeill/CVPR23-VNE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeill Kim, Suhyun Kang, Duhun Hwang, Jungwook Shin, Wonjong Rhee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01434">VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the introduction of deep learning, a wide scope of representation properties, such as decorrelation, whitening, disentanglement, rank, isotropy, and mutual information, have been studied to improve the quality of representation. However, manipulating such properties can be challenging in terms of implementational effectiveness and general applicability. To address these limitations, we propose to regularize von Neumann entropy~(VNE) of representation. First, we demonstrate that the mathematical formulation of VNE is superior in effectively manipulating the eigenvalues of the representation autocorrelation matrix. Then, we demonstrate that it is widely applicable in improving state-of-the-art algorithms or popular benchmark algorithms by investigating domain-generalization, meta-learning, self-supervised learning, and generative models. In addition, we formally establish theoretical connections with rank, disentanglement, and isotropy of representation. Finally, we provide discussions on the dimension control of VNE and the relationship with Shannon entropy. Code is available at: https://github.com/jaeill/CVPR23-VNE.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2303.18031.pdf' target='_blank'>https://arxiv.org/pdf/2303.18031.pdf</a></span>   <span><a href='https://github.com/shiralab/OpenDG-Eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Masashi Noguchi, Shinichi Shirakawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.18031">Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world applications, a machine learning model is required to handle an open-set recognition (OSR), where unknown classes appear during the inference, in addition to a domain shift, where the distribution of data differs between the training and inference phases. Domain generalization (DG) aims to handle the domain shift situation where the target domain of the inference phase is inaccessible during model training. Open domain generalization (ODG) takes into account both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a method targeting ODG but has a complicated learning process. On the other hand, although various DG methods have been proposed, they have not been evaluated in ODG situations. This work comprehensively evaluates existing DG methods in ODG and shows that two simple DG methods, CORrelation ALignment (CORAL) and Maximum Mean Discrepancy (MMD), are competitive with DAML in several cases. In addition, we propose simple extensions of CORAL and MMD by introducing the techniques used in DAML, such as ensemble learning and Dirichlet mixup data augmentation. The experimental evaluation demonstrates that the extended CORAL and MMD can perform comparably to DAML with lower computational costs. This suggests that the simple DG methods and their simple extensions are strong baselines for ODG. The code used in the experiments is available at https://github.com/shiralab/OpenDG-Eval.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2303.15698.pdf' target='_blank'>https://arxiv.org/pdf/2303.15698.pdf</a></span>   <span><a href='https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo A. Vargas Hakim, David Osowiechi, Ismail Ben Ayed, Christian Desrosiers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15698">TFS-ViT: Token-Level Feature Stylization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Standard deep learning models such as convolutional neural networks (CNNs) lack the ability of generalizing to domains which have not been seen during training. This problem is mainly due to the common but often wrong assumption of such models that the source and target data come from the same i.i.d. distribution. Recently, Vision Transformers (ViTs) have shown outstanding performance for a broad range of computer vision tasks. However, very few studies have investigated their ability to generalize to new domains. This paper presents a first Token-level Feature Stylization (TFS-ViT) approach for domain generalization, which improves the performance of ViTs to unseen data by synthesizing new domains. Our approach transforms token features by mixing the normalization statistics of images from different domains. We further improve this approach with a novel strategy for attention-aware stylization, which uses the attention maps of class (CLS) tokens to compute and mix normalization statistics of tokens corresponding to different image regions. The proposed method is flexible to the choice of backbone model and can be easily applied to any ViT-based architecture with a negligible increase in computational complexity. Comprehensive experiments show that our approach is able to achieve state-of-the-art performance on five challenging benchmarks for domain generalization, and demonstrate its ability to deal with different types of domain shifts. The implementation is available at: https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2303.13813.pdf' target='_blank'>https://arxiv.org/pdf/2303.13813.pdf</a></span>   <span><a href='https://github.com/PKU-ML/Generalist' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjun Wang, Yisen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13813">Generalist: Decoupling Natural and Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks obtained by standard training have been constantly plagued by adversarial examples. Although adversarial training demonstrates its capability to defend against adversarial examples, unfortunately, it leads to an inevitable drop in the natural generalization. To address the issue, we decouple the natural generalization and the robust generalization from joint training and formulate different training strategies for each one. Specifically, instead of minimizing a global loss on the expectation over these two generalization errors, we propose a bi-expert framework called \emph{Generalist} where we simultaneously train base learners with task-aware strategies so that they can specialize in their own fields. The parameters of base learners are collected and combined to form a global learner at intervals during the training process. The global learner is then distributed to the base learners as initialized parameters for continued training. Theoretically, we prove that the risks of Generalist will get lower once the base learners are well trained. Extensive experiments verify the applicability of Generalist to achieve high accuracy on natural examples while maintaining considerable robustness to adversarial ones. Code is available at https://github.com/PKU-ML/Generalist.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2303.12314.pdf' target='_blank'>https://arxiv.org/pdf/2303.12314.pdf</a></span>   <span><a href='https://github.com/beepkh/SUPMER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaihang Pan, Juncheng Li, Hongye Song, Jun Lin, Xiaozhong Liu, Siliang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12314">Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they fail to data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with MEta-gradient Regularization for few-shot generalization (SUPMER). SUPMER leverages self-supervised meta-learning with a diverse set of well-designed meta-training tasks to learn a universal prompt initialization for efficient adaptation using only unlabeled data. Additionally, it jointly meta-learns a gradient regularization function to transform raw gradients into a domain-generalizable direction, thus alleviating the problem of overfitting. Extensive experiments show that SUPMER achieves better performance for different few-shot downstream tasks, and also exhibits a stronger domain generalization ability. The code for SUPMER will be available at https://github.com/beepkh/SUPMER.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2303.11674.pdf' target='_blank'>https://arxiv.org/pdf/2303.11674.pdf</a></span>   <span><a href='https://github.com/lingeringlight/ALOFT/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lingeringlight/ALOFT/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Guo, Na Wang, Lei Qi, Yinghuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11674">ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most state-of-the-art CNN-based methods. The baseline can learn global structure representations with a filter to suppress structure irrelevant information in the frequency space. Moreover, we propose a dynAmic LOw-Frequency spectrum Transform (ALOFT) that can perturb local texture features while preserving global structure features, thus enabling the filter to remove structure-irrelevant information sufficiently. Extensive experiments on four benchmarks have demonstrated that our method can achieve great performance improvement with a small number of parameters compared to SOTA CNN-based DG methods. Our code is available at https://github.com/lingeringlight/ALOFT/.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2303.10902.pdf' target='_blank'>https://arxiv.org/pdf/2303.10902.pdf</a></span>   <span><a href='https://github.com/SakurajimaMaiii/TSD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, Rui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10902">Feature Alignment and Uniformity for Test Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test time adaptation (TTA) aims to adapt deep neural networks when receiving out of distribution test domain samples. In this setting, the model can only access online unlabeled test samples and pre-trained models on the training domains. We first address TTA as a feature revision problem due to the domain gap between source domains and target domains. After that, we follow the two measurements alignment and uniformity to discuss the test time feature revision. For test time feature uniformity, we propose a test time self-distillation strategy to guarantee the consistency of uniformity between representations of the current batch and all the previous batches. For test time feature alignment, we propose a memorized spatial local clustering strategy to align the representations among the neighborhood samples for the upcoming batch. To deal with the common noisy label problem, we propound the entropy and consistency filters to select and drop the possible noisy labels. To prove the scalability and efficacy of our method, we conduct experiments on four domain generalization benchmarks and four medical image segmentation tasks with various backbones. Experiment results show that our method not only improves baseline stably but also outperforms existing state-of-the-art test time adaptation methods. Code is available at \href{https://github.com/SakurajimaMaiii/TSD}{https://github.com/SakurajimaMaiii/TSD}.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2303.10353.pdf' target='_blank'>https://arxiv.org/pdf/2303.10353.pdf</a></span>   <span><a href='https://github.com/Wang-pengfei/SAGM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10353">Sharpness-Aware Gradient Matching for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of domain generalization (DG) is to enhance the generalization capability of the model learned from a source domain to other unseen domains. The recently developed Sharpness-Aware Minimization (SAM) method aims to achieve this goal by minimizing the sharpness measure of the loss landscape. Though SAM and its variants have demonstrated impressive DG performance, they may not always converge to the desired flat region with a small loss value. In this paper, we present two conditions to ensure that the model could converge to a flat minimum with a small loss, and present an algorithm, named Sharpness-Aware Gradient Matching (SAGM), to meet the two conditions for improving model generalization capability. Specifically, the optimization objective of SAGM will simultaneously minimize the empirical risk, the perturbed loss (i.e., the maximum loss within a neighborhood in the parameter space), and the gap between them. By implicitly aligning the gradient directions between the empirical risk and the perturbed loss, SAGM improves the generalization capability over SAM and its variants without increasing the computational cost. Extensive experimental results show that our proposed SAGM method consistently outperforms the state-of-the-art methods on five DG benchmarks, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Codes are available at https://github.com/Wang-pengfei/SAGM.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2303.01233.pdf' target='_blank'>https://arxiv.org/pdf/2303.01233.pdf</a></span>   <span><a href='https://github.com/workerbcd/DCT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyu Guo, Brian Lovell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01233">Domain-aware Triplet loss in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress being made in the field of object recognition with the advances of deep learning, there are still several factors negatively affecting the performance of deep learning models. Domain shift is one of these factors and is caused by discrepancies in the distributions of the testing and training data. In this paper, we focus on the problem of compact feature clustering in domain generalization to help optimize the embedding space from multi-domain data. We design a domainaware triplet loss for domain generalization to help the model to not only cluster similar semantic features, but also to disperse features arising from the domain. Unlike previous methods focusing on distribution alignment, our algorithm is designed to disperse domain information in the embedding space. The basic idea is motivated based on the assumption that embedding features can be clustered based on domain information, which is mathematically and empirically supported in this paper. In addition, during our exploration of feature clustering in domain generalization, we note that factors affecting the convergence of metric learning loss in domain generalization are more important than the pre-defined domains. To solve this issue, we utilize two methods to normalize the embedding space, reducing the internal covariate shift of the embedding features. The ablation study demonstrates the effectiveness of our algorithm. Moreover, the experiments on the benchmark datasets, including PACS, VLCS and Office-Home, show that our method outperforms related methods focusing on domain discrepancy. In particular, our results on RegnetY-16 are significantly better than state-of-the-art methods on the benchmark datasets. Our code will be released at https://github.com/workerbcd/DCT
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2303.00455.pdf' target='_blank'>https://arxiv.org/pdf/2303.00455.pdf</a></span>   <span><a href='https://github.com/nttcslab/dcase2023_task2_baseline_ae' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Noboru Harada, Daisuke Niizumi, Yasunori Ohishi, Daiki Takeuchi, Masahiro Yasuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00455">First-shot anomaly sound detection for machine condition monitoring: A domain generalization baseline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides a baseline system for First-shot-compliant unsupervised anomaly detection (ASD) for machine condition monitoring. First-shot ASD does not allow systems to do machine-type dependent hyperparameter tuning or tool ensembling based on the performance metric calculated with the grand truth. To show benchmark performance for First-shot ASD, this paper proposes an anomaly sound detection system that works on the domain generalization task in the Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 Challenge Task 2: "Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Technique" while complying with the First-shot requirements introduced in the DCASE 2023 Challenge Task 2 (DCASE2023T2). A simple autoencoder based implementation combined with selective Mahalanobis metric is implemented as a baseline system. The performance evaluation is conducted to set the target benchmark for the forthcoming DCASE2023T2. Source code of the baseline system will be available on GitHub: https://github.com/nttcslab/dcase2023_task2_baseline_ae .
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2302.06637.pdf' target='_blank'>https://arxiv.org/pdf/2302.06637.pdf</a></span>   <span><a href='https://github.com/NVlabs/PerAda' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chulin Xie, De-An Huang, Wenda Chu, Daguang Xu, Chaowei Xiao, Bo Li, Anima Anandkumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06637">PerAda: Parameter-Efficient Federated Learning Personalization with Generalization Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Personalized Federated Learning (pFL) has emerged as a promising solution to tackle data heterogeneity across clients in FL. However, existing pFL methods either (1) introduce high communication and computation costs or (2) overfit to local data, which can be limited in scope, and are vulnerable to evolved test samples with natural shifts. In this paper, we propose PerAda, a parameter-efficient pFL framework that reduces communication and computational costs and exhibits superior generalization performance, especially under test-time distribution shifts. PerAda reduces the costs by leveraging the power of pretrained models and only updates and communicates a small number of additional parameters from adapters. PerAda has good generalization since it regularizes each client's personalized adapter with a global adapter, while the global adapter uses knowledge distillation to aggregate generalized information from all clients. Theoretically, we provide generalization bounds to explain why PerAda improves generalization, and we prove its convergence to stationary points under non-convex settings. Empirically, PerAda demonstrates competitive personalized performance (+4.85% on CheXpert) and enables better out-of-distribution generalization (+5.23% on CIFAR-10-C) on different datasets across natural and medical domains compared with baselines, while only updating 12.6% of parameters per model based on the adapter. Our code is available at https://github.com/NVlabs/PerAda.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2302.05936.pdf' target='_blank'>https://arxiv.org/pdf/2302.05936.pdf</a></span>   <span><a href='https://github.com/yawencui/CMoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yawen Cui, Zitong Yu, Rizhao Cai, Xun Wang, Alex C. Kot, Li Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05936">Generalized Few-Shot Continual Learning with Contrastive Mixture of Adapters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of Few-Shot Continual Learning (FSCL) is to incrementally learn novel tasks with limited labeled samples and preserve previous capabilities simultaneously, while current FSCL methods are all for the class-incremental purpose. Moreover, the evaluation of FSCL solutions is only the cumulative performance of all encountered tasks, but there is no work on exploring the domain generalization ability. Domain generalization is a challenging yet practical task that aims to generalize beyond training domains. In this paper, we set up a Generalized FSCL (GFSCL) protocol involving both class- and domain-incremental situations together with the domain generalization assessment. Firstly, two benchmark datasets and protocols are newly arranged, and detailed baselines are provided for this unexplored configuration. We find that common continual learning methods have poor generalization ability on unseen domains and cannot better cope with the catastrophic forgetting issue in cross-incremental tasks. In this way, we further propose a rehearsal-free framework based on Vision Transformer (ViT) named Contrastive Mixture of Adapters (CMoA). Due to different optimization targets of class increment and domain increment, the CMoA contains two parts: (1) For the class-incremental issue, the Mixture of Adapters (MoA) module is incorporated into ViT, then cosine similarity regularization and the dynamic weighting are designed to make each adapter learn specific knowledge and concentrate on particular classes. (2) For the domain-related issues and domain-invariant representation learning, we alleviate the inner-class variation by prototype-calibrated contrastive learning. The codes and protocols are available at https://github.com/yawencui/CMoA.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2301.13530.pdf' target='_blank'>https://arxiv.org/pdf/2301.13530.pdf</a></span>   <span><a href='https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.13530">Domain-Generalizable Multiple-Domain Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme. We demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2301.13323.pdf' target='_blank'>https://arxiv.org/pdf/2301.13323.pdf</a></span>   <span><a href='https://github.com/pth1993/FATDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thai-Hoang Pham, Xueru Zhang, Ping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.13323">Fairness and Accuracy under Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning (ML) algorithms are increasingly used in high-stakes applications, concerns have arisen that they may be biased against certain social groups. Although many approaches have been proposed to make ML models fair, they typically rely on the assumption that data distributions in training and deployment are identical. Unfortunately, this is commonly violated in practice and a model that is fair during training may lead to an unexpected outcome during its deployment. Although the problem of designing robust ML models under dataset shifts has been widely studied, most existing works focus only on the transfer of accuracy. In this paper, we study the transfer of both fairness and accuracy under domain generalization where the data at test time may be sampled from never-before-seen domains. We first develop theoretical bounds on the unfairness and expected loss at deployment, and then derive sufficient conditions under which fairness and accuracy can be perfectly transferred via invariant representation learning. Guided by this, we design a learning algorithm such that fair ML models learned with training data still have high fairness and accuracy when deployment environments change. Experiments on real-world data validate the proposed algorithm. Model implementation is available at https://github.com/pth1993/FATDM.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2301.12643.pdf' target='_blank'>https://arxiv.org/pdf/2301.12643.pdf</a></span>   <span><a href='https://github.com/YBZh/AdvStyle' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/YBZh/AdvStyle' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yabin Zhang, Bin Deng, Ruihuang Li, Kui Jia, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12643">Adversarial Style Augmentation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is well-known that the performance of well-trained deep neural networks may degrade significantly when they are applied to data with even slightly shifted distributions. Recent studies have shown that introducing certain perturbation on feature statistics (\eg, mean and standard deviation) during training can enhance the cross-domain generalization ability. Existing methods typically conduct such perturbation by utilizing the feature statistics within a mini-batch, limiting their representation capability. Inspired by the domain generalization objective, we introduce a novel Adversarial Style Augmentation (ASA) method, which explores broader style spaces by generating more effective statistics perturbation via adversarial training. Specifically, we first search for the most sensitive direction and intensity for statistics perturbation by maximizing the task loss. By updating the model against the adversarial statistics perturbation during training, we allow the model to explore the worst-case domain and hence improve its generalization performance. To facilitate the application of ASA, we design a simple yet effective module, namely AdvStyle, which instantiates the ASA method in a plug-and-play manner. We justify the efficacy of AdvStyle on tasks of cross-domain classification and instance retrieval. It achieves higher mean accuracy and lower performance fluctuation. Especially, our method significantly outperforms its competitors on the PACS dataset under the single source generalization setting, \eg, boosting the classification accuracy from 61.2\% to 67.1\% with a ResNet50 backbone. Our code will be available at \url{https://github.com/YBZh/AdvStyle}.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2301.08834.pdf' target='_blank'>https://arxiv.org/pdf/2301.08834.pdf</a></span>   <span><a href='https://github.com/ycq091044/ManyDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoqi Yang, M. Brandon Westover, Jimeng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08834">ManyDG: Many-domain Generalization for Healthcare Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vast amount of health data has been continuously collected for each patient, providing opportunities to support diverse healthcare predictive tasks such as seizure detection and hospitalization prediction. Existing models are mostly trained on other patients data and evaluated on new patients. Many of them might suffer from poor generalizability. One key reason can be overfitting due to the unique information related to patient identities and their data collection environments, referred to as patient covariates in the paper. These patient covariates usually do not contribute to predicting the targets but are often difficult to remove. As a result, they can bias the model training process and impede generalization. In healthcare applications, most existing domain generalization methods assume a small number of domains. In this paper, considering the diversity of patient covariates, we propose a new setting by treating each patient as a separate domain (leading to many domains). We develop a new domain generalization method ManyDG, that can scale to such many-domain problems. Our method identifies the patient domain covariates by mutual reconstruction and removes them via an orthogonal projection step. Extensive experiments show that ManyDG can boost the generalization performance on multiple real-world healthcare tasks (e.g., 3.7% Jaccard improvements on MIMIC drug recommendation) and support realistic but challenging settings such as insufficient data and continuous learning.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2301.06442.pdf' target='_blank'>https://arxiv.org/pdf/2301.06442.pdf</a></span>   <span><a href='https://github.com/lixiaotong97/DSU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotong Li, Zixuan Hu, Jun Liu, Yixiao Ge, Yongxing Dai, Ling-Yu Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06442">Modeling Uncertain Feature Representation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Though deep neural networks have achieved impressive success on various vision tasks, obvious performance degradation still exists when models are tested in out-of-distribution scenarios. In addressing this limitation, we ponder that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization ability of deep learning models. Existing methods commonly consider feature statistics as deterministic values measured from the learned features and do not explicitly model the uncertain statistics discrepancy caused by potential domain shifts during testing. In this paper, we improve the network generalization ability by modeling domain shifts with uncertainty (DSU), i.e., characterizing the feature statistics as uncertain distributions during training. Specifically, we hypothesize that the feature statistic, after considering the potential uncertainties, follows a multivariate Gaussian distribution. During inference, we propose an instance-wise adaptation strategy that can adaptively deal with the unforeseeable shift and further enhance the generalization ability of the trained model with negligible additional cost. We also conduct theoretical analysis on the aspects of generalization error bound and the implicit regularization effect, showing the efficacy of our method. Extensive experiments demonstrate that our method consistently improves the network generalization ability on multiple vision tasks, including image classification, semantic segmentation, instance retrieval, and pose estimation. Our methods are simple yet effective and can be readily integrated into networks without additional trainable parameters or loss constraints. Code will be released in https://github.com/lixiaotong97/DSU.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2301.04796.pdf' target='_blank'>https://arxiv.org/pdf/2301.04796.pdf</a></span>   <span><a href='https://github.com/hikvision-research/OOD-CV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhao, Binbin Chen, Weijie Chen, Shicai Yang, Di Xie, Shiliang Pu, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04796">1st Place Solution for ECCV 2022 OOD-CV Challenge Object Detection Track</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>OOD-CV challenge is an out-of-distribution generalization task. To solve this problem in object detection track, we propose a simple yet effective Generalize-then-Adapt (G&A) framework, which is composed of a two-stage domain generalization part and a one-stage domain adaptation part. The domain generalization part is implemented by a Supervised Model Pretraining stage using source data for model warm-up and a Weakly Semi-Supervised Model Pretraining stage using both source data with box-level label and auxiliary data (ImageNet-1K) with image-level label for performance boosting. The domain adaptation part is implemented as a Source-Free Domain Adaptation paradigm, which only uses the pre-trained model and the unlabeled target data to further optimize in a self-supervised training manner. The proposed G&A framework help us achieve the first place on the object detection leaderboard of the OOD-CV challenge. Code will be released in https://github.com/hikvision-research/OOD-CV.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2301.04795.pdf' target='_blank'>https://arxiv.org/pdf/2301.04795.pdf</a></span>   <span><a href='https://github.com/hikvision-research/OOD-CV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilu Guo, Xingyue Shi, Weijie Chen, Shicai Yang, Di Xie, Shiliang Pu, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04795">1st Place Solution for ECCV 2022 OOD-CV Challenge Image Classification Track</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>OOD-CV challenge is an out-of-distribution generalization task. In this challenge, our core solution can be summarized as that Noisy Label Learning Is A Strong Test-Time Domain Adaptation Optimizer. Briefly speaking, our main pipeline can be divided into two stages, a pre-training stage for domain generalization and a test-time training stage for domain adaptation. We only exploit labeled source data in the pre-training stage and only exploit unlabeled target data in the test-time training stage. In the pre-training stage, we propose a simple yet effective Mask-Level Copy-Paste data augmentation strategy to enhance out-of-distribution generalization ability so as to resist shape, pose, context, texture, occlusion, and weather domain shifts in this challenge. In the test-time training stage, we use the pre-trained model to assign noisy label for the unlabeled target data, and propose a Label-Periodically-Updated DivideMix method for noisy label learning. After integrating Test-Time Augmentation and Model Ensemble strategies, our solution ranks the first place on the Image Classification Leaderboard of the OOD-CV Challenge. Code will be released in https://github.com/hikvision-research/OOD-CV.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2212.10534.pdf' target='_blank'>https://arxiv.org/pdf/2212.10534.pdf</a></span>   <span><a href='https://github.com/eric11eca/disco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle Richardson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.10534">DISCO: Distilling Counterfactuals with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCO generated counterfactuals are more robust (6% absolute) and generalize better across distributions (2%) compared to models trained without data augmentation. Furthermore, DISCO augmented models are 10% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO augmentation enables models to more reliably learn causal representations. Our repository is available at: https://github.com/eric11eca/disco
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2212.10445.pdf' target='_blank'>https://arxiv.org/pdf/2212.10445.pdf</a></span>   <span><a href='https://github.com/facebookresearch/ModelRatatouille' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre RamÃ©, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, LÃ©on Bottou, David Lopez-Paz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.10445">Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks. Empirically, it improves the state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, this work contributes to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to reliably update machine learning models. Our code is released: https://github.com/facebookresearch/ModelRatatouille.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2212.09950.pdf' target='_blank'>https://arxiv.org/pdf/2212.09950.pdf</a></span>   <span><a href='https://github.com/freshman97/CSU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Bin Wang, Debesh Jha, Ugur Demir, Ulas Bagci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.09950">Domain Generalization with Correlated Style Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) approaches intend to extract domain invariant features that can lead to a more robust deep learning model. In this regard, style augmentation is a strong DG method taking advantage of instance-specific feature statistics containing informative style characteristics to synthetic novel domains. While it is one of the state-of-the-art methods, prior works on style augmentation have either disregarded the interdependence amongst distinct feature channels or have solely constrained style augmentation to linear interpolation. To address these research gaps, in this work, we introduce a novel augmentation approach, named Correlated Style Uncertainty (CSU), surpassing the limitations of linear interpolation in style statistic space and simultaneously preserving vital correlation information. Our method's efficacy is established through extensive experimentation on diverse cross-domain computer vision and medical imaging classification tasks: PACS, Office-Home, and Camelyon17 datasets, and the Duke-Market1501 instance retrieval task. The results showcase a remarkable improvement margin over existing state-of-the-art techniques. The source code is available https://github.com/freshman97/CSU.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2212.09068.pdf' target='_blank'>https://arxiv.org/pdf/2212.09068.pdf</a></span>   <span><a href='https://github.com/HeliosZhao/SHADE-VisualDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Zhao, Zhun Zhong, Na Zhao, Nicu Sebe, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.09068">Style-Hallucinated Dual Consistency Learning: A Unified Framework for Visual Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift widely exists in the visual world, while modern deep neural networks commonly suffer from severe performance degradation under domain shift due to the poor generalization ability, which limits the real-world applications. The domain shift mainly lies in the limited source environmental variations and the large distribution gap between source and unseen target data. To this end, we propose a unified framework, Style-HAllucinated Dual consistEncy learning (SHADE), to handle such domain shift in various visual tasks. Specifically, SHADE is constructed based on two consistency constraints, Style Consistency (SC) and Retrospection Consistency (RC). SC enriches the source situations and encourages the model to learn consistent representation across style-diversified samples. RC leverages general visual knowledge to prevent the model from overfitting to source data and thus largely keeps the representation consistent between the source and general visual models. Furthermore, we present a novel style hallucination module (SHM) to generate style-diversified samples that are essential to consistency learning. SHM selects basis styles from the source distribution, enabling the model to dynamically generate diverse and realistic samples during training. Extensive experiments demonstrate that our versatile SHADE can significantly enhance the generalization in various visual recognition tasks, including image classification, semantic segmentation and object detection, with different models, i.e., ConvNets and Transformer.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2212.04245.pdf' target='_blank'>https://arxiv.org/pdf/2212.04245.pdf</a></span>   <span><a href='https://github.com/JulesSanchez/3DLabelProp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jules Sanchez, Jean-Emmanuel Deschaud, Francois Goulette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04245">Domain generalization of 3D semantic segmentation in autonomous driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using deep learning, 3D autonomous driving semantic segmentation has become a well-studied subject, with methods that can reach very high performance. Nonetheless, because of the limited size of the training datasets, these models cannot see every type of object and scene found in real-world applications. The ability to be reliable in these various unknown environments is called \textup{domain generalization}.
  Despite its importance, domain generalization is relatively unexplored in the case of 3D autonomous driving semantic segmentation. To fill this gap, this paper presents the first benchmark for this application by testing state-of-the-art methods and discussing the difficulty of tackling Laser Imaging Detection and Ranging (LiDAR) domain shifts.
  We also propose the first method designed to address this domain generalization, which we call 3DLabelProp. This method relies on leveraging the geometry and sequentiality of the LiDAR data to enhance its generalization performances by working on partially accumulated point clouds. It reaches a mean Intersection over Union (mIoU) of 50.4% on SemanticPOSS and of 55.2% on PandaSet solid-state LiDAR while being trained only on SemanticKITTI, making it the state-of-the-art method for generalization (+5% and +33% better, respectively, than the second best method).
  The code for this method is available on GitHub: https://github.com/JulesSanchez/3DLabelProp.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2212.00979.pdf' target='_blank'>https://arxiv.org/pdf/2212.00979.pdf</a></span>   <span><a href='https://github.com/prithv1/PASTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithvijit Chattopadhyay, Kartik Sarangmath, Vivek Vijaykumar, Judy Hoffman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.00979">PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real), object detection (Sim10K-to-Real), and object recognition (VisDA-C Syn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complementary to the same.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2212.00259.pdf' target='_blank'>https://arxiv.org/pdf/2212.00259.pdf</a></span>   <span><a href='https://github.com/Lizw14/Super-CLEVR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Lizw14/Super-CLEVR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, Alan Yuille
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.00259">Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Question Answering (VQA) models often perform poorly on out-of-distribution data and struggle on domain generalization. Due to the multi-modal nature of this task, multiple factors of variation are intertwined, making generalization difficult to analyze. This motivates us to introduce a virtual benchmark, Super-CLEVR, where different factors in VQA domain shifts can be isolated in order that their effects can be studied independently. Four factors are considered: visual complexity, question redundancy, concept distribution and concept compositionality. With controllably generated data, Super-CLEVR enables us to test VQA methods in situations where the test data differs from the training data along each of these axes. We study four existing methods, including two neural symbolic methods NSCL and NSVQA, and two non-symbolic methods FiLM and mDETR; and our proposed method, probabilistic NSVQA (P-NSVQA), which extends NSVQA with uncertainty reasoning. P-NSVQA outperforms other methods on three of the four domain shift factors. Our results suggest that disentangling reasoning and perception, combined with probabilistic uncertainty, form a strong VQA model that is more robust to domain shifts. The dataset and code are released at https://github.com/Lizw14/Super-CLEVR.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2211.11460.pdf' target='_blank'>https://arxiv.org/pdf/2211.11460.pdf</a></span>   <span><a href='https://github.com/gzoumpourlis/Ensemble-MI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Zoumpourlis, Ioannis Patras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11460">Motor Imagery Decoding Using Ensemble Curriculum Learning and Collaborative Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we study the problem of cross-subject motor imagery (MI) decoding from electroencephalography (EEG) data. Multi-subject EEG datasets present several kinds of domain shifts due to various inter-individual differences (e.g. brain anatomy, personality and cognitive profile). These domain shifts render multi-subject training a challenging task and also impede robust cross-subject generalization. Inspired by the importance of domain generalization techniques for tackling such issues, we propose a two-stage model ensemble architecture built with multiple feature extractors (first stage) and a shared classifier (second stage), which we train end-to-end with two novel loss terms. The first loss applies curriculum learning, forcing each feature extractor to specialize to a subset of the training subjects and promoting feature diversity. The second loss is an intra-ensemble distillation objective that allows collaborative exchange of knowledge between the models of the ensemble. We compare our method against several state-of-the-art techniques, conducting subject-independent experiments on two large MI datasets, namely PhysioNet and OpenBMI. Our algorithm outperforms all of the methods in both 5-fold cross-validation and leave-one-subject-out evaluation settings, using a substantially lower number of trainable parameters. We demonstrate that our model ensembling approach combining the powers of curriculum learning and collaborative training, leads to high learning capacity and robust performance. Our work addresses the issue of domain shifts in multi-subject EEG datasets, paving the way for calibration-free brain-computer interfaces. We make our code publicly available at: https://github.com/gzoumpourlis/Ensemble-MI
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2211.05228.pdf' target='_blank'>https://arxiv.org/pdf/2211.05228.pdf</a></span>   <span><a href='https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/jindongwang/transferlearning/tree/master/code/deep/fixed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Lu, Jindong Wang, Han Yu, Lei Huang, Xiang Zhang, Yiqiang Chen, Xing Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.05228">FIXED: Frustratingly Easy Domain Generalization with Mixup</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn a generalizable model from multiple training domains such that it can perform well on unseen target domains. A popular strategy is to augment training data to benefit generalization through methods such as Mixup~\cite{zhang2018mixup}. While the vanilla Mixup can be directly applied, theoretical and empirical investigations uncover several shortcomings that limit its performance. Firstly, Mixup cannot effectively identify the domain and class information that can be used for learning invariant representations. Secondly, Mixup may introduce synthetic noisy data points via random interpolation, which lowers its discrimination capability. Based on the analysis, we propose a simple yet effective enhancement for Mixup-based DG, namely domain-invariant Feature mIXup (FIX). It learns domain-invariant representations for Mixup. To further enhance discrimination, we leverage existing techniques to enlarge margins among classes to further propose the domain-invariant Feature MIXup with Enhanced Discrimination (FIXED) approach. We present theoretical insights about guarantees on its effectiveness. Extensive experiments on seven public datasets across two modalities including image classification (Digits-DG, PACS, Office-Home) and time series (DSADS, PAMAP2, UCI-HAR, and USC-HAD) demonstrate that our approach significantly outperforms nine state-of-the-art related methods, beating the best performing baseline by 6.5\% on average in terms of test accuracy. Code is available at: https://github.com/jindongwang/transferlearning/tree/master/code/deep/fixed.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2211.03553.pdf' target='_blank'>https://arxiv.org/pdf/2211.03553.pdf</a></span>   <span><a href='https://github.com/Genentech/sVAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Romain Lopez, NataÅ¡a Tagasovska, Stephen Ra, Kyunghyn Cho, Jonathan K. Pritchard, Aviv Regev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.03553">Learning Causal Representations of Single Cells via Sparse Mechanism Shift Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent variable models such as the Variational Auto-Encoder (VAE) have become a go-to tool for analyzing biological data, especially in the field of single-cell genomics. One remaining challenge is the interpretability of latent variables as biological processes that define a cell's identity. Outside of biological applications, this problem is commonly referred to as learning disentangled representations. Although several disentanglement-promoting variants of the VAE were introduced, and applied to single-cell genomics data, this task has been shown to be infeasible from independent and identically distributed measurements, without additional structure. Instead, recent methods propose to leverage non-stationary data, as well as the sparse mechanism shift assumption in order to learn disentangled representations with a causal semantic. Here, we extend the application of these methodological advances to the analysis of single-cell genomics data with genetic or chemical perturbations. More precisely, we propose a deep generative model of single-cell gene expression data for which each perturbation is treated as a stochastic intervention targeting an unknown, but sparse, subset of latent variables. We benchmark these methods on simulated single-cell data to evaluate their performance at latent units recovery, causal target identification and out-of-domain generalization. Finally, we apply those approaches to two real-world large-scale gene perturbation data sets and find that models that exploit the sparse mechanism shift hypothesis surpass contemporary methods on a transfer learning task. We implement our new model and benchmarks using the scvi-tools library, and release it as open-source software at https://github.com/Genentech/sVAE.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2211.02843.pdf' target='_blank'>https://arxiv.org/pdf/2211.02843.pdf</a></span>   <span><a href='https://github.com/yongduosui/AIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongduo Sui, Qitian Wu, Jiancan Wu, Qing Cui, Longfei Li, Jun Zhou, Xiang Wang, Xiangnan He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.02843">Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The issue of distribution shifts is emerging as a critical concern in graph representation learning. From the perspective of invariant learning and stable learning, a recently well-established paradigm for out-of-distribution generalization, stable features of the graph are assumed to causally determine labels, while environmental features tend to be unstable and can lead to the two primary types of distribution shifts. The correlation shift is often caused by the spurious correlation between environmental features and labels that differs between the training and test data; the covariate shift often stems from the presence of new environmental features in test data. However, most strategies, such as invariant learning or graph augmentation, typically struggle with limited training environments or perturbed stable features, thus exposing limitations in handling the problem of covariate shift. To address this challenge, we propose a simple-yet-effective data augmentation strategy, Adversarial Invariant Augmentation (AIA), to handle the covariate shift on graphs. Specifically, given the training data, AIA aims to extrapolate and generate new environments, while concurrently preserving the original stable features during the augmentation process. Such a design equips the graph classification model with an enhanced capability to identify stable features in new environments, thereby effectively tackling the covariate shift in data. Extensive experiments with in-depth empirical analysis demonstrate the superiority of our approach. The implementation codes are publicly available at https://github.com/yongduosui/AIA.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2211.00933.pdf' target='_blank'>https://arxiv.org/pdf/2211.00933.pdf</a></span>   <span><a href='https://github.com/JeremyXSC/DMF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suncheng Xiang, Hao Chen, Wei Ran, Zefang Yu, Ting Liu, Dahong Qian, Yuzhuo Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.00933">Deep Multimodal Fusion for Generalizable Person Re-identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person re-identification plays a significant role in realistic scenarios due to its various applications in public security and video surveillance. Recently, leveraging the supervised or semi-unsupervised learning paradigms, which benefits from the large-scale datasets and strong computing performance, has achieved a competitive performance on a specific target domain. However, when Re-ID models are directly deployed in a new domain without target samples, they always suffer from considerable performance degradation and poor domain generalization. To address this challenge, we propose a Deep Multimodal Fusion network to elaborate rich semantic knowledge for assisting in representation learning during the pre-training. Importantly, a multimodal fusion strategy is introduced to translate the features of different modalities into the common space, which can significantly boost generalization capability of Re-ID model. As for the fine-tuning stage, a realistic dataset is adopted to fine-tune the pre-trained model for better distribution alignment with real-world data. Comprehensive experiments on benchmarks demonstrate that our method can significantly outperform previous domain generalization or meta-learning methods with a clear margin. Our source code will also be publicly available at https://github.com/JeremyXSC/DMF.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2211.00692.pdf' target='_blank'>https://arxiv.org/pdf/2211.00692.pdf</a></span>   <span><a href='https://github.com/smahdavi4/clrs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadegh Mahdavi, Kevin Swersky, Thomas Kipf, Milad Hashemi, Christos Thrampoulidis, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.00692">Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study the OOD generalization of neural algorithmic reasoning tasks, where the goal is to learn an algorithm (e.g., sorting, breadth-first search, and depth-first search) from input-output pairs using deep neural networks. First, we argue that OOD generalization in this setting is significantly different than common OOD settings. For example, some phenomena in OOD generalization of image classifications such as \emph{accuracy on the line} are not observed here, and techniques such as data augmentation methods do not help as assumptions underlying many augmentation techniques are often violated. Second, we analyze the main challenges (e.g., input distribution shift, non-representative data generation, and uninformative validation metrics) of the current leading benchmark, i.e., CLRS \citep{deepmind2021clrs}, which contains 30 algorithmic reasoning tasks. We propose several solutions, including a simple-yet-effective fix to the input distribution shift and improved data generation. Finally, we propose an attention-based 2WL-graph neural network (GNN) processor which complements message-passing GNNs so their combination outperforms the state-of-the-art model by a 3% margin averaged over all algorithms. Our code is available at: \url{https://github.com/smahdavi4/clrs}.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2210.10175.pdf' target='_blank'>https://arxiv.org/pdf/2210.10175.pdf</a></span>   <span><a href='https://github.com/boschresearch/ISSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Li, Dan Zhang, Margret Keuper, Anna Khoreva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.10175">Intra-Source Style Augmentation for Improved Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by $3\%$ mIoU in Cityscapes to Dark ZÃ¼rich.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2210.07347.pdf' target='_blank'>https://arxiv.org/pdf/2210.07347.pdf</a></span>   <span><a href='https://github.com/facebookresearch/disentangling-correlated-factors' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, Diane Bouchacourt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07347">Disentanglement of Correlated Factors via Hausdorff Factorized Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a model's representation with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption: that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we consider the use of a relaxed disentanglement criterion -- the Hausdorff Factorized Support (HFS) criterion -- that encourages only pairwise factorized \emph{support}, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over $+60\%$ in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization. Code available at https://github.com/facebookresearch/disentangling-correlated-factors.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2210.05845.pdf' target='_blank'>https://arxiv.org/pdf/2210.05845.pdf</a></span>   <span><a href='https://github.com/sunchipsster1/ConSpec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Sun, Wannan Yang, Thomas Jiralerspong, Dane Malenfant, Benjamin Alsbury-Nealy, Yoshua Bengio, Blake Richards
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05845">Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2209.15451.pdf' target='_blank'>https://arxiv.org/pdf/2209.15451.pdf</a></span>   <span><a href='https://github.com/MAWanqin2002/STACOM2022Ma' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanqin Ma, Huifeng Yao, Yiqun Lin, Jiarong Guo, Xiaomeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.15451">Semi-Supervised Domain Generalization for Cardiac Magnetic Resonance Image Segmentation with High Quality Pseudo Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing a deep learning method for medical segmentation tasks heavily relies on a large amount of labeled data. However, the annotations require professional knowledge and are limited in number. Recently, semi-supervised learning has demonstrated great potential in medical segmentation tasks. Most existing methods related to cardiac magnetic resonance images only focus on regular images with similar domains and high image quality. A semi-supervised domain generalization method was developed in [2], which enhances the quality of pseudo labels on varied datasets. In this paper, we follow the strategy in [2] and present a domain generalization method for semi-supervised medical segmentation. Our main goal is to improve the quality of pseudo labels under extreme MRI Analysis with various domains. We perform Fourier transformation on input images to learn low-level statistics and cross-domain information. Then we feed the augmented images as input to the double cross pseudo supervision networks to calculate the variance among pseudo labels. We evaluate our method on the CMRxMotion dataset [1]. With only partially labeled data and without domain labels, our approach consistently generates accurate segmentation results of cardiac magnetic resonance images with different respiratory motions. Code is available at: https://github.com/MAWanqin2002/STACOM2022Ma
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2209.01121.pdf' target='_blank'>https://arxiv.org/pdf/2209.01121.pdf</a></span>   <span><a href='https://github.com/PIC4SeR/Back-to-Bones' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Angarano, Mauro Martini, Francesco Salvetti, Vittorio Mazzia, Marcello Chiaberge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01121">Back-to-Bones: Rediscovering the Role of Backbones in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) studies the capability of a deep learning model to generalize to out-of-training distributions. In the last decade, literature has been massively filled with training methodologies that claim to obtain more abstract and robust data representations to tackle domain shifts. Recent research has provided a reproducible benchmark for DG, pointing out the effectiveness of naive empirical risk minimization (ERM) over existing algorithms. Nevertheless, researchers persist in using the same outdated feature extractors, and no attention has been given to the effects of different backbones yet. In this paper, we start back to the backbones proposing a comprehensive analysis of their intrinsic generalization capabilities, which so far have been ignored by the research community. We evaluate a wide variety of feature extractors, from standard residual solutions to transformer-based architectures, finding an evident linear correlation between large-scale single-domain classification accuracy and DG capability. Our extensive experimentation shows that by adopting competitive backbones in conjunction with effective data augmentation, plain ERM outperforms recent DG solutions and achieves state-of-the-art accuracy. Moreover, our additional qualitative studies reveal that novel backbones give more similar representations to same-class samples, separating different domains in the feature space. This boost in generalization capabilities leaves marginal room for DG algorithms. It suggests a new paradigm for investigating the problem, placing backbones in the spotlight and encouraging the development of consistent algorithms on top of them. The code is available at https://github.com/PIC4SeR/Back-to-Bones.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2208.05788.pdf' target='_blank'>https://arxiv.org/pdf/2208.05788.pdf</a></span>   <span><a href='https://github.com/visinf/self-adaptive' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/visinf/self-adaptive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sherwin Bahmani, Oliver Hahn, Eduard Zamfir, Nikita Araslanov, Daniel Cremers, Stefan Roth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.05788">Semantic Self-adaptation: Enhancing Generalization with a Single Sample</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The lack of out-of-domain generalization is a critical weakness of deep networks for semantic segmentation. Previous studies relied on the assumption of a static model, i. e., once the training process is complete, model parameters remain fixed at test time. In this work, we challenge this premise with a self-adaptive approach for semantic segmentation that adjusts the inference process to each input sample. Self-adaptation operates on two levels. First, it fine-tunes the parameters of convolutional layers to the input image using consistency regularization. Second, in Batch Normalization layers, self-adaptation interpolates between the training and the reference distribution derived from a single test sample. Despite both techniques being well known in the literature, their combination sets new state-of-the-art accuracy on synthetic-to-real generalization benchmarks. Our empirical study suggests that self-adaptation may complement the established practice of model regularization at training time for improving deep network generalization to out-of-domain data. Our code and pre-trained models are available at https://github.com/visinf/self-adaptive.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2208.05516.pdf' target='_blank'>https://arxiv.org/pdf/2208.05516.pdf</a></span>   <span><a href='https://github.com/mlfoundations/clip_quality_not_quantity' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, Ludwig Schmidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.05516">Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Web-crawled datasets have enabled remarkable generalization capabilities in recent image-text models such as CLIP (Contrastive Language-Image pre-training) or Flamingo, but little is known about the dataset creation processes. In this work, we introduce a testbed of six publicly available data sources - YFCC, LAION, Conceptual Captions, WIT, RedCaps, Shutterstock - to investigate how pre-training distributions induce robustness in CLIP. We find that the performance of the pre-training data varies substantially across distribution shifts, with no single data source dominating. Moreover, we systematically study the interactions between these data sources and find that combining multiple sources does not necessarily yield better models, but rather dilutes the robustness of the best individual data source. We complement our empirical findings with theoretical insights from a simple setting, where combining the training data also results in diluted robustness. In addition, our theoretical model provides a candidate explanation for the success of the CLIP-based data filtering technique recently employed in the LAION dataset. Overall our results demonstrate that simply gathering a large amount of data from the web is not the most effective way to build a pre-training dataset for robust generalization, necessitating further study into dataset design. Code is available at https://github.com/mlfoundations/clip_quality_not_quantity.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2208.03462.pdf' target='_blank'>https://arxiv.org/pdf/2208.03462.pdf</a></span>   <span><a href='https://github.com/simpleshinobu/IRMCon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Qi, Kaihua Tang, Qianru Sun, Xian-Sheng Hua, Hanwang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.03462">Class Is Invariant to Context and Vice Versa: On Learning Invariance for Out-Of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-Of-Distribution generalization (OOD) is all about learning invariance against environmental changes. If the context in every class is evenly distributed, OOD would be trivial because the context can be easily removed due to an underlying principle: class is invariant to context. However, collecting such a balanced dataset is impractical. Learning on imbalanced data makes the model bias to context and thus hurts OOD. Therefore, the key to OOD is context balance. We argue that the widely adopted assumption in prior work, the context bias can be directly annotated or estimated from biased class prediction, renders the context incomplete or even incorrect. In contrast, we point out the everoverlooked other side of the above principle: context is also invariant to class, which motivates us to consider the classes (which are already labeled) as the varying environments to resolve context bias (without context labels). We implement this idea by minimizing the contrastive loss of intra-class sample similarity while assuring this similarity to be invariant across all classes. On benchmarks with various context biases and domain gaps, we show that a simple re-weighting based classifier equipped with our context estimation achieves state-of-the-art performance. We provide the theoretical justifications in Appendix and codes on https://github.com/simpleshinobu/IRMCon.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2207.14466.pdf' target='_blank'>https://arxiv.org/pdf/2207.14466.pdf</a></span>   <span><a href='https://github.com/YvanYin/FillDepth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangkai Xu, Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Simon Chen, Jia-Wang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.14466">Towards Domain-agnostic Depth Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing depth completion methods are often targeted at a specific sparse depth type and generalize poorly across task domains. We present a method to complete sparse/semi-dense, noisy, and potentially low-resolution depth maps obtained by various range sensors, including those in modern mobile phones, or by multi-view reconstruction algorithms. Our method leverages a data-driven prior in the form of a single image depth prediction network trained on large-scale datasets, the output of which is used as an input to our model. We propose an effective training scheme where we simulate various sparsity patterns in typical task domains. In addition, we design two new benchmarks to evaluate the generalizability and the robustness of depth completion methods. Our simple method shows superior cross-domain generalization ability against state-of-the-art depth completion methods, introducing a practical solution to high-quality depth capture on a mobile device. The code is available at: https://github.com/YvanYin/FillDepth.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2207.12194.pdf' target='_blank'>https://arxiv.org/pdf/2207.12194.pdf</a></span>   <span><a href='https://github.com/ForeverPs/PoER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Pei, Jiaxi Sun, Richard Yi Da Xu, Shiming Xiang, Gaofeng Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.12194">Domain Decorrelation with Potential Energy Ranking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning systems, especially the methods based on deep learning, enjoy great success in modern computer vision tasks under experimental settings. Generally, these classic deep learning methods are built on the \emph{i.i.d.} assumption, supposing the training and test data are drawn from a similar distribution independently and identically. However, the aforementioned \emph{i.i.d.} assumption is in general unavailable in the real-world scenario, and as a result, leads to sharp performance decay of deep learning algorithms. Behind this, domain shift is one of the primary factors to be blamed. In order to tackle this problem, we propose using \textbf{Po}tential \textbf{E}nergy \textbf{R}anking (PoER) to decouple the object feature and the domain feature (\emph{i.e.,} appearance feature) in given images, promoting the learning of label-discriminative features while filtering out the irrelevant correlations between the objects and the background. PoER helps the neural networks to capture label-related features which contain the domain information first in shallow layers and then distills the label-discriminative representations out progressively, enforcing the neural networks to be aware of the characteristic of objects and background which is vital to the generation of domain-invariant features. PoER reports superior performance on domain generalization benchmarks, improving the average top-1 accuracy by at least 1.20\% compared to the existing methods. Moreover, we use PoER in the ECCV 2022 NICO Challenge\footnote{https://nicochallenge.com}, achieving top place with only a vanilla ResNet-18. The code has been made available at https://github.com/ForeverPs/PoER.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2207.03132.pdf' target='_blank'>https://arxiv.org/pdf/2207.03132.pdf</a></span>   <span><a href='https://github.com/WentaoTan/Interleaved-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Tan, Changxing Ding, Pengfei Wang, Mingming Gong, Kui Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.03132">Style Interleaved Learning for Generalizable Person Re-identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) for person re-identification (ReID) is a challenging problem, as access to target domain data is not permitted during the training process. Most existing DG ReID methods update the feature extractor and classifier parameters based on the same features. This common practice causes the model to overfit to existing feature styles in the source domain, resulting in sub-optimal generalization ability on target domains. To solve this problem, we propose a novel style interleaved learning (IL) framework. Unlike conventional learning strategies, IL incorporates two forward propagations and one backward propagation for each iteration. We employ the features of interleaved styles to update the feature extractor and classifiers using different forward propagations, which helps to prevent the model from overfitting to certain domain styles. To generate interleaved feature styles, we further propose a new feature stylization approach. It produces a wide range of meaningful styles that are both different and independent from the original styles in the source domain, which caters to the IL methodology. Extensive experimental results show that our model not only consistently outperforms state-of-the-art methods on large-scale benchmarks for DG ReID, but also has clear advantages in computational efficiency. The code is available at https://github.com/WentaoTan/Interleaved-Learning.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2206.05149.pdf' target='_blank'>https://arxiv.org/pdf/2206.05149.pdf</a></span>   <span><a href='https://github.com/JizhiziLi/RIM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/JizhiziLi/RIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jizhizi Li, Jing Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.05149">Referring Image Matting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Different from conventional image matting, which either requires user-defined scribbles/trimap to extract a specific foreground object or directly extracts all the foreground objects in the image indiscriminately, we introduce a new task named Referring Image Matting (RIM) in this paper, which aims to extract the meticulous alpha matte of the specific object that best matches the given natural language description, thus enabling a more natural and simpler instruction for image matting. First, we establish a large-scale challenging dataset RefMatte by designing a comprehensive image composition and expression generation engine to automatically produce high-quality images along with diverse text attributes based on public datasets. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions. Additionally, we construct a real-world test set with 100 high-resolution natural images and manually annotate complex phrases to evaluate the out-of-domain generalization abilities of RIM methods. Furthermore, we present a novel baseline method CLIPMat for RIM, including a context-embedded prompt, a text-driven semantic pop-up, and a multi-level details extractor. Extensive experiments on RefMatte in both keyword and expression settings validate the superiority of CLIPMat over representative methods. We hope this work could provide novel insights into image matting and encourage more follow-up studies. The dataset, code and models are available at https://github.com/JizhiziLi/RIM.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2205.11100.pdf' target='_blank'>https://arxiv.org/pdf/2205.11100.pdf</a></span>   <span><a href='https://github.com/Mowenyii/CPKP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangmeng Li, Wenyi Mo, Wenwen Qiang, Bing Su, Changwen Zheng, Hui Xiong, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.11100">Supporting Vision-Language Model Inference with Confounder-pruning Knowledge Prompt</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models are pre-trained by aligning image-text pairs in a common space to deal with open-set visual concepts. To boost the transferability of the pre-trained models, recent works adopt fixed or learnable prompts, i.e., classification weights are synthesized from natural language describing task-relevant categories, to reduce the gap between tasks in the training and test phases. However, how and what prompts can improve inference performance remains unclear. In this paper, we explicitly clarify the importance of including semantic information in prompts, while existing prompting methods generate prompts without exploring the semantic information of textual labels. Manually constructing prompts with rich semantics requires domain expertise and is extremely time-consuming. To cope with this issue, we propose a semantic-aware prompt learning method, namely CPKP, which retrieves an ontological knowledge graph by treating the textual label as a query to extract task-relevant semantic information. CPKP further introduces a double-tier confounder-pruning procedure to refine the derived semantic information. The graph-tier confounders are gradually identified and phased out, inspired by the principle of Granger causality. The feature-tier confounders are demolished by following the maximum entropy principle in information theory. Empirically, the evaluations demonstrate the effectiveness of CPKP, e.g., with two shots, CPKP outperforms the manual-prompt method by 4.64% and the learnable-prompt method by 1.09% on average, and the superiority of CPKP in domain generalization compared to benchmark approaches. Our implementation is available at https://github.com/Mowenyii/CPKP.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2203.16828.pdf' target='_blank'>https://arxiv.org/pdf/2203.16828.pdf</a></span>   <span><a href='https://github.com/ViTAE-Transformer/P3M-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Ma, Jizhizi Li, Jing Zhang, He Zhang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.16828">Rethinking Portrait Matting with Privacy Preserving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been an increasing concern about the privacy issue raised by identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable images. To fill the gap, we present P3M-10k, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting (P3M). P3M-10k consists of 10,421 high resolution face-blurred portrait images along with high-quality alpha mattes, which enables us to systematically evaluate both trimap-free and trimap-based matting methods and obtain some useful findings about model generalization ability under the privacy preserving training (PPT) setting. We also present a unified matting model dubbed P3M-Net that is compatible with both CNN and transformer backbones. To further mitigate the cross-domain performance gap issue under the PPT setting, we devise a simple yet effective Copy and Paste strategy (P3M-CP), which borrows facial information from public celebrity images and directs the network to reacquire the face context at both data and feature level. Extensive experiments on P3M-10k and public benchmarks demonstrate the superiority of P3M-Net over state-of-the-art methods and the effectiveness of P3M-CP in improving the cross-domain generalization ability, implying a great significance of P3M for future research and real-world applications.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2203.14276.pdf' target='_blank'>https://arxiv.org/pdf/2203.14276.pdf</a></span>   <span><a href='https://github.com/TomerVolk/Hyper-PADA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomer Volk, Eyal Ben-David, Ohad Amosy, Gal Chechik, Roi Reichart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.14276">Example-based Hypernetworks for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Natural Language Processing (NLP) algorithms continually achieve new milestones, out-of-distribution generalization remains a significant challenge. This paper addresses the issue of multi-source adaptation for unfamiliar domains: We leverage labeled data from multiple source domains to generalize to unknown target domains at training. Our innovative framework employs example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates a unique signature from an input example, embedding it within the source domains' semantic space. This signature is subsequently utilized by a Hypernetwork to generate the task classifier's weights. We evaluated our method across two tasks - sentiment classification and natural language inference - in 29 adaptation scenarios, where it outpaced established algorithms. In an advanced version, the signature also enriches the input example's representation. We also compare our finetuned architecture to few-shot GPT-3, demonstrating its effectiveness in essential use cases. To our knowledge, this marks the first application of Hypernetworks to the adaptation for unknown domains.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2202.09517.pdf' target='_blank'>https://arxiv.org/pdf/2202.09517.pdf</a></span>   <span><a href='https://github.com/jmjmalik22/Hate-Speech-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jitendra Singh Malik, Hezhe Qiao, Guansong Pang, Anton van den Hengel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.09517">Deep Learning for Hate Speech Detection: A Comparative Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated hate speech detection is an important tool in combating the spread of hate speech, particularly in social media. Numerous methods have been developed for the task, including a recent proliferation of deep-learning based approaches. A variety of datasets have also been developed, exemplifying various manifestations of the hate-speech detection problem. We present here a large-scale empirical comparison of deep and shallow hate-speech detection methods, mediated through the three most commonly used datasets. Our goal is to illuminate progress in the area, and identify strengths and weaknesses in the current state-of-the-art. We particularly focus our analysis on measures of practical performance, including detection accuracy, computational efficiency, capability in using pre-trained models, and domain generalization. In doing so we aim to provide guidance as to the use of hate-speech detection in practice, quantify the state-of-the-art, and identify future research directions. Code and dataset are available at https://github.com/jmjmalik22/Hate-Speech-Detection.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2112.03676.pdf' target='_blank'>https://arxiv.org/pdf/2112.03676.pdf</a></span>   <span><a href='https://github.com/lingeringlight/PLACEdropout' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lingeringlight/PLACEdropout' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Guo, Lei Qi, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.03676">PLACE dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this paper, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then randomly selects its channels to conduct dropout. Particularly, the proposed method can generate a variety of data variants to better deal with the overfitting issue. We also provide theoretical analysis for our dropout method and prove that it can effectively reduce the generalization error bound. Besides, we leverage the progressive scheme to increase the dropout ratio with the training progress, which can gradually boost the difficulty of training the model to enhance its robustness. Extensive experiments on three standard benchmark datasets have demonstrated that our method outperforms several state-of-the-art DG methods. Our code is available at https://github.com/lingeringlight/PLACEdropout.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2111.12525.pdf' target='_blank'>https://arxiv.org/pdf/2111.12525.pdf</a></span>   <span><a href='https://github.com/cheng-01037/Causality-Medical-Image-Domain-Generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Ouyang, Chen Chen, Surui Li, Zeju Li, Chen Qin, Wenjia Bai, Daniel Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.12525">Causality-inspired Single-source Domain Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models usually suffer from domain shift issues, where models trained on one source domain do not generalize well to other unseen domains. In this work, we investigate the single-source domain generalization problem: training a deep network that is robust to unseen domains, under the condition that training data is only available from one source domain, which is common in medical imaging applications. We tackle this problem in the context of cross-domain medical image segmentation. Under this scenario, domain shifts are mainly caused by different acquisition processes. We propose a simple causality-inspired data augmentation approach to expose a segmentation model to synthesized domain-shifted training examples. Specifically, 1) to make the deep model robust to discrepancies in image intensities and textures, we employ a family of randomly-weighted shallow networks. They augment training images using diverse appearance transformations. 2) Further we show that spurious correlations among objects in an image are detrimental to domain robustness. These correlations might be taken by the network as domain-specific clues for making predictions, and they may break on unseen domains. We remove these spurious correlations via causal intervention. This is achieved by resampling the appearances of potentially correlated objects independently. The proposed approach is validated on three cross-domain segmentation tasks: cross-modality (CT-MRI) abdominal image segmentation, cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-center prostate MRI segmentation. The proposed approach yields consistent performance gains compared with competitive methods when tested on unseen domains.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2107.02053.pdf' target='_blank'>https://arxiv.org/pdf/2107.02053.pdf</a></span>   <span><a href='https://github.com/KaiyangZhou/mixstyle-release' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.02053">MixStyle Neural Networks for Domain Generalization and Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks do not generalize well to unseen data with domain shifts -- a longstanding problem in machine learning and AI. To overcome the problem, we propose MixStyle, a simple plug-and-play, parameter-free module that can improve domain generalization performance without the need to collect more data or increase model capacity. The design of MixStyle is simple: it mixes the feature statistics of two random instances in a single forward pass during training. The idea is grounded by the finding from recent style transfer research that feature statistics capture image style information, which essentially defines visual domains. Therefore, mixing feature statistics can be seen as an efficient way to synthesize new domains in the feature space, thus achieving data augmentation. MixStyle is easy to implement with a few lines of code, does not require modification to training objectives, and can fit a variety of learning paradigms including supervised domain generalization, semi-supervised domain generalization, and unsupervised domain adaptation. Our experiments show that MixStyle can significantly boost out-of-distribution generalization performance across a wide range of tasks including image recognition, instance retrieval and reinforcement learning.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2006.12797.pdf' target='_blank'>https://arxiv.org/pdf/2006.12797.pdf</a></span>   <span><a href='https://github.com/gallenszl/PCWNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhelun Shen, Yuchao Dai, Xibin Song, Zhibo Rao, Dingfu Zhou, Liangjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2006.12797">PCW-Net: Pyramid Combination and Warping Cost Volume for Stereo Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing deep learning based stereo matching methods either focus on achieving optimal performances on the target dataset while with poor generalization for other datasets or focus on handling the cross-domain generalization by suppressing the domain sensitive features which results in a significant sacrifice on the performance. To tackle these problems, we propose PCW-Net, a Pyramid Combination and Warping cost volume-based network to achieve good performance on both cross-domain generalization and stereo matching accuracy on various benchmarks. In particular, our PCW-Net is designed for two purposes. First, we construct combination volumes on the upper levels of the pyramid and develop a cost volume fusion module to integrate them for initial disparity estimation. Multi-scale receptive fields can be covered by fusing multi-scale combination volumes, thus, domain-invariant features can be extracted. Second, we construct the warping volume at the last level of the pyramid for disparity refinement. The proposed warping volume can narrow down the residue searching range from the initial disparity searching range to a fine-grained one, which can dramatically alleviate the difficulty of the network to find the correct residue in an unconstrained residue searching space. When training on synthetic datasets and generalizing to unseen real datasets, our method shows strong cross-domain generalization and outperforms existing state-of-the-arts with a large margin. After fine-tuning on the real datasets, our method ranks first on KITTI 2012, second on KITTI 2015, and first on the Argoverse among all published methods as of 7, March 2022. The code will be available at https://github.com/gallenszl/PCWNet.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2508.03700.pdf' target='_blank'>https://arxiv.org/pdf/2508.03700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liujian Tang, Shaokang Dong, Yijia Huang, Minqi Xiang, Hongtao Ruan, Bin Wang, Shuo Li, Zhiheng Xi, Zhihui Cao, Hailiang Pang, Heng Kong, He Yang, Mingxu Chai, Zhilin Gao, Xingyu Liu, Yingnan Fu, Jiaming Liu, Xuanjing Huang, Yu-Gang Jiang, Tao Gui, Qi Zhang, Kang Wang, Yunke Zhang, Yuran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03700">MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents MagicGUI, a foundational mobile GUI agent designed to address critical challenges in perception, grounding, and reasoning within real-world mobile GUI environments. The framework is underpinned by following six key components: (1) a comprehensive and accurate dataset, constructed via the scalable GUI Data Pipeline, which aggregates the largest and most diverse GUI-centric multimodal data to date from open-source repositories, automated crawling, and targeted manual annotation; (2) enhanced perception and grounding capabilities, facilitating fine-grained multimodal alignment for UI element referencing, grounding, and screen comprehension; (3) a comprehensive and unified action space, encompassing both fundamental UI operations and complex interactive intents to support human-agent interactions; (4) planning-oriented reasoning mechanisms that enable the model to decompose complex user instructions into sequential actions with explicit intermediate meta-paln reasoning; (5) an iterative two-stage training procedure, combining large-scale continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing a spatially enhanced composite reward and dual filtering strategy; and (6) competitive performance on both the proprietary Magic-RICH benchmark and over a dozen public benchmarks, achieving superior performance across GUI perception and agent tasks, while demonstrating robust generalization and real-world deployment potential in practical mobile GUI scenarios, as detailed in Figure 1.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2507.05197.pdf' target='_blank'>https://arxiv.org/pdf/2507.05197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shihan Dou, Shichun Liu, Yuming Yang, Yicheng Zou, Yunhua Zhou, Shuhao Xing, Chenhao Huang, Qiming Ge, Demin Song, Haijun Lv, Songyang Gao, Chengqi Lv, Enyu Zhou, Honglin Guo, Zhiheng Xi, Wenwei Zhang, Qipeng Guo, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Tao Gui, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05197">Pre-Trained Policy Discriminators are General Reward Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2505.13886.pdf' target='_blank'>https://arxiv.org/pdf/2505.13886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13886">Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs General Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world vision language reasoning scenarios often include diverse and complex tasks. However, vision language reinforcement learning has primarily focused on a narrow set of tasks (e.g. geometry or chart reasoning), limiting the improvement of Vision Language Models' (VLMs) general reasoning. Therefore, we propose a novel Code2Logic approach, using Large Language Models (LLMs) to synthesize verifiable game reasoning tasks at scale via adapting game code. Using the Code2Logic, we developed the GameQA dataset to train and evaluate VLMs. GameQA is verifiable and scalable, offers controllable difficulty gradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL, which is simple reinforcement learning on GameQA. Surprisingly, despite training solely on game tasks, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language benchmarks. Our code, dataset and models are available at the GitHub repository.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2505.07596.pdf' target='_blank'>https://arxiv.org/pdf/2505.07596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07596">Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2402.14536.pdf' target='_blank'>https://arxiv.org/pdf/2402.14536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyin Wang, Jie Zhou, Qin Chen, Qi Zhang, Tao Gui, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14536">Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaption has been widely adapted for cross-domain sentiment analysis to transfer knowledge from the source domain to the target domain. Whereas, most methods are proposed under the assumption that the target (test) domain is known, making them fail to generalize well on unknown test data that is not always available in practice. In this paper, we focus on the problem of domain generalization for cross-domain sentiment analysis. Specifically, we propose a backdoor adjustment-based causal model to disentangle the domain-specific and domain-invariant representations that play essential roles in tackling domain shift. First, we rethink the cross-domain sentiment analysis task in a causal view to model the causal-and-effect relationships among different variables. Then, to learn an invariant feature representation, we remove the effect of domain confounders (e.g., domain knowledge) using the backdoor adjustment. A series of experiments over many homologous and diverse datasets show the great performance and robustness of our model by comparing it with the state-of-the-art domain generalization baselines.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2311.01459.pdf' target='_blank'>https://arxiv.org/pdf/2311.01459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jameel Hassan, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Shahbaz Khan, Salman Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01459">Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promising zero-shot generalization of vision-language models such as CLIP has led to their adoption using prompt learning for numerous downstream tasks. Previous works have shown test-time prompt tuning using entropy minimization to adapt text prompts for unseen domains. While effective, this overlooks the key cause for performance degradation to unseen domains -- distribution shift. In this work, we explicitly handle this problem by aligning the out-of-distribution (OOD) test sample statistics to those of the source data using prompt tuning. We use a single test sample to adapt multi-modal prompts at test time by minimizing the feature distribution shift to bridge the gap in the test domain. Evaluating against the domain generalization benchmark, our method improves zero-shot top- 1 accuracy beyond existing prompt-learning techniques, with a 3.08% improvement over the baseline MaPLe. In cross-dataset generalization with unseen categories across 10 datasets, our method improves consistently across all datasets compared to the existing state-of-the-art. Our source code and models are available at https://jameelhassan.github.io/promptalign.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2305.12123.pdf' target='_blank'>https://arxiv.org/pdf/2305.12123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Wu, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12123">Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising, in practice factors like expensive annotations and privacy preclude the availability of group labels. More crucially, when taking a closer look at the failure modes of out-of-distribution generalization, the typical procedure of reweighting in group DRO loses efficiency. Hinged on the limitations, in this work, we reformulate the group DRO framework by proposing Q-Diversity. Characterized by an interactive training mode, Q-Diversity relaxes the group identification from annotation into direct parameterization. Furthermore, a novel mixing strategy across groups is presented to diversify the under-represented groups. In a series of experiments on both synthetic and real-world text classification tasks, results demonstrate that Q-Diversity can consistently improve worst-case accuracy under different distributional shifts, outperforming state-of-the-art alternatives.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2509.21871.pdf' target='_blank'>https://arxiv.org/pdf/2509.21871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyang Liu, Yifan Hu, Senjie Jin, Shihan Dou, Gonglei Shi, Jie Shao, Tao Gui, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21871">Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) are well suited to image aesthetic assessment, as they can capture high-level aesthetic features leveraging their cross-modal understanding capacity. However, the scarcity of multimodal aesthetic reasoning data and the inherently subjective nature of aesthetic judgment make it difficult for MLLMs to generate accurate aesthetic judgments with interpretable rationales. To this end, we propose Aes-R1, a comprehensive aesthetic reasoning framework with reinforcement learning (RL). Concretely, Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality chain-of-thought aesthetic reasoning data used for cold-start. After teaching the model to generate structured explanations prior to scoring, we then employ the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that jointly optimizes absolute score regression and relative ranking order, improving both per-image accuracy and cross-image preference judgments. Aes-R1 enables MLLMs to generate grounded explanations alongside faithful scores, thereby enhancing aesthetic scoring and reasoning in a unified framework. Extensive experiments demonstrate that Aes-R1 improves the backbone's average PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar size. More ablation studies validate Aes-R1's robust generalization under limited supervision and in out-of-distribution scenarios.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2505.19797.pdf' target='_blank'>https://arxiv.org/pdf/2505.19797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Zhang, Hao Li, Chenxu Wang, Linyao Chen, Qiaosheng Zhang, Peng Ye, Shi Feng, Daling Wang, Zhen Wang, Xinrun Wang, Jia Xu, Lei Bai, Wanli Ouyang, Shuyue Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19797">The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proprietary giants are increasingly dominating the race for ever-larger language models. Can open-source, smaller models remain competitive across a broad range of tasks? In this paper, we present the Avengers -- a simple recipe that leverages the collective intelligence of these smaller models. The Avengers builds upon four lightweight operations: (i) embedding: encode queries using a text embedding model; (ii) clustering: group queries based on their semantic similarity; (iii) scoring: scores each model's performance within each cluster; and (iv) voting: improve outputs via repeated sampling and voting. At inference time, each query is embedded and assigned to its nearest cluster. The top-performing model(s) within that cluster are selected to generate the response with repeated sampling. Remarkably, with 10 open-source models (~7B parameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in average performance across 15 diverse datasets spanning mathematics, coding, logical reasoning, general knowledge, and affective tasks. In particular, it surpasses GPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the Avengers delivers superior out-of-distribution generalization, and remains robust across various embedding models, clustering algorithms, ensemble strategies, and values of its sole parameter -- the number of clusters.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2508.17715.pdf' target='_blank'>https://arxiv.org/pdf/2508.17715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Huang, Keping Bi, Yinqiong Cai, Wei Chen, Jiafeng Guo, Xueqi Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17715">How Do LLM-Generated Texts Impact Term-Based Retrieval Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As more content generated by large language models (LLMs) floods into the Internet, information retrieval (IR) systems now face the challenge of distinguishing and handling a blend of human-authored and machine-generated texts. Recent studies suggest that neural retrievers may exhibit a preferential inclination toward LLM-generated content, while classic term-based retrievers like BM25 tend to favor human-written documents. This paper investigates the influence of LLM-generated content on term-based retrieval models, which are valued for their efficiency and robust generalization across domains. Our linguistic analysis reveals that LLM-generated texts exhibit smoother high-frequency and steeper low-frequency Zipf slopes, higher term specificity, and greater document-level diversity. These traits are aligned with LLMs being trained to optimize reader experience through diverse and precise expressions. Our study further explores whether term-based retrieval models demonstrate source bias, concluding that these models prioritize documents whose term distributions closely correspond to those of the queries, rather than displaying an inherent source bias. This work provides a foundation for understanding and addressing potential biases in term-based IR systems managing mixed-source content.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2306.11730.pdf' target='_blank'>https://arxiv.org/pdf/2306.11730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lian Zhang, Zhengliang Liu, Lu Zhang, Zihao Wu, Xiaowei Yu, Jason Holmes, Hongying Feng, Haixing Dai, Xiang Li, Quanzheng Li, Dajiang Zhu, Tianming Liu, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11730">Segment Anything Model (SAM) for Radiation Oncology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we evaluate the performance of the Segment Anything Model (SAM) in clinical radiotherapy. Our results indicate that SAM's 'segment anything' mode can achieve clinically acceptable segmentation results in most organs-at-risk (OARs) with Dice scores higher than 0.7. SAM's 'box prompt' mode further improves the Dice scores by 0.1 to 0.5. Considering the size of the organ and the clarity of its boundary, SAM displays better performance for large organs with clear boundaries but performs worse for smaller organs with unclear boundaries. Given that SAM, a model pre-trained purely on natural images, can handle the delineation of OARs from medical images with clinically acceptable accuracy, these results highlight SAM's robust generalization capabilities with consistent accuracy in automatic segmentation for radiotherapy. In other words, SAM can achieve delineation of different OARs at different sites using a generic automatic segmentation model. SAM's generalization capabilities across different disease sites suggest that it is technically feasible to develop a generic model for automatic segmentation in radiotherapy.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2408.12590.pdf' target='_blank'>https://arxiv.org/pdf/2408.12590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Qin, Congying Xia, Krithika Ramakrishnan, Michael Ryoo, Lifu Tu, Yihao Feng, Manli Shu, Honglu Zhou, Anas Awadalla, Jun Wang, Senthil Purushwalkam, Le Xue, Yingbo Zhou, Huan Wang, Silvio Savarese, Juan Carlos Niebles, Zeyuan Chen, Ran Xu, Caiming Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12590">xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of producing realistic scenes from textual descriptions. Building on recent advancements, such as OpenAI's Sora, we explore the latent diffusion model (LDM) architecture and introduce a video variational autoencoder (VidVAE). VidVAE compresses video data both spatially and temporally, significantly reducing the length of visual tokens and the computational demands associated with generating long-sequence videos. To further address the computational costs, we propose a divide-and-merge strategy that maintains temporal consistency across video segments. Our Diffusion Transformer (DiT) model incorporates spatial and temporal self-attention layers, enabling robust generalization across different timeframes and aspect ratios. We have devised a data processing pipeline from the very beginning and collected over 13M high-quality video-text pairs. The pipeline includes multiple steps such as clipping, text detection, motion estimation, aesthetics scoring, and dense captioning based on our in-house video-LLM model. Training the VidVAE and DiT models required approximately 40 and 642 H100 days, respectively. Our model supports over 14-second 720p video generation in an end-to-end way and demonstrates competitive performance against state-of-the-art T2V models.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2405.16224.pdf' target='_blank'>https://arxiv.org/pdf/2405.16224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16224">Negative as Positive: Enhancing Out-of-distribution Generalization for Graph Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph contrastive learning (GCL), standing as the dominant paradigm in the realm of graph pre-training, has yielded considerable progress. Nonetheless, its capacity for out-of-distribution (OOD) generalization has been relatively underexplored. In this work, we point out that the traditional optimization of InfoNCE in GCL restricts the cross-domain pairs only to be negative samples, which inevitably enlarges the distribution gap between different domains. This violates the requirement of domain invariance under OOD scenario and consequently impairs the model's OOD generalization performance. To address this issue, we propose a novel strategy "Negative as Positive", where the most semantically similar cross-domain negative pairs are treated as positive during GCL. Our experimental results, spanning a wide array of datasets, confirm that this method substantially improves the OOD generalization performance of GCL.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2301.07507.pdf' target='_blank'>https://arxiv.org/pdf/2301.07507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, Yongbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07507">Graphix-T5: Mixing Pre-Trained Transformers with Graph-Aware Layers for Text-to-SQL Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of text-to-SQL parsing, which aims at converting natural language questions into executable SQL queries, has garnered increasing attention in recent years, as it can assist end users in efficiently extracting vital information from databases without the need for technical background. One of the major challenges in text-to-SQL parsing is domain generalization, i.e., how to generalize well to unseen databases. Recently, the pre-trained text-to-text transformer model, namely T5, though not specialized for text-to-SQL parsing, has achieved state-of-the-art performance on standard benchmarks targeting domain generalization. In this work, we explore ways to further augment the pre-trained T5 model with specialized components for text-to-SQL parsing. Such components are expected to introduce structural inductive bias into text-to-SQL parsers thus improving model's capacity on (potentially multi-hop) reasoning, which is critical for generating structure-rich SQLs. To this end, we propose a new architecture GRAPHIX-T5, a mixed model with the standard pre-trained transformer model augmented by some specially-designed graph-aware layers. Extensive experiments and analysis demonstrate the effectiveness of GRAPHIX-T5 across four text-to-SQL benchmarks: SPIDER, SYN, REALISTIC and DK. GRAPHIX-T5 surpass all other T5-based parsers with a significant margin, achieving new state-of-the-art performance. Notably, GRAPHIX-T5-large reach performance superior to the original T5-large by 5.7% on exact match (EM) accuracy and 6.6% on execution accuracy (EX). This even outperforms the T5-3B by 1.2% on EM and 1.5% on EX.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2507.18484.pdf' target='_blank'>https://arxiv.org/pdf/2507.18484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yang, Lingxuan Wu, Lizhong Wang, Chengyang Ying, Hang Su, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18484">Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2403.12968.pdf' target='_blank'>https://arxiv.org/pdf/2403.12968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor RÃ¼hle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12968">LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.
  To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.
  We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x. Our code is available at https://aka.ms/LLMLingua-2.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2408.04400.pdf' target='_blank'>https://arxiv.org/pdf/2408.04400.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04400">DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck approaches, have achieved partial success but struggle to overcome simplicity bias, often reinforcing spurious correlations. To tackle this, we propose DIVE, training a collection of models to focus on all label-predictive subgraphs by encouraging the models to foster divergence on the subgraph mask, which circumvents the limitation of a model solely focusing on the subgraph corresponding to simple structural patterns. Specifically, we employs a regularizer to punish overlap in extracted subgraphs across models, thereby encouraging different models to concentrate on distinct structural patterns. Model selection for robust OOD performance is achieved through validation accuracy. Tested across four datasets from GOOD benchmark and one dataset from DrugOOD benchmark, our approach demonstrates significant improvement over existing methods, effectively addressing the simplicity bias and enhancing generalization in graph machine learning.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2508.04117.pdf' target='_blank'>https://arxiv.org/pdf/2508.04117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwen Ruan, Yun Chen, Yutao Hou, Peng Li, Yang Liu, Guanhua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04117">Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pretrained large language models (LLMs) are finetuned with labeled data for better instruction following ability and alignment with human values. In this paper, we study the learning dynamics of LLM finetuning on reasoning tasks and reveal the uncovered over-memorization phenomenon during a specific stage of LLM finetuning. At this stage, the LLMs have excessively memorized training data and exhibit high test perplexity while maintaining good test accuracy. We investigate the conditions that lead to LLM over-memorization and find that training epochs and large learning rates contribute to this issue. Although models with over-memorization demonstrate comparable test accuracy to normal models, they suffer from reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. Our experiments unveil the over-memorization to be broadly applicable across different tasks, models, and finetuning methods. Our research highlights that overparameterized, extensively finetuned LLMs exhibit unique learning dynamics distinct from traditional machine learning models. Based on our observations of over-memorization, we provide recommendations on checkpoint and learning rate selection during finetuning.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2508.04117.pdf' target='_blank'>https://arxiv.org/pdf/2508.04117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwen Ruan, Yun Chen, Yutao Hou, Peng Li, Yang Liu, Guanhua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04117">Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pretrained large language models (LLMs) are finetuned with labeled data for better instruction following ability and alignment with human values. In this paper, we study the learning dynamics of LLM finetuning on reasoning tasks and reveal the uncovered over-memorization phenomenon during a specific stage of LLM finetuning. At this stage, the LLMs have excessively memorized training data and exhibit high test perplexity while maintaining good test accuracy. We explore the conditions that contribute to over-memorization and discover that this issue is prevalent across various tasks, models, and fine-tuning methods, with prolonged training and large learning rates exacerbating the problem. Although models with over-memorization demonstrate comparable test accuracy to normal models, they suffer from reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. In light of our findings on over-memorization, we offer recommendations for checkpoint selection and propose techniques such as checkpoint merging and memorization-aware reweighting to mitigate this effect.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2406.18844.pdf' target='_blank'>https://arxiv.org/pdf/2406.18844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Liang, Jiawei Liang, Tianyu Pang, Chao Du, Aishan Liu, Mingli Zhu, Xiaochun Cao, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18844">Revisiting Backdoor Attacks against Large Vision-Language Models from Domain Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instruction tuning enhances large vision-language models (LVLMs) but increases their vulnerability to backdoor attacks due to their open design. Unlike prior studies in static settings, this paper explores backdoor attacks in LVLM instruction tuning across mismatched training and testing domains. We introduce a new evaluation dimension, backdoor domain generalization, to assess attack robustness under visual and text domain shifts. Our findings reveal two insights: (1) backdoor generalizability improves when distinctive trigger patterns are independent of specific data domains or model architectures, and (2) the competitive interaction between trigger patterns and clean semantic regions, where guiding the model to predict triggers enhances attack generalizability. Based on these insights, we propose a multimodal attribution backdoor attack (MABA) that injects domain-agnostic triggers into critical areas using attributional interpretation. Experiments with OpenFlamingo, Blip-2, and Otter show that MABA significantly boosts the attack success rate of generalization by 36.4%, achieving a 97% success rate at a 0.2% poisoning rate. This study reveals limitations in current evaluations and highlights how enhanced backdoor generalizability poses a security threat to LVLMs, even without test data access.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2505.11166.pdf' target='_blank'>https://arxiv.org/pdf/2505.11166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huashan Sun, Shengyi Liao, Yansen Han, Yu Bai, Yang Gao, Cheng Fu, Weizhou Shen, Fanqi Wan, Ming Yan, Ji Zhang, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11166">SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite advances in pretraining with extended context lengths, large language models (LLMs) still face challenges in effectively utilizing real-world long-context information, primarily due to insufficient long-context alignment caused by data quality issues, training inefficiencies, and the lack of well-designed optimization objectives. To address these limitations, we propose a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng $\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling long-context preference optimization (PO) into two components: short-context PO and short-to-long reward alignment (SoLo-RA), supported by both theoretical and empirical evidence. Specifically, short-context PO leverages preference pairs sampled from short contexts to enhance the model's contextual knowledge utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score consistency utilization for the responses when conditioned on both short and long contexts that contain identical task-relevant information. This facilitates transferring the model's ability to handle short contexts into long-context scenarios. SoLoPO is compatible with mainstream preference optimization algorithms, while substantially improving the efficiency of data construction and training processes. Experimental results show that SoLoPO enhances all these algorithms with respect to stronger length and domain generalization abilities across various long-context benchmarks, while achieving notable improvements in both computational and memory efficiency.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2310.14747.pdf' target='_blank'>https://arxiv.org/pdf/2310.14747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, Ji Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14747">MCC-KD: Multi-CoT Consistent Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence between the answer distributions. We investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results not only confirm MCC-KD's superior performance on in-distribution datasets but also highlight its robust generalization ability on out-of-distribution datasets.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2312.12475.pdf' target='_blank'>https://arxiv.org/pdf/2312.12475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyu Chen, Teng Xiao, Kun Kuang, Zheqi Lv, Min Zhang, Jinluan Yang, Chengqiang Lu, Hongxia Yang, Fei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12475">Learning to Reweight for Graph Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) show promising results for graph tasks. However, existing GNNs' generalization ability will degrade when there exist distribution shifts between testing and training graph data. The cardinal impetus underlying the severe degeneration is that the GNNs are architected predicated upon the I.I.D assumptions. In such a setting, GNNs are inclined to leverage imperceptible statistical correlations subsisting in the training set to predict, albeit it is a spurious correlation. In this paper, we study the problem of the generalization ability of GNNs in Out-Of-Distribution (OOD) settings. To solve this problem, we propose the Learning to Reweight for Generalizable Graph Neural Network (L2R-GNN) to enhance the generalization ability for achieving satisfactory performance on unseen testing graphs that have different distributions with training graphs. We propose a novel nonlinear graph decorrelation method, which can substantially improve the out-of-distribution generalization ability and compares favorably to previous methods in restraining the over-reduced sample size. The variables of the graph representation are clustered based on the stability of the correlation, and the graph decorrelation method learns weights to remove correlations between the variables of different clusters rather than any two variables. Besides, we interpose an efficacious stochastic algorithm upon bi-level optimization for the L2R-GNN framework, which facilitates simultaneously learning the optimal weights and GNN parameters, and avoids the overfitting problem. Experimental results show that L2R-GNN greatly outperforms baselines on various graph prediction benchmarks under distribution shifts.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2305.15889.pdf' target='_blank'>https://arxiv.org/pdf/2305.15889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunze Tong, Junkun Yuan, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, Kun Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15889">Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under the invariant learning framework. With contrastive learning, we propose a learning potential-guided metric for domain heterogeneity by promoting learning variant features. Then we notice the differences between seeking variance-based heterogeneity and training invariance-based generalizable model. We thus propose a novel method called Heterogeneity-based Two-stage Contrastive Learning (HTCL) for the DG task. In the first stage, we generate the most heterogeneous dividing pattern with our contrastive metric. In the second stage, we employ an invariance-aimed contrastive learning by re-building pairs with the stable relation hinted by domains and classes, which better utilizes generated domain labels for generalization learning. Extensive experiments show HTCL better digs heterogeneity and yields great generalization performance.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2110.06736.pdf' target='_blank'>https://arxiv.org/pdf/2110.06736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkun Yuan, Xu Ma, Defang Chen, Fei Wu, Lanfen Lin, Kun Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.06736">Collaborative Semantic Aggregation and Calibration for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. The existing DG methods usually exploit the fusion of shared multi-source data to train a generalizable model. However, tremendous data is distributed across lots of places nowadays that can not be shared due to privacy policies. In this paper, we tackle the problem of federated domain generalization where the source datasets can only be accessed and learned locally for privacy protection. We propose a novel framework called Collaborative Semantic Aggregation and Calibration (CSAC) to enable this challenging problem. To fully absorb multi-source semantic information while avoiding unsafe data fusion, we conduct data-free semantic aggregation by fusing the models trained on the separated domains layer-by-layer. To address the semantic dislocation problem caused by domain shift, we further design cross-layer semantic calibration with an attention mechanism to align each semantic level and enhance domain invariance. We unify multi-source semantic learning and alignment in a collaborative way by repeating the semantic aggregation and calibration alternately, keeping each dataset localized, and the data privacy is carefully protected. Extensive experiments show the significant performance of our method in addressing this challenging problem.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2110.01438.pdf' target='_blank'>https://arxiv.org/pdf/2110.01438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkun Yuan, Xu Ma, Ruoxuan Xiong, Mingming Gong, Xiangyu Liu, Fei Wu, Lanfen Lin, Kun Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.01438">Instrumental Variable-Driven Domain Generalization with Unobserved Confounders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn from multiple source domains a model that can generalize well on unseen target domains. Existing DG methods mainly learn the representations with invariant marginal distribution of the input features, however, the invariance of the conditional distribution of the labels given the input features is more essential for unknown domain prediction. Meanwhile, the existing of unobserved confounders which affect the input features and labels simultaneously cause spurious correlation and hinder the learning of the invariant relationship contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we propose an instrumental variable-driven DG method (IV-DG) by removing the bias of the unobserved confounders with two-stage learning. In the first stage, it learns the conditional distribution of the input features of one domain given input features of another domain. In the second stage, it estimates the relationship by predicting labels with the learned conditional distribution. Theoretical analyses and simulation experiments show that it accurately captures the invariant relationship. Extensive experiments on real-world datasets demonstrate that IV-DG method yields state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2509.22403.pdf' target='_blank'>https://arxiv.org/pdf/2509.22403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanjin Meng, Yuan Yuan, Jingtao Ding, Jie Feng, Chonghua Han, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22403">MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobility Foundation Models (MFMs) have advanced the modeling of human movement patterns, yet they face a ceiling due to limitations in data scale and semantic understanding. While Large Language Models (LLMs) offer powerful semantic reasoning, they lack the innate understanding of spatio-temporal statistics required for generating physically plausible mobility trajectories. To address these gaps, we propose MoveFM-R, a novel framework that unlocks the full potential of mobility foundation models by leveraging language-driven semantic reasoning capabilities. It tackles two key challenges: the vocabulary mismatch between continuous geographic coordinates and discrete language tokens, and the representation gap between the latent vectors of MFMs and the semantic world of LLMs. MoveFM-R is built on three core innovations: a semantically enhanced location encoding to bridge the geography-language gap, a progressive curriculum to align the LLM's reasoning with mobility patterns, and an interactive self-reflection mechanism for conditional trajectory generation. Extensive experiments demonstrate that MoveFM-R significantly outperforms existing MFM-based and LLM-based baselines. It also shows robust generalization in zero-shot settings and excels at generating realistic trajectories from natural language instructions. By synthesizing the statistical power of MFMs with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm that enables a more comprehensive, interpretable, and powerful modeling of human mobility. The implementation of MoveFM-R is available online at https://anonymous.4open.science/r/MoveFM-R-CDE7/.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2507.02288.pdf' target='_blank'>https://arxiv.org/pdf/2507.02288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>De Cheng, Zhipeng Xu, Xinyang Jiang, Dongsheng Li, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02288">Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) seeks to develop a versatile model capable of performing effectively on unseen target domains. Notably, recent advances in pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated considerable potential in enhancing the generalization capabilities of deep learning models. Despite the increasing attention toward VFM-based domain prompt tuning within DG, the effective design of prompts capable of disentangling invariant features across diverse domains remains a critical challenge. In this paper, we propose addressing this challenge by leveraging the controllable and flexible language prompt of the VFM. Noting that the text modality of VFMs is naturally easier to disentangle, we introduce a novel framework for text feature-guided visual prompt tuning. This framework first automatically disentangles the text prompt using a large language model (LLM) and then learns domain-invariant visual representation guided by the disentangled text feature. However, relying solely on language to guide visual feature disentanglement has limitations, as visual features can sometimes be too complex or nuanced to be fully captured by descriptive text. To address this, we introduce Worst Explicit Representation Alignment (WERA), which extends text-guided visual prompts by incorporating an additional set of abstract prompts. These prompts enhance source domain diversity through stylized image augmentations, while alignment constraints ensure that visual representations remain consistent across both the original and augmented distributions. Experiments conducted on major DG datasets, including PACS, VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method outperforms state-of-the-art DG methods.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2310.16391.pdf' target='_blank'>https://arxiv.org/pdf/2310.16391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Huang, Muyang Li, Li Shen, Jun Yu, Chen Gong, Bo Han, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16391">Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-Distribution (OOD) Generalization aims to learn robust models that generalize well to various environments without fitting to distribution-specific features. Recent studies based on Lottery Ticket Hypothesis (LTH) address this problem by minimizing the learning target to find some of the parameters that are critical to the task. However, in OOD problems, such solutions are suboptimal as the learning task contains severe distribution noises, which can mislead the optimization process. Therefore, apart from finding the task-related parameters (i.e., invariant parameters), we propose Exploring Variant parameters for Invariant Learning (EVIL) which also leverages the distribution knowledge to find the parameters that are sensitive to distribution shift (i.e., variant parameters). Once the variant parameters are left out of invariant learning, a robust subnetwork that is resistant to distribution shift can be found. Additionally, the parameters that are relatively stable across distributions can be considered invariant ones to improve invariant learning. By fully exploring both variant and invariant parameters, our EVIL can effectively identify a robust subnetwork to improve OOD generalization. In extensive experiments on integrated testbed: DomainBed, EVIL can effectively and efficiently enhance many popular methods, such as ERM, IRM, SAM, etc.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2303.13087.pdf' target='_blank'>https://arxiv.org/pdf/2303.13087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Huang, Miaoxi Zhu, Xiaobo Xia, Li Shen, Jun Yu, Chen Gong, Bo Han, Bo Du, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13087">Robust Generalization against Photon-Limited Corruptions via Worst-Case Sharpness Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust generalization aims to tackle the most challenging data distributions which are rare in the training set and contain severe noises, i.e., photon-limited corruptions. Common solutions such as distributionally robust optimization (DRO) focus on the worst-case empirical risk to ensure low training error on the uncommon noisy distributions. However, due to the over-parameterized model being optimized on scarce worst-case data, DRO fails to produce a smooth loss landscape, thus struggling on generalizing well to the test set. Therefore, instead of focusing on the worst-case risk minimization, we propose SharpDRO by penalizing the sharpness of the worst-case distribution, which measures the loss changes around the neighbor of learning parameters. Through worst-case sharpness minimization, the proposed method successfully produces a flat loss curve on the corrupted distributions, thus achieving robust generalization. Moreover, by considering whether the distribution annotation is available, we apply SharpDRO to two problem settings and design a worst-case selection process for robust generalization. Theoretically, we show that SharpDRO has a great convergence guarantee. Experimentally, we simulate photon-limited corruptions using CIFAR10/100 and ImageNet30 datasets and show that SharpDRO exhibits a strong generalization ability against severe corruptions and exceeds well-known baseline methods with large performance gains.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2510.03896.pdf' target='_blank'>https://arxiv.org/pdf/2510.03896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Liu, Zheng Huang, Xiaoyi Lin, Muzhi Zhu, Canyu Zhao, Zongze Du, Yating Wang, Haoyi Zhu, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03896">Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although Vision-Language Models (VLM) have demonstrated impressive planning and reasoning capabilities, translating these abilities into the physical world introduces significant challenges. Conventional Vision-Language-Action (VLA) models, which integrate reasoning and action into a monolithic architecture, generalize poorly because they are constrained by scarce, narrow-domain data. While recent dual-system approaches attempt to decouple "thinking" from "acting", they are often constrained by semantic ambiguities within the action module. This ambiguity makes large-scale, cross-task training infeasible. Consequently, these systems typically necessitate fine-tuning on newly collected data when deployed to novel environments, and the cooperation mechanism between the two systems remains ill-defined. To address these limitations, we introduce, for the first time, a framework centered around a generalizable action expert. Our approach utilizes sparse 3D trajectories as an intermediate representation, effectively bridging the high-level planning capabilities of the VLM with the low-level physical action module. During the planning phase, the VLM is only required to generate coarse 3D waypoints. These waypoints are then processed by our generalizable action expert, which refines them into dense, executable action sequences by sampling real-time point cloud observations of the environment. To promote training efficiency and robust generalization, we introduce a novel "Action Pre-training, Pointcloud Fine-tuning" paradigm. Our method combines the broad generalization capabilities of VLMs in visual understanding and planning with the fine-grained, action-level generalization of action expert.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2505.20256.pdf' target='_blank'>https://arxiv.org/pdf/2505.20256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhong, Muzhi Zhu, Zongze Du, Zheng Huang, Canyu Zhao, Mingyu Liu, Wen Wang, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20256">Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon video-audio reasoning and fine-grained pixel understanding impose conflicting requirements on omnimodal models: dense temporal coverage demands many low-resolution frames, whereas precise grounding calls for high-resolution inputs. We tackle this trade-off with a two-system architecture: a Global Reasoning System selects informative keyframes and rewrites the task at low spatial cost, while a Detail Understanding System performs pixel-level grounding on the selected high-resolution snippets. Because ``optimal'' keyframe selection and reformulation are ambiguous and hard to supervise, we formulate them as a reinforcement learning (RL) problem and present Omni-R1, an end-to-end RL framework built on Group Relative Policy Optimization. Omni-R1 trains the Global Reasoning System through hierarchical rewards obtained via online collaboration with the Detail Understanding System, requiring only one epoch of RL on small task splits.
  Experiments on two challenging benchmarks, namely Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show that Omni-R1 not only surpasses strong supervised baselines but also outperforms specialized state-of-the-art models, while substantially improving out-of-domain generalization and mitigating multimodal hallucination. Our results demonstrate the first successful application of RL to large-scale omnimodal reasoning and highlight a scalable path toward universally foundation models.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2404.11326.pdf' target='_blank'>https://arxiv.org/pdf/2404.11326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiangang Du, Jinlong Peng, Xu Chen, Qingdong He, Liren He, Qiang Nie, Wenbing Zhu, Mingmin Chi, Yabiao Wang, Chengjie Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11326">Single-temporal Supervised Remote Change Detection for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Change detection is widely applied in remote sensing image analysis. Existing methods require training models separately for each dataset, which leads to poor domain generalization. Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical. In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization. Additionally, we propose a dynamic context optimization for prompt learning. Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN). This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization. Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods. Code will be available.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2303.09181.pdf' target='_blank'>https://arxiv.org/pdf/2303.09181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyang Han, Yong Liu, Jun Hao Liew, Henghui Ding, Yunchao Wei, Jiajun Liu, Yitong Wang, Yansong Tang, Yujiu Yang, Jiashi Feng, Yao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09181">Global Knowledge Calibration for Fast Open-Vocabulary Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in pre-trained vision-language models, such as CLIP, have enabled the segmentation of arbitrary concepts solely from textual inputs, a process commonly referred to as open-vocabulary semantic segmentation (OVS). However, existing OVS techniques confront a fundamental challenge: the trained classifier tends to overfit on the base classes observed during training, resulting in suboptimal generalization performance to unseen classes. To mitigate this issue, recent studies have proposed the use of an additional frozen pre-trained CLIP for classification. Nonetheless, this approach incurs heavy computational overheads as the CLIP vision encoder must be repeatedly forward-passed for each mask, rendering it impractical for real-world applications. To address this challenge, our objective is to develop a fast OVS model that can perform comparably or better without the extra computational burden of the CLIP image encoder during inference. To this end, we propose a core idea of preserving the generalizable representation when fine-tuning on known classes. Specifically, we introduce a text diversification strategy that generates a set of synonyms for each training category, which prevents the learned representation from collapsing onto specific known category names. Additionally, we employ a text-guided knowledge distillation method to preserve the generalizable knowledge of CLIP. Extensive experiments demonstrate that our proposed model achieves robust generalization performance across various datasets. Furthermore, we perform a preliminary exploration of open-vocabulary video segmentation and present a benchmark that can facilitate future open-vocabulary research in the video domain.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2505.22855.pdf' target='_blank'>https://arxiv.org/pdf/2505.22855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruining Deng, Junchao Zhu, Juming Xiong, Can Cui, Tianyuan Yao, Junlin Guo, Siqi Lu, Marilyn Lionts, Mengmeng Yin, Yu Wang, Shilin Zhao, Yucheng Tang, Yihe Yang, Paul Dennis Simonson, Mert R. Sabuncu, Haichun Yang, Yuankai Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22855">IRS: Incremental Relationship-guided Segmentation for Digital Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual learning is rapidly emerging as a key focus in computer vision, aiming to develop AI systems capable of continuous improvement, thereby enhancing their value and practicality in diverse real-world applications. In healthcare, continual learning holds great promise for continuously acquired digital pathology data, which is collected in hospitals on a daily basis. However, panoramic segmentation on digital whole slide images (WSIs) presents significant challenges, as it is often infeasible to obtain comprehensive annotations for all potential objects, spanning from coarse structures (e.g., regions and unit objects) to fine structures (e.g., cells). This results in temporally and partially annotated data, posing a major challenge in developing a holistic segmentation framework. Moreover, an ideal segmentation model should incorporate new phenotypes, unseen diseases, and diverse populations, making this task even more complex. In this paper, we introduce a novel and unified Incremental Relationship-guided Segmentation (IRS) learning scheme to address temporally acquired, partially annotated data while maintaining out-of-distribution (OOD) continual learning capacity in digital pathology. The key innovation of IRS lies in its ability to realize a new spatial-temporal OOD continual learning paradigm by mathematically modeling anatomical relationships between existing and newly introduced classes through a simple incremental universal proposition matrix. Experimental results demonstrate that the IRS method effectively handles the multi-scale nature of pathological segmentation, enabling precise kidney segmentation across various structures (regions, units, and cells) as well as OOD disease lesions at multiple magnifications. This capability significantly enhances domain generalization, making IRS a robust approach for real-world digital pathology applications.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2505.14681.pdf' target='_blank'>https://arxiv.org/pdf/2505.14681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14681">Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2404.05145.pdf' target='_blank'>https://arxiv.org/pdf/2404.05145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haimei Zhao, Jing Zhang, Zhuo Chen, Shanshan Zhao, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05145">UniMix: Towards Domain Adaptive and Generalizable LiDAR Semantic Segmentation in Adverse Weather</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR semantic segmentation (LSS) is a critical task in autonomous driving and has achieved promising progress. However, prior LSS methods are conventionally investigated and evaluated on datasets within the same domain in clear weather. The robustness of LSS models in unseen scenes and all weather conditions is crucial for ensuring safety and reliability in real applications. To this end, we propose UniMix, a universal method that enhances the adaptability and generalizability of LSS models. UniMix first leverages physically valid adverse weather simulation to construct a Bridge Domain, which serves to bridge the domain gap between the clear weather scenes and the adverse weather scenes. Then, a Universal Mixing operator is defined regarding spatial, intensity, and semantic distributions to create the intermediate domain with mixed samples from given domains. Integrating the proposed two techniques into a teacher-student framework, UniMix efficiently mitigates the domain gap and enables LSS models to learn weather-robust and domain-invariant representations. We devote UniMix to two main setups: 1) unsupervised domain adaption, adapting the model from the clear weather source domain to the adverse weather target domain; 2) domain generalization, learning a model that generalizes well to unseen scenes in adverse weather. Extensive experiments validate the effectiveness of UniMix across different tasks and datasets, all achieving superior performance over state-of-the-art methods. The code will be released.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2508.06026.pdf' target='_blank'>https://arxiv.org/pdf/2508.06026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, Wei Ye, Shikun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06026">Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose \textbf{Temporal Self-Rewarding Language Models} that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) \textit{Anchored Rejection} - fixing rejected responses using the past initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2501.03271.pdf' target='_blank'>https://arxiv.org/pdf/2501.03271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amitava Das, Suranjana Trivedy, Danush Khanna, Rajarshi Roy, Gurpreet Singh, Basab Ghosh, Yaswanth Narsupalli, Vinija Jain, Vasu Sharma, Aishwarya Naresh Reganti, Aman Chadha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03271">DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid rise of large language models (LLMs) has unlocked many applications but also underscores the challenge of aligning them with diverse values and preferences. Direct Preference Optimization (DPO) is central to alignment but constrained by fixed divergences and limited feature transformations. We propose DPO-Kernels, which integrates kernel methods to address these issues through four key contributions: (i) Kernelized Representations with polynomial, RBF, Mahalanobis, and spectral kernels for richer transformations, plus a hybrid loss combining embedding-based and probability-based objectives; (ii) Divergence Alternatives (Jensen-Shannon, Hellinger, Renyi, Bhattacharyya, Wasserstein, and f-divergences) for greater stability; (iii) Data-Driven Selection metrics that automatically choose the best kernel-divergence pair; and (iv) a Hierarchical Mixture of Kernels for both local precision and global modeling. Evaluations on 12 datasets demonstrate state-of-the-art performance in factuality, safety, reasoning, and instruction following. Grounded in Heavy-Tailed Self-Regularization, DPO-Kernels maintains robust generalization for LLMs, offering a comprehensive resource for further alignment research.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2411.02149.pdf' target='_blank'>https://arxiv.org/pdf/2411.02149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanqi Yao, Gang Wu, Kui Jiang, Siao Liu, Jian Kuai, Xianming Liu, Junjun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02149">Improving Domain Generalization in Self-supervised Monocular Depth Estimation via Stabilized Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a self-supervised Monocular Depth Estimation (MDE) model with great generalization remains significantly challenging. Despite the success of adversarial augmentation in the supervised learning generalization, naively incorporating it into self-supervised MDE models potentially causes over-regularization, suffering from severe performance degradation. In this paper, we conduct qualitative analysis and illuminate the main causes: (i) inherent sensitivity in the UNet-alike depth network and (ii) dual optimization conflict caused by over-regularization. To tackle these issues, we propose a general adversarial training framework, named Stabilized Conflict-optimization Adversarial Training (SCAT), integrating adversarial data augmentation into self-supervised MDE methods to achieve a balance between stability and generalization. Specifically, we devise an effective scaling depth network that tunes the coefficients of long skip connection and effectively stabilizes the training process. Then, we propose a conflict gradient surgery strategy, which progressively integrates the adversarial gradient and optimizes the model toward a conflict-free direction. Extensive experiments on five benchmarks demonstrate that SCAT can achieve state-of-the-art performance and significantly improve the generalization capability of existing self-supervised MDE methods.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2407.15498.pdf' target='_blank'>https://arxiv.org/pdf/2407.15498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingyao Yu, Yang An, Wei Ye, Xiongfeng Xiao, Shaoguang Mao, Tao Ge, Shikun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15498">Refining Corpora from a Model Calibration Perspective for Chinese Spelling Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality corpora, due to the labor-intensive labeling of spelling errors in real-life human writing or typing scenarios. Two data augmentation methods are widely adopted: (1) \textit{Random Replacement} with the guidance of confusion sets and (2) \textit{OCR/ASR-based Generation} that simulates character misusing. However, both methods inevitably introduce noisy data (e.g., false spelling errors), potentially leading to over-correction. By carefully analyzing the two types of corpora, we find that though the latter achieves more robust generalization performance, the former yields better-calibrated CSC models. We then provide a theoretical analysis of this empirical observation, based on which a corpus refining strategy is proposed. Specifically, OCR/ASR-based data samples are fed into a well-calibrated CSC model trained on random replacement-based corpora and then filtered based on prediction confidence. By learning a simple BERT-based model on the refined OCR/ASR-based corpus, we set up impressive state-of-the-art performance on three widely-used benchmarks, while significantly alleviating over-correction (e.g., lowering false positive predictions).
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2303.07527.pdf' target='_blank'>https://arxiv.org/pdf/2303.07527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07527">Domain Generalization via Nuclear Norm Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to generalize to unseen domains is crucial for machine learning systems deployed in the real world, especially when we only have data from limited training domains. In this paper, we propose a simple and effective regularization method based on the nuclear norm of the learned features for domain generalization. Intuitively, the proposed regularizer mitigates the impacts of environmental features and encourages learning domain-invariant features. Theoretically, we provide insights into why nuclear norm regularization is more effective compared to ERM and alternative regularization methods. Empirically, we conduct extensive experiments on both synthetic and real datasets. We show nuclear norm regularization achieves strong performance compared to baselines in a wide range of domain generalization tasks. Moreover, our regularizer is broadly applicable with various methods such as ERM and SWAD with consistently improved performance, e.g., 1.7% and 0.9% test accuracy improvements respectively on the DomainBed benchmark.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2303.06571.pdf' target='_blank'>https://arxiv.org/pdf/2303.06571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, Wei Ji, Qi Tian, Tat-Seng Chua, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06571">Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt tuning, a recently emerging paradigm, enables the powerful vision-language pre-training models to adapt to downstream tasks in a parameter -- and data -- efficient way, by learning the ``soft prompts'' to condition frozen pre-training models. Though effective, it is particularly problematic in the few-shot scenario, where prompt tuning performance is sensitive to the initialization and requires a time-consuming process to find a good initialization, thus restricting the fast adaptation ability of the pre-training models. In addition, prompt tuning could undermine the generalizability of the pre-training models, because the learnable prompt tokens are easy to overfit to the limited training samples. To address these issues, we introduce a novel Gradient-RegulAted Meta-prompt learning (GRAM) framework that jointly meta-learns an efficient soft prompt initialization for better adaptation and a lightweight gradient regulating function for strong cross-domain generalizability in a meta-learning paradigm using only the unlabeled image-text pre-training data. Rather than designing a specific prompt tuning method, our GRAM can be easily incorporated into various prompt tuning methods in a model-agnostic way, and comprehensive experiments show that GRAM brings about consistent improvement for them in several settings (i.e., few-shot learning, cross-domain generalization, cross-dataset generalization, etc.) over 11 datasets. Further, experiments show that GRAM enables the orthogonal methods of textual and visual prompt tuning to work in a mutually-enhanced way, offering better generalizability beyond the uni-modal prompt tuning methods.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2211.12509.pdf' target='_blank'>https://arxiv.org/pdf/2211.12509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Tan, Zhangyang Gao, Siyuan Li, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12509">SimVPv2: Towards Simple yet Powerful Spatiotemporal Predictive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed remarkable advances in spatiotemporal predictive learning, with methods incorporating auxiliary inputs, complex neural architectures, and sophisticated training strategies. While SimVP has introduced a simpler, CNN-based baseline for this task, it still relies on heavy Unet-like architectures for spatial and temporal modeling, which still suffers from high complexity and computational overhead. In this paper, we propose SimVPv2, a streamlined model that eliminates the need for Unet architectures and demonstrates that plain stacks of convolutional layers, enhanced with an efficient Gated Spatiotemporal Attention mechanism, can deliver state-of-the-art performance. SimVPv2 not only simplifies the model architecture but also improves both performance and computational efficiency. On the standard Moving MNIST benchmark, SimVPv2 achieves superior performance compared to SimVP, with fewer FLOPs, about half the training time, and 60% faster inference efficiency. Extensive experiments across eight diverse datasets, including real-world tasks such as traffic forecasting and climate prediction, further demonstrate that SimVPv2 offers a powerful yet straightforward solution, achieving robust generalization across various spatiotemporal learning scenarios. We believe the proposed SimVPv2 can serve as a solid baseline to benefit the spatiotemporal predictive learning community.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2506.16406.pdf' target='_blank'>https://arxiv.org/pdf/2506.16406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Liang, Dongwen Tang, Yuhao Zhou, Xuanlei Zhao, Mingjia Shi, Wangbo Zhao, Zekai Li, Peihao Wang, Konstantin SchÃ¼rholt, Damian Borth, Michael M. Bronstein, Yang You, Zhangyang Wang, Kai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16406">Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to \textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains up to \textbf{30\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at \href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2308.02287.pdf' target='_blank'>https://arxiv.org/pdf/2308.02287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Wang, Jindong Wang, Xixu Hu, Shujun Wang, Xing Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02287">Frustratingly Easy Model Generalization by Dummy Risk Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the performance under all tasks with an almost free lunch manner. Furthermore, we show that DuRM is compatible with existing generalization techniques and we discuss possible limitations. We hope that DuRM could trigger new interest in the fundamental research on risk minimization.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2211.08073.pdf' target='_blank'>https://arxiv.org/pdf/2211.08073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.08073">GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2209.00652.pdf' target='_blank'>https://arxiv.org/pdf/2209.00652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Lu, Jindong Wang, Yidong Wang, Xing Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00652">Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The distribution shifts between training and test data typically undermine the performance of models. In recent years, lots of work pays attention to domain generalization (DG) where distribution shifts exist, and target data are unseen. Despite the progress in algorithm design, two foundational factors have long been ignored: 1) the optimization for regularization-based objectives, and 2) the model selection for DG since no knowledge about the target domain can be utilized. In this paper, we propose Mixup guided optimization and selection techniques for DG. For optimization, we utilize an adapted Mixup to generate an out-of-distribution dataset that can guide the preference direction and optimize with Pareto optimization. For model selection, we generate a validation dataset with a closer distance to the target distribution, and thereby it can better represent the target data. We also present some theoretical insights behind our proposals. Comprehensive experiments demonstrate that our model optimization and selection techniques can largely improve the performance of existing domain generalization algorithms and even achieve new state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2208.08661.pdf' target='_blank'>https://arxiv.org/pdf/2208.08661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Fan Zhang, Jindong Wang, Jian Liang, Zhang Zhang, Baosheng Yu, Liang Wang, Dacheng Tao, Xing Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.08661">Domain-Specific Risk Minimization for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent domain generalization (DG) approaches typically use the hypothesis learned on source domains for inference on the unseen target domain. However, such a hypothesis can be arbitrarily far from the optimal one for the target domain, induced by a gap termed ``adaptivity gap''. Without exploiting the domain information from the unseen test samples, adaptivity gap estimation and minimization are intractable, which hinders us to robustify a model to any unknown distribution. In this paper, we first establish a generalization bound that explicitly considers the adaptivity gap. Our bound motivates two strategies to reduce the gap: the first one is ensembling multiple classifiers to enrich the hypothesis space, then we propose effective gap estimation methods for guiding the selection of a better hypothesis for the target. The other method is minimizing the gap directly by adapting model parameters using online target samples. We thus propose \textbf{Domain-specific Risk Minimization (DRM)}. During training, DRM models the distributions of different source domains separately; for inference, DRM performs online model steering using the source hypothesis for each arriving target sample. Extensive experiments demonstrate the effectiveness of the proposed DRM for domain generalization with the following advantages: 1) it significantly outperforms competitive baselines on different distributional shift settings; 2) it achieves either comparable or superior accuracies on all source domains compared to vanilla empirical risk minimization; 3) it remains simple and efficient during training, and 4) it is complementary to invariant learning approaches.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2506.20923.pdf' target='_blank'>https://arxiv.org/pdf/2506.20923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, Qian Chen, Youcheng Pan, Yang Xiang, Meishan Zhang, Haofen Wang, Jun Yu, Baotian Hu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20923">KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose KaLM-Embedding-V2, a versatile and compact embedding model, which achieves impressive performance in general-purpose text embedding tasks by leveraging superior training techniques and data. Our key innovations include: (1) To better align the architecture with representation learning, we remove the causal attention mask and adopt a fully bidirectional transformer with simple yet effective mean-pooling to produce fixed-length embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on large-scale weakly supervised open-source corpora; (ii) fine-tuning on high-quality retrieval and non-retrieval datasets; and (iii) model-soup parameter averaging for robust generalization. Besides, we introduce a focal-style reweighting mechanism that concentrates learning on difficult samples and an online hard-negative mixing strategy to continuously enrich hard negatives without expensive offline mining; (3) We collect over 20 categories of data for pre-training and 100 categories of data for fine-tuning, to boost both the performance and generalization of the embedding model. Extensive evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English show that our model significantly outperforms others of comparable size, and competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new standard for a versatile and compact embedding model with less than 1B parameters.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2505.12629.pdf' target='_blank'>https://arxiv.org/pdf/2505.12629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchang Sun, Yanxi Chen, Yaliang Li, Bolin Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12629">Enhancing Latent Computation in Transformers with Latent Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmenting large language models (LLMs) with auxiliary tokens has emerged as a promising strategy for enhancing model performance. In this work, we introduce a lightweight method termed latent tokens; these are dummy tokens that may be non-interpretable in natural language but steer the autoregressive decoding process of a Transformer-based LLM via the attention mechanism. The proposed latent tokens can be seamlessly integrated with a pre-trained Transformer, trained in a parameter-efficient manner, and applied flexibly at inference time, while adding minimal complexity overhead to the existing infrastructure of standard Transformers. We propose several hypotheses about the underlying mechanisms of latent tokens and design synthetic tasks accordingly to verify them. Numerical results confirm that the proposed method noticeably outperforms the baselines, particularly in the out-of-distribution generalization scenarios, highlighting its potential in improving the adaptability of LLMs.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2504.03450.pdf' target='_blank'>https://arxiv.org/pdf/2504.03450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Van-Anh Nguyen, Thanh-Toan Do, Mehrtash Harandi, Dinh Phung, Trung Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03450">Optimizing Specific and Shared Parameters for Efficient Parameter Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models, with a vast number of parameters and pretraining on massive datasets, achieve state-of-the-art performance across various applications. However, efficiently adapting them to downstream tasks with minimal computational overhead remains a challenge. Parameter-Efficient Transfer Learning (PETL) addresses this by fine-tuning only a small subset of parameters while preserving pre-trained knowledge. In this paper, we propose SaS, a novel PETL method that effectively mitigates distributional shifts during fine-tuning. SaS integrates (1) a shared module that captures common statistical characteristics across layers using low-rank projections and (2) a layer-specific module that employs hypernetworks to generate tailored parameters for each layer. This dual design ensures an optimal balance between performance and parameter efficiency while introducing less than 0.05% additional parameters, making it significantly more compact than existing methods. Extensive experiments on diverse downstream tasks, few-shot settings and domain generalization demonstrate that SaS significantly enhances performance while maintaining superior parameter efficiency compared to existing methods, highlighting the importance of capturing both shared and layer-specific information in transfer learning. Code and data are available at https://anonymous.4open.science/r/SaS-PETL-3565.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2502.10716.pdf' target='_blank'>https://arxiv.org/pdf/2502.10716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long-Tung Vuong, Vy Vo, Hien Dang, Van-Anh Nguyen, Thanh-Toan Do, Mehrtash Harandi, Trung Le, Dinh Phung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10716">Why Domain Generalization Fail? A View of Necessity and Sufficiency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite a strong theoretical foundation, empirical experiments reveal that existing domain generalization (DG) algorithms often fail to consistently outperform the ERM baseline. We argue that this issue arises because most DG studies focus on establishing theoretical guarantees for generalization under unrealistic assumptions, such as the availability of sufficient, diverse (or even infinite) domains or access to target domain knowledge. As a result, the extent to which domain generalization is achievable in scenarios with limited domains remains largely unexplored. This paper seeks to address this gap by examining generalization through the lens of the conditions necessary for its existence and learnability. Specifically, we systematically establish a set of necessary and sufficient conditions for generalization. Our analysis highlights that existing DG methods primarily act as regularization mechanisms focused on satisfying sufficient conditions, while often neglecting necessary ones. However, sufficient conditions cannot be verified in settings with limited training domains. In such cases, regularization targeting sufficient conditions aims to maximize the likelihood of generalization, whereas regularization targeting necessary conditions ensures its existence. Using this analysis, we reveal the shortcomings of existing DG algorithms by showing that, while they promote sufficient conditions, they inadvertently violate necessary conditions. To validate our theoretical insights, we propose a practical method that promotes the sufficient condition while maintaining the necessary conditions through a novel subspace representation alignment strategy. This approach highlights the advantages of preserving the necessary conditions on well-established DG benchmarks.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2311.00288.pdf' target='_blank'>https://arxiv.org/pdf/2311.00288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, Nanyun Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00288">Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and prediction probability. We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit, underscoring the importance of task selection for instruction tuning.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2209.14105.pdf' target='_blank'>https://arxiv.org/pdf/2209.14105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aishan Liu, Shiyu Tang, Siyuan Liang, Ruihao Gong, Boxi Wu, Xianglong Liu, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14105">Exploring the Relationship between Architecture and Adversarially Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial training has been demonstrated to be one of the most effective remedies for defending adversarial examples, yet it often suffers from the huge robustness generalization gap on unseen testing adversaries, deemed as the adversarially robust generalization problem. Despite the preliminary understandings devoted to adversarially robust generalization, little is known from the architectural perspective. To bridge the gap, this paper for the first time systematically investigated the relationship between adversarially robust generalization and architectural design. Inparticular, we comprehensively evaluated 20 most representative adversarially trained architectures on ImageNette and CIFAR-10 datasets towards multiple `p-norm adversarial attacks. Based on the extensive experiments, we found that, under aligned settings, Vision Transformers (e.g., PVT, CoAtNet) often yield better adversarially robust generalization while CNNs tend to overfit on specific attacks and fail to generalize on multiple adversaries. To better understand the nature behind it, we conduct theoretical analysis via the lens of Rademacher complexity. We revealed the fact that the higher weight sparsity contributes significantly towards the better adversarially robust generalization of Transformers, which can be often achieved by the specially-designed attention blocks. We hope our paper could help to better understand the mechanism for designing robust DNNs. Our model weights can be found at http://robust.art.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2203.00553.pdf' target='_blank'>https://arxiv.org/pdf/2203.00553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoang Phan, Trung Le, Trung Phung, Tuan Anh Bui, Nhat Ho, Dinh Phung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.00553">Global-Local Regularization Via Distributional Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite superior performance in many situations, deep neural networks are often vulnerable to adversarial examples and distribution shifts, limiting model generalization ability in real-world applications. To alleviate these problems, recent approaches leverage distributional robustness optimization (DRO) to find the most challenging distribution, and then minimize loss function over this most challenging distribution. Regardless of achieving some improvements, these DRO approaches have some obvious limitations. First, they purely focus on local regularization to strengthen model robustness, missing a global regularization effect which is useful in many real-world applications (e.g., domain adaptation, domain generalization, and adversarial machine learning). Second, the loss functions in the existing DRO approaches operate in only the most challenging distribution, hence decouple with the original distribution, leading to a restrictive modeling capability. In this paper, we propose a novel regularization technique, following the veins of Wasserstein-based DRO framework. Specifically, we define a particular joint distribution and Wasserstein-based uncertainty, allowing us to couple the original and most challenging distributions for enhancing modeling capability and applying both local and global regularizations. Empirical studies on different learning problems demonstrate that our proposed approach significantly outperforms the existing regularization approaches in various domains: semi-supervised learning, domain adaptation, domain generalization, and adversarial machine learning.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2111.13822.pdf' target='_blank'>https://arxiv.org/pdf/2111.13822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Trung Phung, Trung Le, Long Vuong, Toan Tran, Anh Tran, Hung Bui, Dinh Phung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.13822">On Learning Domain-Invariant Representations for Transfer Learning with Multiple Sources</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptation (DA) benefits from the rigorous theoretical works that study its insightful characteristics and various aspects, e.g., learning domain-invariant representations and its trade-off. However, it seems not the case for the multiple source DA and domain generalization (DG) settings which are remarkably more complicated and sophisticated due to the involvement of multiple source domains and potential unavailability of target domain during training. In this paper, we develop novel upper-bounds for the target general loss which appeal to us to define two kinds of domain-invariant representations. We further study the pros and cons as well as the trade-offs of enforcing learning each domain-invariant representation. Finally, we conduct experiments to inspect the trade-off of these representations for offering practical hints regarding how to use them in practice and explore other interesting properties of our developed theory.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2310.15008.pdf' target='_blank'>https://arxiv.org/pdf/2310.15008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15008">Wonder3D: Single Image to 3D using Cross-Domain Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce Wonder3D, a novel method for efficiently generating high-fidelity textured meshes from single-view images.Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of image-to-3D tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure consistency, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a geometry-aware normal fusion algorithm that extracts high-quality surfaces from the multi-view 2D representations. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and reasonably good efficiency compared to prior works.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2503.18987.pdf' target='_blank'>https://arxiv.org/pdf/2503.18987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiran Wang, Jian Zhang, Lei Qi, Yinghuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18987">Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization is proposed to address distribution shift, arising from statistical disparities between training source and unseen target domains. The widely used first-order meta-learning algorithms demonstrate strong performance for domain generalization by leveraging the gradient matching theory, which aims to establish balanced parameters across source domains to reduce overfitting to any particular domain. However, our analysis reveals that there are actually numerous directions to achieve gradient matching, with current methods representing just one possible path. These methods actually overlook another critical factor that the balanced parameters should be close to the centroid of optimal parameters of each source domain. To address this, we propose a simple yet effective arithmetic meta-learning with arithmetic-weighted gradients. This approach, while adhering to the principles of gradient matching, promotes a more precise balance by estimating the centroid between domain-specific optimal parameters. Experimental results validate the effectiveness of our strategy.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2407.15085.pdf' target='_blank'>https://arxiv.org/pdf/2407.15085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajun Hu, Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15085">Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to avoid the performance degradation of the model when the distribution shift between the limited training data and unseen test data occurs. Recently, foundation models with enormous parameters have been pre-trained with huge datasets, demonstrating strong generalization ability and showing promising direction for solving the DG problem. However, fully Fine-Tuning (FT) the foundation models results in unsatisfactory out-of-distribution accuracy due to the destroyed pre-trained generalized features. Recently, Parameter-Efficient Fine-Tuning (PEFT) alleviates the above problem by fine-tuning a small portion of the model parameters while keeping the rest frozen, which achieves better generalization performance compared to FT. Nevertheless, PEFT still suffers from the issue of overfitting to the training domains. To address the above issue, we propose Parameter-Efficient Group with Orthogonal regularization (PEGO) for vision transformers, which effectively preserves the generalization ability of the pre-trained network and learns more diverse knowledge compared with conventional PEFT. Specifically, we inject a group of trainable Low-Rank Adaptation (LoRA) modules into the pre-trained model and propose an orthogonal regularization loss to enhance the generalization ability of the model. Our framework achieves SOTA performance on five DG benchmarks, while only requiring training a small number of parameters without adding additional testing cost.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2406.03051.pdf' target='_blank'>https://arxiv.org/pdf/2406.03051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minglei Li, Peng Ye, Yongqi Huang, Lin Zhang, Tao Chen, Tong He, Jiayuan Fan, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03051">Adapter-X: A Novel General Parameter-Efficient Fine-Tuning Framework for Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parameter-efficient fine-tuning (PEFT) has become increasingly important as foundation models continue to grow in both popularity and size. Adapter has been particularly well-received due to their potential for parameter reduction and adaptability across diverse tasks. However, striking a balance between high efficiency and robust generalization across tasks remains a challenge for adapter-based methods. We analyze existing methods and find that: 1) parameter sharing is the key to reducing redundancy; 2) more tunable parameters, dynamic allocation, and block-specific design are keys to improving performance. Unfortunately, no previous work considers all these factors. Inspired by this insight, we introduce a novel framework named Adapter-X. First, a Sharing Mixture of Adapters (SMoA) module is proposed to fulfill token-level dynamic allocation, increased tunable parameters, and inter-block sharing at the same time. Second, some block-specific designs like Prompt Generator (PG) are introduced to further enhance the ability of adaptation. Extensive experiments across 2D image and 3D point cloud modalities demonstrate that Adapter-X represents a significant milestone as it is the first to outperform full fine-tuning in both 2D image and 3D point cloud modalities with significantly fewer parameters, i.e., only 0.20% and 1.88% of original trainable parameters for 2D and 3D classification tasks. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2405.16489.pdf' target='_blank'>https://arxiv.org/pdf/2405.16489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiwen Li, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Jialong Wang, Yang Li, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16489">Causal-aware Graph Neural Architecture Search under Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph NAS has emerged as a promising approach for autonomously designing GNN architectures by leveraging the correlations between graphs and architectures. Existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions. We propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts. The problem remains unexplored with following challenges: how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, and how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures. To address these challenges, we propose Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts. Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions. Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements. Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures. Extensive experiments demonstrate that CARNAS achieves advanced out-of-distribution generalization ability.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2403.16075.pdf' target='_blank'>https://arxiv.org/pdf/2403.16075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Xu, Weiran Shen, Xiao Zhang, Jun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16075">IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an inverse batched contextual bandit (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming problem by utilizing the behavioral evolution history of the batched contextual bandit with inaccessible rewards. We demonstrate that IBCB is a unified framework for both deterministic and randomized bandit policies. The experimental results indicate that IBCB outperforms several existing imitation learning algorithms on synthetic and real-world data and significantly reduces running time. Additionally, empirical analyses reveal that IBCB exhibits better out-of-distribution generalization and is highly effective in learning the bandit policy from the interaction history of novice experts.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2401.05752.pdf' target='_blank'>https://arxiv.org/pdf/2401.05752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Na Wang, Lei Qi, Jintao Guo, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05752">Learning Generalizable Models via Disentangling Spurious and Enhancing Potential Correlations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) intends to train a model on multiple source domains to ensure that it can generalize well to an arbitrary unseen target domain. The acquisition of domain-invariant representations is pivotal for DG as they possess the ability to capture the inherent semantic information of the data, mitigate the influence of domain shift, and enhance the generalization capability of the model. Adopting multiple perspectives, such as the sample and the feature, proves to be effective. The sample perspective facilitates data augmentation through data manipulation techniques, whereas the feature perspective enables the extraction of meaningful generalization features. In this paper, we focus on improving the generalization ability of the model by compelling it to acquire domain-invariant representations from both the sample and feature perspectives by disentangling spurious correlations and enhancing potential correlations. 1) From the sample perspective, we develop a frequency restriction module, guiding the model to focus on the relevant correlations between object features and labels, thereby disentangling spurious correlations. 2) From the feature perspective, the simple Tail Interaction module implicitly enhances potential correlations among all samples from all source domains, facilitating the acquisition of domain-invariant representations across multiple domains for the model. The experimental results show that Convolutional Neural Networks (CNNs) or Multi-Layer Perceptrons (MLPs) with a strong baseline embedded with these two modules can achieve superior results, e.g., an average accuracy of 92.30% on Digits-DG.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2308.00918.pdf' target='_blank'>https://arxiv.org/pdf/2308.00918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjia Zhao, Lei Qi, Xiao Shi, Yinghuan Shi, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00918">A Novel Cross-Perturbation for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization aims to enhance the ability of the model to generalize to unknown domains when trained on a single source domain. However, the limited diversity in the training data hampers the learning of domain-invariant features, resulting in compromised generalization performance. To address this, data perturbation (augmentation) has emerged as a crucial method to increase data diversity. Nevertheless, existing perturbation methods often focus on either image-level or feature-level perturbations independently, neglecting their synergistic effects. To overcome these limitations, we propose CPerb, a simple yet effective cross-perturbation method. Specifically, CPerb utilizes both horizontal and vertical operations. Horizontally, it applies image-level and feature-level perturbations to enhance the diversity of the training data, mitigating the issue of limited diversity in single-source domains. Vertically, it introduces multi-route perturbation to learn domain-invariant features from different perspectives of samples with the same semantic category, thereby enhancing the generalization capability of the model. Additionally, we propose MixPatch, a novel feature-level perturbation method that exploits local image style information to further diversify the training data. Extensive experiments on various benchmark datasets validate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2307.13492.pdf' target='_blank'>https://arxiv.org/pdf/2307.13492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Qi, Hongpeng Yang, Yinghuan Shi, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13492">NormAUG: Normalization-guided Augmentation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has made significant advancements in supervised learning. However, models trained in this setting often face challenges due to domain shift between training and test sets, resulting in a significant drop in performance during testing. To address this issue, several domain generalization methods have been developed to learn robust and domain-invariant features from multiple training domains that can generalize well to unseen test domains. Data augmentation plays a crucial role in achieving this goal by enhancing the diversity of the training data. In this paper, inspired by the observation that normalizing an image with different statistics generated by different batches with various domains can perturb its feature, we propose a simple yet effective method called NormAUG (Normalization-guided Augmentation). Our method includes two paths: the main path and the auxiliary (augmented) path. During training, the auxiliary path includes multiple sub-paths, each corresponding to batch normalization for a single domain or a random combination of multiple domains. This introduces diverse information at the feature level and improves the generalization of the main path. Moreover, our NormAUG method effectively reduces the existing upper boundary for generalization based on theoretical perspectives. During the test stage, we leverage an ensemble strategy to combine the predictions from the auxiliary path of our model, further boosting performance. Extensive experiments are conducted on multiple benchmark datasets to validate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2306.11991.pdf' target='_blank'>https://arxiv.org/pdf/2306.11991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Qi, Ziang Liu, Yinghuan Shi, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11991">Generalizable Metric Network for Cross-domain Person Re-identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person Re-identification (Re-ID) is a crucial technique for public security and has made significant progress in supervised settings. However, the cross-domain (i.e., domain generalization) scene presents a challenge in Re-ID tasks due to unseen test domains and domain-shift between the training and test sets. To tackle this challenge, most existing methods aim to learn domain-invariant or robust features for all domains. In this paper, we observe that the data-distribution gap between the training and test sets is smaller in the sample-pair space than in the sample-instance space. Based on this observation, we propose a Generalizable Metric Network (GMN) to further explore sample similarity in the sample-pair space. Specifically, we add a Metric Network (M-Net) after the main network and train it on positive and negative sample-pair features, which is then employed during the test stage. Additionally, we introduce the Dropout-based Perturbation (DP) module to enhance the generalization capability of the metric network by enriching the sample-pair diversity. Moreover, we develop a Pair-Identity Center (PIC) loss to enhance the model's discrimination by ensuring that sample-pair features with the same pair-identity are consistent. We validate the effectiveness of our proposed method through a lot of experiments on multiple benchmark datasets and confirm the value of each module in our GMN.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2208.05853.pdf' target='_blank'>https://arxiv.org/pdf/2208.05853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Qi, Hongpeng Yang, Yinghuan Shi, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.05853">MultiMatch: Multi-task Learning for Semi-supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims at learning a model on source domains to well generalize on the unseen target domain. Although it has achieved great success, most of existing methods require the label information for all training samples in source domains, which is time-consuming and expensive in the real-world application. In this paper, we resort to solving the semi-supervised domain generalization (SSDG) task, where there are a few label information in each source domain. To address the task, we first analyze the theory of the multi-domain learning, which highlights that 1) mitigating the impact of domain gap and 2) exploiting all samples to train the model can effectively reduce the generalization error in each source domain so as to improve the quality of pseudo-labels. According to the analysis, we propose MultiMatch, i.e., extending FixMatch to the multi-task learning framework, producing the high-quality pseudo-label for SSDG. To be specific, we consider each training domain as a single task (i.e., local task) and combine all training domains together (i.e., global task) to train an extra task for the unseen test domain. In the multi-task framework, we utilize the independent BN and classifier for each task, which can effectively alleviate the interference from different domains during pseudo-labeling. Also, most of parameters in the framework are shared, which can be trained by all training samples sufficiently. Moreover, to further boost the pseudo-label accuracy and the model's generalization, we fuse the predictions from the global task and local task during training and testing, respectively. A series of experiments validate the effectiveness of the proposed method, and it outperforms the existing semi-supervised methods and the SSDG method on several benchmark DG datasets.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2202.07987.pdf' target='_blank'>https://arxiv.org/pdf/2202.07987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Li, Xin Wang, Ziwei Zhang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.07987">Out-Of-Distribution Generalization on Graphs: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph machine learning has been extensively studied in both academia and industry. Although booming with a vast number of emerging methods and techniques, most of the literature is built on the in-distribution hypothesis, i.e., testing and training graph data are identically distributed. However, this in-distribution hypothesis can hardly be satisfied in many real-world graph scenarios where the model performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this critical problem, out-of-distribution (OOD) generalization on graphs, which goes beyond the in-distribution hypothesis, has made great progress and attracted ever-increasing attention from the research community. In this paper, we comprehensively survey OOD generalization on graphs and present a detailed review of recent advances in this area. First, we provide a formal problem definition of OOD generalization on graphs. Second, we categorize existing methods into three classes from conceptually different perspectives, i.e., data, model, and learning strategy, based on their positions in the graph machine learning pipeline, followed by detailed discussions for each category. We also review the theories related to OOD generalization on graphs and introduce the commonly used graph datasets for thorough evaluations. Finally, we share our insights on future research directions. This paper is the first systematic and comprehensive review of OOD generalization on graphs, to the best of our knowledge.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2111.15077.pdf' target='_blank'>https://arxiv.org/pdf/2111.15077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Qi, Jiaqi Liu, Lei Wang, Yinghuan Shi, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.15077">Unsupervised Domain Generalization for Person Re-identification: A Domain-specific Adaptive Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) has attracted much attention in person re-identification (ReID) recently. It aims to make a model trained on multiple source domains generalize to an unseen target domain. Although achieving promising progress, existing methods usually need the source domains to be labeled, which could be a significant burden for practical ReID tasks. In this paper, we turn to investigate unsupervised domain generalization for ReID, by assuming that no label is available for any source domains.
  To address this challenging setting, we propose a simple and efficient domain-specific adaptive framework, and realize it with an adaptive normalization module designed upon the batch and instance normalization techniques. In doing so, we successfully yield reliable pseudo-labels to implement training and also enhance the domain generalization capability of the model as required. In addition, we show that our framework can even be applied to improve person ReID under the settings of supervised domain generalization and unsupervised domain adaptation, demonstrating competitive performance with respect to relevant methods. Extensive experimental study on benchmark datasets is conducted to validate the proposed framework. A significance of our work lies in that it shows the potential of unsupervised domain generalization for person ReID and sets a strong baseline for the further research on this topic.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2508.01850.pdf' target='_blank'>https://arxiv.org/pdf/2508.01850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lala Shakti Swarup Ray, Vitor Fortes Rey, Bo Zhou, Paul Lukowicz, Sungho Suh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01850">ChairPose: Pressure-based Chair Morphology Grounded Sitting Pose Estimation through Simulation-Assisted Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prolonged seated activity is increasingly common in modern environments, raising concerns around musculoskeletal health, ergonomics, and the design of responsive interactive systems. Existing posture sensing methods such as vision-based or wearable approaches face limitations including occlusion, privacy concerns, user discomfort, and restricted deployment flexibility. We introduce ChairPose, the first full body, wearable free seated pose estimation system that relies solely on pressure sensing and operates independently of chair geometry. ChairPose employs a two stage generative model trained on pressure maps captured from a thin, chair agnostic sensing mattress. Unlike prior approaches, our method explicitly incorporates chair morphology into the inference process, enabling accurate, occlusion free, and privacy preserving pose estimation. To support generalization across diverse users and chairs, we introduce a physics driven data augmentation pipeline that simulates realistic variations in posture and seating conditions. Evaluated across eight users and four distinct chairs, ChairPose achieves a mean per joint position error of 89.4 mm when both the user and the chair are unseen, demonstrating robust generalization to novel real world generalizability. ChairPose expands the design space for posture aware interactive systems, with potential applications in ergonomics, healthcare, and adaptive user interfaces.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2501.07408.pdf' target='_blank'>https://arxiv.org/pdf/2501.07408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07408">Initial Findings on Sensor based Open Vocabulary Activity Recognition via Text Embedding Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional human activity recognition (HAR) relies on classifiers trained to predict discrete activity classes, inherently limiting recognition to activities explicitly present in the training set. Such classifiers would invariably fail, putting zero likelihood, when encountering unseen activities. We propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this limitation by first converting each activity into natural language and breaking it into a sequence of elementary motions. This descriptive text is then encoded into a fixed-size embedding. The model is trained to regress this embedding, which is subsequently decoded back into natural language using a pre-trained embedding inversion model. Unlike other works that rely on auto-regressive large language models (LLMs) at their core, OV-HAR achieves open vocabulary recognition without the computational overhead of such models. The generated text can be transformed into a single activity class using LLM prompt engineering. We have evaluated our approach on different modalities, including vision (pose), IMU, and pressure sensors, demonstrating robust generalization across unseen activities and modalities, offering a fundamentally different paradigm from contemporary classifiers.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2404.19652.pdf' target='_blank'>https://arxiv.org/pdf/2404.19652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Liu, Mingxin Huang, Hao Yan, Linger Deng, Weijia Wu, Hao Lu, Chunhua Shen, Lianwen Jin, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19652">VimTS: A Unified Video and Image Text Spotter for Enhancing the Cross-domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the https://VimTextSpotter.github.io.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2509.22295.pdf' target='_blank'>https://arxiv.org/pdf/2509.22295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Wu, Jianxin Jin, Wanghui Qiu, Peng Chen, Yang Shu, Bin Yang, Chenjuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22295">Aurora: Towards Universal Generative Multimodal Time Series Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain generalization is very important in Time Series Forecasting because similar historical information may lead to distinct future trends due to the domain-specific characteristics. Recent works focus on building unimodal time series foundation models and end-to-end multimodal supervised models. Since domain-specific knowledge is often contained in modalities like texts, the former lacks the explicit utilization of them, thus hindering the performance. The latter is tailored for end-to-end scenarios and does not support zero-shot inference for cross-domain scenarios. In this work, we introduce Aurora, a Multimodal Time Series Foundation Model, which supports multimodal inputs and zero-shot inference. Pretrained on Corss-domain Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key domain knowledge contained in corrsponding text or image modalities, thus possessing strong Cross-domain generalization capability. Through tokenization, encoding, and distillation, Aurora can extract multimodal domain knowledge as guidance and then utilizes a Modality-Guided Multi-head Self-Attention to inject them into the modeling of temporal representations. In the decoding phase, the multimodal representations are used to generate the conditions and prototypes of future tokens, contributing to a novel Prototype-Guided Flow Matching for generative probabilistic forecasting. Comprehensive experiments on well-recognized benchmarks, including TimeMMD, TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art performance of Aurora on both unimodal and multimodal scenarios.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2506.03167.pdf' target='_blank'>https://arxiv.org/pdf/2506.03167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Tan Le, Senura Hansaja Wanasekara, Zerun Niu, Yansong Shi, Nguyen H. Tran, Phuong Vo, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong, H. Vincent Poor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03167">Distributionally Robust Wireless Semantic Communication with Large AI Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>6G wireless systems are expected to support massive volumes of data with ultra-low latency. However, conventional bit-level transmission strategies cannot support the efficiency and adaptability required by modern, data-intensive applications. The concept of semantic communication (SemCom) addresses this limitation by focusing on transmitting task-relevant semantic information instead of raw data. While recent efforts incorporating deep learning and large-scale AI models have improved SemCom's performance, existing systems remain vulnerable to both semantic-level and transmission-level noise because they often rely on domain-specific architectures that hinder generalizability. In this paper, a novel and generalized semantic communication framework called WaSeCom is proposed to systematically address uncertainty and enhance robustness. In particular, Wasserstein distributionally robust optimization is employed to provide resilience against semantic misinterpretation and channel perturbations. A rigorous theoretical analysis is performed to establish the robust generalization guarantees of the proposed framework. Experimental results on image and text transmission demonstrate that WaSeCom achieves improved robustness under noise and adversarial perturbations. These results highlight its effectiveness in preserving semantic fidelity across varying wireless conditions.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2503.03222.pdf' target='_blank'>https://arxiv.org/pdf/2503.03222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou, Mingtao Pei, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03222">Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering absolute human motion from monocular inputs is challenging due to two main issues. First, existing methods depend on 3D training data collected from limited environments, constraining out-of-distribution generalization. The second issue is the difficulty of estimating metric-scale poses from monocular input. To address these challenges, we introduce Mocap-2-to-3, a novel framework that performs multi-view lifting from monocular input by leveraging 2D data pre-training, enabling the reconstruction of metrically accurate 3D motions with absolute positions. To leverage abundant 2D data, we decompose complex 3D motion into multi-view syntheses. We first pretrain a single-view diffusion model on extensive 2D datasets, then fine-tune a multi-view model using public 3D data to enable view-consistent motion generation from monocular input, allowing the model to acquire action priors and diversity through 2D data. Furthermore, to recover absolute poses, we propose a novel human motion representation that decouples the learning of local pose and global movements, while encoding geometric priors of the ground to accelerate convergence. This enables progressive recovery of motion in absolute space during inference. Experimental results on in-the-wild benchmarks demonstrate that our method surpasses state-of-the-art approaches in both camera-space motion realism and world-grounded human positioning, while exhibiting superior generalization capability. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2407.21264.pdf' target='_blank'>https://arxiv.org/pdf/2407.21264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, Huan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21264">Model Attribution in LLM-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model attribution for LLM-generated disinformation poses a significant challenge in understanding its origins and mitigating its spread. This task is especially challenging because modern large language models (LLMs) produce disinformation with human-like quality. Additionally, the diversity in prompting methods used to generate disinformation complicates accurate source attribution. These methods introduce domain-specific features that can mask the fundamental characteristics of the models. In this paper, we introduce the concept of model attribution as a domain generalization problem, where each prompting method represents a unique domain. We argue that an effective attribution model must be invariant to these domain-specific features. It should also be proficient in identifying the originating models across all scenarios, reflecting real-world detection challenges. To address this, we introduce a novel approach based on Supervised Contrastive Learning. This method is designed to enhance the model's robustness to variations in prompts and focuses on distinguishing between different source LLMs. We evaluate our model through rigorous experiments involving three common prompting methods: ``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs: ``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the effectiveness of our approach in model attribution tasks, achieving state-of-the-art performance across diverse and unseen datasets.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2407.20053.pdf' target='_blank'>https://arxiv.org/pdf/2407.20053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Ronghui Xu, Jilin Hu, Zhong Peng, Xi Lu, Chenjuan Guo, Bin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20053">Orca: Ocean Significant Wave Height Estimation with Spatio-temporally Aware Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant wave height (SWH) is a vital metric in marine science, and accurate SWH estimation is crucial for various applications, e.g., marine energy development, fishery, early warning systems for potential risks, etc. Traditional SWH estimation methods that are based on numerical models and physical theories are hindered by computational inefficiencies. Recently, machine learning has emerged as an appealing alternative to improve accuracy and reduce computational time. However, due to limited observational technology and high costs, the scarcity of real-world data restricts the potential of machine learning models. To overcome these limitations, we propose an ocean SWH estimation framework, namely Orca. Specifically, Orca enhances the limited spatio-temporal reasoning abilities of classic LLMs with a novel spatiotemporal aware encoding module. By segmenting the limited buoy observational data temporally, encoding the buoys' locations spatially, and designing prompt templates, Orca capitalizes on the robust generalization ability of LLMs to estimate significant wave height effectively with limited data. Experimental results on the Gulf of Mexico demonstrate that Orca achieves state-of-the-art performance in SWH estimation.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2507.17849.pdf' target='_blank'>https://arxiv.org/pdf/2507.17849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyue Yin, Qiushi Sun, Zhiyuan Zeng, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17849">Dynamic and Generalizable Process Reward Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Process Reward Models (PRMs) are crucial for guiding Large Language Models (LLMs) in complex scenarios by providing dense reward signals. However, existing PRMs primarily rely on heuristic approaches, which struggle with cross-domain generalization. While LLM-as-judge has been proposed to provide generalized rewards, current research has focused mainly on feedback results, overlooking the meaningful guidance embedded within the text. Additionally, static and coarse-grained evaluation criteria struggle to adapt to complex process supervision. To tackle these challenges, we propose Dynamic and Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to capture and store fine-grained, multi-dimensional reward criteria. DG-PRM dynamically selects reward signals for step-wise reward scoring. To handle multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation to identify discriminative positive and negative pairs. Experimental results show that DG-PRM achieves stunning performance on prevailing benchmarks, significantly boosting model performance across tasks with dense rewards. Further analysis reveals that DG-PRM adapts well to out-of-distribution scenarios, demonstrating exceptional generalizability.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2505.16675.pdf' target='_blank'>https://arxiv.org/pdf/2505.16675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenwen Qiang, Jingyao Wang, Zeen Song, Jiangmeng Li, Changwen Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16675">On the Out-of-Distribution Generalization of Self-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we focus on the out-of-distribution (OOD) generalization of self-supervised learning (SSL). By analyzing the mini-batch construction during the SSL training phase, we first give one plausible explanation for SSL having OOD generalization. Then, from the perspective of data generation and causal inference, we analyze and conclude that SSL learns spurious correlations during the training process, which leads to a reduction in OOD generalization. To address this issue, we propose a post-intervention distribution (PID) grounded in the Structural Causal Model. PID offers a scenario where the spurious variable and label variable is mutually independent. Besides, we demonstrate that if each mini-batch during SSL training satisfies PID, the resulting SSL model can achieve optimal worst-case OOD performance. This motivates us to develop a batch sampling strategy that enforces PID constraints through the learning of a latent variable model. Through theoretical analysis, we demonstrate the identifiability of the latent variable model and validate the effectiveness of the proposed sampling strategy. Experiments conducted on various downstream OOD tasks demonstrate the effectiveness of the proposed sampling strategy.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2308.00906.pdf' target='_blank'>https://arxiv.org/pdf/2308.00906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasheng Sun, Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han Hu, Lili Qiu, Hideki Koike
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00906">ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While language-guided image manipulation has made remarkable progress, the challenge of how to instruct the manipulation process faithfully reflecting human intentions persists. An accurate and comprehensive description of a manipulation task using natural language is laborious and sometimes even impossible, primarily due to the inherent uncertainty and ambiguity present in linguistic expressions. Is it feasible to accomplish image manipulation without resorting to external cross-modal language information? If this possibility exists, the inherent modality gap would be effortlessly eliminated. In this paper, we propose a novel manipulation methodology, dubbed ImageBrush, that learns visual instructions for more accurate image editing. Our key idea is to employ a pair of transformation images as visual instructions, which not only precisely captures human intention but also facilitates accessibility in real-world scenarios. Capturing visual instructions is particularly challenging because it involves extracting the underlying intentions solely from visual demonstrations and then applying this operation to a new image. To address this challenge, we formulate visual instruction learning as a diffusion-based inpainting problem, where the contextual information is fully exploited through an iterative process of generation. A visual prompting encoder is carefully devised to enhance the model's capacity in uncovering human intent behind the visual instructions. Extensive experiments show that our method generates engaging manipulation results conforming to the transformations entailed in demonstrations. Moreover, our model exhibits robust generalization capabilities on various downstream tasks such as pose transfer, image translation and video inpainting.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2510.07755.pdf' target='_blank'>https://arxiv.org/pdf/2510.07755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyu Wu, Yinlin Zhu, Xunkai Li, Ziang Qiu, Rong-Hua Li, Guoren Wang, Chenghu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07755">FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models have shown remarkable cross-domain generalization in language and vision, inspiring the development of graph foundation models (GFMs). However, existing GFMs typically assume centralized access to multi-domain graphs, which is often infeasible due to privacy and institutional constraints. Federated Graph Foundation Models (FedGFMs) address this limitation, but their effectiveness fundamentally hinges on constructing a robust global codebook that achieves intra-domain coherence by consolidating mutually reinforcing semantics within each domain, while also maintaining inter-domain diversity by retaining heterogeneous knowledge across domains. To this end, we propose FedBook, a unified federated graph foundation codebook that systematically aggregates clients' local codebooks during server-side federated pre-training. FedBook follows a two-phase process: (1) Intra-domain Collaboration, where low-frequency tokens are refined by referencing more semantically reliable high-frequency tokens across clients to enhance domain-specific coherence; and (2) Inter-domain Integration, where client contributions are weighted by the semantic distinctiveness of their codebooks during the aggregation of the global GFM, thereby preserving cross-domain diversity. Extensive experiments on 8 benchmarks across multiple domains and tasks demonstrate that FedBook consistently outperforms 21 baselines, including isolated supervised learning, FL/FGL, federated adaptations of centralized GFMs, and FedGFM techniques.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2509.08401.pdf' target='_blank'>https://arxiv.org/pdf/2509.08401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08401">Two Facets of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the success of LLMs, GFMs are designed to learn the optimal embedding functions from multi-domain text-attributed graphs for the downstream cross-task generalization capability. Among the diverse architectures, graph VQ-MAE stands out among the increasingly diverse landscape of GFM. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2309.09866.pdf' target='_blank'>https://arxiv.org/pdf/2309.09866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Pan, Bin Wang, Zheyuan Zhang, Xin Zhu, Debesh Jha, Ahmet Enis Cetin, Concetto Spampinato, Ulas Bagci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09866">Domain Generalization with Fourier Transform and Soft Thresholding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to train models on multiple source domains so that they can generalize well to unseen target domains. Among many domain generalization methods, Fourier-transform-based domain generalization methods have gained popularity primarily because they exploit the power of Fourier transformation to capture essential patterns and regularities in the data, making the model more robust to domain shifts. The mainstream Fourier-transform-based domain generalization swaps the Fourier amplitude spectrum while preserving the phase spectrum between the source and the target images. However, it neglects background interference in the amplitude spectrum. To overcome this limitation, we introduce a soft-thresholding function in the Fourier domain. We apply this newly designed algorithm to retinal fundus image segmentation, which is important for diagnosing ocular diseases but the neural network's performance can degrade across different sources due to domain shifts. The proposed technique basically enhances fundus image augmentation by eliminating small values in the Fourier domain and providing better generalization. The innovative nature of the soft thresholding fused with Fourier-transform-based domain generalization improves neural network models' performance by reducing the target images' background interference significantly. Experiments on public data validate our approach's effectiveness over conventional and state-of-the-art methods with superior segmentation metrics.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2308.08344.pdf' target='_blank'>https://arxiv.org/pdf/2308.08344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Lu, Xiaoying Gan, Ze Zhao, Shiyu Liang, Luoyi Fu, Xinbing Wang, Chenghu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08344">Graph Out-of-Distribution Generalization with Controllable Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \texttt{OOD-GMixup} to jointly manipulate the training distribution with \emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale representation domain to obtain potential OOD training samples. Finally, we propose OOD calibration to measure the distribution deviation of virtual samples by leveraging Extreme Value Theory, and further actively control the training distribution by emphasizing the impact of virtual OOD samples. Extensive studies on several real-world datasets on graph classification demonstrate the superiority of our proposed method over state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2306.11732.pdf' target='_blank'>https://arxiv.org/pdf/2306.11732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Renrui Zhang, Yi Wang, Yu Qiao, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11732">Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Question Answering (VideoQA) has been significantly advanced from the scaling of recent Large Language Models (LLMs). The key idea is to convert the visual information into the language feature space so that the capacity of LLMs can be fully exploited. Existing VideoQA methods typically take two paradigms: (1) learning cross-modal alignment, and (2) using an off-the-shelf captioning model to describe the visual data. However, the first design needs costly training on many extra multi-modal data, whilst the second is further limited by limited domain generalization. To address these limitations, a simple yet effective Retrieving-to-Answer (R2A) framework is proposed.Given an input video, R2A first retrieves a set of semantically similar texts from a generic text corpus using a pre-trained multi-modal model (e.g., CLIP). With both the question and the retrieved texts, a LLM (e.g., DeBERTa) can be directly used to yield a desired answer. Without the need for cross-modal fine-tuning, R2A allows for all the key components (e.g., LLM, retrieval model, and text corpus) to plug-and-play. Extensive experiments on several VideoQA benchmarks show that despite with 1.3B parameters and no fine-tuning, our R2A can outperform the 61 times larger Flamingo-80B model even additionally trained on nearly 2.1B multi-modal data.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2304.02720.pdf' target='_blank'>https://arxiv.org/pdf/2304.02720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Bin Wang, Lanhong Yao, Ugur Demir, Debesh Jha, Ismail Baris Turkbey, Boqing Gong, Ulas Bagci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02720">Domain Generalization with Adversarial Intensity Attack for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most statistical learning algorithms rely on an over-simplified assumption, that is, the train and test data are independent and identically distributed. In real-world scenarios, however, it is common for models to encounter data from new and different domains to which they were not exposed to during training. This is often the case in medical imaging applications due to differences in acquisition devices, imaging protocols, and patient characteristics. To address this problem, domain generalization (DG) is a promising direction as it enables models to handle data from previously unseen domains by learning domain-invariant features robust to variations across different domains. To this end, we introduce a novel DG method called Adversarial Intensity Attack (AdverIN), which leverages adversarial training to generate training data with an infinite number of styles and increase data diversity while preserving essential content information. We conduct extensive evaluation experiments on various multi-domain segmentation datasets, including 2D retinal fundus optic disc/cup and 3D prostate MRI. Our results demonstrate that AdverIN significantly improves the generalization ability of the segmentation models, achieving significant improvement on these challenging datasets. Code is available upon publication.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2302.09509.pdf' target='_blank'>https://arxiv.org/pdf/2302.09509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexing Qi, Shuhao Li, Zhixin Guo, Yusheng Huang, Chenghu Zhou, Weinan Zhang, Xinbing Wang, Zhouhan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09509">Text Classification in the Wild: a Large-scale Long-tailed Name Normalization Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world data usually exhibits a long-tailed distribution,with a few frequent labels and a lot of few-shot labels. The study of institution name normalization is a perfect application case showing this phenomenon. There are many institutions worldwide with enormous variations of their names in the publicly available literature. In this work, we first collect a large-scale institution name normalization dataset LoT-insts1, which contains over 25k classes that exhibit a naturally long-tailed distribution. In order to isolate the few-shot and zero-shot learning scenarios from the massive many-shot classes, we construct our test set from four different subsets: many-, medium-, and few-shot sets, as well as a zero-shot open set. We also replicate several important baseline methods on our data, covering a wide range from search-based methods to neural network methods that use the pretrained BERT model. Further, we propose our specially pretrained, BERT-based model that shows better out-of-distribution generalization on few-shot and zero-shot test sets. Compared to other datasets focusing on the long-tailed phenomenon, our dataset has one order of magnitude more training data than the largest existing long-tailed datasets and is naturally long-tailed rather than manually synthesized. We believe it provides an important and different scenario to study this problem. To our best knowledge, this is the first natural language dataset that focuses on long-tailed and open-set classification problems.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2112.04764.pdf' target='_blank'>https://arxiv.org/pdf/2112.04764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Mohammad-Ali Nikouei Mahani, Nassir Navab, Benjamin Busam, Federico Tombari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.04764">3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As 3D object detection on point clouds relies on the geometrical relationships between the points, non-standard object shapes can hinder a method's detection capability. However, in safety-critical settings, robustness to out-of-domain and long-tail samples is fundamental to circumvent dangerous issues, such as the misdetection of damaged or rare cars. In this work, we substantially improve the generalization of 3D object detectors to out-of-domain data by deforming point clouds during training. We achieve this with 3D-VField: a novel data augmentation method that plausibly deforms objects via vector fields learned in an adversarial fashion. Our approach constrains 3D points to slide along their sensor view rays while neither adding nor removing any of them. The obtained vectors are transferable, sample-independent and preserve shape and occlusions. Despite training only on a standard dataset, such as KITTI, augmenting with our vector fields significantly improves the generalization to differently shaped objects and scenes. Towards this end, we propose and share CrashD: a synthetic dataset of realistic damaged and rare cars, with a variety of crash scenarios. Extensive experiments on KITTI, Waymo, our CrashD and SUN RGB-D show the generalizability of our techniques to out-of-domain data, different models and sensors, namely LiDAR and ToF cameras, for both indoor and outdoor scenes. Our CrashD dataset is available at https://crashd-cars.github.io.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2506.18939.pdf' target='_blank'>https://arxiv.org/pdf/2506.18939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui An, Yifeng Zhang, Ziran Liang, Wenqi Fan, Yuxuan Liang, Xuequn Shang, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18939">Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training urban spatio-temporal foundation models that generalize well across diverse regions and cities is critical for deploying urban services in unseen or data-scarce regions. Recent studies have typically focused on fusing cross-domain spatio-temporal data to train unified Transformer-based models. However, these models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment. Inspired by the efficiency of Mamba, a state space model with linear time complexity, we explore its potential for efficient urban spatio-temporal prediction. However, directly applying Mamba as a spatio-temporal backbone leads to negative transfer and severe performance degradation. This is primarily due to spatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden state updates, which limit cross-domain generalization. To overcome these challenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for efficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear complexity advantage while significantly enhancing its adaptability to heterogeneous domains. Specifically, we introduce two core innovations: (1) a domain-adaptive state space model that partitions the latent representation space into a shared subspace for learning cross-domain commonalities and independent, domain-specific subspaces for capturing intra-domain discriminative features; (2) three distinct Domain Adapters, which serve as domain-aware proxies to bridge disparate domain distributions and facilitate the alignment of cross-domain commonalities. Extensive experiments demonstrate the generalization and efficiency of Damba-ST. It achieves state-of-the-art performance on prediction tasks and demonstrates strong zero-shot generalization, enabling seamless deployment in new urban environments without extensive retraining or fine-tuning.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2501.17384.pdf' target='_blank'>https://arxiv.org/pdf/2501.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengpeng Xie, Jiahang Cao, Yulong Zhang, Qiang Zhang, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17384">A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent's policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2509.21261.pdf' target='_blank'>https://arxiv.org/pdf/2509.21261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng-Qi Cui, Jinyang Huang, Anyang Tong, Ziyu Jia, Jie Zhang, Zhi Liu, Dan Guo, Jianwei Lu, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21261">Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-action Recognition is vital for psychological assessment and human-computer interaction. However, existing methods often fail in real-world scenarios because inter-person variability causes the same action to manifest differently, hindering robust generalization. To address this, we propose the Person Independence Universal Micro-action Recognition Framework, which integrates Distributionally Robust Optimization principles to learn person-agnostic representations. Our framework contains two plug-and-play components operating at the feature and loss levels. At the feature level, the Temporal-Frequency Alignment Module normalizes person-specific motion characteristics with a dual-branch design: the temporal branch applies Wasserstein-regularized alignment to stabilize dynamic trajectories, while the frequency branch introduces variance-guided perturbations to enhance robustness against person-specific spectral differences. A consistency-driven fusion mechanism integrates both branches. At the loss level, the Group-Invariant Regularized Loss partitions samples into pseudo-groups to simulate unseen person-specific distributions. By up-weighting boundary cases and regularizing subgroup variance, it forces the model to generalize beyond easy or frequent samples, thus enhancing robustness to difficult variations. Experiments on the large-scale MA-52 dataset demonstrate that our framework outperforms existing methods in both accuracy and robustness, achieving stable generalization under fine-grained conditions.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2509.21261.pdf' target='_blank'>https://arxiv.org/pdf/2509.21261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng-Qi Cui, Jinyang Huang, Anyang Tong, Ziyu Jia, Jie Zhang, Zhi Liu, Dan Guo, Jianwei Lu, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21261">Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-action Recognition is vital for psychological assessment and human-computer interaction. However, existing methods often fail in real-world scenarios because inter-person variability causes the same action to manifest differently, hindering robust generalization. To address this, we propose the Person Independence Universal Micro-action Recognition Framework, which integrates Distributionally Robust Optimization principles to learn person-agnostic representations. Our framework contains two plug-and-play components operating at the feature and loss levels. At the feature level, the Temporal-Frequency Alignment Module normalizes person-specific motion characteristics with a dual-branch design: the temporal branch applies Wasserstein-regularized alignment to stabilize dynamic trajectories, while the frequency branch introduces variance-guided perturbations to enhance robustness against person-specific spectral differences. A consistency-driven fusion mechanism integrates both branches. At the loss level, the Group-Invariant Regularized Loss partitions samples into pseudo-groups to simulate unseen person-specific distributions. By up-weighting boundary cases and regularizing subgroup variance, it forces the model to generalize beyond easy or frequent samples, thus enhancing robustness to difficult variations. Experiments on the large-scale MA-52 dataset demonstrate that our framework outperforms existing methods in both accuracy and robustness, achieving stable generalization under fine-grained conditions.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2409.12370.pdf' target='_blank'>https://arxiv.org/pdf/2409.12370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Yifan Peng, Yichen Lu, Xuankai Chang, Ruihua Song, Shinji Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12370">Robust Audiovisual Speech Recognition Models with Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual signals can enhance audiovisual speech recognition accuracy by providing additional contextual information. Given the complexity of visual signals, an audiovisual speech recognition model requires robust generalization capabilities across diverse video scenarios, presenting a significant challenge. In this paper, we introduce EVA, leveraging the mixture-of-Experts for audioVisual ASR to perform robust speech recognition for ``in-the-wild'' videos. Specifically, we first encode visual information into visual tokens sequence and map them into speech space by a lightweight projection. Then, we build EVA upon a robust pretrained speech recognition model, ensuring its generalization ability. Moreover, to incorporate visual information effectively, we inject visual information into the ASR model through a mixture-of-experts module. Experiments show our model achieves state-of-the-art results on three benchmarks, which demonstrates the generalization ability of EVA across diverse video domains.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2309.01437.pdf' target='_blank'>https://arxiv.org/pdf/2309.01437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Zhu, Changhe Song, Zhiyong Wu, Helen Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01437">SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, excellent progress has been made in speech recognition. However, pure data-driven approaches have struggled to solve the problem in domain-mismatch and long-tailed data. Considering that knowledge-driven approaches can help data-driven approaches alleviate their flaws, we introduce sememe-based semantic knowledge information to speech recognition (SememeASR). Sememe, according to the linguistic definition, is the minimum semantic unit in a language and is able to represent the implicit semantic information behind each word very well. Our experiments show that the introduction of sememe information can improve the effectiveness of speech recognition. In addition, our further experiments show that sememe knowledge can improve the model's recognition of long-tailed data and enhance the model's domain generalization ability.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2504.21414.pdf' target='_blank'>https://arxiv.org/pdf/2504.21414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Fan, Kaiqi Liu, Nian Liu, Hisham Cholakkal, Rao Muhammad Anwer, Wenbin Li, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21414">Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2501.00895.pdf' target='_blank'>https://arxiv.org/pdf/2501.00895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyang Liu, Keyan Chen, Rui Zhao, Zhengxia Zou, Zhenwei Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00895">Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative foundation models have advanced large-scale text-driven natural image generation, becoming a prominent research trend across various vertical domains. However, in the remote sensing field, there is still a lack of research on large-scale text-to-image (text2image) generation technology. Existing remote sensing image-text datasets are small in scale and confined to specific geographic areas and scene types. Besides, existing text2image methods have struggled to achieve global-scale, multi-resolution controllable, and unbounded image generation. To address these challenges, this paper presents two key contributions: the Git-10M dataset and the Text2Earth foundation model. Git-10M is a global-scale image-text dataset comprising 10.5 million image-text pairs, 5 times larger than the previous largest one. The dataset covers a wide range of geographic scenes and contains resolution information, significantly surpassing existing datasets in both size and diversity. Building on Git-10M, we propose Text2Earth, a 1.3 billion parameter generative foundation model based on the diffusion framework to model global-scale remote sensing scenes. Text2Earth integrates a resolution guidance mechanism, enabling users to specify image resolutions. A dynamic condition adaptation strategy is proposed for training and inference to improve image quality. Text2Earth excels in zero-shot text2image generation and demonstrates robust generalization and flexibility across multiple tasks, including unbounded scene construction, image editing, and cross-modal image generation. This robust capability surpasses previous models restricted to the basic fixed size and limited scene types. On the previous benchmark dataset, Text2Earth outperforms previous models with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA metric.Our project page is https://chen-yang-liu.github.io/Text2Earth
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2411.17772.pdf' target='_blank'>https://arxiv.org/pdf/2411.17772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Liu, Xiaomei Zhang, Zhiyuan Ma, Xiangyu Zhu, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17772">MVBoost: Boost 3D Reconstruction with Multi-View Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D object reconstruction have been remarkable, yet most current 3D models rely heavily on existing 3D datasets. The scarcity of diverse 3D datasets results in limited generalization capabilities of 3D reconstruction models. In this paper, we propose a novel framework for boosting 3D reconstruction with multi-view refinement (MVBoost) by generating pseudo-GT data. The key of MVBoost is combining the advantages of the high accuracy of the multi-view generation model and the consistency of the 3D reconstruction model to create a reliable data source. Specifically, given a single-view input image, we employ a multi-view diffusion model to generate multiple views, followed by a large 3D reconstruction model to produce consistent 3D data. MVBoost then adaptively refines these multi-view images, rendered from the consistent 3D data, to build a large-scale multi-view dataset for training a feed-forward 3D reconstruction model. Additionally, the input view optimization is designed to optimize the corresponding viewpoints based on the user's input image, ensuring that the most important viewpoint is accurately tailored to the user's needs. Extensive evaluations demonstrate that our method achieves superior reconstruction results and robust generalization compared to prior works.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2410.04587.pdf' target='_blank'>https://arxiv.org/pdf/2410.04587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04587">Hammer: Robust Function-Calling for On-Device Language Models via Function Masking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function calling capabilities. This paper identifies a critical gap in existing function calling models, where performance varies significantly across benchmarks, often due to being misled by specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models' sensitivity to irrelevant functions and incorporates function masking techniques to minimize misleading. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving sota results. Our open source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function calling performance.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2510.06955.pdf' target='_blank'>https://arxiv.org/pdf/2510.06955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06955">High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensembling fine-tuned models initialized from powerful pre-trained weights is a common strategy to improve robustness under distribution shifts, but it comes with substantial computational costs due to the need to train and store multiple models. Dropout offers a lightweight alternative by simulating ensembles through random neuron deactivation; however, when applied to pre-trained models, it tends to over-regularize and disrupt critical representations necessary for generalization. In this work, we investigate Mixout, a stochastic regularization technique that provides an alternative to Dropout for domain generalization. Rather than deactivating neurons, Mixout mitigates overfitting by probabilistically swapping a subset of fine-tuned weights with their pre-trained counterparts during training, thereby maintaining a balance between adaptation and retention of prior knowledge. Our study reveals that achieving strong performance with Mixout on domain generalization benchmarks requires a notably high masking probability of 0.9 for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it yields two key advantages for domain generalization: (1) higher masking rates more strongly penalize deviations from the pre-trained parameters, promoting better generalization to unseen domains; and (2) high-rate masking substantially reduces computational overhead, cutting gradient computation by up to 45% and gradient memory usage by up to 90%. Experiments across five domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, using ResNet and ViT architectures, show that our approach, High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based methods while significantly reducing training costs.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2507.17001.pdf' target='_blank'>https://arxiv.org/pdf/2507.17001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Li, Guangyi Chen, Yunlong Deng, Zijian Li, Zeyu Tang, Anpeng Wu, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17001">Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing methods for adapting models to out-of-distribution (OOD) domains rely on invariant representation learning to eliminate the influence of biased features. However, should bias always be eliminated -- and if not, when should it be retained, and how can it be leveraged? To address these questions, we first present a theoretical analysis that explores the conditions under which biased features can be identified and effectively utilized. Building on this theoretical foundation, we introduce a novel framework that strategically leverages bias to complement invariant representations during inference. The framework comprises two key components that leverage bias in both direct and indirect ways: (1) using invariance as guidance to extract predictive ingredients from bias, and (2) exploiting identified bias to estimate the environmental condition and then use it to explore appropriate bias-aware predictors to alleviate environment gaps. We validate our approach through experiments on both synthetic datasets and standard domain generalization benchmarks. Results consistently demonstrate that our method outperforms existing approaches, underscoring its robustness and adaptability.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2505.06575.pdf' target='_blank'>https://arxiv.org/pdf/2505.06575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengfeng Wang, Wei Zhai, Yuhang Yang, Yang Cao, Zhengjun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06575">GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the geometry level of human-scene contact aims to ground specific contact surface points at 3D human geometries, which provides a spatial prior and bridges the interaction between human and scene, supporting applications such as human behavior analysis, embodied AI, and AR/VR. To complete the task, existing approaches predominantly rely on parametric human models (e.g., SMPL), which establish correspondences between images and contact regions through fixed SMPL vertex sequences. This actually completes the mapping from image features to an ordered sequence. However, this approach lacks consideration of geometry, limiting its generalizability in distinct human geometries. In this paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates a point cloud encoder-decoder architecture along with a hierarchical feature extraction and fusion module, enabling the effective integration of 3D human geometric structures with 2D interaction semantics derived from images. Guided by visual cues, GRACE establishes an implicit mapping from geometric features to the vertex space of the 3D human mesh, thereby achieving accurate modeling of contact regions. This design ensures high prediction accuracy and endows the framework with strong generalization capability across diverse human geometries. Extensive experiments on multiple benchmark datasets demonstrate that GRACE achieves state-of-the-art performance in contact estimation, with additional results further validating its robust generalization to unstructured human point clouds.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2504.11218.pdf' target='_blank'>https://arxiv.org/pdf/2504.11218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11218">3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2504.09439.pdf' target='_blank'>https://arxiv.org/pdf/2504.09439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Xu, Jingjing Chen, Yang Jiao, Jiacheng Zhang, Zhiyu Tan, Hao Li, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09439">Identity-Aware Vision-Language Model for Explainable Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative artificial intelligence have enabled the creation of highly realistic image forgeries, raising significant concerns about digital media authenticity. While existing detection methods demonstrate promising results on benchmark datasets, they face critical limitations in real-world applications. First, existing detectors typically fail to detect semantic inconsistencies with the person's identity, such as implausible behaviors or incompatible environmental contexts in given images. Second, these methods rely heavily on low-level visual cues, making them effective for known forgeries but less reliable against new or unseen manipulation techniques. To address these challenges, we present a novel personalized vision-language model (VLM) that integrates low-level visual artifact analysis and high-level semantic inconsistency detection. Unlike previous VLM-based methods, our approach avoids resource-intensive supervised fine-tuning that often struggles to preserve distinct identity characteristics. Instead, we employ a lightweight method that dynamically encodes identity-specific information into specialized identifier tokens. This design enables the model to learn distinct identity characteristics while maintaining robust generalization capabilities. We further enhance detection capabilities through a lightweight detection adapter that extracts fine-grained information from shallow features of the vision encoder, preserving critical low-level evidence. Comprehensive experiments demonstrate that our approach achieves 94.25% accuracy and 94.08% F1 score, outperforming both traditional forgery detectors and general VLMs while requiring only 10 extra tokens.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2411.02920.pdf' target='_blank'>https://arxiv.org/pdf/2411.02920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengkun Jiao, Na Zhao, Jingjing Chen, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02920">Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts. The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition. In this paper, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes. Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples. Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information. Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers. In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization. Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2407.07544.pdf' target='_blank'>https://arxiv.org/pdf/2407.07544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An Zhang, Han Wang, Xiang Wang, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07544">Disentangling Masked Autoencoders for Unsupervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG), designed to enhance out-of-distribution (OOD) generalization, is all about learning invariance against domain shifts utilizing sufficient supervision signals. Yet, the scarcity of such labeled data has led to the rise of unsupervised domain generalization (UDG) - a more important yet challenging task in that models are trained across diverse domains in an unsupervised manner and eventually tested on unseen domains. UDG is fast gaining attention but is still far from well-studied. To close the research gap, we propose a novel learning framework designed for UDG, termed the Disentangled Masked Auto Encoder (DisMAE), aiming to discover the disentangled representations that faithfully reveal the intrinsic features and superficial variations without access to the class label. At its core is the distillation of domain-invariant semantic features, which cannot be distinguished by domain classifier, while filtering out the domain-specific variations (for example, color schemes and texture patterns) that are unstable and redundant. Notably, DisMAE co-trains the asymmetric dual-branch architecture with semantic and lightweight variation encoders, offering dynamic data manipulation and representation level augmentation capabilities. Extensive experiments on four benchmark datasets (i.e., DomainNet, PACS, VLCS, Colored MNIST) with both DG and UDG tasks demonstrate that DisMAE can achieve competitive OOD performance compared with the state-of-the-art DG and UDG baselines, which shed light on potential research line in improving the generalization ability with large-scale unlabeled data.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2312.13923.pdf' target='_blank'>https://arxiv.org/pdf/2312.13923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyi Cai, Ye Shi, Wei Huang, Jingya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13923">Fed-CO2: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) has emerged as a promising distributed learning paradigm that enables multiple clients to learn a global model collaboratively without sharing their private data. However, the effectiveness of FL is highly dependent on the quality of the data that is being used for training. In particular, data heterogeneity issues, such as label distribution skew and feature skew, can significantly impact the performance of FL. Previous studies in FL have primarily focused on addressing label distribution skew data heterogeneity, while only a few recent works have made initial progress in tackling feature skew issues. Notably, these two forms of data heterogeneity have been studied separately and have not been well explored within a unified FL framework. To address this gap, we propose Fed-CO$_{2}$, a universal FL framework that handles both label distribution skew and feature skew within a \textbf{C}ooperation mechanism between the \textbf{O}nline and \textbf{O}ffline models. Specifically, the online model learns general knowledge that is shared among all clients, while the offline model is trained locally to learn the specialized knowledge of each individual client. To further enhance model cooperation in the presence of feature shifts, we design an intra-client knowledge transfer mechanism that reinforces mutual learning between the online and offline models, and an inter-client knowledge transfer mechanism to increase the models' domain generalization ability. Extensive experiments show that our Fed-CO$_{2}$ outperforms a wide range of existing personalized federated learning algorithms in terms of handling label distribution skew and feature skew, both individually and collectively. The empirical results are supported by our convergence analyses in a simplified setting.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2310.09002.pdf' target='_blank'>https://arxiv.org/pdf/2310.09002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jixuan Cui, Jun Li, Zhen Mei, Kang Wei, Sha Wei, Ming Ding, Wen Chen, Song Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09002">Federated Meta-Learning for Few-Shot Fault Diagnosis with Representation Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based fault diagnosis (FD) approaches require a large amount of training data, which are difficult to obtain since they are located across different entities. Federated learning (FL) enables multiple clients to collaboratively train a shared model with data privacy guaranteed. However, the domain discrepancy and data scarcity problems among clients deteriorate the performance of the global FL model. To tackle these issues, we propose a novel framework called representation encoding-based federated meta-learning (REFML) for few-shot FD. First, a novel training strategy based on representation encoding and meta-learning is developed. It harnesses the inherent heterogeneity among training clients, effectively transforming it into an advantage for out-of-distribution generalization on unseen working conditions or equipment types. Additionally, an adaptive interpolation method that calculates the optimal combination of local and global models as the initialization of local training is proposed. This helps to further utilize local information to mitigate the negative effects of domain discrepancy. As a result, high diagnostic accuracy can be achieved on unseen working conditions or equipment types with limited training data. Compared with the state-of-the-art methods, such as FedProx, the proposed REFML framework achieves an increase in accuracy by 2.17%-6.50% when tested on unseen working conditions of the same equipment type and 13.44%-18.33% when tested on totally unseen equipment types, respectively.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2204.12037.pdf' target='_blank'>https://arxiv.org/pdf/2204.12037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Yushen Wei, Hong Yan, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.12037">Causal Reasoning Meets Visual Representation Learning: A Prospective Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual representation learning is ubiquitous in various real-world applications, including visual comprehension, video understanding, multi-modal analysis, human-computer interaction, and urban computing. Due to the emergence of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal data in big data era, the lack of interpretability, robustness, and out-of-distribution generalization are becoming the challenges of the existing visual models. The majority of the existing methods tend to fit the original data/variable distributions and ignore the essential causal relations behind the multi-modal knowledge, which lacks unified guidance and analysis about why modern visual representation learning methods easily collapse into data bias and have limited generalization and cognitive abilities. Inspired by the strong inference ability of human-level agents, recent years have therefore witnessed great effort in developing causal reasoning paradigms to realize robust representation and model learning with good cognitive ability. In this paper, we conduct a comprehensive review of existing causal reasoning methods for visual representation learning, covering fundamental theories, models, and datasets. The limitations of current methods and datasets are also discussed. Moreover, we propose some prospective challenges, opportunities, and future research directions for benchmarking causal reasoning algorithms in visual representation learning. This paper aims to provide a comprehensive overview of this emerging field, attract attention, encourage discussions, bring to the forefront the urgency of developing novel causal reasoning methods, publicly available benchmarks, and consensus-building standards for reliable visual representation learning and related real-world applications more efficiently.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2506.17137.pdf' target='_blank'>https://arxiv.org/pdf/2506.17137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuonan Liang, Dongnan Liu, Jianan Fan, Yaxuan Song, Qiang Qu, Yu Yao, Peng Fu, Weidong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17137">On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object counting models suffer when deployed across domains with differing density variety, since density shifts are inherently task-relevant and violate standard domain adaptation assumptions. To address this, we propose a theoretical framework of conditional feature alignment. We first formalize the notion of conditional divergence by partitioning each domain into subsets (e.g., object vs. background) and measuring divergences per condition. We then derive a joint error bound showing that, under discrete label spaces treated as condition sets, aligning distributions conditionally leads to tighter bounds on the combined source-target decision error than unconditional alignment. These insights motivate a general conditional adaptation principle: by preserving task-relevant variations while filtering out nuisance shifts, one can achieve superior cross-domain generalization for counting. We provide both defining conditional divergence then proving its benefit in lowering joint error and a practical adaptation strategy that preserves task-relevant information in unsupervised domain-adaptive counting. We demonstrate the effectiveness of our approach through extensive experiments on multiple counting datasets with varying density distributions. The results show that our method outperforms existing unsupervised domain adaptation methods, empirically validating the theoretical insights on conditional feature alignment.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2505.02152.pdf' target='_blank'>https://arxiv.org/pdf/2505.02152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02152">Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown great promise for generalist robotic manipulation in the physical world. However, existing models are restricted to robot observations and text-only instructions, lacking the flexibility of interleaved multimodal instructions enabled by recent advances in foundation models in the digital world. In this paper, we present Interleave-VLA, the first framework capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world. It offers a flexible, model-agnostic paradigm that extends state-of-the-art VLA models with minimal modifications and strong zero-shot generalization. A key challenge in realizing Interleave-VLA is the absence of large-scale interleaved embodied datasets. To bridge this gap, we develop an automatic pipeline that converts text-only instructions from real-world datasets in Open X-Embodiment into interleaved image-text instructions, resulting in the first large-scale real-world interleaved embodied dataset with 210k episodes. Through comprehensive evaluation on simulation benchmarks and real-robot experiments, we demonstrate that Interleave-VLA offers significant benefits: 1) it improves out-of-domain generalization to unseen objects by 2-3x compared to state-of-the-art baselines, 2) supports flexible task interfaces, and 3) handles diverse user-provided image instructions in a zero-shot manner, such as hand-drawn sketches. We further analyze the factors behind Interleave-VLA's strong zero-shot performance, showing that the interleaved paradigm effectively leverages heterogeneous datasets and diverse instruction images, including those from the Internet, which demonstrates strong potential for scaling up. Our model and dataset will be open-sourced.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2505.02152.pdf' target='_blank'>https://arxiv.org/pdf/2505.02152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02152">Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of foundation models paves the way for generalist robot policies in the physical world. Existing methods relying on text-only instructions often struggle to generalize to unseen scenarios. We argue that interleaved image-text inputs offer richer and less biased context and enable robots to better handle unseen tasks with more versatile human-robot interaction. Building on this insight, Interleave-VLA, the first robot learning paradigm capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world, is introduced. It offers a natural, flexible, and model-agnostic paradigm that extends state-of-the-art vision-language-action (VLA) models with minimal modifications while achieving strong zero-shot generalization. Interleave-VLA also includes an automatic pipeline that converts text instructions from Open X-Embodiment into interleaved image-text instructions, resulting in a large-scale real-world interleaved embodied dataset with 210k episodes. Comprehensive evaluation in simulation and the real world shows that Interleave-VLA offers two major benefits: (1) improves out-of-domain generalization to unseen objects by 2x compared to text input baselines, (2) supports flexible task interfaces and diverse instructions in a zero-shot manner, such as hand-drawn sketches. We attribute Interleave-VLA's strong zero-shot capability to the use of instruction images, which effectively mitigate hallucinations, and the inclusion of heterogeneous multimodal datasets, enriched with Internet-sourced images, offering potential for scalability. More information is available at https://interleave-vla.github.io/Interleave-VLA-Anonymous/
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2503.21847.pdf' target='_blank'>https://arxiv.org/pdf/2503.21847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Xie, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21847">ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ReCoM, an efficient framework for generating high-fidelity and generalizable human body motions synchronized with speech. The core innovation lies in the Recurrent Embedded Transformer (RET), which integrates Dynamic Embedding Regularization (DER) into a Vision Transformer (ViT) core architecture to explicitly model co-speech motion dynamics. This architecture enables joint spatial-temporal dependency modeling, thereby enhancing gesture naturalness and fidelity through coherent motion synthesis. To enhance model robustness, we incorporate the proposed DER strategy, which equips the model with dual capabilities of noise resistance and cross-domain generalization, thereby improving the naturalness and fluency of zero-shot motion generation for unseen speech inputs. To mitigate inherent limitations of autoregressive inference, including error accumulation and limited self-correction, we propose an iterative reconstruction inference (IRI) strategy. IRI refines motion sequences via cyclic pose reconstruction, driven by two key components: (1) classifier-free guidance improves distribution alignment between generated and real gestures without auxiliary supervision, and (2) a temporal smoothing process eliminates abrupt inter-frame transitions while ensuring kinematic continuity. Extensive experiments on benchmark datasets validate ReCoM's effectiveness, achieving state-of-the-art performance across metrics. Notably, it reduces the FrÃ©chet Gesture Distance (FGD) from 18.70 to 2.48, demonstrating an 86.7% improvement in motion realism. Our project page is https://yong-xie-xy.github.io/ReCoM/.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2502.11617.pdf' target='_blank'>https://arxiv.org/pdf/2502.11617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarthak Mittal, Yoshua Bengio, Nikolay Malkin, Guillaume Lajoie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11617">In-Context Parametric Inference: Point or Distribution Estimators?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian and frequentist inference are two fundamental paradigms in statistical estimation. Bayesian methods treat hypotheses as random variables, incorporating priors and updating beliefs via Bayes' theorem, whereas frequentist methods assume fixed but unknown hypotheses, relying on estimators like maximum likelihood. While extensive research has compared these approaches, the frequentist paradigm of obtaining point estimates has become predominant in deep learning, as Bayesian inference is challenging due to the computational complexity and the approximation gap of posterior estimation methods. However, a good understanding of trade-offs between the two approaches is lacking in the regime of amortized estimators, where in-context learners are trained to estimate either point values via maximum likelihood or maximum a posteriori estimation, or full posteriors using normalizing flows, score-based diffusion samplers, or diagonal Gaussian approximations, conditioned on observations. To help resolve this, we conduct a rigorous comparative analysis spanning diverse problem settings, from linear models to shallow neural networks, with a robust evaluation framework assessing both in-distribution and out-of-distribution generalization on tractable tasks. Our experiments indicate that amortized point estimators generally outperform posterior inference, though the latter remain competitive in some low-dimensional problems, and we further discuss why this might be the case.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2501.09783.pdf' target='_blank'>https://arxiv.org/pdf/2501.09783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiliang Tang, Jia-Hui Pan, Yun-Hui Liu, Masayoshi Tomizuka, Li Erran Li, Chi-Wing Fu, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09783">GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GeoManip, a framework to enable generalist robots to leverage essential conditions derived from object and part relationships, as geometric constraints, for robot manipulation. For example, cutting the carrot requires adhering to a geometric constraint: the blade of the knife should be perpendicular to the carrot's direction. By interpreting these constraints through symbolic language representations and translating them into low-level actions, GeoManip bridges the gap between natural language and robotic execution, enabling greater generalizability across diverse even unseen tasks, objects, and scenarios. Unlike vision-language-action models that require extensive training, operates training-free by utilizing large foundational models: a constraint generation module that predicts stage-specific geometric constraints and a geometry parser that identifies object parts involved in these constraints. A solver then optimizes trajectories to satisfy inferred constraints from task descriptions and the scene. Furthermore, GeoManip learns in-context and provides five appealing human-robot interaction features: on-the-fly policy adaptation, learning from human demonstrations, learning from failure cases, long-horizon action planning, and efficient data collection for imitation learning. Extensive evaluations on both simulations and real-world scenarios demonstrate GeoManip's state-of-the-art performance, with superior out-of-distribution generalization while avoiding costly model training.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2403.05018.pdf' target='_blank'>https://arxiv.org/pdf/2403.05018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichong Meng, Changdi Yang, Jun Liu, Hao Tang, Pu Zhao, Yanzhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05018">InstructGIE: Towards Generalizable Image Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in image editing have been driven by the development of denoising diffusion models, marking a significant leap forward in this field. Despite these advances, the generalization capabilities of recent image editing approaches remain constrained. In response to this challenge, our study introduces a novel image editing framework with enhanced generalization robustness by boosting in-context learning capability and unifying language instruction. This framework incorporates a module specifically optimized for image editing tasks, leveraging the VMamba Block and an editing-shift matching strategy to augment in-context learning. Furthermore, we unveil a selective area-matching technique specifically engineered to address and rectify corrupted details in generated images, such as human facial features, to further improve the quality. Another key innovation of our approach is the integration of a language unification technique, which aligns language embeddings with editing semantics to elevate the quality of image editing. Moreover, we compile the first dataset for image editing with visual prompts and editing instructions that could be used to enhance in-context capability. Trained on this dataset, our methodology not only achieves superior synthesis quality for trained tasks, but also demonstrates robust generalization capability across unseen vision tasks through tailored prompts.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2306.14275.pdf' target='_blank'>https://arxiv.org/pdf/2306.14275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianjin Huang, Shiwei Liu, Tianlong Chen, Meng Fang, Li Shen, Vlaod Menkovski, Lu Yin, Yulong Pei, Mykola Pechenizkiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14275">Enhancing Adversarial Training via Reweighting Optimization Trajectory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with the existing adversarial training methods and consistently overcomes the robust overfitting issue, resulting in better adversarial robustness. For example, WOT boosts the robust accuracy of AT-PGD under AA-$L_{\infty}$ attack by 1.53\% $\sim$ 6.11\% and meanwhile increases the clean accuracy by 0.55\%$\sim$5.47\% across SVHN, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2502.07968.pdf' target='_blank'>https://arxiv.org/pdf/2502.07968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Wang, Zhen Tan, Yaochen Zhu, Chuxu Zhang, Jundong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07968">Generative Risk Minimization for Out-of-Distribution Generalization on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2501.04102.pdf' target='_blank'>https://arxiv.org/pdf/2501.04102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Wang, Xiaodong Yang, Rashidul Islam, Huiyuan Chen, Minghua Xu, Jundong Li, Yiwei Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04102">Enhancing Distribution and Label Consistency for Graph Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To deal with distribution shifts in graph data, various graph out-of-distribution (OOD) generalization techniques have been recently proposed. These methods often employ a two-step strategy that first creates augmented environments and subsequently identifies invariant subgraphs to improve generalizability. Nevertheless, this approach could be suboptimal from the perspective of consistency. First, the process of augmenting environments by altering the graphs while preserving labels may lead to graphs that are not realistic or meaningfully related to the origin distribution, thus lacking distribution consistency. Second, the extracted subgraphs are obtained from directly modifying graphs, and may not necessarily maintain a consistent predictive relationship with their labels, thereby impacting label consistency. In response to these challenges, we introduce an innovative approach that aims to enhance these two types of consistency for graph OOD generalization. We propose a modifier to obtain both augmented and invariant graphs in a unified manner. With the augmented graphs, we enrich the training data without compromising the integrity of label-graph relationships. The label consistency enhancement in our framework further preserves the supervision information in the invariant graph. We conduct extensive experiments on real-world datasets to demonstrate the superiority of our framework over other state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2404.12090.pdf' target='_blank'>https://arxiv.org/pdf/2404.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyuan Jiang, Ziyue Li, Hua Wei, Xuantang Xiong, Jingqing Ruan, Jiaming Lu, Hangyu Mao, Rui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12090">X-Light: Cross-City Traffic Signal Control Using Transformer on Transformer as Meta Multi-Agent Reinforcement Learner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effectiveness of traffic light control has been significantly improved by current reinforcement learning-based approaches via better cooperation among multiple traffic lights. However, a persisting issue remains: how to obtain a multi-agent traffic signal control algorithm with remarkable transferability across diverse cities? In this paper, we propose a Transformer on Transformer (TonT) model for cross-city meta multi-agent traffic signal control, named as X-Light: We input the full Markov Decision Process trajectories, and the Lower Transformer aggregates the states, actions, rewards among the target intersection and its neighbors within a city, and the Upper Transformer learns the general decision trajectories across different cities. This dual-level approach bolsters the model's robust generalization and transferability. Notably, when directly transferring to unseen scenarios, ours surpasses all baseline methods with +7.91% on average, and even +16.3% in some cases, yielding the best results.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2310.10639.pdf' target='_blank'>https://arxiv.org/pdf/2310.10639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10639">Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot's own training data. We propose SuSIE, a method that leverages an image-editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller can accomplish. Specifically, we finetune InstructPix2Pix on video data, consisting of both human videos and robot rollouts, such that it outputs hypothetical future "subgoal" observations given the robot's current observation and a language command. We also use the robot data to train a low-level goal-conditioned policy to act as the aforementioned low-level controller. We find that the high-level subgoal predictions can utilize Internet-scale pretraining and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization and precision than conventional language-conditioned policies. We achieve state-of-the-art results on the CALVIN benchmark, and also demonstrate robust generalization on real-world manipulation tasks, beating strong baselines that have access to privileged information or that utilize orders of magnitude more compute and training data. The project website can be found at http://rail-berkeley.github.io/susie .
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2302.00194.pdf' target='_blank'>https://arxiv.org/pdf/2302.00194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>YiFan Zhang, Xue Wang, Jian Liang, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00194">Free Lunch for Domain Adversarial Training: Environment Label Smoothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A fundamental challenge for machine learning models is how to generalize learned models for out-of-distribution (OOD) data. Among various approaches, exploiting invariant features by Domain Adversarial Training (DAT) received widespread attention. Despite its success, we observe training instability from DAT, mostly due to over-confident domain discriminator and environment label noise. To address this issue, we proposed Environment Label Smoothing (ELS), which encourages the discriminator to output soft probability, which thus reduces the confidence of the discriminator and alleviates the impact of noisy environment labels. We demonstrate, both experimentally and theoretically, that ELS can improve training stability, local convergence, and robustness to noisy environment labels. By incorporating ELS with DAT methods, we are able to yield state-of-art results on a wide range of domain generalization/adaptation tasks, particularly when the environment labels are highly noisy.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2212.10390.pdf' target='_blank'>https://arxiv.org/pdf/2212.10390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Fei, Siyuan Huang, Jiakang Yuan, Botian Shi, Bo Zhang, Weidong Yang, Min Dou, Yikang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.10390">UniDA3D: Unified Domain Adaptive 3D Semantic Segmentation Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art 3D semantic segmentation models are trained on off-the-shelf public benchmarks, but they will inevitably face the challenge of recognition accuracy drop when these well-trained models are deployed to a new domain. In this paper, we introduce a Unified Domain Adaptive 3D semantic segmentation pipeline (UniDA3D) to enhance the weak generalization ability, and bridge the point distribution gap between domains. Different from previous studies that only focus on a single adaptation task, UniDA3D can tackle several adaptation tasks in 3D segmentation field, by designing a unified source-and-target active sampling strategy, which selects a maximally-informative subset from both source and target domains for effective model adaptation. Besides, benefiting from the rise of multi-modal 2D-3D datasets, UniDA3D investigates the possibility of achieving a multi-modal sampling strategy, by developing a cross-modality feature interaction module that can extract a representative pair of image and point features to achieve a bi-directional image-point feature interaction for safe model adaptation. Experimentally, UniDA3D is verified to be effective in many adaptation tasks including: 1) unsupervised domain adaptation, 2) unsupervised few-shot domain adaptation; 3) active domain adaptation. Their results demonstrate that, by easily coupling UniDA3D with off-the-shelf 3D segmentation baselines, domain generalization ability of these baselines can be enhanced.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2210.05178.pdf' target='_blank'>https://arxiv.org/pdf/2210.05178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aviral Kumar, Anikait Singh, Frederik Ebert, Mitsuhiko Nakamoto, Yanlai Yang, Chelsea Finn, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05178">Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Progress in deep learning highlights the tremendous potential of utilizing diverse robotic datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice, we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. To our knowledge, PTR is the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. We also demonstrate that PTR can enable effective autonomous fine-tuning and improvement in a handful of trials, without needing any demonstrations. An accompanying overview video can be found in the supplementary material and at thi URL: https://sites.google.com/view/ptr-final/
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2508.05234.pdf' target='_blank'>https://arxiv.org/pdf/2508.05234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Shangguan, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Ge Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05234">Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The surge in rich multimodal content on social media platforms has greatly advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs) further accelerating progress in this field. Current approaches primarily leverage the knowledge and reasoning capabilities of parameter-heavy (Multimodal) LLMs for sentiment classification, overlooking autonomous multimodal sentiment reasoning generation in resource-constrained environments. Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment Reasoning and Classification task, JMSRC, which simultaneously performs multimodal sentiment reasoning chain generation and sentiment classification only with a lightweight model. We propose a Multimodal Chain-of-Thought Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a "Teacher-Assistant-Student" distillation paradigm to address deployment constraints in resource-limited environments. We first leverage a high-performance Multimodal Large Language Model (MLLM) to generate the initial reasoning dataset and train a medium-sized assistant model with a multi-task learning mechanism. A lightweight student model is jointly trained to perform efficient multimodal sentiment reasoning generation and classification. Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B parameters achieves strong performance on JMSRC, while exhibiting robust generalization and enhanced interpretability.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2506.10966.pdf' target='_blank'>https://arxiv.org/pdf/2506.10966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Gao, Yilun Chen, Shuai Yang, Xinyi Chen, Yang Tian, Hao Li, Haifeng Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10966">GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation in real-world settings remains challenging, especially regarding robust generalization. Existing simulation platforms lack sufficient support for exploring how policies adapt to varied instructions and scenarios. Thus, they lag behind the growing interest in instruction-following foundation models like LLMs, whose adaptability is crucial yet remains underexplored in fair comparisons. To bridge this gap, we introduce GenManip, a realistic tabletop simulation platform tailored for policy generalization studies. It features an automatic pipeline via LLM-driven task-oriented scene graph to synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To systematically assess generalization, we present GenManip-Bench, a benchmark of 200 scenarios refined via human-in-the-loop corrections. We evaluate two policy types: (1) modular manipulation systems integrating foundation models for perception, reasoning, and planning, and (2) end-to-end policies trained through scalable data collection. Results show that while data scaling benefits end-to-end methods, modular systems enhanced with foundation models generalize more effectively across diverse scenarios. We anticipate this platform to facilitate critical insights for advancing policy generalization in realistic conditions. Project Page: https://genmanip.axi404.top/.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2412.10345.pdf' target='_blank'>https://arxiv.org/pdf/2412.10345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal DaumÃ©, Andrey Kolobov, Furong Huang, Jianwei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10345">TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation. In this work, we introduce visual trace prompting, a simple yet effective approach to facilitate VLA models' spatial-temporal awareness for action prediction by encoding state-action trajectories visually. We develop a new TraceVLA model by finetuning OpenVLA on our own collected dataset of 150K robot manipulation trajectories using visual trace prompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and 3.5x on real-robot tasks and exhibiting robust generalization across diverse embodiments and scenarios. To further validate the effectiveness and generality of our method, we present a compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while significantly improving inference efficiency.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2412.02825.pdf' target='_blank'>https://arxiv.org/pdf/2412.02825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Wenhui Zhu, Xuanzhao Dong, Yanxi Chen, Xin Li, Peijie Qiu, Xiwen Chen, Vamsi Krishna Vasa, Yujian Xiong, Oana M. Dumitrascu, Abolfazl Razi, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02825">Many-MobileNet: Multi-Model Augmentation for Robust Retinal Disease Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose Many-MobileNet, an efficient model fusion strategy for retinal disease classification using lightweight CNN architecture. Our method addresses key challenges such as overfitting and limited dataset variability by training multiple models with distinct data augmentation strategies and different model complexities. Through this fusion technique, we achieved robust generalization in data-scarce domains while balancing computational efficiency with feature extraction capabilities.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2508.05008.pdf' target='_blank'>https://arxiv.org/pdf/2508.05008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xusheng Liang, Lihua Zhou, Nianxin Li, Miao Xu, Ziyang Song, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05008">Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable zero-shot capabilities in various computer vision tasks. However, their application to medical imaging remains challenging due to the high variability and complexity of medical data. Specifically, medical images often exhibit significant domain shifts caused by various confounders, including equipment differences, procedure artifacts, and imaging modes, which can lead to poor generalization when models are applied to unseen domains. To address this limitation, we propose Multimodal Causal-Driven Representation Learning (MCDRL), a novel framework that integrates causal inference with the VLM to tackle domain generalization in medical image segmentation. MCDRL is implemented in two steps: first, it leverages CLIP's cross-modal capabilities to identify candidate lesion regions and construct a confounder dictionary through text prompts, specifically designed to represent domain-specific variations; second, it trains a causal intervention network that utilizes this dictionary to identify and eliminate the influence of these domain-specific variations while preserving the anatomical structural information critical for segmentation tasks. Extensive experiments demonstrate that MCDRL consistently outperforms competing methods, yielding superior segmentation accuracy and exhibiting robust generalizability.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2507.23110.pdf' target='_blank'>https://arxiv.org/pdf/2507.23110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Linkai Peng, Wanying Dou, Cuiling Sun, Halil Ertugrul Aktas, Andrea M. Bejar, Elif Keles, Gorkem Durak, Ulas Bagci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23110">Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences whose appearance differs more than the acquisition sites that produce them. Existing domain-generalization benchmarks focus almost on cross-center shifts and overlook this dominant source of variability. Pancreas segmentation remains a major challenge in abdominal imaging: the gland is small, irregularly, surrounded by organs and fat, and often suffers from low T1 contrast. State-of-the-art deep networks that already achieve >90% Dice on the liver or kidneys still miss 20-30% of the pancreas. The organ is also systematically under-represented in public cross-domain benchmarks, despite its clinical importance in early cancer detection, surgery, and diabetes research. To close this gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas segmentation dataset for investigating domain generalization in medical imaging. The dataset comprises 563 MRI scans from six institutions, spanning both venous phase and out-of-phase sequences, enabling study of both cross-center and cross-sequence variations with pixel-accurate pancreas masks created by a double-blind, two-pass protocol. Through comprehensive analysis, we reveal three insights: (i) limited sampling introduces significant variance that may be mistaken for distribution shifts, (ii) cross-center performance correlates with source domain performance for identical sequences, and (iii) cross-sequence shifts require specialized solutions. We also propose a semi-supervised approach that leverages anatomical invariances, significantly outperforming state-of-the-art domain generalization techniques with 61.63% Dice score improvements and 87.00% on two test centers for cross-sequence segmentation. PancreasDG sets a new benchmark for domain generalization in medical imaging. Dataset, code, and models will be available at https://pancreasdg.netlify.app.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2507.09961.pdf' target='_blank'>https://arxiv.org/pdf/2507.09961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lihua Zhou, Mao Ye, Nianxin Li, Shuaifeng Li, Jinlin Wu, Xiatian Zhu, Lei Deng, Hongbin Liu, Jiebo Luo, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09961">Text-Driven Causal Representation Learning for Source-Free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning often struggles when training and test data distributions differ. Traditional domain generalization (DG) tackles this by including data from multiple source domains, which is impractical due to expensive data collection and annotation. Recent vision-language models like CLIP enable source-free domain generalization (SFDG) by using text prompts to simulate visual representations, reducing data demands. However, existing SFDG methods struggle with domain-specific confounders, limiting their generalization capabilities. To address this issue, we propose TDCRL (\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation \textbf{L}earning), the first method to integrate causal inference into the SFDG setting. TDCRL operates in two steps: first, it employs data augmentation to generate style word vectors, combining them with class information to generate text embeddings to simulate visual representations; second, it trains a causal intervention network with a confounder dictionary to extract domain-invariant features. Grounded in causal learning, our approach offers a clear and effective mechanism to achieve robust, domain-invariant features, ensuring robust generalization. Extensive experiments on PACS, VLCS, OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL effectiveness in SFDG.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2506.08772.pdf' target='_blank'>https://arxiv.org/pdf/2506.08772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Song, Kaiyu Li, Xiangyong Cao, Deyu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08772">RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation in remote sensing images is crucial for various applications, yet its performance is heavily reliant on large-scale, high-quality pixel-wise annotations, which are notoriously expensive and time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a promising alternative to mitigate this data dependency. However, existing SSS methods often struggle with the inherent distribution mismatch between limited labeled data and abundant unlabeled data, leading to suboptimal generalization. To alleviate this issue, we attempt to introduce the Vision Foundation Models (VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs possess robust generalization capabilities that can effectively bridge this distribution gap and provide strong semantic priors for SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework that leverages the powerful semantic knowledge embedded in VFMs to guide semi-supervised learning in remote sensing. Specifically, RS-MTDF employs multiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing feature-level distillation to align student features with their robust representations. To further enhance discriminative power, the distilled knowledge is seamlessly fused into the student decoder. Extensive experiments on three challenging remote sensing datasets demonstrate that RS-MTDF consistently achieves state-of-the-art performance. Notably, our method outperforms existing approaches across various label ratios on LoveDA and secures the highest IoU in the majority of semantic categories. These results underscore the efficacy of multi-teacher VFM guidance in significantly enhancing both generalization and semantic understanding for remote sensing segmentation. Ablation studies further validate the contribution of each proposed module.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2505.18531.pdf' target='_blank'>https://arxiv.org/pdf/2505.18531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhou, Jiaming Ji, Boyuan Chen, Jiapeng Sun, Wenqi Chen, Donghai Hong, Sirui Han, Yike Guo, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18531">Generative RLHF-V: Learning Principles from Multi-modal Human Preference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training multi-modal large language models (MLLMs) that align with human intentions is a long-term challenge. Traditional score-only reward models for alignment suffer from low accuracy, weak generalization, and poor interpretability, blocking the progress of alignment methods, e.g., reinforcement learning from human feedback (RLHF). Generative reward models (GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate pair-wise responses, but their pair-wise paradigm makes it hard to generalize to learnable rewards. We introduce Generative RLHF-V, a novel alignment framework that integrates GRMs with multi-modal RLHF. We propose a two-stage pipeline: $\textbf{multi-modal generative reward modeling from RL}$, where RL guides GRMs to actively capture human intention, then predict the correct pair-wise scores; and $\textbf{RL optimization from grouped comparison}$, which enhances multi-modal RL scoring precision by grouped responses comparison. Experimental results demonstrate that, besides out-of-distribution generalization of RM discrimination, our framework improves 4 MLLMs' performance across 7 benchmarks by $18.1\%$, while the baseline RLHF is only $5.3\%$. We further validate that Generative RLHF-V achieves a near-linear improvement with an increasing number of candidate responses. Our code and models can be found at https://generative-rlhf-v.github.io.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2505.07395.pdf' target='_blank'>https://arxiv.org/pdf/2505.07395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07395">ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2504.10957.pdf' target='_blank'>https://arxiv.org/pdf/2504.10957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongkang Li, Yihua Zhang, Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10957">When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2503.19469.pdf' target='_blank'>https://arxiv.org/pdf/2503.19469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fred Philippy, Siwen Guo, Cedric Lothritz, Jacques Klein, TegawendÃ© F. BissyandÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19469">Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. While pretrained language models (PLMs) have shown promise in ZSC, they often rely on large training datasets or external knowledge, limiting their applicability in multilingual and low-resource scenarios. Recent approaches leveraging natural language prompts reduce the dependence on large training datasets but struggle to effectively incorporate available labeled data from related classification tasks, especially when these datasets originate from different languages or distributions. Moreover, existing prompt-based methods typically rely on manually crafted prompts in a specific language, limiting their adaptability and effectiveness in cross-lingual settings. To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts. RoSPrompt is designed for small multilingual PLMs, enabling them to leverage high-resource languages to improve performance in low-resource settings without requiring extensive fine-tuning or high computational costs. We evaluate our approach on multiple multilingual PLMs across datasets covering 106 languages, demonstrating strong cross-lingual transfer performance and robust generalization capabilities over unseen classes.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2503.03480.pdf' target='_blank'>https://arxiv.org/pdf/2503.03480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03480">SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, this exploration yields an 83.58% safety improvement compared to the current state-of-the-art method, while also maintaining task performance (+3.85%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2402.11733.pdf' target='_blank'>https://arxiv.org/pdf/2402.11733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vijaya Raghavan T Ramkumar, Bahram Zonooz, Elahe Arani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11733">The Effectiveness of Random Forgetting for Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called "Forget to Mitigate Overfitting (FOMO)". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2401.14948.pdf' target='_blank'>https://arxiv.org/pdf/2401.14948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shruthi Gowda, Bahram Zonooz, Elahe Arani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14948">Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating "robust overfitting". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2311.17510.pdf' target='_blank'>https://arxiv.org/pdf/2311.17510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiepeng Wang, Hao Pan, Yang Liu, Xin Tong, Taku Komura, Wenping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17510">StructRe: Rewriting for Structured Shape Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Man-made 3D shapes are naturally organized in parts and hierarchies; such structures provide important constraints for shape reconstruction and generation. Modeling shape structures is difficult, because there can be multiple hierarchies for a given shape, causing ambiguity, and across different categories the shape structures are correlated with semantics, limiting generalization. We present StructRe, a structure rewriting system, as a novel approach to structured shape modeling. Given a 3D object represented by points and components, StructRe can rewrite it upward into more concise structures, or downward into more detailed structures; by iterating the rewriting process, hierarchies are obtained. Such a localized rewriting process enables probabilistic modeling of ambiguous structures and robust generalization across object categories. We train StructRe on PartNet data and show its generalization to cross-category and multiple object hierarchies, and test its extension to ShapeNet. We also demonstrate the benefits of probabilistic and generalizable structure modeling for shape reconstruction, generation and editing tasks.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2303.02343.pdf' target='_blank'>https://arxiv.org/pdf/2303.02343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihua Zhang, Pranay Sharma, Parikshit Ram, Mingyi Hong, Kush Varshney, Sijia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02343">What Is Missing in IRM Training and Evaluation? Challenges and Solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invariant risk minimization (IRM) has received increasing attention as a way to acquire environment-agnostic data representations and predictions, and as a principled solution for preventing spurious correlations from being learned and for improving models' out-of-distribution generalization. Yet, recent works have found that the optimality of the originally-proposed IRM optimization (IRM) may be compromised in practice or could be impossible to achieve in some scenarios. Therefore, a series of advanced IRM algorithms have been developed that show practical improvement over IRM. In this work, we revisit these recent IRM advancements, and identify and resolve three practical limitations in IRM training and evaluation. First, we find that the effect of batch size during training has been chronically overlooked in previous studies, leaving room for further improvement. We propose small-batch training and highlight the improvements over a set of large-batch optimization techniques. Second, we find that improper selection of evaluation environments could give a false sense of invariance for IRM. To alleviate this effect, we leverage diversified test-time environments to precisely characterize the invariance of IRM when applied in practice. Third, we revisit (Ahuja et al. (2020))'s proposal to convert IRM into an ensemble game and identify a limitation when a single invariant predictor is desired instead of an ensemble of individual predictors. We propose a new IRM variant to address this limitation based on a novel viewpoint of ensemble IRM games as consensus-constrained bi-level optimization. Lastly, we conduct extensive experiments (covering 7 existing IRM variants and 7 datasets) to justify the practical significance of revisiting IRM training and evaluation in a principled manner.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2508.17784.pdf' target='_blank'>https://arxiv.org/pdf/2508.17784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17784">Proximal Supervised Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised fine-tuning (SFT) of foundation models often leads to poor generalization, where prior capabilities deteriorate after tuning on new tasks or domains. Inspired by trust-region policy optimization (TRPO) and proximal policy optimization (PPO) in reinforcement learning (RL), we propose Proximal SFT (PSFT). This fine-tuning objective incorporates the benefits of trust-region, effectively constraining policy drift during SFT while maintaining competitive tuning. By viewing SFT as a special case of policy gradient methods with constant positive advantages, we derive PSFT that stabilizes optimization and leads to generalization, while leaving room for further optimization in subsequent post-training stages. Experiments across mathematical and human-value domains show that PSFT matches SFT in-domain, outperforms it in out-of-domain generalization, remains stable under prolonged training without causing entropy collapse, and provides a stronger foundation for the subsequent optimization.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2507.11969.pdf' target='_blank'>https://arxiv.org/pdf/2507.11969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaohong Huang, Yuxin Zhang, Jingjing Xie, Fei Chao, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11969">GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in test-time adaptation (TTA) for Vision-Language Models (VLMs) have garnered increasing attention, particularly through the use of multiple augmented views of a single image to boost zero-shot generalization. Unfortunately, existing methods fail to strike a satisfactory balance between performance and efficiency, either due to excessive overhead of tuning text prompts or unstable benefits from handcrafted, training-free visual feature enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias), an efficient and effective TTA paradigm that incorporates two learnable biases during TTA, unfolded as the global bias and spatial bias. Particularly, the global bias captures the global semantic features of a test image by learning consistency across augmented views, while spatial bias learns the semantic coherence between regions in the image's spatial visual representation. It is worth highlighting that these two sets of biases are directly added to the logits outputed by the pretrained VLMs, which circumvent the full backpropagation through VLM that hinders the efficiency of existing TTA methods. This endows GS-Bias with extremely high efficiency while achieving state-of-the-art performance on 15 benchmark datasets. For example, it achieves a 2.23% improvement over TPT in cross-dataset generalization and a 2.72% improvement in domain generalization, while requiring only 6.5% of TPT's memory usage on ImageNet.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2505.18819.pdf' target='_blank'>https://arxiv.org/pdf/2505.18819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guofeng Mei, Bin Ren, Juan Liu, Luigi Riz, Xiaoshui Huang, Xu Zheng, Yongshun Gong, Ming-Hsuan Yang, Nicu Sebe, Fabio Poiesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18819">Self-Supervised and Generalizable Tokenization for CLIP-Based 3D Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models like CLIP can offer a promising foundation for 3D scene understanding when extended with 3D tokenizers. However, standard approaches, such as k-nearest neighbor or radius-based tokenization, struggle with cross-domain generalization due to sensitivity to dataset-specific spatial scales. We present a universal 3D tokenizer designed for scale-invariant representation learning with a frozen CLIP backbone. We show that combining superpoint-based grouping with coordinate scale normalization consistently outperforms conventional methods through extensive experimental analysis. Specifically, we introduce S4Token, a tokenization pipeline that produces semantically-informed tokens regardless of scene scale. Our tokenizer is trained without annotations using masked point modeling and clustering-based objectives, along with cross-modal distillation to align 3D tokens with 2D multi-view image features. For dense prediction tasks, we propose a superpoint-level feature propagation module to recover point-level detail from sparse tokens.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2502.19655.pdf' target='_blank'>https://arxiv.org/pdf/2502.19655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, Hoifung Poon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19655">Med-RLVR: Emerging Medical Reasoning from a 3B base model via reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning from verifiable rewards (RLVR) has recently gained attention for its ability to elicit self-evolved reasoning capabilitie from base language models without explicit reasoning supervisions, as demonstrated by DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical and coding domains, its applicability to other tasks and domains remains unexplored. In this work, we investigate whether medical reasoning can emerge from RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical domain leveraging medical multiple-choice question answering (MCQA) data as verifiable labels. Our results demonstrate that RLVR is not only effective for math and coding but also extends successfully to medical question answering. Notably, Med-RLVR achieves performance comparable to traditional supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization, with an 8-point accuracy gain. Further analysis of training dynamics reveals that, with no explicit reasoning supervision, reasoning emerges from the 3B-parameter base model. These findings underscore the potential of RLVR in domains beyond math and coding, opening new avenues for its application in knowledge-intensive fields such as medicine.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2501.02048.pdf' target='_blank'>https://arxiv.org/pdf/2501.02048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu, Xi Chen, Ser-Nam Lim, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02048">DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary panoptic segmentation has received significant attention due to its applicability in the real world. Despite claims of robust generalization, we find that the advancements of previous works are attributed mainly on trained categories, exposing a lack of generalization to novel classes. In this paper, we explore boosting existing models from a data-centric perspective. We propose DreamMask, which systematically explores how to generate training data in the open-vocabulary setting, and how to train the model with both real and synthetic data. For the first part, we propose an automatic data generation pipeline with off-the-shelf models. We propose crucial designs for vocabulary expansion, layout arrangement, data filtering, etc. Equipped with these techniques, our generated data could significantly outperform the manually collected web data. To train the model with generated data, a synthetic-real alignment loss is designed to bridge the representation gap, bringing noticeable improvements across multiple benchmarks. In general, DreamMask significantly simplifies the collection of large-scale training data, serving as a plug-and-play enhancement for existing methods. For instance, when trained on COCO and tested on ADE20K, the model equipped with DreamMask outperforms the previous state-of-the-art by a substantial margin of 2.1% mIoU.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2412.08906.pdf' target='_blank'>https://arxiv.org/pdf/2412.08906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengchao Chen, Guodong Long, Jing Jiang, Chengqi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08906">Federated Foundation Models on Heterogeneous Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a general-purpose time series foundation models with robust generalization capabilities across diverse applications from scratch is still an open challenge. Efforts are primarily focused on fusing cross-domain time series datasets to extract shared subsequences as tokens for training models on Transformer architecture. However, due to significant statistical heterogeneity across domains, this cross-domain fusing approach doesn't work effectively as the same as fusing texts and images. To tackle this challenge, this paper proposes a novel federated learning approach to address the heterogeneity in time series foundation models training, namely FFTS. Specifically, each data-holding organization is treated as an independent client in a collaborative learning framework with federated settings, and then many client-specific local models will be trained to preserve the unique characteristics per dataset. Moreover, a new regularization mechanism will be applied to both client-side and server-side, thus to align the shared knowledge across heterogeneous datasets from different domains. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed federated learning approach. The newly learned time series foundation models achieve superior generalization capabilities on cross-domain time series analysis tasks, including forecasting, imputation, and anomaly detection.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2303.07123.pdf' target='_blank'>https://arxiv.org/pdf/2303.07123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanqing Qu, Yingwei Pan, Guang Chen, Ting Yao, Changjun Jiang, Tao Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07123">Modality-Agnostic Debiasing for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) usually fail to generalize well to outside of distribution (OOD) data, especially in the extreme case of single domain generalization (single-DG) that transfers DNNs from single domain to multiple unseen domains. Existing single-DG techniques commonly devise various data-augmentation algorithms, and remould the multi-source domain generalization methodology to learn domain-generalized (semantic) features. Nevertheless, these methods are typically modality-specific, thereby being only applicable to one single modality (e.g., image). In contrast, we target a versatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that enables generalization for different modalities. Technically, MAD introduces a novel two-branch classifier: a biased-branch encourages the classifier to identify the domain-specific (superficial) features, and a general-branch captures domain-generalized features based on the knowledge from biased-branch. Our MAD is appealing in view that it is pluggable to most single-DG models. We validate the superiority of our MAD in a variety of single-DG scenarios with different modalities, including recognition on 1D texts, 2D images, 3D point clouds, and semantic segmentation on 2D images. More remarkably, for recognition on 3D point clouds and semantic segmentation on 2D images, MAD improves DSU by 2.82\% and 1.5\% in accuracy and mIOU.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2510.08558.pdf' target='_blank'>https://arxiv.org/pdf/2510.08558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianwei Yang, Ning Zhang, Xian Li, Ashish Shah, Dat Huynh, Hengduo Li, Zi Yang, Sara Cao, Lawrence Jang, Shuyan Zhou, Jiacheng Zhu, Huan Sun, Jason Weston, Yu Su, Yifan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08558">Agent Learning via Early Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2506.08989.pdf' target='_blank'>https://arxiv.org/pdf/2506.08989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, Weizhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08989">SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2505.05785.pdf' target='_blank'>https://arxiv.org/pdf/2505.05785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henan Sun, Xunkai Li, Lei Zhu, Junyi Han, Guang Zeng, Ronghua Li, Guoren Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05785">Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-Of-Distribution (OOD) generalization has gained increasing attentions for machine learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation under distribution shifts. Existing graph OOD methods tend to follow the basic ideas of invariant risk minimization and structural causal models, interpreting the invariant knowledge across datasets under various distribution shifts as graph topology or graph spectrum. However, these interpretations may be inconsistent with real-world scenarios, as neither invariant topology nor spectrum is assured. In this paper, we advocate the learnable random walk (LRW) perspective as the instantiation of invariant knowledge, and propose LRW-OOD to realize graph OOD generalization learning. Instead of employing fixed probability transition matrix (i.e., degree-normalized adjacency matrix), we parameterize the transition matrix with an LRW-sampler and a path encoder. Furthermore, we propose the kernel density estimation (KDE)-based mutual information (MI) loss to generate random walk sequences that adhere to OOD principles. Extensive experiment demonstrates that our model can effectively enhance graph OOD generalization under various types of distribution shifts and yield a significant accuracy improvement of 3.87% over state-of-the-art graph OOD generalization baselines.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2504.15686.pdf' target='_blank'>https://arxiv.org/pdf/2504.15686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phuong Quynh Le, Christin Seifert, JÃ¶rg SchlÃ¶tterer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15686">Invariant Learning with Annotation-free Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invariant learning is a promising approach to improve domain generalization compared to Empirical Risk Minimization (ERM). However, most invariant learning methods rely on the assumption that training examples are pre-partitioned into different known environments. We instead infer environments without the need for additional annotations, motivated by observations of the properties within the representation space of a trained ERM model. We show the preliminary effectiveness of our approach on the ColoredMNIST benchmark, achieving performance comparable to methods requiring explicit environment labels and on par with an annotation-free method that poses strong restrictions on the ERM reference model.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2504.04470.pdf' target='_blank'>https://arxiv.org/pdf/2504.04470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Guo, Ajian Liu, Yunfeng Diao, Jin Zhang, Hui Ma, Bo Zhao, Richang Hong, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04470">Domain Generalization for Face Anti-spoofing via Content-aware Composite Prompt Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The challenge of Domain Generalization (DG) in Face Anti-Spoofing (FAS) is the significant interference of domain-specific signals on subtle spoofing clues. Recently, some CLIP-based algorithms have been developed to alleviate this interference by adjusting the weights of visual classifiers. However, our analysis of this class-wise prompt engineering suffers from two shortcomings for DG FAS: (1) The categories of facial categories, such as real or spoof, have no semantics for the CLIP model, making it difficult to learn accurate category descriptions. (2) A single form of prompt cannot portray the various types of spoofing. In this work, instead of class-wise prompts, we propose a novel Content-aware Composite Prompt Engineering (CCPE) that generates instance-wise composite prompts, including both fixed template and learnable prompts. Specifically, our CCPE constructs content-aware prompts from two branches: (1) Inherent content prompt explicitly benefits from abundant transferred knowledge from the instruction-based Large Language Model (LLM). (2) Learnable content prompts implicitly extract the most informative visual content via Q-Former. Moreover, we design a Cross-Modal Guidance Module (CGM) that dynamically adjusts unimodal features for fusion to achieve better generalized FAS. Finally, our CCPE has been validated for its effectiveness in multiple cross-domain experiments and achieves state-of-the-art (SOTA) results.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2411.16898.pdf' target='_blank'>https://arxiv.org/pdf/2411.16898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyi Li, Michael Niemeyer, Zeyu Chen, Nassir Navab, Federico Tombari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16898">MonoGSDF: Exploring Monocular Geometric Cues for Gaussian Splatting-Guided Implicit Surface Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate meshing from monocular images remains a key challenge in 3D vision. While state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at synthesizing photorealistic novel views through rasterization-based rendering, their reliance on sparse, explicit primitives severely limits their ability to recover watertight and topologically consistent 3D surfaces.We introduce MonoGSDF, a novel method that couples Gaussian-based primitives with a neural Signed Distance Field (SDF) for high-quality reconstruction. During training, the SDF guides Gaussians' spatial distribution, while at inference, Gaussians serve as priors to reconstruct surfaces, eliminating the need for memory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a scaling strategy for robust generalization. A multi-resolution training scheme further refines details and monocular geometric cues from off-the-shelf estimators enhance reconstruction quality. Experiments on real-world datasets show MonoGSDF outperforms prior methods while maintaining efficiency.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2411.16898.pdf' target='_blank'>https://arxiv.org/pdf/2411.16898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyi Li, Michael Niemeyer, Zeyu Chen, Nassir Navab, Federico Tombari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16898">MonoGSDF: Exploring Monocular Geometric Cues for Gaussian Splatting-Guided Implicit Surface Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate meshing from monocular images remains a key challenge in 3D vision. While state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at synthesizing photorealistic novel views through rasterization-based rendering, their reliance on sparse, explicit primitives severely limits their ability to recover watertight and topologically consistent 3D surfaces.We introduce MonoGSDF, a novel method that couples Gaussian-based primitives with a neural Signed Distance Field (SDF) for high-quality reconstruction. During training, the SDF guides Gaussians' spatial distribution, while at inference, Gaussians serve as priors to reconstruct surfaces, eliminating the need for memory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a scaling strategy for robust generalization. A multi-resolution training scheme further refines details and monocular geometric cues from off-the-shelf estimators enhance reconstruction quality. Experiments on real-world datasets show MonoGSDF outperforms prior methods while maintaining efficiency.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2410.07408.pdf' target='_blank'>https://arxiv.org/pdf/2410.07408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyuan Dai, Josiah Wong, Yunfan Jiang, Chen Wang, Cem Gokmen, Ruohan Zhang, Jiajun Wu, Li Fei-Fei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07408">Automated Creation of Digital Cousins for Robust Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training robot policies in the real world can be unsafe, costly, and difficult to scale. Simulation serves as an inexpensive and potentially limitless source of training data, but suffers from the semantics and physics disparity between simulated and real-world environments. These discrepancies can be minimized by training in digital twins, which serve as virtual replicas of a real scene but are expensive to generate and cannot produce cross-domain generalization. To address these limitations, we propose the concept of digital cousins, a virtual asset or scene that, unlike a digital twin, does not explicitly model a real-world counterpart but still exhibits similar geometric and semantic affordances. As a result, digital cousins simultaneously reduce the cost of generating an analogous virtual environment while also facilitating better robustness during sim-to-real domain transfer by providing a distribution of similar training scenes. Leveraging digital cousins, we introduce a novel method for their automated creation, and propose a fully automated real-to-sim-to-real pipeline for generating fully interactive scenes and training robot policies that can be deployed zero-shot in the original scene. We find that digital cousin scenes that preserve geometric and semantic affordances can be produced automatically, and can be used to train policies that outperform policies trained on digital twins, achieving 90% vs. 25% success rates under zero-shot sim-to-real transfer. Additional details are available at https://digital-cousins.github.io/.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2404.13504.pdf' target='_blank'>https://arxiv.org/pdf/2404.13504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Feng, Lizhen Qu, Zhuang Li, Haolan Zhan, Yuncheng Hua, Gholamreza Haffari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13504">IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models have made incredible progress, but they still struggle when applied to examples from unseen domains. This study focuses on a specific problem of domain generalization, where a model is trained on one source domain and tested on multiple target domains that are unseen during training. We propose IMO: Invariant features Masks for Out-of-Distribution text classification, to achieve OOD generalization by learning invariant features. During training, IMO would learn sparse mask layers to remove irrelevant features for prediction, where the remaining features keep invariant. Additionally, IMO has an attention module at the token level to focus on tokens that are useful for prediction. Our comprehensive experiments show that IMO substantially outperforms strong baselines in terms of various evaluation metrics and settings.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2306.08328.pdf' target='_blank'>https://arxiv.org/pdf/2306.08328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runpeng Yu, Songhua Liu, Xingyi Yang, Xinchao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08328">Distribution Shift Inversion for Out-of-Distribution Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning society has witnessed the emergence of a myriad of Out-of-Distribution (OoD) algorithms, which address the distribution shift between the training and the testing distribution by searching for a unified predictor or invariant feature representation. However, the task of directly mitigating the distribution shift in the unseen testing set is rarely investigated, due to the unavailability of the testing distribution during the training phase and thus the impossibility of training a distribution translator mapping between the training and testing distribution. In this paper, we explore how to bypass the requirement of testing distribution for distribution translator training and make the distribution translation useful for OoD prediction. We propose a portable Distribution Shift Inversion algorithm, in which, before being fed into the prediction model, the OoD testing samples are first linearly combined with additional Gaussian noise and then transferred back towards the training distribution using a diffusion model trained only on the source distribution. Theoretical analysis reveals the feasibility of our method. Experimental results, on both multiple-domain generalization datasets and single-domain generalization datasets, show that our method provides a general performance gain when plugged into a wide range of commonly used OoD algorithms.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2305.14104.pdf' target='_blank'>https://arxiv.org/pdf/2305.14104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linyi Yang, Yaoxiao Song, Xuan Ren, Chenyang Lyu, Yidong Wang, Lingqiao Liu, Jindong Wang, Jennifer Foster, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14104">Out-of-Distribution Generalization in Text Classification: Past, Present, and Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) systems in natural language processing (NLP) face significant challenges in generalizing to out-of-distribution (OOD) data, where the test distribution differs from the training data distribution. This poses important questions about the robustness of NLP models and their high accuracy, which may be artificially inflated due to their underlying sensitivity to systematic biases. Despite these challenges, there is a lack of comprehensive surveys on the generalization challenge from an OOD perspective in text classification. Therefore, this paper aims to fill this gap by presenting the first comprehensive review of recent progress, methods, and evaluations on this topic. We furth discuss the challenges involved and potential future research directions. By providing quick access to existing work, we hope this survey will encourage future research in this area.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2305.08208.pdf' target='_blank'>https://arxiv.org/pdf/2305.08208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Niu, Linyi Yang, Ruihai Dong, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08208">Learning to Generalize for Cross-domain QA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2509.20807.pdf' target='_blank'>https://arxiv.org/pdf/2509.20807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhan Wu, Xiaoyang Qu, Zhangcheng Huang, Jianzong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20807">Federated Domain Generalization with Domain-specific Soft Prompts Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has become an efficient paradigm for adapting CLIP to downstream tasks. Compared with traditional fine-tuning, prompt learning optimizes a few parameters yet yields highly competitive results, especially appealing in federated learning for computational efficiency. engendering domain shift among clients and posing a formidable challenge for downstream-task adaptation. Existing federated domain generalization (FDG) methods based on prompt learning typically learn soft prompts from training samples, replacing manually designed prompts to enhance the generalization ability of federated models. However, these learned prompts exhibit limited diversity and tend to ignore information from unknown domains. We propose a novel and effective method from a generative perspective for handling FDG tasks, namely federated domain generalization with domain-specific soft prompts generation (FedDSPG). Specifically, during training, we introduce domain-specific soft prompts (DSPs) for each domain and integrate content and domain knowledge into the generative model among clients. In the inference phase, the generator is utilized to obtain DSPs for unseen target domains, thus guiding downstream tasks in unknown domains. Comprehensive evaluations across several public datasets confirm that our method outperforms existing strong baselines in FDG, achieving state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2509.14921.pdf' target='_blank'>https://arxiv.org/pdf/2509.14921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tahar Chettaoui, Naser Damer, Fadi Boutros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14921">Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models such as CLIP have demonstrated exceptional zero- and few-shot transfer capabilities across diverse vision tasks. However, when fine-tuned for highly specialized biometric tasks, face recognition (FR), morphing attack detection (MAD), and presentation attack detection (PAD), these models may suffer from over-specialization. Thus, they may lose one of their foundational strengths, cross-domain generalization. In this work, we systematically quantify these trade-offs by evaluating three instances of CLIP fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the original CLIP baseline on 14 general vision datasets under zero-shot and linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our results indicate that fine-tuned models suffer from over-specialization, especially when fine-tuned for complex tasks of FR. Also, our results pointed out that task complexity and classification head design, multi-class (FR) vs. binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The FRoundation model with the ViT-L backbone outperforms other approaches on the large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%. However, it experiences a substantial performance drop on ImageNetV2, reaching only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover, the larger CLIP architecture consistently preserves more of the model's original generalization ability than the smaller variant, indicating that increased model capacity may help mitigate over-specialization.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2508.14076.pdf' target='_blank'>https://arxiv.org/pdf/2508.14076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, Di Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14076">PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward models (RMs), which are central to existing post-training methods, aim to align LLM outputs with human values by providing feedback signals during fine-tuning. However, existing RMs struggle to capture nuanced, user-specific preferences, especially under limited data and across diverse domains. Thus, we introduce PersRM-R1, the first reasoning-based reward modeling framework specifically designed to identify and represent personal factors from only one or a few personal exemplars. To address challenges including limited data availability and the requirement for robust generalization, our approach combines synthetic data generation with a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement fine-tuning. Experimental results demonstrate that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability, paving the way for more effective personalized LLMs.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2507.12821.pdf' target='_blank'>https://arxiv.org/pdf/2507.12821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lance Ying, Katherine M. Collins, Prafull Sharma, Cedric Colas, Kaiya Ivy Zhao, Adrian Weller, Zenna Tavares, Phillip Isola, Samuel J. Gershman, Jacob D. Andreas, Thomas L. Griffiths, Francois Chollet, Kelsey R. Allen, Joshua B. Tenenbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12821">Assessing Adaptive World Models in Machines with Novel Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on massive corpora of data, instead of the efficiency and efficacy in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this class of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2506.10145.pdf' target='_blank'>https://arxiv.org/pdf/2506.10145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajeev Yasarla, Shizhong Han, Hsin-Pai Cheng, Litian Liu, Shweta Mahajan, Apratim Bhattacharyya, Yunxiao Shi, Risheek Garrepalli, Hong Cai, Fatih Porikli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10145">RoCA: Robust Cross-Domain End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end (E2E) autonomous driving has recently emerged as a new paradigm, offering significant potential. However, few studies have looked into the practical challenge of deployment across domains (e.g., cities). Although several works have incorporated Large Language Models (LLMs) to leverage their open-world knowledge, LLMs do not guarantee cross-domain driving performance and may incur prohibitive retraining costs during domain adaptation. In this paper, we propose RoCA, a novel framework for robust cross-domain E2E autonomous driving. RoCA formulates the joint probabilistic distribution over the tokens that encode ego and surrounding vehicle information in the E2E pipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of basis tokens with corresponding trajectories, which span diverse driving scenarios. Then, given any driving scene, it is able to probabilistically infer the future trajectory. By using RoCA together with a base E2E model in source-domain training, we improve the generalizability of the base model, without requiring extra inference computation. In addition, RoCA enables robust adaptation on new target domains, significantly outperforming direct finetuning. We extensively evaluate RoCA on various cross-domain scenarios and show that it achieves strong domain generalization and adaptation performance.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2503.13617.pdf' target='_blank'>https://arxiv.org/pdf/2503.13617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Yubin Xiao, Ke Liang, Mengzhu Wang, Long Lan, Kenli Li, Xinwang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13617">Let Synthetic Data Shine: Domain Reassembly and Soft-Fusion for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Domain Generalization (SDG) aims to train models with consistent performance across diverse scenarios using data from a single source. While using latent diffusion models (LDMs) show promise in augmenting limited source data, we demonstrate that directly using synthetic data can be detrimental due to significant feature distribution discrepancies between synthetic and real target domains, leading to performance degradation. To address this issue, we propose Discriminative Domain Reassembly and Soft-Fusion (DRSF), a training framework leveraging synthetic data to improve model generalization. We employ LDMs to produce diverse pseudo-target domain samples and introduce two key modules to handle distribution bias. First, Discriminative Feature Decoupling and Reassembly (DFDR) module uses entropy-guided attention to recalibrate channel-level features, suppressing synthetic noise while preserving semantic consistency. Second, Multi-pseudo-domain Soft Fusion (MDSF) module uses adversarial training with latent-space feature interpolation, creating continuous feature transitions between domains. Extensive SDG experiments on object detection and semantic segmentation tasks demonstrate that DRSF achieves substantial performance gains with only marginal computational overhead. Notably, DRSF's plug-and-play architecture enables seamless integration with unsupervised domain adaptation paradigms, underscoring its broad applicability in addressing diverse and real-world domain challenges.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2412.13815.pdf' target='_blank'>https://arxiv.org/pdf/2412.13815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Xiangyuan Yang, Mengzhu Wang, Long Lan, Ke Liang, Xinwang Liu, Kenli Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13815">Object Style Diffusion for Generalized Object Detection in Urban Scene</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection is a critical task in computer vision, with applications in various domains such as autonomous driving and urban scene monitoring. However, deep learning-based approaches often demand large volumes of annotated data, which are costly and difficult to acquire, particularly in complex and unpredictable real-world environments. This dependency significantly hampers the generalization capability of existing object detection techniques. To address this issue, we introduce a novel single-domain object detection generalization method, named GoDiff, which leverages a pre-trained model to enhance generalization in unseen domains. Central to our approach is the Pseudo Target Data Generation (PTDG) module, which employs a latent diffusion model to generate pseudo-target domain data that preserves source domain characteristics while introducing stylistic variations. By integrating this pseudo data with source domain data, we diversify the training dataset. Furthermore, we introduce a cross-style instance normalization technique to blend style features from different domains generated by the PTDG module, thereby increasing the detector's robustness. Experimental results demonstrate that our method not only enhances the generalization ability of existing detectors but also functions as a plug-and-play enhancement for other single-domain generalization methods, achieving state-of-the-art performance in autonomous driving scenarios.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2411.15421.pdf' target='_blank'>https://arxiv.org/pdf/2411.15421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Hu, Kun Yuan, Yaling Shen, Feilong Tang, Xiaohao Xu, Lin Zhou, Wei Li, Ying Chen, Zhongxing Xu, Zelin Peng, Siyuan Yan, Vinkle Srivastav, Diping Song, Tianbin Li, Danli Shi, Jin Ye, Nicolas Padoy, Nassir Navab, Junjun He, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15421">OphCLIP: Hierarchical Retrieval-Augmented Learning for Ophthalmic Surgical Video-Language Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical practice involves complex visual interpretation, procedural skills, and advanced medical knowledge, making surgical vision-language pretraining (VLP) particularly challenging due to this complexity and the limited availability of annotated data. To address the gap, we propose OphCLIP, a hierarchical retrieval-augmented vision-language pretraining framework specifically designed for ophthalmic surgical workflow understanding. OphCLIP leverages the OphVL dataset we constructed, a large-scale and comprehensive collection of over 375K hierarchically structured video-text pairs with tens of thousands of different combinations of attributes (surgeries, phases/operations/actions, instruments, medications, as well as more advanced aspects like the causes of eye diseases, surgical objectives, and postoperative recovery recommendations, etc). These hierarchical video-text correspondences enable OphCLIP to learn both fine-grained and long-term visual representations by aligning short video clips with detailed narrative descriptions and full videos with structured titles, capturing intricate surgical details and high-level procedural insights, respectively. Our OphCLIP also designs a retrieval-augmented pretraining framework to leverage the underexplored large-scale silent surgical procedure videos, automatically retrieving semantically relevant content to enhance the representation learning of narrative videos. Evaluation across 11 datasets for phase recognition and multi-instrument identification shows OphCLIP's robust generalization and superior performance.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2305.04106.pdf' target='_blank'>https://arxiv.org/pdf/2305.04106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, Houari Sahraoui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04106">On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model needs to learn from a stream of programs containing new, unseen APIs over time. We study two widely used PLM architectures, i.e., a GPT2 decoder and a RoBERTa encoder, on two downstream tasks, API call and API usage prediction. We demonstrate that the most commonly used fine-tuning technique from prior work is not robust enough to handle the dynamic nature of APIs, leading to the loss of previously acquired knowledge i.e., catastrophic forgetting. To address these issues, we implement five continual learning approaches, including replay-based and regularization-based methods. Our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in PLMs across both downstream tasks while achieving comparable or superior performance.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2211.01407.pdf' target='_blank'>https://arxiv.org/pdf/2211.01407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilia Sucholutsky, Ruairidh M. Battleday, Katherine M. Collins, Raja Marjieh, Joshua C. Peterson, Pulkit Singh, Umang Bhatt, Nori Jacoby, Adrian Weller, Thomas L. Griffiths
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.01407">On the Informativeness of Supervision Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., "this is a dog"), soft labels provide information about the object's relationship with multiple classes (e.g., "this is most likely a dog, but it could also be a wolf or a coyote"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalization. We validate these results empirically in a series of experiments with over 1 million crowdsourced image annotations and conduct a cost-benefit analysis to establish a tradeoff curve that enables users to optimize the cost of supervising representation learning on their own datasets.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2501.07769.pdf' target='_blank'>https://arxiv.org/pdf/2501.07769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Ming Yang, Lan-Zhe Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07769">BMIP: Bi-directional Modality Interaction Prompt Learning for VLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have exhibited remarkable generalization capabilities, and prompt learning for VLMs has attracted great attention for the ability to adapt pre-trained VLMs to specific downstream tasks. However, existing studies mainly focus on single-modal prompts or uni-directional modality interaction, overlooking the powerful alignment effects resulting from the interaction between the vision and language modalities. To this end, we propose a novel prompt learning method called $\underline{\textbf{B}}i-directional \underline{\textbf{M}}odality \underline{\textbf{I}}nteraction \underline{\textbf{P}}rompt (BMIP)$, which dynamically weights bi-modal information through learning the information of the attention layer, enhancing trainability and inter-modal consistency compared to simple information aggregation methods. To evaluate the effectiveness of prompt learning methods, we propose a more realistic evaluation paradigm called open-world generalization complementing the widely adopted cross-dataset transfer and domain generalization tasks. Comprehensive experiments on various datasets reveal that BMIP not only outperforms current state-of-the-art methods across all three evaluation paradigms but is also flexible enough to be combined with other prompt-based methods for consistent performance enhancement.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2412.20895.pdf' target='_blank'>https://arxiv.org/pdf/2412.20895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, Tieniu Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20895">Towards Compatible Fine-tuning for Vision-Language Model Updates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>So far, efficient fine-tuning has become a popular strategy for enhancing the capabilities of foundation models on downstream tasks by learning plug-and-play modules. However, existing methods overlook a crucial issue: if the underlying foundation model is updated, are these plug-and-play modules still effective? In this paper, we first conduct a detailed analysis of various fine-tuning methods on the CLIP in terms of their compatibility with model updates. The study reveals that many high-performing fine-tuning methods fail to be compatible with the upgraded models. To address this, we propose a novel approach, Class-conditioned Context Optimization (ContCoOp), which integrates learnable prompts with class embeddings using an attention layer before inputting them into the text encoder. Consequently, the prompts can dynamically adapt to the changes in embedding space (due to model updates), ensuring continued effectiveness. Extensive experiments over 15 datasets show that our ContCoOp achieves the highest compatibility over the baseline methods, and exhibits robust out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2407.18568.pdf' target='_blank'>https://arxiv.org/pdf/2407.18568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjun Yi, Qi Bi, Hao Zheng, Haolan Zhan, Wei Ji, Yawen Huang, Yuexiang Li, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18568">Learning Spectral-Decomposed Tokens for Domain Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of Vision Foundation Model (VFM) brings inherent out-domain generalization for a variety of down-stream tasks. Among them, domain generalized semantic segmentation (DGSS) holds unique challenges as the cross-domain images share common pixel-wise content information but vary greatly in terms of the style. In this paper, we present a novel Spectral-dEcomposed Token (SET) learning framework to advance the frontier. Delving into further than existing fine-tuning token & frozen backbone paradigm, the proposed SET especially focuses on the way learning style-invariant features from these learnable tokens. Particularly, the frozen VFM features are first decomposed into the phase and amplitude components in the frequency space, which mainly contain the information of content and style, respectively, and then separately processed by learnable tokens for task-specific information extraction. After the decomposition, style variation primarily impacts the token-based feature enhancement within the amplitude branch. To address this issue, we further develop an attention optimization method to bridge the gap between style-affected representation and static tokens during inference. Extensive cross-domain experiments show its state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2311.09765.pdf' target='_blank'>https://arxiv.org/pdf/2311.09765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunji Lee, Luca Soldaini, Arman Cohan, Minjoon Seo, Kyle Lo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09765">Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prevailing research practice today often relies on training dense retrievers on existing large datasets such as MSMARCO and then experimenting with ways to improve zero-shot generalization capabilities to unseen domains. While prior work has tackled this challenge through resource-intensive steps such as data augmentation, architectural modifications, increasing model size, or even further base model pretraining, comparatively little investigation has examined whether the training procedures themselves can be improved to yield better generalization capabilities in the resulting models. In this work, we recommend a simple recipe for training dense encoders: Train on MSMARCO with parameter-efficient methods, such as LoRA, and opt for using in-batch negatives unless given well-constructed hard negatives. We validate these recommendations using the BEIR benchmark and find results are persistent across choice of dense encoder and base model size and are complementary to other resource-intensive strategies for out-of-domain generalization such as architectural modifications or additional pretraining. We hope that this thorough and impartial study around various training techniques, which augments other resource-intensive methods, offers practical insights for developing a dense retrieval model that effectively generalizes, even when trained on a single dataset.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2510.00413.pdf' target='_blank'>https://arxiv.org/pdf/2510.00413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Liu, Junyi Li, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00413">PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graphical User Interface (GUI) agents powered by Multimodal Large Language Models (MLLMs) promise human-like interaction with software applications, yet long-horizon tasks remain challenging due to memory limitations. Existing approaches either truncate history or rely on simple textual summaries, which risk losing critical information when past visual details become necessary for future decisions. In this paper, we propose \textbf{PAL-UI} (\textbf{P}lanning with \textbf{A}ctive \textbf{L}ook-back), a novel framework that enables GUI agents to adaptively retrieve past observations when required. PAL-UI combines a dual-level summarization agent, capturing both observation-level cues and action-level outcomes, with a dedicated retrieval tool that allows the agent to recall specific historical screenshots during planning. We curate a step-level instruction dataset of 8.6K samples from mobile GUI navigation trajectories and train \textbf{PAL-UI-3B} and \textbf{PAL-UI-7B} models based on Qwen2.5-VL. Extensive experiments demonstrate that PAL-UI significantly outperforms baseline models and prior methods in mobile GUI navigation tasks, even under data-efficient settings. Moreover, PAL-UI exhibits strong cross-domain generalization, achieving notable improvements in web navigation without additional training. Our work highlights the potential of active memory retrieval for long-horizon planning capabilities of vision-based GUI agents.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2509.03131.pdf' target='_blank'>https://arxiv.org/pdf/2509.03131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sashuai Zhou, Weinan Gan, Qijiong Liu, Ke Lei, Jieming Zhu, Hai Huang, Yan Xia, Ruiming Tang, Zhenhua Dong, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03131">RecBase: Generative Foundation Model Pretraining for Zero-Shot Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in LLM-based recommendation have shown promise, yet their cross-domain generalization is hindered by a fundamental mismatch between language-centric pretraining and the recommendation task. Existing methods, relying on language-level knowledge, fail to capture dynamic, item-level user interests across domains. To bridge this gap, we propose RecBase, a domain-agnostic foundational model pretrained with a recommendation-oriented objective. RecBase leverages a large-scale, heterogeneous, cross-domain corpus with unified textual representations and feature mappings to enhance cross-domain generalization. To further align item semantics across domains, we introduce a unified item tokenizer that encodes items into hierarchical concept identifiers, enabling structured representation and efficient vocabulary sharing. The model is trained using an autoregressive objective to capture complex item-level sequential patterns. On eight real-world datasets, our 1.5B-parameter model matches or surpasses the performance of LLM baselines up to 7B parameters in zero-shot and cross-domain recommendation tasks.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2508.13401.pdf' target='_blank'>https://arxiv.org/pdf/2508.13401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Dumitriu, Florin Miron, Florin Tatui, Radu Tudor Ionescu, Radu Timofte, Aakash Ralhan, Florin-Alexandru Vasluianu, Shenyang Qian, Mitchell Harley, Imran Razzak, Yang Song, Pu Luo, Yumei Li, Cong Xu, Jinming Chai, Kexin Zhang, Licheng Jiao, Lingling Li, Siqi Yu, Chao Zhang, Kehuan Song, Fang Liu, Puhua Chen, Xu Liu, Jin Hu, Jinyang Xu, Biao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13401">AIM 2025 Rip Current Segmentation (RipSeg) Challenge Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents an overview of the AIM 2025 RipSeg Challenge, a competition designed to advance techniques for automatic rip current segmentation in still images. Rip currents are dangerous, fast-moving flows that pose a major risk to beach safety worldwide, making accurate visual detection an important and underexplored research task. The challenge builds on RipVIS, the largest available rip current dataset, and focuses on single-class instance segmentation, where precise delineation is critical to fully capture the extent of rip currents. The dataset spans diverse locations, rip current types, and camera orientations, providing a realistic and challenging benchmark. In total, $75$ participants registered for this first edition, resulting in $5$ valid test submissions. Teams were evaluated on a composite score combining $F_1$, $F_2$, $AP_{50}$, and $AP_{[50:95]}$, ensuring robust and application-relevant rankings. The top-performing methods leveraged deep learning architectures, domain adaptation techniques, pretrained models, and domain generalization strategies to improve performance under diverse conditions. This report outlines the dataset details, competition framework, evaluation metrics, and final results, providing insights into the current state of rip current segmentation. We conclude with a discussion of key challenges, lessons learned from the submissions, and future directions for expanding RipSeg.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2508.11265.pdf' target='_blank'>https://arxiv.org/pdf/2508.11265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei He, Lingling Li, Licheng Jiao, Ronghua Shang, Fang Liu, Shuang Wang, Xu Liu, Wenping Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11265">Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization in 3D segmentation is a critical challenge in deploying models to unseen environments. Current methods mitigate the domain shift by augmenting the data distribution of point clouds. However, the model learns global geometric patterns in point clouds while ignoring the category-level distribution and alignment. In this paper, a category-level geometry learning framework is proposed to explore the domain-invariant geometric features for domain generalized 3D semantic segmentation. Specifically, Category-level Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric properties of point cloud features, which constructs the geometric properties of each class and couples geometric embedding to semantic learning. Secondly, Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D distribution and align the category-level geometric embeddings, allowing the model to focus on the geometric invariant information to improve generalization. Experimental results verify the effectiveness of the proposed method, which has very competitive segmentation accuracy compared with the state-of-the-art domain generalized point cloud methods.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2507.10281.pdf' target='_blank'>https://arxiv.org/pdf/2507.10281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Tian, Liyao Li, Wentao Ye, Haobo Wang, Lingxin Wang, Lihua Yu, Zujie Ren, Gang Chen, Junbo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10281">Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tables are fundamental in domains such as finance, healthcare, and public administration, yet real-world table tasks often involve noise, structural heterogeneity, and semantic complexity--issues underexplored in existing research that primarily targets clean academic datasets. This survey focuses on LLM-based Table Agents, which aim to automate table-centric workflows by integrating preprocessing, reasoning, and domain adaptation. We define five core competencies--C1: Table Structure Understanding, C2: Table and Query Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze and compare current approaches. In addition, a detailed examination of the Text-to-SQL Agent reveals a performance gap between academic benchmarks and real-world scenarios, especially for open-source models. Finally, we provide actionable insights to improve the robustness, generalization, and efficiency of LLM-based Table Agents in practical settings.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2504.09532.pdf' target='_blank'>https://arxiv.org/pdf/2504.09532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congcong Wen, Geeta Chandra Raju Bethala, Yu Hao, Niraj Pudasaini, Hao Huang, Shuaihang Yuan, Baoru Huang, Anh Nguyen, Anthony Tzes, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09532">Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions. Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings. In this paper, we propose Humanoid-COA, the first humanoid agent framework that integrates foundation model reasoning with an Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation. Within the perception--reasoning--action paradigm, our key contribution lies in the reasoning stage, where the proposed CoA mechanism decomposes high-level human instructions into structured sequences of locomotion and manipulation primitives through affordance analysis, spatial inference, and whole-body action reasoning. Extensive experiments on two humanoid robots, Unitree H1-2 and G1, in both an open test area and an apartment environment, demonstrate that our framework substantially outperforms prior baselines across manipulation, locomotion, and loco-manipulation tasks, achieving robust generalization to long-horizon and unstructured scenarios. Project page: https://humanoid-coa.github.io/
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2504.09532.pdf' target='_blank'>https://arxiv.org/pdf/2504.09532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congcong Wen, Geeta Chandra Raju Bethala, Yu Hao, Niraj Pudasaini, Hao Huang, Shuaihang Yuan, Baoru Huang, Anh Nguyen, Mengyu Wang, Anthony Tzes, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09532">Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions. Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings. In this paper, we propose Humanoid-COA, the first humanoid agent framework that integrates foundation model reasoning with an Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation. Within the perception--reasoning--action paradigm, our key contribution lies in the reasoning stage, where the proposed CoA mechanism decomposes high-level human instructions into structured sequences of locomotion and manipulation primitives through affordance analysis, spatial inference, and whole-body action reasoning. Extensive experiments on two humanoid robots, Unitree H1-2 and G1, in both an open test area and an apartment environment, demonstrate that our framework substantially outperforms prior baselines across manipulation, locomotion, and loco-manipulation tasks, achieving robust generalization to long-horizon and unstructured scenarios. Project page: https://humanoid-coa.github.io/
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2407.19174.pdf' target='_blank'>https://arxiv.org/pdf/2407.19174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuran Ma, Weiying Xie, Daixun Li, Haowei Li, Yunsong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19174">Reducing Spurious Correlation for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of multimedia has provided a large amount of data with different distributions for visual tasks, forming different domains. Federated Learning (FL) can efficiently use this diverse data distributed on different client media in a decentralized manner through model sharing. However, in open-world scenarios, there is a challenge: global models may struggle to predict well on entirely new domain data captured by certain media, which were not encountered during training. Existing methods still rely on strong statistical correlations between samples and labels to address this issue, which can be misleading, as some features may establish spurious short-cut correlations with the predictions. To comprehensively address this challenge, we introduce FedCD (Cross-Domain Invariant Federated Learning), an overall optimization framework at both the local and global levels. We introduce the Spurious Correlation Intervener (SCI), which employs invariance theory to locally generate interventers for features in a self-supervised manner to reduce the model's susceptibility to spurious correlated features. Our approach requires no sharing of data or features, only the gradients related to the model. Additionally, we develop the simple yet effective Risk Extrapolation Aggregation strategy (REA), determining aggregation coefficients through mathematical optimization to facilitate global causal invariant predictions. Extensive experiments and ablation studies highlight the effectiveness of our approach. In both classification and object detection generalization tasks, our method outperforms the baselines by an average of at least 1.45% in Acc, 4.8% and 1.27% in mAP50.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2406.05149.pdf' target='_blank'>https://arxiv.org/pdf/2406.05149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Benkert, Mohit Prabhushankar, Ghassan AlRegib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05149">Effective Data Selection for Seismic Interpretation through Disagreement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a discussion on data selection for deep learning in the field of seismic interpretation. In order to achieve a robust generalization to the target volume, it is crucial to identify the specific samples are the most informative to the training process. The selection of the training set from a target volume is a critical factor in determining the effectiveness of the deep learning algorithm for interpreting seismic volumes. This paper proposes the inclusion of interpretation disagreement as a valuable and intuitive factor in the process of selecting training sets. The development of a novel data selection framework is inspired by established practices in seismic interpretation. The framework we have developed utilizes representation shifts to effectively model interpretation disagreement within neural networks. Additionally, it incorporates the disagreement measure to enhance attention towards geologically interesting regions throughout the data selection workflow. By combining this approach with active learning, a well-known machine learning paradigm for data selection, we arrive at a comprehensive and innovative framework for training set selection in seismic interpretation. In addition, we offer a specific implementation of our proposed framework, which we have named ATLAS. This implementation serves as a means for data selection. In this study, we present the results of our comprehensive experiments, which clearly indicate that ATLAS consistently surpasses traditional active learning frameworks in the field of seismic interpretation. Our findings reveal that ATLAS achieves improvements of up to 12% in mean intersection-over-union.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2404.00656.pdf' target='_blank'>https://arxiv.org/pdf/2404.00656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00656">WavLLM: Towards Robust and Adaptive Speech Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \url{aka.ms/wavllm}.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2310.10068.pdf' target='_blank'>https://arxiv.org/pdf/2310.10068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Li, Guanshuo Wang, Yichao Yan, Fufu Yu, Qiong Jia, Jie Qin, Shouhong Ding, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10068">Generalizable Person Search on Open-world User-Generated Video Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person search is a challenging task that involves detecting and retrieving individuals from a large set of un-cropped scene images. Existing person search applications are mostly trained and deployed in the same-origin scenarios. However, collecting and annotating training samples for each scene is often difficult due to the limitation of resources and the labor cost. Moreover, large-scale intra-domain data for training are generally not legally available for common developers, due to the regulation of privacy and public security. Leveraging easily accessible large-scale User Generated Video Contents (\emph{i.e.} UGC videos) to train person search models can fit the open-world distribution, but still suffering a performance gap from the domain difference to surveillance scenes. In this work, we explore enhancing the out-of-domain generalization capabilities of person search models, and propose a generalizable framework on both feature-level and data-level generalization to facilitate downstream tasks in arbitrary scenarios. Specifically, we focus on learning domain-invariant representations for both detection and ReID by introducing a multi-task prototype-based domain-specific batch normalization, and a channel-wise ID-relevant feature decorrelation strategy. We also identify and address typical sources of noise in open-world training frames, including inaccurate bounding boxes, the omission of identity labels, and the absence of cross-camera data. Our framework achieves promising performance on two challenging person search benchmarks without using any human annotation or samples from the target domain.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2310.03007.pdf' target='_blank'>https://arxiv.org/pdf/2310.03007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Chen, Qi Zhang, Zenan Huang, Haobo Wang, Junbo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03007">Towards Domain-Specific Features Disentanglement for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distributional shift between domains poses great challenges to modern machine learning algorithms. The domain generalization (DG) signifies a popular line targeting this issue, where these methods intend to uncover universal patterns across disparate distributions. Noted, the crucial challenge behind DG is the existence of irrelevant domain features, and most prior works overlook this information. Motivated by this, we propose a novel contrastive-based disentanglement method CDDG, to effectively utilize the disentangled features to exploit the over-looked domain-specific features, and thus facilitating the extraction of the desired cross-domain category features for DG tasks. Specifically, CDDG learns to decouple inherent mutually exclusive features by leveraging them in the latent space, thus making the learning discriminative. Extensive experiments conducted on various benchmark datasets demonstrate the superiority of our method compared to other state-of-the-art approaches. Furthermore, visualization evaluations confirm the potential of our method in achieving effective feature disentanglement.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2307.08423.pdf' target='_blank'>https://arxiv.org/pdf/2307.08423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Alex Strasser, Haiyang Yu, YuQing Xie, Xiang Fu, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes StÃ¤rk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, AlÃ¡n Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro LiÃ², Rose Yu, Stephan GÃ¼nnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08423">Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in artificial intelligence (AI) are fueling a new paradigm of discoveries in natural sciences. Today, AI has started to advance natural sciences by improving, accelerating, and enabling our understanding of natural phenomena at a wide range of spatial and temporal scales, giving rise to a new area of research known as AI for science (AI4Science). Being an emerging research paradigm, AI4Science is unique in that it is an enormous and highly interdisciplinary area. Thus, a unified and technical treatment of this field is needed yet challenging. This work aims to provide a technically thorough account of a subarea of AI4Science; namely, AI for quantum, atomistic, and continuum systems. These areas aim at understanding the physical world from the subatomic (wavefunctions and electron density), atomic (molecules, proteins, materials, and interactions), to macro (fluids, climate, and subsurface) scales and form an important subarea of AI4Science. A unique advantage of focusing on these areas is that they largely share a common set of challenges, thereby allowing a unified and foundational treatment. A key common challenge is how to capture physics first principles, especially symmetries, in natural systems by deep learning methods. We provide an in-depth yet intuitive account of techniques to achieve equivariance to symmetry transformations. We also discuss other common technical challenges, including explainability, out-of-distribution generalization, knowledge transfer with foundation and large language models, and uncertainty quantification. To facilitate learning and education, we provide categorized lists of resources that we found to be useful. We strive to be thorough and unified and hope this initial effort may trigger more community interests and efforts to further advance AI4Science.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2111.14290.pdf' target='_blank'>https://arxiv.org/pdf/2111.14290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Yan, Junjie Li, Shengcai Liao, Jie Qin, Bingbing Ni, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.14290">TAL: Two-stream Adaptive Learning for Generalizable Person Re-identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalizable person re-identification aims to apply a trained model to unseen domains. Prior works either combine the data in all the training domains to capture domain-invariant features, or adopt a mixture of experts to investigate domain-specific information. In this work, we argue that both domain-specific and domain-invariant features are crucial for improving the generalization ability of re-id models. To this end, we design a novel framework, which we name two-stream adaptive learning (TAL), to simultaneously model these two kinds of information. Specifically, a domain-specific stream is proposed to capture training domain statistics with batch normalization (BN) parameters, while an adaptive matching layer is designed to dynamically aggregate domain-level information. In the meantime, we design an adaptive BN layer in the domain-invariant stream, to approximate the statistics of various unseen domains. These two streams work adaptively and collaboratively to learn generalizable re-id features. Our framework can be applied to both single-source and multi-source domain generalization tasks, where experimental results show that our framework notably outperforms the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2509.10503.pdf' target='_blank'>https://arxiv.org/pdf/2509.10503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolin Yuan, Jingtao Li, Weiming Zhuang, Chen Chen, Lingjuan Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10503">FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Object Detection (FOD) enables clients to collaboratively train a global object detection model without accessing their local data from diverse domains. However, significant variations in environment, weather, and other domain specific factors hinder performance, making cross domain generalization a key challenge. Existing FOD methods often overlook the hardware constraints of edge devices and introduce local training regularizations that incur high computational costs, limiting real-world applicability. In this paper, we propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without introducing additional local computational overhead. FEDEXCHANGE employs a server side dynamic model exchange strategy that enables each client to gain insights from other clients' domain data without direct data sharing. Specifically, FEDEXCHANGE allows the server to alternate between model aggregation and model exchange. During aggregation rounds, the server aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters and exchanges local models based on distance measures, allowing local models to learn from a variety of domains. As all operations are performed on the server side, clients can achieve improved cross domain utility without any additional computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE enhances FOD performance, achieving 1.6X better mean average precision in challenging domains, such as rainy conditions, while requiring only 0.8X the computational resources compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2506.12307.pdf' target='_blank'>https://arxiv.org/pdf/2506.12307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotian Zhang, Yuan Wang, Zhaopeng Feng, Ruizhe Chen, Zhijie Zhou, Yan Zhang, Hongxia Xu, Jian Wu, Zuozhu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12307">Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve comprehensive medical understanding is still largely unexplored. In this paper, we present Med-U1, a unified framework for robust reasoning across medical QA tasks with diverse output formats, ranging from MCQs to complex generation and computation tasks. Med-U1 employs pure large-scale reinforcement learning with mixed rule-based binary reward functions, incorporating a length penalty to manage output verbosity. With multi-objective reward optimization, Med-U1 directs LLMs to produce concise and verifiable reasoning chains. Empirical results reveal that Med-U1 significantly improves performance across multiple challenging Med-QA benchmarks, surpassing even larger specialized and proprietary models. Furthermore, Med-U1 demonstrates robust generalization to out-of-distribution (OOD) tasks. Extensive analysis presents insights into training strategies, reasoning chain length control, and reward design for medical LLMs. Our code is available here.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2402.06223.pdf' target='_blank'>https://arxiv.org/pdf/2402.06223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Liu, Zhen Zhang, Dong Gong, Erdun Gao, Biwei Huang, Mingming Gong, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06223">Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directed acyclic graphs (DAGs) are fundamental graph structures in causal modeling, but identifying the desired DAG from observational data often requires strong assumptions that may not hold in real-world scenarios, especially for latent causal models and complex multimodal data. This raises the question of whether we can relax or bypass the DAG assumption while maintaining practical utility. In this work, we propose a novel latent partial causal model for multimodal data, featuring two latent coupled variables, connected by an undirected edge, to represent the transfer of knowledge across modalities. Under specific statistical assumptions, we establish an identifiability result, demonstrating that representations learned by multimodal contrastive learning correspond to the latent coupled variables up to a trivial transformation. This result deepens our understanding of the why multimodal contrastive learning works, highlights its potential for disentanglement, and expands the utility of pre-trained models like CLIP. Synthetic experiments confirm the robustness of our findings, even when the assumptions are partially violated. Most importantly, experiments on a pre-trained CLIP model embodies disentangled representations, enabling few-shot learning and improving domain generalization across diverse real-world datasets. Together, these contributions push the boundaries of multimodal contrastive learning, both theoretically and, crucially, in practical applications.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2401.09716.pdf' target='_blank'>https://arxiv.org/pdf/2401.09716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Tongliang Liu, Lina Yao, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09716">HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents a significant advancement in the field, setting itself apart with a unique generative approach to prompts, alongside an explicit model structure and specialized loss functions. Differing from traditional visual prompts that are often shared across entire datasets, HCVP utilizes a hierarchical prompt generation network enhanced by prompt contrastive learning. These generative prompts are instance-dependent, catering to the unique characteristics inherent to different domains and tasks. Additionally, we devise a prompt modulation network that serves as a bridge, effectively incorporating the generated visual prompts into the vision transformer backbone. Experiments conducted on five DG datasets demonstrate the effectiveness of HCVP, outperforming both established DG algorithms and adaptation protocols.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2312.10988.pdf' target='_blank'>https://arxiv.org/pdf/2312.10988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianrui Jia, Haoyang Li, Cheng Yang, Tao Tao, Chuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10988">Graph Invariant Learning with Subgraph Co-mixup for Out-Of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural networks (GNNs) have been demonstrated to perform well in graph representation learning, but always lacking in generalization capability when tackling out-of-distribution (OOD) data. Graph invariant learning methods, backed by the invariance principle among defined multiple environments, have shown effectiveness in dealing with this issue. However, existing methods heavily rely on well-predefined or accurately generated environment partitions, which are hard to be obtained in practice, leading to sub-optimal OOD generalization performances. In this paper, we propose a novel graph invariant learning method based on invariant and variant patterns co-mixup strategy, which is capable of jointly generating mixed multiple environments and capturing invariant patterns from the mixed graph data. Specifically, we first adopt a subgraph extractor to identify invariant subgraphs. Subsequently, we design one novel co-mixup strategy, i.e., jointly conducting environment Mixup and invariant Mixup. For the environment Mixup, we mix the variant environment-related subgraphs so as to generate sufficiently diverse multiple environments, which is important to guarantee the quality of the graph invariant learning. For the invariant Mixup, we mix the invariant subgraphs, further encouraging to capture invariant patterns behind graphs while getting rid of spurious correlations for OOD generalization. We demonstrate that the proposed environment Mixup and invariant Mixup can mutually promote each other. Extensive experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-art under various distribution shifts.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2303.12649.pdf' target='_blank'>https://arxiv.org/pdf/2303.12649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Bi, Zhongliang Jiang, Ricarda Clarenbach, Reza Ghotbi, Angelos Karlas, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12649">MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization capabilities of learning-based medical image segmentation across domains are currently limited by the performance degradation caused by the domain shift, particularly for ultrasound (US) imaging. The quality of US images heavily relies on carefully tuned acoustic parameters, which vary across sonographers, machines, and settings. To improve the generalizability on US images across domains, we propose MI-SegNet, a novel mutual information (MI) based framework to explicitly disentangle the anatomical and domain feature representations; therefore, robust domain-independent segmentation can be expected. Two encoders are employed to extract the relevant features for the disentanglement. The segmentation only uses the anatomical feature map for its prediction. In order to force the encoders to learn meaningful feature representations a cross-reconstruction method is used during training. Transformations, specific to either domain or anatomy are applied to guide the encoders in their respective feature extraction task. Additionally, any MI present in both feature maps is punished to further promote separate feature spaces. We validate the generalizability of the proposed domain-independent segmentation approach on several datasets with varying parameters and machines. Furthermore, we demonstrate the effectiveness of the proposed MI-SegNet serving as a pre-trained model by comparing it with state-of-the-art networks.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2509.10951.pdf' target='_blank'>https://arxiv.org/pdf/2509.10951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Wilkinghoff, Haici Yang, Janek Ebbers, François G. Germain, Gordon Wichern, Jonathan Le Roux
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10951">Local Density-Based Anomaly Score Normalization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art anomalous sound detection (ASD) systems in domain-shifted conditions rely on projecting audio signals into an embedding space and using distance-based outlier detection to compute anomaly scores. One of the major difficulties to overcome is the so-called domain mismatch between the anomaly score distributions of a source domain and a target domain that differ acoustically and in terms of the amount of training data provided. A decision threshold that is optimal for one domain may be highly sub-optimal for the other domain and vice versa. This significantly degrades the performance when only using a single decision threshold, as is required when generalizing to multiple data domains that are possibly unseen during training while still using the same trained ASD system as in the source domain. To reduce this mismatch between the domains, we propose a simple local-density-based anomaly score normalization scheme. In experiments conducted on several ASD datasets, we show that the proposed normalization scheme consistently improves performance for various types of embedding-based ASD systems and yields better results than existing anomaly score normalization approaches.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2507.14783.pdf' target='_blank'>https://arxiv.org/pdf/2507.14783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Derek Li, Jiaming Zhou, Amirreza Kazemi, Qianyi Sun, Abbas Ghaddar, Mohammad Ali Alomrani, Liheng Ma, Yu Luo, Dong Li, Feng Wen, Jianye Hao, Mark Coates, Yingxue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14783">Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of general-purpose artificial intelligence relies on large language models (LLMs) that excel across a wide range of tasks, from structured reasoning to creative generation. However, post-training methods like Supervised Fine-Tuning (SFT) often struggle with generalization, favoring memorization over transferable learning. In this work, we introduce Omni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM performance across diverse tasks by combining rule-based verifiable rewards with generative preference signals via LLM-as-a-Judge evaluations. Our approach enables consistent optimization across task types and scales RL-based training to subjective domains. We further investigate training strategies, demonstrating that a curriculum-based progression that orders tasks from structured to open-ended improves performance and reduces forgetting. Experimental results across four domains reveal that curriculum learning improves performance by 5.2% over joint training and 9.1% over model merging. These results highlight the importance of task-aware sampling and hybrid supervision in scaling RL-based post-training for general-purpose LLMs.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2506.07603.pdf' target='_blank'>https://arxiv.org/pdf/2506.07603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhui Wei, Zikai Xiao, Danyu Sun, Luqi Gong, Zongxin Yang, Zuozhu Liu, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07603">SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical video understanding is pivotal for enabling automated intraoperative decision-making, skill assessment, and postoperative quality improvement. However, progress in developing surgical video foundation models (FMs) remains hindered by the scarcity of large-scale, diverse datasets for pretraining and systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a unified surgical video benchmarking framework comprising a pretraining dataset, \textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}. SurgBench offers extensive coverage of diverse surgical scenarios, with SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11 specialties, and SurgBench-E providing robust evaluation across six categories (phase classification, camera motion, tool recognition, disease diagnosis, action classification, and organ detection) spanning 72 fine-grained tasks. Extensive experiments reveal that existing video FMs struggle to generalize across varied surgical video analysis tasks, whereas pretraining on SurgBench-P yields substantial performance improvements and superior cross-domain generalization to unseen procedures and modalities. Our dataset and code are available upon request.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2503.12049.pdf' target='_blank'>https://arxiv.org/pdf/2503.12049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Lu, Yixin Chen, Yu Liu, Jiaxiang Tang, Junfeng Ni, Diwen Wan, Gang Zeng, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12049">TACO: Taming Diffusion for in-the-wild Video Amodal Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans can infer complete shapes and appearances of objects from limited visual cues, relying on extensive prior knowledge of the physical world. However, completing partially observable objects while ensuring consistency across video frames remains challenging for existing models, especially for unstructured, in-the-wild videos. This paper tackles the task of Video Amodal Completion (VAC), which aims to generate the complete object consistently throughout the video given a visual prompt specifying the object of interest. Leveraging the rich, consistent manifolds learned by pre-trained video diffusion models, we propose a conditional diffusion model, TACO, that repurposes these manifolds for VAC. To enable its effective and robust generalization to challenging in-the-wild scenarios, we curate a large-scale synthetic dataset with multiple difficulty levels by systematically imposing occlusions onto un-occluded videos. Building on this, we devise a progressive fine-tuning paradigm that starts with simpler recovery tasks and gradually advances to more complex ones. We demonstrate TACO's versatility on a wide range of in-the-wild videos from Internet, as well as on diverse, unseen datasets commonly used in autonomous driving, robotic manipulation, and scene understanding. Moreover, we show that TACO can be effectively applied to various downstream tasks like object reconstruction and pose estimation, highlighting its potential to facilitate physical world understanding and reasoning. Our project page is available at https://jason-aplp.github.io/TACO.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2502.01778.pdf' target='_blank'>https://arxiv.org/pdf/2502.01778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stavros Orfanoudakis, Nanda Kishor Panda, Peter Palensky, Pedro P. Vergara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01778">GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient exploration of the solution space. This study introduces GNN-DT, a novel Decision Transformer (DT) architecture that integrates Graph Neural Network (GNN) embedders with a novel residual connection between input and output tokens crucial for handling dynamic environments. By learning from previously collected trajectories, GNN-DT tackles the sparse rewards limitations of online RL algorithms and delivers high-quality solutions in real-time. We evaluate GNN-DT on the complex electric vehicle (EV) charging optimization problem and prove that its performance is superior and requires significantly fewer training trajectories, thus improving sample efficiency compared to existing DT and offline RL baselines. Furthermore, GNN-DT exhibits robust generalization to unseen environments and larger action spaces, addressing a critical gap in prior offline and online RL approaches.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2502.01778.pdf' target='_blank'>https://arxiv.org/pdf/2502.01778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stavros Orfanoudakis, Nanda Kishor Panda, Peter Palensky, Pedro P. Vergara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01778">GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient exploration of the solution space. This study introduces GNN-DT, a novel Decision Transformer (DT) architecture that integrates Graph Neural Network (GNN) embedders with a novel residual connection between input and output tokens crucial for handling dynamic environments. By learning from previously collected trajectories, GNN-DT tackles the sparse rewards limitations of online RL algorithms and delivers high-quality solutions in real-time. We evaluate GNN-DT on the complex electric vehicle (EV) charging optimization problem and prove that its performance is superior and requires significantly fewer training trajectories, thus improving sample efficiency compared to existing DT and offline RL baselines. Furthermore, GNN-DT exhibits robust generalization to unseen environments and larger action spaces, addressing a critical gap in prior offline and online RL approaches.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2501.00759.pdf' target='_blank'>https://arxiv.org/pdf/2501.00759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianshi Zheng, Jiazheng Wang, Zihao Wang, Jiaxin Bai, Hang Yin, Zheye Deng, Yangqiu Song, Jianxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00759">Enhancing Transformers for Generalizable First-Order Logical Entailment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformers, as the fundamental deep learning architecture, have demonstrated great capability in reasoning. This paper studies the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and how to improve it. Transformers' capability of first-order reasoning is further captured by whether they can conduct first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish the connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) unseen knowledge and query settings discussed in the task of knowledge graph query answering, which makes it possible to characterize the fine-grained generalizability. Results on our comprehensive dataset showed that transformers \textit{outperform} previous methods designed particularly for this task and provided detailed empirical evidence about the impact of the input query syntax, token embedding, and transformer architectures on their reasoning capability. Interestingly, our results revealed the mismatch of positional encoding and other design choices of transformer architectures in previous practices. Motivated by this, we propose TEGA, a logic-aware architecture that significantly improves the performance in generalizable first-order logical entailment.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2509.24803.pdf' target='_blank'>https://arxiv.org/pdf/2509.24803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Guan, Zijie Meng, Dianqi Li, Shiyu Wang, Chao-Han Huck Yang, Qingsong Wen, Zuozhu Liu, Sabato Marco Siniscalchi, Ming Jin, Shirui Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24803">TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2509.22407.pdf' target='_blank'>https://arxiv.org/pdf/2509.22407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhehao Dong, Xiaofeng Wang, Zheng Zhu, Yirui Wang, Yang Wang, Yukun Zhou, Boyuan Wang, Chaojun Ni, Runqi Ouyang, Wenkang Qin, Xinze Chen, Yun Ye, Guan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22407">EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2507.17512.pdf' target='_blank'>https://arxiv.org/pdf/2507.17512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17512">Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2505.16590.pdf' target='_blank'>https://arxiv.org/pdf/2505.16590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renyi Zhong, Yichen Li, Guangba Yu, Wenwei Gu, Jinxi Kuang, Yintong Huo, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16590">Larger Is Not Always Better: Exploring Small Open-source Language Models in Logging Statement Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developers use logging statements to create logs that document system behavior and aid in software maintenance. As such, high-quality logging is essential for effective maintenance; however, manual logging often leads to errors and inconsistency. Recent methods emphasize using large language models (LLMs) for automated logging statement generation, but these present privacy and resource issues, hindering their suitability for enterprise use. This paper presents the first large-scale empirical study evaluating small open-source language models (SOLMs) for automated logging statement generation. We evaluate four prominent SOLMs using various prompt strategies and parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG). Our results show that fine-tuned SOLMs with LoRA and RAG prompts, particularly Qwen2.5-coder-14B, outperform existing tools and LLM baselines in predicting logging locations and generating high-quality statements, with robust generalization across diverse repositories. These findings highlight SOLMs as a privacy-preserving, efficient alternative for automated logging.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2505.12762.pdf' target='_blank'>https://arxiv.org/pdf/2505.12762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenlin Ming, Chendi Qu, Mengzhang Cai, Qizhi Pei, Zhuoshi Pan, Yu Li, Xiaoming Duan, Lijun Wu, Conghui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12762">IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have achieved impressive performance through Supervised Fine-tuning (SFT) on diverse instructional datasets. When training on multiple capabilities simultaneously, the mixture training dataset, governed by volumes of data from different domains, is a critical factor that directly impacts the final model's performance. Unlike many studies that focus on enhancing the quality of training datasets through data selection methods, few works explore the intricate relationship between the compositional quantity of mixture training datasets and the emergent capabilities of LLMs. Given the availability of a high-quality multi-domain training dataset, understanding the impact of data from each domain on the model's overall capabilities is crucial for preparing SFT data and training a well-balanced model that performs effectively across diverse domains. In this work, we introduce IDEAL, an innovative data equilibrium adaptation framework designed to effectively optimize volumes of data from different domains within mixture SFT datasets, thereby enhancing the model's alignment and performance across multiple capabilities. IDEAL employs a gradient-based approach to iteratively refine the training data distribution, dynamically adjusting the volumes of domain-specific data based on their impact on downstream task performance. By leveraging this adaptive mechanism, IDEAL ensures a balanced dataset composition, enabling the model to achieve robust generalization and consistent proficiency across diverse tasks. Experiments across different capabilities demonstrate that IDEAL outperforms conventional uniform data allocation strategies, achieving a comprehensive improvement of approximately 7% in multi-task evaluation scores.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2504.06235.pdf' target='_blank'>https://arxiv.org/pdf/2504.06235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahryar Zehtabi, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06235">Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Much of federated learning (FL) focuses on settings where local dataset statistics remain the same between training and testing. However, this assumption often does not hold in practice due to distribution shifts, motivating the development of domain generalization (DG) approaches that leverage source domain data to train models capable of generalizing to unseen target domains. In this paper, we are motivated by two major gaps in existing work on FL and DG: (1) the lack of formal mathematical analysis of DG objectives; and (2) DG research in FL being limited to the star-topology architecture. We develop Decentralized Federated Domain Generalization with Style Sharing ($\textit{StyleDDG}$), a decentralized DG algorithm which allows devices in a peer-to-peer network to achieve DG based on sharing style information inferred from their datasets. Additionally, we provide the first systematic approach to analyzing style-based DG training in decentralized networks. We cast existing centralized DG algorithms within our framework, and employ their formalisms to model $\textit{StyleDDG}$. We then obtain analytical conditions under which convergence of $\textit{StyleDDG}$ can be guaranteed. Through experiments on popular DG datasets, we demonstrate that $\textit{StyleDDG}$ can obtain significant improvements in accuracy across target domains with minimal communication overhead compared to baseline decentralized gradient methods.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2504.06235.pdf' target='_blank'>https://arxiv.org/pdf/2504.06235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahryar Zehtabi, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06235">Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Much of federated learning (FL) focuses on settings where local dataset statistics remain the same between training and testing. However, this assumption often does not hold in practice due to distribution shifts, motivating the development of domain generalization (DG) approaches that leverage source domain data to train models capable of generalizing to unseen target domains. In this paper, we are motivated by two major gaps in existing work on FL and DG: (1) the lack of formal mathematical analysis of DG objectives; and (2) DG research in FL being limited to the star-topology architecture. We develop Decentralized Federated Domain Generalization with Style Sharing ($\textit{StyleDDG}$), a decentralized DG algorithm which allows devices in a peer-to-peer network to achieve DG based on sharing style information inferred from their datasets. Additionally, we provide the first systematic approach to analyzing style-based DG training in decentralized networks. We cast existing centralized DG algorithms within our framework, and employ their formalisms to model $\textit{StyleDDG}$. We then obtain analytical conditions under which convergence of $\textit{StyleDDG}$ can be guaranteed. Through experiments on popular DG datasets, we demonstrate that $\textit{StyleDDG}$ can obtain significant improvements in accuracy across target domains with minimal communication overhead compared to baseline decentralized gradient methods.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2503.13935.pdf' target='_blank'>https://arxiv.org/pdf/2503.13935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Yuan, Yuxia Fu, Zijian Wang, Yadan Luo, Zi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13935">SCORE: Soft Label Compression-Centric Dataset Condensation via Coding Rate Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dataset Condensation (DC) aims to obtain a condensed dataset that allows models trained on the condensed dataset to achieve performance comparable to those trained on the full dataset. Recent DC approaches increasingly focus on encoding knowledge into realistic images with soft labeling, for their scalability to ImageNet-scale datasets and strong capability of cross-domain generalization. However, this strong performance comes at a substantial storage cost which could significantly exceed the storage cost of the original dataset. We argue that the three key properties to alleviate this performance-storage dilemma are informativeness, discriminativeness, and compressibility of the condensed data. Towards this end, this paper proposes a \textbf{S}oft label compression-centric dataset condensation framework using \textbf{CO}ding \textbf{R}at\textbf{E} (SCORE). SCORE formulates dataset condensation as a min-max optimization problem, which aims to balance the three key properties from an information-theoretic perspective. In particular, we theoretically demonstrate that our coding rate-inspired objective function is submodular, and its optimization naturally enforces low-rank structure in the soft label set corresponding to each condensed data. Extensive experiments on large-scale datasets, including ImageNet-1K and Tiny-ImageNet, demonstrate that SCORE outperforms existing methods in most cases. Even with 30$\times$ compression of soft labels, performance decreases by only 5.5\% and 2.7\% for ImageNet-1K with IPC 10 and 50, respectively. Code will be released upon paper acceptance.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2411.05824.pdf' target='_blank'>https://arxiv.org/pdf/2411.05824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixian Su, Jingwei Guo, Xi Yang, Qiufeng Wang, Frans Coenen, Kaizhu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05824">Navigating Distribution Shifts in Medical Image Analysis: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical Image Analysis (MedIA) has become indispensable in modern healthcare, enhancing clinical diagnostics and personalized treatment. Despite the remarkable advancements supported by deep learning (DL) technologies, their practical deployment faces challenges due to distribution shifts, where models trained on specific datasets underperform across others from varying hospitals, regions, or patient populations. To navigate this issue, researchers have been actively developing strategies to increase the adaptability and robustness of DL models, enabling their effective use in unfamiliar and diverse environments. This paper systematically reviews approaches that apply DL techniques to MedIA systems affected by distribution shifts. Unlike traditional categorizations based on technical specifications, our approach is grounded in the real-world operational constraints faced by healthcare institutions. Specifically, we categorize the existing body of work into Joint Training, Federated Learning, Fine-tuning, and Domain Generalization, with each method tailored to distinct scenarios caused by Data Accessibility, Privacy Concerns, and Collaborative Protocols. This perspective equips researchers with a nuanced understanding of how DL can be strategically deployed to address distribution shifts in MedIA, ensuring diverse and robust medical applications. By delving deeper into these topics, we highlight potential pathways for future research that not only address existing limitations but also push the boundaries of deployable MedIA technologies.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2411.00553.pdf' target='_blank'>https://arxiv.org/pdf/2411.00553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Mancusi, Mattia Bernardi, Aniello Panariello, Angelo Porrello, Rita Cucchiara, Simone Calderara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00553">Is Multiple Object Tracking a Matter of Specialization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end transformer-based trackers have achieved remarkable performance on most human-related datasets. However, training these trackers in heterogeneous scenarios poses significant challenges, including negative interference - where the model learns conflicting scene-specific parameters - and limited domain generalization, which often necessitates expensive fine-tuning to adapt the models to new domains. In response to these challenges, we introduce Parameter-efficient Scenario-specific Tracking Architecture (PASTA), a novel framework that combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL). Specifically, we define key scenario attributes (e.g, camera-viewpoint, lighting condition) and train specialized PEFT modules for each attribute. These expert modules are combined in parameter space, enabling systematic generalization to new domains without increasing inference time. Extensive experiments on MOTSynth, along with zero-shot evaluations on MOT17 and PersonPath22 demonstrate that a neural tracker built from carefully selected modules surpasses its monolithic counterpart. We release models and code.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2410.04492.pdf' target='_blank'>https://arxiv.org/pdf/2410.04492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04492">Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2409.13527.pdf' target='_blank'>https://arxiv.org/pdf/2409.13527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avi Deb Raha, Apurba Adhikary, Mrityunjoy Gain, Yu Qiao, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13527">Boosting Federated Domain Generalization: Understanding the Role of Advanced Pre-Trained Architectures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we explore the efficacy of advanced pre-trained architectures, such as Vision Transformers (ViT), ConvNeXt, and Swin Transformers in enhancing Federated Domain Generalization. These architectures capture global contextual features and model long-range dependencies, making them promising candidates for improving cross-domain generalization. We conduct a broad study with in-depth analysis and systematically evaluate different variants of these architectures, using extensive pre-training datasets such as ImageNet-1K, ImageNet-21K, JFT-300M, and ImageNet-22K. Additionally, we compare self-supervised and supervised pre-training strategies to assess their impact on FDG performance. Our findings suggest that self-supervised techniques, which focus on reconstructing masked image patches, can better capture the intrinsic structure of images, thereby outperforming their supervised counterparts. Comprehensive evaluations on the Office-Home and PACS datasets demonstrate that adopting advanced architectures pre-trained on larger datasets establishes new benchmarks, achieving average accuracies of 84.46\% and 92.55\%, respectively. Additionally, we observe that certain variants of these advanced models, despite having fewer parameters, outperform larger ResNet models. This highlights the critical role of utilizing sophisticated architectures and diverse pre-training strategies to enhance FDG performance, especially in scenarios with limited computational resources where model efficiency is crucial. Our results indicate that federated learning systems can become more adaptable and efficient by leveraging these advanced methods, offering valuable insights for future research in FDG.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2401.13898.pdf' target='_blank'>https://arxiv.org/pdf/2401.13898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huy Q. Le, Chu Myaet Thwal, Yu Qiao, Ye Lin Tun, Minh N. H. Nguyen, Eui-Nam Huh, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13898">Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal federated learning (MFL) has emerged as a decentralized machine learning paradigm, allowing multiple clients with different modalities to collaborate on training a global model across diverse data sources without sharing their private data. However, challenges, such as data heterogeneity and severely missing modalities, pose crucial hindrances to the robustness of MFL, significantly impacting the performance of global model. The occurrence of missing modalities in real-world applications, such as autonomous driving, often arises from factors like sensor failures, leading knowledge gaps during the training process. Specifically, the absence of a modality introduces misalignment during the local training phase, stemming from zero-filling in the case of clients with missing modalities. Consequently, achieving robust generalization in global model becomes imperative, especially when dealing with clients that have incomplete data. In this paper, we propose $\textbf{Multimodal Federated Cross Prototype Learning (MFCPL)}$, a novel approach for MFL under severely missing modalities. Our MFCPL leverages the complete prototypes to provide diverse modality knowledge in modality-shared level with the cross-modal regularization and modality-specific level with cross-modal contrastive mechanism. Additionally, our approach introduces the cross-modal alignment to provide regularization for modality-specific features, thereby enhancing the overall performance, particularly in scenarios involving severely missing modalities. Through extensive experiments on three multimodal datasets, we demonstrate the effectiveness of MFCPL in mitigating the challenges of data heterogeneity and severely missing modalities while improving the overall performance and robustness of MFL.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2311.16754.pdf' target='_blank'>https://arxiv.org/pdf/2311.16754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Senkang Hu, Zhengru Fang, Yiqin Deng, Xianhao Chen, Yuguang Fang, Sam Kwong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16754">Towards Full-scene Domain Generalization in Multi-agent Collaborative Bird's Eye View Segmentation for Connected and Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative perception has recently gained significant attention in autonomous driving, improving perception quality by enabling the exchange of additional information among vehicles. However, deploying collaborative perception systems can lead to domain shifts due to diverse environmental conditions and data heterogeneity among connected and autonomous vehicles (CAVs). To address these challenges, we propose a unified domain generalization framework to be utilized during the training and inference stages of collaborative perception. In the training phase, we introduce an Amplitude Augmentation (AmpAug) method to augment low-frequency image variations, broadening the model's ability to learn across multiple domains. We also employ a meta-consistency training scheme to simulate domain shifts, optimizing the model with a carefully designed consistency loss to acquire domain-invariant representations. In the inference phase, we introduce an intra-system domain alignment mechanism to reduce or potentially eliminate the domain discrepancy among CAVs prior to inference. Extensive experiments substantiate the effectiveness of our method in comparison with the existing state-of-the-art works.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2311.00227.pdf' target='_blank'>https://arxiv.org/pdf/2311.00227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungwuk Park, Dong-Jun Han, Jinho Kim, Shiqiang Wang, Christopher G. Brinton, Jaekyun Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00227">StableFDG: Style and Attention Based Learning for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client's local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2206.04046.pdf' target='_blank'>https://arxiv.org/pdf/2206.04046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.04046">Sparse Mixture-of-Experts are Domain Generalizable Learners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human visual perception can easily generalize to out-of-distributed visual data, which is far beyond the capability of modern machine learning models. Domain generalization (DG) aims to close this gap, with existing DG methods mainly focusing on the loss function design. In this paper, we propose to explore an orthogonal direction, i.e., the design of the backbone architecture. It is motivated by an empirical finding that transformer-based models trained with empirical risk minimization (ERM) outperform CNN-based models employing state-of-the-art (SOTA) DG algorithms on multiple DG datasets. We develop a formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset. This analysis guides us to propose a novel DG model built upon vision transformers, namely Generalizable Mixture-of-Experts (GMoE). Extensive experiments on DomainBed demonstrate that GMoE trained with ERM outperforms SOTA DG baselines by a large margin. Moreover, GMoE is complementary to existing DG methods and its performance is substantially improved when trained with DG algorithms.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2108.11726.pdf' target='_blank'>https://arxiv.org/pdf/2108.11726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, Mahsa Baktashmotlagh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.11726">Learning to Diversify for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to generalize a model trained on multiple source (i.e., training) domains to a distributionally different target (i.e., test) domain. In contrast to the conventional DG that strictly requires the availability of multiple source domains, this paper considers a more realistic yet challenging scenario, namely Single Domain Generalization (Single-DG), where only one source domain is available for training. In this scenario, the limited diversity may jeopardize the model generalization on unseen target domains. To tackle this problem, we propose a style-complement module to enhance the generalization power of the model by synthesizing images from diverse distributions that are complementary to the source ones. More specifically, we adopt a tractable upper bound of mutual information (MI) between the generated and source samples and perform a two-step optimization iteratively: (1) by minimizing the MI upper bound approximation for each sample pair, the generated images are forced to be diversified from the source samples; (2) subsequently, we maximize the MI between the samples from the same semantic category, which assists the network to learn discriminative features from diverse-styled images. Extensive experiments on three benchmark datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 25.14%.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2510.04225.pdf' target='_blank'>https://arxiv.org/pdf/2510.04225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikun Ji, Yan Hong, Bowen Deng, jun lan, Huijia Zhu, Weiqiang Wang, Liqing Zhang, Jianfu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04225">Zoom-In to Sort AI-Generated Images Out</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of AI-generated imagery has blurred the boundary between real and synthetic content, raising critical concerns for digital integrity. Vision-language models (VLMs) offer interpretability through explanations but often fail to detect subtle artifacts in high-quality synthetic images. We propose ZoomIn, a two-stage forensic framework that improves both accuracy and interpretability. Mimicking human visual inspection, ZoomIn first scans an image to locate suspicious regions and then performs a focused analysis on these zoomed-in areas to deliver a grounded verdict. To support training, we introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images annotated with bounding boxes and forensic explanations, generated through an automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust generalization, while providing human-understandable explanations grounded in visual evidence.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2509.12275.pdf' target='_blank'>https://arxiv.org/pdf/2509.12275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghua Zhao, Hang Su, Lichun Fan, Zhenbo Luo, Hui Wang, Haoqin Sun, Yong Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12275">Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid progress of large audio-language models (LALMs), audio question answering (AQA) has emerged as a challenging task requiring both fine-grained audio understanding and complex reasoning. While current methods mainly rely on constructing new datasets via captioning or reasoning traces, existing high-quality AQA data remains underutilized. To address this, we propose Omni-CLST, an error-aware Curriculum Learning framework with guided Selective Chain-of-Thought. The framework efficiently leverages existing high-quality dataset through two key strategies: an error-aware curriculum that organizes samples by difficulty, and a guided thought dropout mechanism that focuses reasoning on challenging cases. Experiments show that Omni-CLST achieves 73.80% on MMAU-mini and a new state of the art of 64.30% on MMAR, demonstrating robust generalization in multimodal audio-language understanding.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2503.13966.pdf' target='_blank'>https://arxiv.org/pdf/2503.13966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Zhang, Yanyuan Qiao, Qunbo Wang, Longteng Guo, Zhihua Wei, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13966">FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aspiration of the Vision-and-Language Navigation (VLN) task has long been to develop an embodied agent with robust adaptability, capable of seamlessly transferring its navigation capabilities across various tasks. Despite remarkable advancements in recent years, most methods necessitate dataset-specific training, thereby lacking the capability to generalize across diverse datasets encompassing distinct types of instructions. Large language models (LLMs) have demonstrated exceptional reasoning and generalization abilities, exhibiting immense potential in robot action planning. In this paper, we propose FlexVLN, an innovative hierarchical approach to VLN that integrates the fundamental navigation ability of a supervised-learning-based Instruction Follower with the robust generalization ability of the LLM Planner, enabling effective generalization across diverse VLN datasets. Moreover, a verification mechanism and a multi-model integration mechanism are proposed to mitigate potential hallucinations by the LLM Planner and enhance execution accuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as out-of-domain datasets for assessing generalization ability. The generalization performance of FlexVLN surpasses that of all the previous methods to a large extent.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2412.05551.pdf' target='_blank'>https://arxiv.org/pdf/2412.05551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Jiang, Yuan Meng, Chen Tang, Han Yu, Qun Li, Zhi Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05551">GAQAT: gradient-adaptive quantization-aware training for domain generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on loss surface geometry, such as Sharpness-Aware Minimization (SAM), shows that flatter minima improve generalization. Recent studies further reveal that flatter minima can also reduce the domain generalization (DG) gap. However, existing flatness-based DG techniques predominantly operate within a full-precision training process, which is impractical for deployment on resource-constrained edge devices that typically rely on lower bit-width representations (e.g., 4 bits, 3 bits). Consequently, low-precision quantization-aware training is critical for optimizing these techniques in real-world applications. In this paper, we observe a significant degradation in performance when applying state-of-the-art DG-SAM methods to quantized models, suggesting that current approaches fail to preserve generalizability during the low-precision training process. To address this limitation, we propose a novel Gradient-Adaptive Quantization-Aware Training (GAQAT) framework for DG. Our approach begins by identifying the scale-gradient conflict problem in low-precision quantization, where the task loss and smoothness loss induce conflicting gradients for the scaling factors of quantizers, with certain layers exhibiting opposing gradient directions. This conflict renders the optimization of quantized weights highly unstable. To mitigate this, we further introduce a mechanism to quantify gradient inconsistencies and selectively freeze the gradients of scaling factors, thereby stabilizing the training process and enhancing out-of-domain generalization. Extensive experiments validate the effectiveness of the proposed GAQAT framework. On PACS, our 3-bit and 4-bit models outperform direct DG-QAT integration by up to 4.5%. On DomainNet, the 4-bit model achieves near-lossless performance compared to full precision, with improvements of 1.39% (4-bit) and 1.06% (3-bit) over the SOTA QAT baseline.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2409.13787.pdf' target='_blank'>https://arxiv.org/pdf/2409.13787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Hu, Chenwei Zhang, Min Yang, Xiaodan Liang, Chengming Li, Xiping Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13787">Learning to Generalize Unseen Domains via Multi-Source Meta Learning for Text Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of deep learning methods, there have been many breakthroughs in the field of text classification. Models developed for this task have been shown to achieve high accuracy. However, most of these models are trained using labeled data from seen domains. It is difficult for these models to maintain high accuracy in a new challenging unseen domain, which is directly related to the generalization of the model. In this paper, we study the multi-source Domain Generalization of text classification and propose a framework to use multiple seen domains to train a model that can achieve high accuracy in an unseen domain. Specifically, we propose a multi-source meta-learning Domain Generalization framework to simulate the process of model generalization to an unseen domain, so as to extract sufficient domain-related features. We introduced a memory mechanism to store domain-specific features, which coordinate with the meta-learning framework. Besides, we adopt the novel "jury" mechanism that enables the model to learn sufficient domain-invariant features. Experiments demonstrate that our meta-learning framework can effectively enhance the ability of the model to generalize to an unseen domain and can outperform the state-of-the-art methods on multi-source text classification datasets.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2408.09138.pdf' target='_blank'>https://arxiv.org/pdf/2408.09138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiao Zhang, Jian Xu, Xu-Yao Zhang, Cheng-Lin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09138">StylePrompter: Enhancing Domain Generalization with Test-Time Style Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world applications, the sample distribution at the inference stage often differs from the one at the training stage, causing performance degradation of trained deep models. The research on domain generalization (DG) aims to develop robust algorithms that can improve the generalized performance in unseen domains by training on a few domains. However, the domain-agnostic vision model, trained on a limited number of domains using traditional domain generalization methods, cannot guarantee its effectiveness in dealing with unseen domains. The introduction of language can break the closed cognition space of the vision model, providing additional semantic information that cannot be inferred from vision-only datasets. In this paper, we propose to overcome the challenge in previous DG methods by introducing the style prompt in the language modality to adapt the trained model dynamically. In particular, we train a style prompter to extract style information of the current image into an embedding in the token embedding space and place it in front of the candidate category words as prior knowledge to prompt the model. Our open space partition of the style token embedding space and the hand-crafted style regularization enable the trained style prompter to handle data from unknown domains effectively. Extensive experiments verify the effectiveness of our method and demonstrate state-of-the-art performances on multiple public datasets. Codes will be available after the acceptance of this paper.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2407.04603.pdf' target='_blank'>https://arxiv.org/pdf/2407.04603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Zhu, Yuyang Ji, Zhiyu Zhao, Gangshan Wu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04603">AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks. However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes. To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space. AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module. We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2404.18758.pdf' target='_blank'>https://arxiv.org/pdf/2404.18758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyuan Wang, Yan Jin, Zhen Chen, Jinlin Wu, Mengke Li, Yang Lu, Hanzi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18758">Transitive Vision-Language Prompt Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision-language pre-training has enabled deep models to make a huge step forward in generalizing across unseen domains. The recent learning method based on the vision-language pre-training model is a great tool for domain generalization and can solve this problem to a large extent. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. In this paper, we introduce a novel prompt learning strategy that leverages deep vision prompts to address domain invariance while utilizing language prompts to ensure class separability, coupled with adaptive weighting mechanisms to balance domain invariance and class separability. Extensive experiments demonstrate that deep vision prompts effectively extract domain-invariant features, significantly improving the generalization ability of deep models and achieving state-of-the-art performance on three datasets.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2404.09011.pdf' target='_blank'>https://arxiv.org/pdf/2404.09011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zining Chen, Weiqiu Wang, Zhicheng Zhao, Fei Su, Aidong Men, Hongying Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09011">PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to resolve distribution shifts between source and target domains, and current DG methods are default to the setting that data from source and target domains share identical categories. Nevertheless, there exists unseen classes from target domains in practical scenarios. To address this issue, Open Set Domain Generalization (OSDG) has emerged and several methods have been exclusively proposed. However, most existing methods adopt complex architectures with slight improvement compared with DG methods. Recently, vision-language models (VLMs) have been introduced in DG following the fine-tuning paradigm, but consume huge training overhead with large vision models. Therefore, in this paper, we innovate to transfer knowledge from VLMs to lightweight vision models and improve the robustness by introducing Perturbation Distillation (PD) from three perspectives, including Score, Class and Instance (SCI), named SCI-PD. Moreover, previous methods are oriented by the benchmarks with identical and fixed splits, ignoring the divergence between source domains. These methods are revealed to suffer from sharp performance decay with our proposed new benchmark Hybrid Domain Generalization (HDG) and a novel metric $H^{2}$-CV, which construct various splits to comprehensively assess the robustness of algorithms. Extensive experiments demonstrate that our method outperforms state-of-the-art algorithms on multiple datasets, especially improving the robustness when confronting data scarcity.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2403.08506.pdf' target='_blank'>https://arxiv.org/pdf/2403.08506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sikai Bai, Jie Zhang, Shuaicheng Li, Song Guo, Jingcai Guo, Jun Hou, Tao Han, Xiaocheng Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08506">DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation. Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2311.11114.pdf' target='_blank'>https://arxiv.org/pdf/2311.11114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Yuan, Qingyun Sun, Xingcheng Fu, Ziwei Zhang, Cheng Ji, Hao Peng, Jianxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11114">Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic graph neural networks (DGNNs) are increasingly pervasive in exploiting spatio-temporal patterns on dynamic graphs. However, existing works fail to generalize under distribution shifts, which are common in real-world scenarios. As the generation of dynamic graphs is heavily influenced by latent environments, investigating their impacts on the out-of-distribution (OOD) generalization is critical. However, it remains unexplored with the following two major challenges: (1) How to properly model and infer the complex environments on dynamic graphs with distribution shifts? (2) How to discover invariant patterns given inferred spatio-temporal environments? To solve these challenges, we propose a novel Environment-Aware dynamic Graph LEarning (EAGLE) framework for OOD generalization by modeling complex coupled environments and exploiting spatio-temporal invariant patterns. Specifically, we first design the environment-aware EA-DGNN to model environments by multi-channel environments disentangling. Then, we propose an environment instantiation mechanism for environment diversification with inferred distributions. Finally, we discriminate spatio-temporal invariant patterns for out-of-distribution prediction by the invariant pattern recognition mechanism and perform fine-grained causal interventions node-wisely with a mixture of instantiated environment samples. Experiments on real-world and synthetic dynamic graph datasets demonstrate the superiority of our method against state-of-the-art baselines under distribution shifts. To the best of our knowledge, we are the first to study OOD generalization on dynamic graphs from the environment learning perspective.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2310.00757.pdf' target='_blank'>https://arxiv.org/pdf/2310.00757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soroosh Tayebi Arasteh, Christiane Kuhl, Marwin-Jonathan Saehn, Peter Isfort, Daniel Truhn, Sven Nebelung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00757">Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing robust artificial intelligence (AI) models that generalize well to unseen datasets is challenging and usually requires large and variable datasets, preferably from multiple institutions. In federated learning (FL), a model is trained collaboratively at numerous sites that hold local datasets without exchanging them. So far, the impact of training strategy, i.e., local versus collaborative, on the diagnostic on-domain and off-domain performance of AI models interpreting chest radiographs has not been assessed. Consequently, using 610,000 chest radiographs from five institutions across the globe, we assessed diagnostic performance as a function of training strategy (i.e., local vs. collaborative), network architecture (i.e., convolutional vs. transformer-based), generalization performance (i.e., on-domain vs. off-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia, atelectasis, consolidation, pneumothorax, and no abnormality), dataset size (i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large datasets not only showed minimal performance gains with FL but, in some instances, even exhibited decreases. In contrast, smaller datasets revealed marked improvements. Thus, on-domain performance was mainly driven by training data size. However, off-domain performance leaned more on training diversity. When trained collaboratively across diverse external institutions, AI models consistently surpassed models trained locally for off-domain tasks, emphasizing FL's potential in leveraging data diversity. In conclusion, FL can bolster diagnostic privacy, reproducibility, and off-domain reliability of AI models and, potentially, optimize healthcare outcomes.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2309.06142.pdf' target='_blank'>https://arxiv.org/pdf/2309.06142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiao Zhang, Xu-Yao Zhang, Cheng-Lin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06142">Towards Reliable Domain Generalization: A New Dataset and Evaluations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There are ubiquitous distribution shifts in the real world. However, deep neural networks (DNNs) are easily biased towards the training set, which causes severe performance degradation when they receive out-of-distribution data. Many methods are studied to train models that generalize under various distribution shifts in the literature of domain generalization (DG). However, the recent DomainBed and WILDS benchmarks challenged the effectiveness of these methods. Aiming at the problems in the existing research, we propose a new domain generalization task for handwritten Chinese character recognition (HCCR) to enrich the application scenarios of DG method research. We evaluate eighteen DG methods on the proposed PaHCC (Printed and Handwritten Chinese Characters) dataset and show that the performance of existing methods on this dataset is still unsatisfactory. Besides, under a designed dynamic DG setting, we reveal more properties of DG methods and argue that only the leave-one-domain-out protocol is unreliable. We advocate that researchers in the DG community refer to dynamic performance of methods for more comprehensive and reliable evaluation. Our dataset and evaluations bring new perspectives to the community for more substantial progress. We will make our dataset public with the article published to facilitate the study of domain generalization.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2309.03208.pdf' target='_blank'>https://arxiv.org/pdf/2309.03208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihai Wang, Lei Chen, Jie Wang, Xing Li, Yinqi Bai, Xijun Li, Mingxuan Yuan, Jianye Hao, Yongdong Zhang, Feng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03208">A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Logic Synthesis (LS) plays a vital role in chip design -- a cornerstone of the semiconductor industry. A key task in LS is to transform circuits -- modeled by directed acyclic graphs (DAGs) -- into simplified circuits with equivalent functionalities. To tackle this task, many LS operators apply transformations to subgraphs -- rooted at each node on an input DAG -- sequentially. However, we found that a large number of transformations are ineffective, which makes applying these operators highly time-consuming. In particular, we notice that the runtime of the Resub and Mfs2 operators often dominates the overall runtime of LS optimization processes. To address this challenge, we propose a novel data-driven LS operator paradigm, namely PruneX, to reduce ineffective transformations. The major challenge of developing PruneX is to learn models that well generalize to unseen circuits, i.e., the out-of-distribution (OOD) generalization problem. Thus, the major technical contribution of PruneX is the novel circuit domain generalization framework, which learns domain-invariant representations based on the transformation-invariant domain-knowledge. To the best of our knowledge, PruneX is the first approach to tackle the OOD problem in LS operators. We integrate PruneX with the aforementioned Resub and Mfs2 operators. Experiments demonstrate that PruneX significantly improves their efficiency while keeping comparable optimization performance on industrial and very large-scale circuits, achieving up to $3.1\times$ faster runtime.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2205.10664.pdf' target='_blank'>https://arxiv.org/pdf/2205.10664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangji Bai, Chen Ling, Liang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.10664">Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal domain generalization is a promising yet extremely challenging area where the goal is to learn models under temporally changing data distributions and generalize to unseen data distributions following the trends of the change. The advancement of this area is challenged by: 1) characterizing data distribution drift and its impacts on models, 2) expressiveness in tracking the model dynamics, and 3) theoretical guarantee on the performance. To address them, we propose a Temporal Domain Generalization with Drift-Aware Dynamic Neural Network (DRAIN) framework. Specifically, we formulate the problem into a Bayesian framework that jointly models the relation between data and model dynamics. We then build a recurrent graph generation scenario to characterize the dynamic graph-structured neural networks learned across different time points. It captures the temporal drift of model parameters and data distributions and can predict models in the future without the presence of future data. In addition, we explore theoretical guarantees of the model performance under the challenging temporal DG setting and provide theoretical analysis, including uncertainty and generalization error. Finally, extensive experiments on several real-world benchmarks with temporal drift demonstrate the effectiveness and efficiency of the proposed method.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2408.03297.pdf' target='_blank'>https://arxiv.org/pdf/2408.03297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, Yasha Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03297">KnowPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors such as contextual ignorance and contextual overinclusion. To this end, we propose a Knowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at achieving adaptive knowledge selection based on contextual relevance in real retrieval scenarios. Concretely, we proposed a general paradigm for constructing knowledge conflict datasets, which comprehensively cover various error types and learn how to avoid these negative signals through preference optimization methods. Simultaneously, we proposed a rewriting strategy and data ratio optimization strategy to address preference imbalances. Experimental results show that KnowPO outperforms previous methods for handling knowledge conflicts by over 37\%, while also exhibiting robust generalization across various out-of-distribution datasets.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2407.09475.pdf' target='_blank'>https://arxiv.org/pdf/2407.09475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinning Li, Jiachen Li, Sangjae Bae, David Isele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09475">Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based trajectory prediction models for autonomous driving often struggle with generalization to out-of-distribution (OOD) scenarios, sometimes performing worse than simple rule-based models. To address this limitation, we propose a novel framework, Adaptive Prediction Ensemble (APE), which integrates deep learning and rule-based prediction experts. A learned routing function, trained concurrently with the deep learning model, dynamically selects the most reliable prediction based on the input scenario. Our experiments on large-scale datasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate improvement in zero-shot generalization across datasets. We show that our method outperforms individual prediction models and other variants, particularly in long-horizon prediction and scenarios with a high proportion of OOD data. This work highlights the potential of hybrid approaches for robust and generalizable motion prediction in autonomous driving. More details can be found on the project page: https://sites.google.com/view/ape-generalization.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2406.11517.pdf' target='_blank'>https://arxiv.org/pdf/2406.11517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Qin, Jiangmeng Li, Yi Li, Xuesong Wu, Yupeng Wang, Wenwen Qiang, Jianwen Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11517">Revisiting Spurious Correlation in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Without loss of generality, existing machine learning techniques may learn spurious correlation dependent on the domain, which exacerbates the generalization of models in out-of-distribution (OOD) scenarios. To address this issue, recent works build a structural causal model (SCM) to describe the causality within data generation process, thereby motivating methods to avoid the learning of spurious correlation by models. However, from the machine learning viewpoint, such a theoretical analysis omits the nuanced difference between the data generation process and representation learning process, resulting in that the causal analysis based on the former cannot well adapt to the latter. To this end, we explore to build a SCM for representation learning process and further conduct a thorough analysis of the mechanisms underlying spurious correlation. We underscore that adjusting erroneous covariates introduces bias, thus necessitating the correct selection of spurious correlation mechanisms based on practical application scenarios. In this regard, we substantiate the correctness of the proposed SCM and further propose to control confounding bias in OOD generalization by introducing a propensity score weighted estimator, which can be integrated into any existing OOD method as a plug-and-play module. The empirical results comprehensively demonstrate the effectiveness of our method on synthetic and large-scale real OOD datasets.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2402.18853.pdf' target='_blank'>https://arxiv.org/pdf/2402.18853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaorui Tan, Xi Yang, Kaizhu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18853">Rethinking Multi-domain Generalization with A General Learning Objective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \textbf{optimize partially the objective} and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2305.13752.pdf' target='_blank'>https://arxiv.org/pdf/2305.13752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Liwei Wu, Yuxi Wang, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13752">Pulling Target to Source: A New Perspective on Domain Adaptive Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptive semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. However, existing methods primarily focus on directly learning qualified target features, making it challenging to guarantee their discrimination in the absence of target labels. This work provides a new perspective. We observe that the features learned with source data manage to keep categorically discriminative during training, thereby enabling us to implicitly learn adequate target representations by simply \textbf{pulling target features close to source features for each category}. To this end, we propose T2S-DA, which we interpret as a form of pulling Target to Source for Domain Adaptation, encouraging the model in learning similar cross-domain features. Also, considering the pixel categories are heavily imbalanced for segmentation datasets, we come up with a dynamic re-weighting strategy to help the model concentrate on those underperforming classes. Extensive experiments confirm that T2S-DA learns a more discriminative and generalizable representation, significantly surpassing the state-of-the-art. We further show that our method is quite qualified for the domain generalization task, verifying its domain-invariant property.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2304.01029.pdf' target='_blank'>https://arxiv.org/pdf/2304.01029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Angarano, Mauro Martini, Alessandro Navone, Marcello Chiaberge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01029">Domain Generalization for Crop Segmentation with Standardized Ensemble Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, precision agriculture has gradually oriented farming closer to automation processes to support all the activities related to field management. Service robotics plays a predominant role in this evolution by deploying autonomous agents that can navigate fields while performing tasks such as monitoring, spraying, and harvesting without human intervention. To execute these precise actions, mobile robots need a real-time perception system that understands their surroundings and identifies their targets in the wild. Existing methods, however, often fall short in generalizing to new crops and environmental conditions. This limit is critical for practical applications where labeled samples are rarely available. In this paper, we investigate the problem of crop segmentation and propose a novel approach to enhance domain generalization using knowledge distillation. In the proposed framework, we transfer knowledge from a standardized ensemble of models individually trained on source domains to a student model that can adapt to unseen realistic scenarios. To support the proposed method, we present a synthetic multi-domain dataset for crop segmentation containing plants of variegate species and covering different terrain styles, weather conditions, and light scenarios for more than 70,000 samples. We demonstrate significant improvements in performance over state-of-the-art methods and superior sim-to-real generalization. Our approach provides a promising solution for domain generalization in crop segmentation and has the potential to enhance a wide variety of agriculture applications.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2211.02733.pdf' target='_blank'>https://arxiv.org/pdf/2211.02733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhai Xu, Han Zhang, Yasaman Sefidgar, Yiyi Ren, Xin Liu, Woosuk Seo, Jennifer Brown, Kevin Kuehn, Mike Merrill, Paula Nurius, Shwetak Patel, Tim Althoff, Margaret E. Morris, Eve Riskin, Jennifer Mankoff, Anind K. Dey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.02733">GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 user-years and 497 unique users' data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithms' generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2209.15042.pdf' target='_blank'>https://arxiv.org/pdf/2209.15042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumail Alhamoud, Hasan Abed Al Kader Hammoud, Motasem Alfarra, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.15042">Generalizability of Adversarial Robustness Under Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in empirical and certified robustness promises to deliver reliable and deployable Deep Neural Networks (DNNs). Despite that success, most existing evaluations of DNN robustness have been done on images sampled from the same distribution on which the model was trained. However, in the real world, DNNs may be deployed in dynamic environments that exhibit significant distribution shifts. In this work, we take a first step towards thoroughly investigating the interplay between empirical and certified adversarial robustness on one hand and domain generalization on another. To do so, we train robust models on multiple domains and evaluate their accuracy and robustness on an unseen domain. We observe that: (1) both empirical and certified robustness generalize to unseen domains, and (2) the level of generalizability does not correlate well with input visual similarity, measured by the FID between source and target domains. We also extend our study to cover a real-world medical application, in which adversarial augmentation significantly boosts the generalization of robustness with minimal effect on clean data accuracy.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2508.20835.pdf' target='_blank'>https://arxiv.org/pdf/2508.20835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Qianyu Zhou, Haijia Sun, Xiangtai Li, Xuequan Lu, Lizhuang Ma, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20835">PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) has been recently explored to enhance the generalizability of Point Cloud Classification (PCC) models toward unseen domains. Prior works are based on convolutional networks, Transformer or Mamba architectures, either suffering from limited receptive fields or high computational cost, or insufficient long-range dependency modeling. RWKV, as an emerging architecture, possesses superior linear complexity, global receptive fields, and long-range dependency. In this paper, we present the first work that studies the generalizability of RWKV models in DG PCC. We find that directly applying RWKV to DG PCC encounters two significant challenges: RWKV's fixed direction token shift methods, like Q-Shift, introduce spatial distortions when applied to unstructured point clouds, weakening local geometric modeling and reducing robustness. In addition, the Bi-WKV attention in RWKV amplifies slight cross-domain differences in key distributions through exponential weighting, leading to attention shifts and degraded generalization. To this end, we propose PointDGRWKV, the first RWKV-based framework tailored for DG PCC. It introduces two key modules to enhance spatial modeling and cross-domain robustness, while maintaining RWKV's linear efficiency. In particular, we present Adaptive Geometric Token Shift to model local neighborhood structures to improve geometric context awareness. In addition, Cross-Domain key feature Distribution Alignment is designed to mitigate attention drift by aligning key feature distributions across domains. Extensive experiments on multiple benchmarks demonstrate that PointDGRWKV achieves state-of-the-art performance on DG PCC.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2508.20835.pdf' target='_blank'>https://arxiv.org/pdf/2508.20835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Qianyu Zhou, Haijia Sun, Xiangtai Li, Xuequan Lu, Lizhuang Ma, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20835">PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) has been recently explored to enhance the generalizability of Point Cloud Classification (PCC) models toward unseen domains. Prior works are based on convolutional networks, Transformer or Mamba architectures, either suffering from limited receptive fields or high computational cost, or insufficient long-range dependency modeling. RWKV, as an emerging architecture, possesses superior linear complexity, global receptive fields, and long-range dependency. In this paper, we present the first work that studies the generalizability of RWKV models in DG PCC. We find that directly applying RWKV to DG PCC encounters two significant challenges: RWKV's fixed direction token shift methods, like Q-Shift, introduce spatial distortions when applied to unstructured point clouds, weakening local geometric modeling and reducing robustness. In addition, the Bi-WKV attention in RWKV amplifies slight cross-domain differences in key distributions through exponential weighting, leading to attention shifts and degraded generalization. To this end, we propose PointDGRWKV, the first RWKV-based framework tailored for DG PCC. It introduces two key modules to enhance spatial modeling and cross-domain robustness, while maintaining RWKV's linear efficiency. In particular, we present Adaptive Geometric Token Shift to model local neighborhood structures to improve geometric context awareness. In addition, Cross-Domain key feature Distribution Alignment is designed to mitigate attention drift by aligning key feature distributions across domains. Extensive experiments on multiple benchmarks demonstrate that PointDGRWKV achieves state-of-the-art performance on DG PCC.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2508.11277.pdf' target='_blank'>https://arxiv.org/pdf/2508.11277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11277">Probing the Representational Power of Sparse Autoencoders in Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2505.11883.pdf' target='_blank'>https://arxiv.org/pdf/2505.11883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihuan Qiu, Yi Xu, Chiyuan He, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11883">MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual model merging integrates independently fine-tuned models sequentially without access to original training data, providing a scalable and efficient solution to continual learning. However, current methods still face critical challenges, notably parameter interference among tasks and limited adaptability to evolving test distributions. The former causes catastrophic forgetting of integrated tasks, while the latter hinders effective adaptation to new tasks. To address these, we propose MINGLE, a novel framework for test-time continual model merging, which leverages test-time adaptation using a small set of unlabeled test samples from the current task to dynamically guide the merging process. MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation. Extensive experiments on standard continual merging benchmarks demonstrate that MINGLE achieves robust generalization, reduces forgetting significantly, and consistently surpasses previous state-of-the-art methods by 7-9\% on average across diverse task orders.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2504.06572.pdf' target='_blank'>https://arxiv.org/pdf/2504.06572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Long, Qianyu Zhou, Xikun Jiang, Chenhao Ying, Lizhuang Ma, Yuan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06572">Domain Generalization via Discrete Codebook Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) strives to address distribution shifts across diverse environments to enhance model's generalizability. Current DG approaches are confined to acquiring robust representations with continuous features, specifically training at the pixel level. However, this DG paradigm may struggle to mitigate distribution gaps in dealing with a large space of continuous features, rendering it susceptible to pixel details that exhibit spurious correlations or noise. In this paper, we first theoretically demonstrate that the domain gaps in continuous representation learning can be reduced by the discretization process. Based on this inspiring finding, we introduce a novel learning paradigm for DG, termed Discrete Domain Generalization (DDG). DDG proposes to use a codebook to quantize the feature map into discrete codewords, aligning semantic-equivalent information in a shared discrete representation space that prioritizes semantic-level information over pixel-level intricacies. By learning at the semantic level, DDG diminishes the number of latent features, optimizing the utilization of the representation space and alleviating the risks associated with the wide-ranging space of continuous features. Extensive experiments across widely employed benchmarks in DG demonstrate DDG's superior performance compared to state-of-the-art approaches, underscoring its potential to reduce the distribution gaps and enhance the model's generalizability.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2501.01453.pdf' target='_blank'>https://arxiv.org/pdf/2501.01453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Rabeh, Ethan Herron, Aditya Balu, Soumik Sarkar, Chinmay Hegde, Adarsh Krishnamurthy, Baskar Ganapathysubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01453">Geometry Matters: Benchmarking Scientific ML Approaches for Flow Prediction around Complex Geometries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid and accurate simulations of fluid dynamics around complicated geometric bodies are critical in a variety of engineering and scientific applications, including aerodynamics and biomedical flows. However, while scientific machine learning (SciML) has shown considerable promise, most studies in this field are limited to simple geometries, and complex, real-world scenarios are underexplored. This paper addresses this gap by benchmarking diverse SciML models, including neural operators and vision transformer-based foundation models, for fluid flow prediction over intricate geometries. Using a high-fidelity dataset of steady-state flows across various geometries, we evaluate the impact of geometric representations -- Signed Distance Fields (SDF) and binary masks -- on model accuracy, scalability, and generalization. Central to this effort is the introduction of a novel, unified scoring framework that integrates metrics for global accuracy, boundary layer fidelity, and physical consistency to enable a robust, comparative evaluation of model performance. Our findings demonstrate that newer foundation models significantly outperform neural operators, particularly in data-limited scenarios, and that SDF representations yield superior results with sufficient training data. Despite these promises, all models struggle with out-of-distribution generalization, highlighting a critical challenge for future SciML applications. By advancing both evaluation models and modeling capabilities, our work paves the way for robust and scalable ML solutions for fluid dynamics across complex geometries.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2412.18281.pdf' target='_blank'>https://arxiv.org/pdf/2412.18281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhou Jin, Li You, Huibin Zhou, Yuanshuo Wang, Xiaofeng Liu, Xinrui Gong, Xiqi Gao, Derrick Wing Kwan Ng, Xiang-Gen Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18281">GDM4MMIMO: Generative Diffusion Models for Massive MIMO Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Massive multiple-input multiple-output (MIMO) offers significant advantages in spectral and energy efficiencies, positioning it as a cornerstone technology of fifth-generation (5G) wireless communication systems and a promising solution for the burgeoning data demands anticipated in sixth-generation (6G) networks. In recent years, with the continuous advancement of artificial intelligence (AI), a multitude of task-oriented generative foundation models (GFMs) have emerged, achieving remarkable performance in various fields such as computer vision (CV), natural language processing (NLP), and autonomous driving. As a pioneering force, these models are driving the paradigm shift in AI towards generative AI (GenAI). Among them, the generative diffusion model (GDM), as one of state-of-the-art families of generative models, demonstrates an exceptional capability to learn implicit prior knowledge and robust generalization capabilities, thereby enhancing its versatility and effectiveness across diverse applications. In this paper, we delve into the potential applications of GDM in massive MIMO communications. Specifically, we first provide an overview of massive MIMO communication, the framework of GFMs, and the working mechanism of GDM. Following this, we discuss recent research advancements in the field and present a case study of near-field channel estimation based on GDM, demonstrating its promising potential for facilitating efficient ultra-dimensional channel statement information (CSI) acquisition in the context of massive MIMO communications. Finally, we highlight several pressing challenges in future mobile communications and identify promising research directions surrounding GDM.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2408.13574.pdf' target='_blank'>https://arxiv.org/pdf/2408.13574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Qianyu Zhou, Haijia Sun, Xiangtai Li, Fengqi Liu, Xuequan Lu, Lizhuang Ma, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13574">PointDGMamba: Domain Generalization of Point Cloud Classification via Generalized State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) has been recently explored to improve the generalizability of point cloud classification (PCC) models toward unseen domains. However, they often suffer from limited receptive fields or quadratic complexity due to using convolution neural networks or vision Transformers. In this paper, we present the first work that studies the generalizability of state space models (SSMs) in DG PCC and find that directly applying SSMs into DG PCC will encounter several challenges: the inherent topology of the point cloud tends to be disrupted and leads to noise accumulation during the serialization stage. Besides, the lack of designs in domain-agnostic feature learning and data scanning will introduce unanticipated domain-specific information into the 3D sequence data. To this end, we propose a novel framework, PointDGMamba, that excels in strong generalizability toward unseen domains and has the advantages of global receptive fields and efficient linear complexity. PointDGMamba consists of three innovative components: Masked Sequence Denoising (MSD), Sequence-wise Cross-domain Feature Aggregation (SCFA), and Dual-level Domain Scanning (DDS). In particular, MSD selectively masks out the noised point tokens of the point cloud sequences, SCFA introduces cross-domain but same-class point cloud features to encourage the model to learn how to extract more generalized features. DDS includes intra-domain scanning and cross-domain scanning to facilitate information exchange between features. In addition, we propose a new and more challenging benchmark PointDG-3to1 for multi-domain generalization. Extensive experiments demonstrate the effectiveness and state-of-the-art performance of PointDGMamba.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2407.08801.pdf' target='_blank'>https://arxiv.org/pdf/2407.08801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jincen Jiang, Qianyu Zhou, Yuhang Li, Xuequan Lu, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08801">DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent point cloud understanding research suffers from performance drops on unseen data, due to the distribution shifts across different domains. While recent studies use Domain Generalization (DG) techniques to mitigate this by learning domain-invariant features, most are designed for a single task and neglect the potential of testing data. Despite In-Context Learning (ICL) showcasing multi-task learning capability, it usually relies on high-quality context-rich data and considers a single dataset, and has rarely been studied in point cloud understanding. In this paper, we introduce a novel, practical, multi-domain multi-task setting, handling multiple domains and multiple tasks within one unified model for domain generalized point cloud understanding. To this end, we propose Domain Generalized Point-In-Context Learning (DG-PIC) that boosts the generalizability across various tasks and domains at testing time. In particular, we develop dual-level source prototype estimation that considers both global-level shape contextual and local-level geometrical structures for representing source domains and a dual-level test-time feature shifting mechanism that leverages both macro-level domain semantic information and micro-level patch positional relationships to pull the target data closer to the source ones during the testing. Our DG-PIC does not require any model updates during the testing and can handle unseen domains and multiple tasks, \textit{i.e.,} point cloud reconstruction, denoising, and registration, within one unified model. We also introduce a benchmark for this new setting. Comprehensive experiments demonstrate that DG-PIC outperforms state-of-the-art techniques significantly.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2404.04188.pdf' target='_blank'>https://arxiv.org/pdf/2404.04188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Vitorino, Miguel Silva, Eva Maia, Isabel PraÃ§a
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04188">Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing cybersecurity threats make it essential to use high-quality data to train Machine Learning (ML) models for network traffic analysis, without noisy or missing data. By selecting the most relevant features for cyber-attack detection, it is possible to improve both the robustness and computational efficiency of the models used in a cybersecurity system. This work presents a feature selection and consensus process that combines multiple methods and applies them to several network datasets. Two different feature sets were selected and were used to train multiple ML models with regular and adversarial training. Finally, an adversarial evasion robustness benchmark was performed to analyze the reliability of the different feature sets and their impact on the susceptibility of the models to adversarial examples. By using an improved dataset with more data diversity, selecting the best time-related features and a more specific feature set, and performing adversarial training, the ML models were able to achieve a better adversarially robust generalization. The robustness of the models was significantly improved without their generalization to regular traffic flows being affected, without increases of false alarms, and without requiring too many computational resources, which enables a reliable detection of suspicious activity and perturbed traffic flows in enterprise computer networks.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2403.19334.pdf' target='_blank'>https://arxiv.org/pdf/2403.19334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Xuequan Lu, Shouhong Ding, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19334">Test-Time Domain Generalization for Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Anti-Spoofing (FAS) is pivotal in safeguarding facial recognition systems against presentation attacks. While domain generalization (DG) methods have been developed to enhance FAS performance, they predominantly focus on learning domain-invariant features during training, which may not guarantee generalizability to unseen data that differs largely from the source distributions. Our insight is that testing data can serve as a valuable resource to enhance the generalizability beyond mere evaluation for DG FAS. In this paper, we introduce a novel Test-Time Domain Generalization (TTDG) framework for FAS, which leverages the testing data to boost the model's generalizability. Our method, consisting of Test-Time Style Projection (TTSP) and Diverse Style Shifts Simulation (DSSS), effectively projects the unseen data to the seen domain space. In particular, we first introduce the innovative TTSP to project the styles of the arbitrarily unseen samples of the testing distribution to the known source space of the training distributions. We then design the efficient DSSS to synthesize diverse style shifts via learnable style bases with two specifically designed losses in a hyperspherical feature space. Our method eliminates the need for model updates at the test time and can be seamlessly integrated into not only the CNN but also ViT backbones. Comprehensive experiments on widely used cross-domain FAS benchmarks demonstrate our method's state-of-the-art performance and effectiveness.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2403.14333.pdf' target='_blank'>https://arxiv.org/pdf/2403.14333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ajian Liu, Shuai Xue, Jianwen Gan, Jun Wan, Yanyan Liang, Jiankang Deng, Sergio Escalera, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14333">CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model's performance on unseen domains. Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization. In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier's weights for exploring generalizable visual features. Specifically, we propose a novel Class Free Prompt Learning (CFPL) paradigm for DG FAS, which utilizes two lightweight transformers, namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic prompts conditioned on content and style features by using a set of learnable query vectors, respectively. Thus, the generalizable prompt can be learned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description. (2) A Diversified Style Prompt (DSP) technology is proposed to diversify the learning of style prompts by mixing feature statistics between instance-specific styles. Finally, the learned text features modulate visual features to generalization through the designed Prompt Modulation (PM). Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2403.06953.pdf' target='_blank'>https://arxiv.org/pdf/2403.06953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddhant Satyanaik, Aditya Murali, Deepak Alapatt, Xin Wang, Pietro Mascagni, Nicolas Padoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06953">Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purpose: Advances in deep learning have resulted in effective models for surgical video analysis; however, these models often fail to generalize across medical centers due to domain shift caused by variations in surgical workflow, camera setups, and patient demographics. Recently, object-centric learning has emerged as a promising approach for improved surgical scene understanding, capturing and disentangling visual and semantic properties of surgical tools and anatomy to improve downstream task performance. In this work, we conduct a multi-centric performance benchmark of object-centric approaches, focusing on Critical View of Safety assessment in laparoscopic cholecystectomy, then propose an improved approach for unseen domain generalization.
  Methods: We evaluate four object-centric approaches for domain generalization, establishing baseline performance. Next, leveraging the disentangled nature of object-centric representations, we dissect one of these methods through a series of ablations (e.g. ignoring either visual or semantic features for downstream classification). Finally, based on the results of these ablations, we develop an optimized method specifically tailored for domain generalization, LG-DG, that includes a novel disentanglement loss function.
  Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over the best baseline approach. More broadly, we show that object-centric approaches are highly effective for domain generalization thanks to their modular approach to representation learning.
  Conclusion: We investigate the use of object-centric methods for unseen domain generalization, identify method-agnostic factors critical for performance, and present an optimized approach that substantially outperforms existing methods.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2401.12733.pdf' target='_blank'>https://arxiv.org/pdf/2401.12733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niqi Liu, Fang Liu, Wenqi Ji, Xinxin Du, Xu Liu, Guozhen Zhao, Wenting Mu, Yong-Jin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12733">TNANet: A Temporal-Noise-Aware Neural Network for Suicidal Ideation Prediction with Noisy Physiological Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The robust generalization of deep learning models in the presence of inherent noise remains a significant challenge, especially when labels are subjective and noise is indiscernible in natural settings. This problem is particularly pronounced in many practical applications. In this paper, we address a special and important scenario of monitoring suicidal ideation, where time-series data, such as photoplethysmography (PPG), is susceptible to such noise. Current methods predominantly focus on image and text data or address artificially introduced noise, neglecting the complexities of natural noise in time-series analysis. To tackle this, we introduce a novel neural network model tailored for analyzing noisy physiological time-series data, named TNANet, which merges advanced encoding techniques with confidence learning, enhancing prediction accuracy. Another contribution of our work is the collection of a specialized dataset of PPG signals derived from real-world environments for suicidal ideation prediction. Employing this dataset, our TNANet achieves the prediction accuracy of 63.33% in a binary classification task, outperforming state-of-the-art models. Furthermore, comprehensive evaluations were conducted on three other well-known public datasets with artificially introduced noise to rigorously test the TNANet's capabilities. These tests consistently demonstrated TNANet's superior performance by achieving an accuracy improvement of more than 10% compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2309.17230.pdf' target='_blank'>https://arxiv.org/pdf/2309.17230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, Tong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17230">Spurious Feature Diversification Improves Out-of-distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. However, the underlying mechanism for their effectiveness remains unclear. In this study, we closely examine WiSE-FT, a popular weight space ensemble method that interpolates between a pre-trained and a fine-tuned model. We observe an unexpected ``FalseFalseTrue" phenomenon, in which WiSE-FT successfully corrects many cases where each individual model makes incorrect predictions, which contributes significantly to its OOD effectiveness. To gain further insights, we conduct theoretical analysis in a multi-class setting with a large number of spurious features. Our analysis predicts the above phenomenon and it further shows that ensemble-based models reduce prediction errors in the OOD settings by utilizing a more diverse set of spurious features. Contrary to the conventional wisdom that focuses on learning invariant features for better OOD performance, our findings suggest that incorporating a large number of diverse spurious features weakens their individual contributions, leading to improved overall OOD generalization performance. Additionally, our findings provide the first explanation for the mysterious phenomenon of weight space ensembles outperforming output space ensembles in OOD. Empirically we demonstrate the effectiveness of utilizing diverse spurious features on a MultiColorMNIST dataset, and our experimental results are consistent with the theoretical analysis. Building upon the new theoretical insights into the efficacy of ensemble methods, we further propose a novel averaging method called BAlaNced averaGing (BANG) which significantly enhances the OOD performance of WiSE-FT.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2309.16483.pdf' target='_blank'>https://arxiv.org/pdf/2309.16483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Long, Qianyu Zhou, Chenhao Ying, Lizhuang Ma, Yuan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16483">Rethinking Domain Generalization: Discriminability and Generalizability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization(DG) endeavors to develop robust models that possess strong generalizability while preserving excellent discriminability. Nonetheless, pivotal DG techniques tend to improve the feature generalizability by learning domain-invariant representations, inadvertently overlooking the feature discriminability. On the one hand, the simultaneous attainment of generalizability and discriminability of features presents a complex challenge, often entailing inherent contradictions. This challenge becomes particularly pronounced when domain-invariant features manifest reduced discriminability owing to the inclusion of unstable factors, i.e., spurious correlations. On the other hand, prevailing domain-invariant methods can be categorized as category-level alignment, susceptible to discarding indispensable features possessing substantial generalizability and narrowing intra-class variations. To surmount these obstacles, we rethink DG from a new perspective that concurrently imbues features with formidable discriminability and robust generalizability, and present a novel framework, namely, Discriminative Microscopic Distribution Alignment~(DMDA). DMDA incorporates two core components: Selective Channel Pruning~(SCP) and Micro-level Distribution Alignment~(MDA). Concretely, SCP attempts to curtail redundancy within neural networks, prioritizing stable attributes conducive to accurate classification. This approach alleviates the adverse effect of spurious domain invariance and amplifies the feature discriminability. Besides, MDA accentuates micro-level alignment within each class, going beyond mere category-level alignment. Extensive experiments on four benchmark datasets corroborate that DMDA achieves comparable results to state-of-the-art methods in DG, underscoring the efficacy of our method.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2309.16460.pdf' target='_blank'>https://arxiv.org/pdf/2309.16460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Long, Qianyu Zhou, Chenhao Ying, Lizhuang Ma, Yuan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16460">Diverse Target and Contribution Scheduling for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization under the distribution shift has been a great challenge in computer vision. The prevailing practice of directly employing the one-hot labels as the training targets in domain generalization~(DG) can lead to gradient conflicts, making it insufficient for capturing the intrinsic class characteristics and hard to increase the intra-class variation. Besides, existing methods in DG mostly overlook the distinct contributions of source (seen) domains, resulting in uneven learning from these domains. To address these issues, we firstly present a theoretical and empirical analysis of the existence of gradient conflicts in DG, unveiling the previously unexplored relationship between distribution shifts and gradient conflicts during the optimization process. In this paper, we present a novel perspective of DG from the empirical source domain's risk and propose a new paradigm for DG called Diverse Target and Contribution Scheduling (DTCS). DTCS comprises two innovative modules: Diverse Target Supervision (DTS) and Diverse Contribution Balance (DCB), with the aim of addressing the limitations associated with the common utilization of one-hot labels and equal contributions for source domains in DG. In specific, DTS employs distinct soft labels as training targets to account for various feature distributions across domains and thereby mitigates the gradient conflicts, and DCB dynamically balances the contributions of source domains by ensuring a fair decline in losses of different source domains. Extensive experiments with analysis on four benchmark datasets show that the proposed method achieves a competitive performance in comparison with the state-of-the-art approaches, demonstrating the effectiveness and advantages of the proposed DTCS.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2309.14356.pdf' target='_blank'>https://arxiv.org/pdf/2309.14356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiep Le, Vasudev Lal, Phillip Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14356">COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2309.00844.pdf' target='_blank'>https://arxiv.org/pdf/2309.00844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueying Jiang, Jiaxing Huang, Sheng Jin, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00844">Domain Generalization via Balancing Training Difficulty and Model Capability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn domain-generalizable models from one or multiple source domains that can perform well in unseen target domains. Despite its recent progress, most existing work suffers from the misalignment between the difficulty level of training samples and the capability of contemporarily trained models, leading to over-fitting or under-fitting in the trained generalization model. We design MoDify, a Momentum Difficulty framework that tackles the misalignment by balancing the seesaw between the model's capability and the samples' difficulties along the training process. MoDify consists of two novel designs that collaborate to fight against the misalignment while learning domain-generalizable models. The first is MoDify-based Data Augmentation which exploits an RGB Shuffle technique to generate difficulty-aware training samples on the fly. The second is MoDify-based Network Optimization which dynamically schedules the training samples for balanced and smooth learning with appropriate difficulty. Without bells and whistles, a simple implementation of MoDify achieves superior performance across multiple benchmarks. In addition, MoDify can complement existing methods as a plug-in, and it is generic and can work for different visual recognition tasks.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2305.11615.pdf' target='_blank'>https://arxiv.org/pdf/2305.11615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingchun Wang, Jingcai Guo, Yi Liu, Song Guo, Weizhan Zhang, Xiangyong Cao, Qinghua Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11615">SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model substructure learning aims to find an invariant network substructure that can have better out-of-distribution (OOD) generalization than the original full structure. Existing works usually search the invariant substructure using modular risk minimization (MRM) with fully exposed out-domain data, which may bring about two drawbacks: 1) Unfairness, due to the dependence of the full exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the equally feature-untargeted pruning on the whole data distribution. Based on the idea that in-distribution (ID) data with spurious features may have a lower experience risk, in this paper, we propose a novel Spurious Feature-targeted model Pruning framework, dubbed SFP, to automatically explore invariant substructures without referring to the above drawbacks. Specifically, SFP identifies spurious features within ID instances during training using our theoretically verified task loss, upon which, SFP attenuates the corresponding feature projections in model space to achieve the so-called spurious feature-targeted pruning. This is typically done by removing network branches with strong dependencies on identified spurious features, thus SFP can push the model learning toward invariant features and pull that out of spurious features and devise optimal OOD generalization. Moreover, we also conduct detailed theoretical analysis to provide the rationality guarantee and a proof framework for OOD structures via model sparsity, and for the first time, reveal how a highly biased data distribution affects the model's OOD generalization. Experiments on various OOD datasets show that SFP can significantly outperform both structure-based and non-structure-based OOD generalization SOTAs, with accuracy improvement up to 4.72% and 23.35%, respectively
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2305.06300.pdf' target='_blank'>https://arxiv.org/pdf/2305.06300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, Jimmy Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06300">Evaluating Embedding APIs for Information Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ever-increasing size of language models curtails their widespread availability to the community, thereby galvanizing many companies into offering access to large language models through APIs. One particular type, suitable for dense retrieval, is a semantic embedding service that builds vector representations of input text. With a growing number of publicly available APIs, our goal in this paper is to analyze existing offerings in realistic retrieval scenarios, to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we investigate the capabilities of existing semantic embedding APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate these services on two standard benchmarks, BEIR and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective in English, in contrast to the standard practice of employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best, albeit at a higher cost. We hope our work lays the groundwork for evaluating semantic embedding APIs that are critical in search and more broadly, for information access.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2212.04983.pdf' target='_blank'>https://arxiv.org/pdf/2212.04983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Aleksandar Bojchevski, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04983">Adversarial Weight Perturbation Improves Generalization in Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A lot of theoretical and empirical evidence shows that the flatter local minima tend to improve generalization. Adversarial Weight Perturbation (AWP) is an emerging technique to efficiently and effectively find such minima. In AWP we minimize the loss w.r.t. a bounded worst-case perturbation of the model parameters thereby favoring local minima with a small loss in a neighborhood around them. The benefits of AWP, and more generally the connections between flatness and generalization, have been extensively studied for i.i.d. data such as images. In this paper, we extensively study this phenomenon for graph data. Along the way, we first derive a generalization bound for non-i.i.d. node classification tasks. Then we identify a vanishing-gradient issue with all existing formulations of AWP and we propose a new Weighted Truncated AWP (WT-AWP) to alleviate this issue. We show that regularizing graph neural networks with WT-AWP consistently improves both natural and robust generalization across many different graph learning tasks and models.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2509.26045.pdf' target='_blank'>https://arxiv.org/pdf/2509.26045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aoming Liu, Kevin Miller, Venkatesh Saligrama, Kate Saenko, Boqing Gong, Ser-Nam Lim, Bryan A. Plummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26045">Scaling Up Temporal Domain Generalization via Temporal Experts Averaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal Domain Generalization (TDG) aims to generalize across temporal distribution shifts, e.g., lexical change over time. Prior work often addresses this by predicting future model weights. However, full model prediction is prohibitively expensive for even reasonably sized models. Thus, recent methods only predict the classifier layer, limiting generalization by failing to adjust other model components. To address this, we propose Temporal Experts Averaging (TEA), a novel and scalable TDG framework that updates the entire model using weight averaging to maximize generalization potential while minimizing computational costs. Our theoretical analysis guides us to two steps that enhance generalization to future domains. First, we create expert models with functional diversity yet parameter similarity by fine-tuning a domain-agnostic base model on individual temporal domains while constraining weight changes. Second, we optimize the bias-variance tradeoff through adaptive averaging coefficients derived from modeling temporal weight trajectories in a principal component subspace. Expert's contributions are based on their projected proximity to future domains. Extensive experiments across 7 TDG benchmarks, 5 models, and 2 TDG settings shows TEA outperforms prior TDG methods by up to 69% while being up to 60x more efficient.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2507.02291.pdf' target='_blank'>https://arxiv.org/pdf/2507.02291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyu Zhang, Lingyi Wang, Wei Wu, Fuhui Zhou, Qihui Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02291">Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven semantic communication is based on superficial statistical patterns, thereby lacking interpretability and generalization, especially for applications with the presence of unseen data. To address these challenges, we propose a novel knowledge graph-enhanced zero-shot semantic communication (KGZS-SC) network. Guided by the structured semantic information from a knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides generalized semantic representations and enables reasoning for unseen cases. Specifically, the KG-SKB aligns the semantic features in a shared category semantics embedding space and enhances the generalization ability of the transmitter through aligned semantic features, thus reducing communication overhead by selectively transmitting compact visual semantics. At the receiver, zero-shot learning (ZSL) is leveraged to enable direct classification for unseen cases without the demand for retraining or additional computational overhead, thereby enhancing the adaptability and efficiency of the classification process in dynamic or resource-constrained environments. The simulation results conducted on the APY datasets show that the proposed KGZS-SC network exhibits robust generalization and significantly outperforms existing SC frameworks in classifying unseen categories across a range of SNR levels.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2506.17718.pdf' target='_blank'>https://arxiv.org/pdf/2506.17718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo He, Shuang Li, Wenze Song, Longhui Yuan, Jian Liang, Han Li, Kun Gai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17718">Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Endowing deep models with the ability to generalize in dynamic scenarios is of vital significance for real-world deployment, given the continuous and complex changes in data distribution. Recently, evolving domain generalization (EDG) has emerged to address distribution shifts over time, aiming to capture evolving patterns for improved model generalization. However, existing EDG methods may suffer from spurious correlations by modeling only the dependence between data and targets across domains, creating a shortcut between task-irrelevant factors and the target, which hinders generalization. To this end, we design a time-aware structural causal model (SCM) that incorporates dynamic causal factors and the causal mechanism drifts, and propose \textbf{S}tatic-D\textbf{YN}amic \textbf{C}ausal Representation Learning (\textbf{SYNC}), an approach that effectively learns time-aware causal representations. Specifically, it integrates specially designed information-theoretic objectives into a sequential VAE framework which captures evolving patterns, and produces the desired representations by preserving intra-class compactness of causal factors both across and within domains. Moreover, we theoretically show that our method can yield the optimal causal predictor for each time domain. Results on both synthetic and real-world datasets exhibit that SYNC can achieve superior temporal generalization performance.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2505.13519.pdf' target='_blank'>https://arxiv.org/pdf/2505.13519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Cai, Yiheng Yao, Guangji Bai, Renhe Jiang, Xuan Song, Ryosuke Shibasaki, Liang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13519">Continuous Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world data distributions often shift continuously across multiple latent factors such as time, geography, and socioeconomic context. However, existing domain generalization approaches typically treat domains as discrete or evolving along a single axis (e.g., time), which fails to capture the complex, multi-dimensional nature of real-world variation. This paper introduces the task of Continuous Domain Generalization (CDG), which aims to generalize predictive models to unseen domains defined by arbitrary combinations of continuous variation descriptors. We present a principled framework grounded in geometric and algebraic theory, showing that optimal model parameters across domains lie on a low-dimensional manifold. To model this structure, we propose a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter transitions by enforcing geometric continuity and algebraic consistency. To handle noisy or incomplete domain descriptors, we introduce a gating mechanism to suppress irrelevant dimensions and a local chart-based strategy for robust generalization. Extensive experiments on synthetic and real-world datasets-including remote sensing, scientific documents, and traffic forecasting-demonstrate that our method significantly outperforms existing baselines in generalization accuracy and robustness under descriptor imperfections.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2504.01512.pdf' target='_blank'>https://arxiv.org/pdf/2504.01512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01512">High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently single-view 3D generation via Gaussian splatting has emerged and developed quickly. They learn 3D Gaussians from 2D RGB images generated from pre-trained multi-view diffusion (MVD) models, and have shown a promising avenue for 3D generation through a single image. Despite the current progress, these methods still suffer from the inconsistency jointly caused by the geometric ambiguity in the 2D images, and the lack of structure of 3D Gaussians, leading to distorted and blurry 3D object generation. In this paper, we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian Reconstruction Model designed to generate high-fidelity 3D objects from single-view images. Our key insight is a structured 3D representation can simultaneously mitigate the afore-mentioned two issues. To this end, we propose a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation contains explicit 3D geometric information, eliminating the geometric ambiguity from 2D images. It also structures Gaussians during learning so that the optimization tends to find better local optima. Our 3D voxel representation is obtained by a fusion module that aligns RGB features and surface normal features, both of which can be estimated from 2D images. Extensive experiments demonstrate the superiority of our methods over prior works in terms of high-quality reconstruction results, robust generalization, and good efficiency.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2503.01422.pdf' target='_blank'>https://arxiv.org/pdf/2503.01422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, Rui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01422">Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time scaling improves large language model performance by adding extra compute during decoding. Best-of-N (BoN) sampling serves as a common scaling technique, broadening the search space for finding better solutions from the model distribution. However, traditional BoN requires N full generations, leading to high GPU memory overhead and time latency. Moreover, some methods depend on reward models, adding computational cost and limiting domain generalization.
  In this paper, we propose Self-Truncation Best-of-N (ST-BoN), a novel decoding method that avoids fully generating all samplings and eliminates the need for reward models. ST-BoN introduces early sampling consistency to estimate the most promising sample, truncating suboptimal ones to free memory and accelerate inference. This pushes the sampling-efficient test-time scaling. Compared to traditional BoN, ST-BoN can reduce dynamic GPU memory overhead by over 90% and time latency by 50%, while achieving comparable or even better performance across reasoning and open-ended domains.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2412.02856.pdf' target='_blank'>https://arxiv.org/pdf/2412.02856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Bryan A. Plummer, Kate Saenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02856">Is Large-Scale Pretraining the Secret to Good Domain Generalization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Source Domain Generalization (DG) is the task of training on multiple source domains and achieving high classification performance on unseen target domains. Recent methods combine robust features from web-scale pretrained backbones with new features learned from source data, and this has dramatically improved benchmark results. However, it remains unclear if DG finetuning methods are becoming better over time, or if improved benchmark performance is simply an artifact of stronger pre-training. Prior studies have shown that perceptual similarity to pre-training data correlates with zero-shot performance, but we find the effect limited in the DG setting. Instead, we posit that having perceptually similar data in pretraining is not enough; and that it is how well these data were learned that determines performance. This leads us to introduce the Alignment Hypothesis, which states that the final DG performance will be high if and only if alignment of image and class label text embeddings is high. Our experiments confirm the Alignment Hypothesis is true, and we use it as an analysis tool of existing DG methods evaluated on DomainBed datasets by splitting evaluation data into In-pretraining (IP) and Out-of-pretraining (OOP). We show that all evaluated DG methods struggle on DomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, our findings highlight the need for DG methods which can generalize beyond pretraining alignment.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2408.14812.pdf' target='_blank'>https://arxiv.org/pdf/2408.14812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yubin Wang, Xinyang Jiang, De Cheng, Wenli Sun, Dongsheng Li, Cairong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14812">HPT++: Hierarchically Prompting Vision-Language Models with Multi-Granularity Knowledge Generation and Improved Structure Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has become a prevalent strategy for adapting vision-language foundation models (VLMs) such as CLIP to downstream tasks. With the emergence of large language models (LLMs), recent studies have explored the potential of using category-related descriptions to enhance prompt effectiveness. However, conventional descriptions lack explicit structured information necessary to represent the interconnections among key elements like entities or attributes with relation to a particular category. Since existing prompt tuning methods give little consideration to managing structured knowledge, this paper advocates leveraging LLMs to construct a graph for each description to prioritize such structured knowledge. Consequently, we propose a novel approach called Hierarchical Prompt Tuning (HPT), enabling simultaneous modeling of both structured and conventional linguistic knowledge. Specifically, we introduce a relationship-guided attention module to capture pair-wise associations among entities and attributes for low-level prompt learning. In addition, by incorporating high-level and global-level prompts modeling overall semantics, the proposed hierarchical structure forges cross-level interlinks and empowers the model to handle more complex and long-term relationships. Finally, by enhancing multi-granularity knowledge generation, redesigning the relationship-driven attention re-weighting module, and incorporating consistent constraints on the hierarchical text encoder, we propose HPT++, which further improves the performance of HPT. Our experiments are conducted across a wide range of evaluation settings, including base-to-new generalization, cross-dataset evaluation, and domain generalization. Extensive results and ablation studies demonstrate the effectiveness of our methods, which consistently outperform existing SOTA methods.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2408.09706.pdf' target='_blank'>https://arxiv.org/pdf/2408.09706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Wang, Yi Yang, Minfeng Zhu, Kecheng Zheng, Shi Liu, Wei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09706">MePT: Multi-Representation Guided Prompt Tuning for Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in pre-trained Vision-Language Models (VLMs) have highlighted the significant potential of prompt tuning for adapting these models to a wide range of downstream tasks. However, existing prompt tuning methods typically map an image to a single representation, limiting the model's ability to capture the diverse ways an image can be described. To address this limitation, we investigate the impact of visual prompts on the model's generalization capability and introduce a novel method termed Multi-Representation Guided Prompt Tuning (MePT). Specifically, MePT employs a three-branch framework that focuses on diverse salient regions, uncovering the inherent knowledge within images which is crucial for robust generalization. Further, we employ efficient self-ensemble techniques to integrate these versatile image representations, allowing MePT to learn all conditional, marginal, and fine-grained distributions effectively. We validate the effectiveness of MePT through extensive experiments, demonstrating significant improvements on both base-to-novel class prediction and domain generalization tasks.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2402.14296.pdf' target='_blank'>https://arxiv.org/pdf/2402.14296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Xingwei Liang, Kam-Fai Wong, Ruifeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14296">Mitigating Biases of Large Language Models in Stance Detection with Counterfactual Augmented Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stance detection is critical for understanding the underlying position or attitude expressed toward a topic. Large language models (LLMs) have demonstrated significant advancements across various natural language processing tasks including stance detection, however, their performance in stance detection is limited by biases and spurious correlations inherent due to their data-driven nature. Our statistical experiment reveals that LLMs are prone to generate biased stances due to sentiment-stance spurious correlations and preference towards certain individuals and topics. Furthermore, the results demonstrate a strong negative correlation between stance bias and stance detection performance, underscoring the importance of mitigating bias to enhance the utility of LLMs in stance detection. Therefore, in this paper, we propose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel calibration network is devised to calibrate potential bias in the stance prediction of LLMs. Further, to address the challenge of effectively learning bias representations and the difficulty in the generalizability of debiasing, we construct counterfactual augmented data. This approach enhances the calibration network, facilitating the debiasing and out-of-domain generalization. Experimental results on in-target and zero-shot stance detection tasks show that the proposed FACTUAL can effectively mitigate biases of LLMs, achieving state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2305.16124.pdf' target='_blank'>https://arxiv.org/pdf/2305.16124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Yang, Wufei Ma, Angtian Wang, Xiaoding Yuan, Alan Yuille, Adam Kortylewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16124">Robust Category-Level 3D Pose Estimation from Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obtaining accurate 3D object poses is vital for numerous computer vision applications, such as 3D reconstruction and scene understanding. However, annotating real-world objects is time-consuming and challenging. While synthetically generated training data is a viable alternative, the domain shift between real and synthetic data is a significant challenge. In this work, we aim to narrow the performance gap between models trained on synthetic data and few real images and fully supervised models trained on large-scale data. We achieve this by approaching the problem from two perspectives: 1) We introduce SyntheticP3D, a new synthetic dataset for object pose estimation generated from CAD models and enhanced with a novel algorithm. 2) We propose a novel approach (CC3D) for training neural mesh models that perform pose estimation via inverse rendering. In particular, we exploit the spatial relationships between features on the mesh surface and a contrastive learning scheme to guide the domain adaptation process. Combined, these two approaches enable our models to perform competitively with state-of-the-art models using only 10% of the respective real training images, while outperforming the SOTA model by 10.4% with a threshold of pi/18 using only 50% of the real training data. Our trained model further demonstrates robust generalization to out-of-distribution scenarios despite being trained with minimal real data.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2305.11389.pdf' target='_blank'>https://arxiv.org/pdf/2305.11389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Wang, Guangji Bai, Qingyang Zhu, Zhaohui Qin, Liang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11389">Domain Generalization Deep Graph Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph transformation that predicts graph transition from one mode to another is an important and common problem. Despite much progress in developing advanced graph transformation techniques in recent years, the fundamental assumption typically required in machine-learning models that the testing and training data preserve the same distribution does not always hold. As a result, domain generalization graph transformation that predicts graphs not available in the training data is under-explored, with multiple key challenges to be addressed including (1) the extreme space complexity when training on all input-output mode combinations, (2) difference of graph topologies between the input and the output modes, and (3) how to generalize the model to (unseen) target domains that are not in the training data. To fill the gap, we propose a multi-input, multi-output, hypernetwork-based graph neural network (MultiHyperGNN) that employs a encoder and a decoder to encode topologies of both input and output modes and semi-supervised link prediction to enhance the graph transformation task. Instead of training on all mode combinations, MultiHyperGNN preserves a constant space complexity with the encoder and the decoder produced by two novel hypernetworks. Comprehensive experiments show that MultiHyperGNN has a superior performance than competing models in both prediction and domain generalization tasks.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2212.04196.pdf' target='_blank'>https://arxiv.org/pdf/2212.04196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cairong Zhao, Yubin Wang, Xinyang Jiang, Yifei Shen, Kaitao Song, Dongsheng Li, Duoqian Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04196">Learning Domain Invariant Prompt for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning is one of the most effective and trending ways to adapt powerful vision-language foundation models like CLIP to downstream datasets by tuning learnable prompt vectors with very few samples. However, although prompt learning achieves excellent performance over in-domain data, it still faces the major challenge of generalizing to unseen classes and domains. Some existing prompt learning methods tackle this issue by adaptively generating different prompts for different tokens or domains but neglecting the ability of learned prompts to generalize to unseen domains. In this paper, we propose a novel prompt learning paradigm that directly generates \emph{domain invariant} prompt that can be generalized to unseen domains, called MetaPrompt. Specifically, a dual-modality prompt tuning network is proposed to generate prompts for input from both image and text modalities. With a novel asymmetric contrastive loss, the representation from the original pre-trained vision-language model acts as supervision to enhance the generalization ability of the learned prompt. More importantly, we propose a meta-learning-based prompt tuning algorithm that explicitly constrains the task-specific prompt tuned for one domain or class to also achieve good performance in another domain or class. Extensive experiments on 11 datasets for base-to-new generalization and 4 datasets for domain generalization demonstrate that our method consistently and significantly outperforms existing methods.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2510.08575.pdf' target='_blank'>https://arxiv.org/pdf/2510.08575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haofei Xu, Daniel Barath, Andreas Geiger, Marc Pollefeys
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08575">ReSplat: Learning Recurrent Gaussian Splats</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \times$ subsampled space, producing $16 \times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \times 256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2509.25172.pdf' target='_blank'>https://arxiv.org/pdf/2509.25172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Jiang, Yuchao Gu, Yiren Song, Ivor Tsang, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25172">Personalized Vision via Visual In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern vision models, trained on large-scale annotated datasets, excel at predefined tasks but struggle with personalized vision -- tasks defined at test time by users with customized objects or novel objectives. Existing personalization approaches rely on costly fine-tuning or synthetic data pipelines, which are inflexible and restricted to fixed task formats. Visual in-context learning (ICL) offers a promising alternative, yet prior methods confine to narrow, in-domain tasks and fail to generalize to open-ended personalization. We introduce Personalized In-Context Operator (PICO), a simple four-panel framework that repurposes diffusion transformers as visual in-context learners. Given a single annotated exemplar, PICO infers the underlying transformation and applies it to new inputs without retraining. To enable this, we construct VisRel, a compact yet diverse tuning dataset, showing that task diversity, rather than scale, drives robust generalization. We further propose an attention-guided seed scorer that improves reliability via efficient inference scaling. Extensive experiments demonstrate that PICO (i) surpasses fine-tuning and synthetic-data baselines, (ii) flexibly adapts to novel user-defined tasks, and (iii) generalizes across both recognition and generation.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2506.23208.pdf' target='_blank'>https://arxiv.org/pdf/2506.23208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runtian Yuan, Qingqiu Li, Junlin Hou, Jilan Xu, Yuejie Zhang, Rui Feng, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23208">Multi-Source COVID-19 Detection via Variance Risk Extrapolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present our solution for the Multi-Source COVID-19 Detection Challenge, which aims to classify chest CT scans into COVID and Non-COVID categories across data collected from four distinct hospitals and medical centers. A major challenge in this task lies in the domain shift caused by variations in imaging protocols, scanners, and patient populations across institutions. To enhance the cross-domain generalization of our model, we incorporate Variance Risk Extrapolation (VREx) into the training process. VREx encourages the model to maintain consistent performance across multiple source domains by explicitly minimizing the variance of empirical risks across environments. This regularization strategy reduces overfitting to center-specific features and promotes learning of domain-invariant representations. We further apply Mixup data augmentation to improve generalization and robustness. Mixup interpolates both the inputs and labels of randomly selected pairs of training samples, encouraging the model to behave linearly between examples and enhancing its resilience to noise and limited data. Our method achieves an average macro F1 score of 0.96 across the four sources on the validation set, demonstrating strong generalization.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2506.10097.pdf' target='_blank'>https://arxiv.org/pdf/2506.10097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoya Nishida, Noboru Harada, Daisuke Niizumi, Davide Albertini, Roberto Sannino, Simone Pradolini, Filippo Augusti, Keisuke Imoto, Kota Dohi, Harsh Purohit, Takashi Endo, Yohei Kawaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10097">Description and Discussion on DCASE 2025 Challenge Task 2: First-shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the task description for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2025 Challenge Task 2, titled "First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring." Building on the DCASE 2024 Challenge Task 2, this task is structured as a first-shot problem within a domain generalization framework. The primary objective of the first-shot approach is to facilitate the rapid deployment of ASD systems for new machine types without requiring machine-specific hyperparameter tunings. For DCASE 2025 Challenge Task 2, sounds from previously unseen machine types have been collected and provided as the evaluation dataset. Results and analysis of the challenge submissions will be added following the challenge's submission deadline.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2506.10097.pdf' target='_blank'>https://arxiv.org/pdf/2506.10097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoya Nishida, Noboru Harada, Daisuke Niizumi, Davide Albertini, Roberto Sannino, Simone Pradolini, Filippo Augusti, Keisuke Imoto, Kota Dohi, Harsh Purohit, Takashi Endo, Yohei Kawaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10097">Description and Discussion on DCASE 2025 Challenge Task 2: First-shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the task description for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2025 Challenge Task 2, titled "First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring". Building on the DCASE 2024 Challenge Task 2, this task is structured as a first-shot problem within a domain generalization framework. The primary objective of the first-shot approach is to facilitate the rapid deployment of ASD systems for new machine types without requiring machine-specific hyperparameter tunings. For DCASE 2025 Challenge Task 2, sounds from previously unseen machine types have been collected and provided as the evaluation dataset. We received 119 submissions from 35 teams, and an analysis of these submissions has been made in this paper. Analysis showed that various approaches can all be competitive, such as fine-tuning pre-trained models, using frozen pre-trained models, and training small models from scratch, when combined with appropriate cost functions, anomaly score normalization, and use of clean machine and noise sounds.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2504.09601.pdf' target='_blank'>https://arxiv.org/pdf/2504.09601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Wei, Xiaoqi Zhao, Jonghye Woo, Jinsong Ouyang, Georges El Fakhri, Qingyu Chen, Xiaofeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09601">Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning methods that encode shape priors often suffer from limited representational power with a small set of offline computed shape elements, or overfitting when the dictionary size grows. Moreover, they are not readily compatible with large foundation models such as the Segment Anything Model (SAM). In this paper, we propose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly integrates the idea of mixture-of-experts (MoE) training into dictionary learning to efficiently capture diverse and robust shape priors. Our method conceptualizes each dictionary atom as a shape expert, which specializes in encoding distinct semantic shape information. A gating network dynamically fuses these shape experts into a robust shape map, with sparse activation guided by SAM encoding to prevent overfitting. We further provide this shape map as a prompt to SAM, utilizing the powerful generalization capability of SAM through bidirectional integration. All modules, including the shape dictionary, are trained in an end-to-end manner. Extensive experiments on multiple public datasets demonstrate its effectiveness.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2502.19634.pdf' target='_blank'>https://arxiv.org/pdf/2502.19634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19634">MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice. Inference model is available at: https://huggingface.co/JZPeterPan/MedVLM-R1.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2410.05345.pdf' target='_blank'>https://arxiv.org/pdf/2410.05345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Ghaznavi, Hesam Asadollahzadeh, Fahimeh Hosseini Noohdani, Soroush Vafaie Tabar, Hosein Hasani, Taha Akbari Alvanagh, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05345">Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or 'minority') groups that lack these attributes, posing significant challenges for both out-of-distribution generalization and fairness objectives. Many studies aim to enhance robustness to spurious correlation, but they sometimes depend on group annotations for training. Additionally, a common limitation in previous research is the reliance on group-annotated validation datasets for model selection. This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are not available. To enhance model robustness with minimal group annotation assumptions, we propose Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses from an ERM-trained model to construct a balanced dataset of high-loss and low-loss samples, mitigating group imbalance in data. This significantly enhances robustness to group shifts when equipped with a simple post-training last layer retraining. By using environment inference methods to create diverse environments with correlation shifts, EVaLS can potentially eliminate the need for group annotation in validation data. In this context, the worst environment accuracy acts as a reliable surrogate throughout the retraining process for tuning hyperparameters and finding a model that performs well across diverse group shifts. EVaLS effectively achieves group robustness, showing that group annotation is not necessary even for validation. It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2410.02512.pdf' target='_blank'>https://arxiv.org/pdf/2410.02512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mucong Ding, Bang An, Yuancheng Xu, Anirudh Satheesh, Furong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02512">SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation, a cornerstone technique in deep learning, is crucial in enhancing model performance, especially with scarce labeled data. While traditional techniques are effective, their reliance on hand-crafted methods limits their applicability across diverse data types and tasks. Although modern learnable augmentation methods offer increased adaptability, they are computationally expensive and challenging to incorporate within prevalent augmentation workflows. In this work, we present a novel, efficient method for data augmentation, effectively bridging the gap between existing augmentation strategies and emerging datasets and learning tasks. We introduce SAFLEX (Self-Adaptive Augmentation via Feature Label EXtrapolation), which learns the sample weights and soft labels of augmented samples provided by any given upstream augmentation pipeline, using a specifically designed efficient bilevel optimization algorithm. Remarkably, SAFLEX effectively reduces the noise and label errors of the upstream augmentation pipeline with a marginal computational cost. As a versatile module, SAFLEX excels across diverse datasets, including natural and medical images and tabular data, showcasing its prowess in few-shot learning and out-of-distribution generalization. SAFLEX seamlessly integrates with common augmentation strategies like RandAug, CutMix, and those from large pre-trained generative models like stable diffusion and is also compatible with frameworks such as CLIP's fine-tuning. Our findings highlight the potential to adapt existing augmentation pipelines for new data types and tasks, signaling a move towards more adaptable and resilient training frameworks.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2408.03256.pdf' target='_blank'>https://arxiv.org/pdf/2408.03256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, Chang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03256">Synthesizing Text-to-SQL Data from Weak and Strong LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2407.05897.pdf' target='_blank'>https://arxiv.org/pdf/2407.05897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Abbasi, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05897">Deciphering the Role of Representation Disentanglement: Investigating Compositional Generalization in CLIP Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CLIP models have recently shown to exhibit Out of Distribution (OoD) generalization capabilities. However, Compositional Out of Distribution (C-OoD) generalization, which is a crucial aspect of a model's ability to understand unseen compositions of known concepts, is relatively unexplored for the CLIP models. Our goal is to address this problem and identify the factors that contribute to the C-OoD in CLIPs. We noted that previous studies regarding compositional understanding of CLIPs frequently fail to ensure that test samples are genuinely novel relative to the CLIP training data. To this end, we carefully synthesized a large and diverse dataset in the single object setting, comprising attributes for objects that are highly unlikely to be encountered in the combined training datasets of various CLIP models. This dataset enables an authentic evaluation of C-OoD generalization. Our observations reveal varying levels of C-OoD generalization across different CLIP models. We propose that the disentanglement of CLIP representations serves as a critical indicator in this context. By utilizing our synthesized datasets and other existing datasets, we assess various disentanglement metrics of text and image representations. Our study reveals that the disentanglement of image and text representations, particularly with respect to their compositional elements, plays a crucial role in improving the generalization of CLIP models in out-of-distribution settings. This finding suggests promising opportunities for advancing out-of-distribution generalization in CLIPs.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2406.07250.pdf' target='_blank'>https://arxiv.org/pdf/2406.07250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoya Nishida, Noboru Harada, Daisuke Niizumi, Davide Albertini, Roberto Sannino, Simone Pradolini, Filippo Augusti, Keisuke Imoto, Kota Dohi, Harsh Purohit, Takashi Endo, Yohei Kawaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07250">Description and Discussion on DCASE 2024 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge Task 2: First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring. Continuing from last year's DCASE 2023 Challenge Task 2, we organize the task as a first-shot problem under domain generalization required settings. The main goal of the first-shot problem is to enable rapid deployment of ASD systems for new kinds of machines without the need for machine-specific hyperparameter tunings. This problem setting was realized by (1) giving only one section for each machine type and (2) having completely different machine types for the development and evaluation datasets. For the DCASE 2024 Challenge Task 2, data of completely new machine types were newly collected and provided as the evaluation dataset. In addition, attribute information such as the machine operation conditions were concealed for several machine types to mimic situations where such information are unavailable. We will add challenge results and analysis of the submissions after the challenge submission deadline.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2404.15718.pdf' target='_blank'>https://arxiv.org/pdf/2404.15718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Constantin Ulrich, Catherine Knobloch, Julius C. Holzschuh, Tassilo Wald, Maximilian R. Rokuss, Maximilian Zenk, Maximilian Fischer, Michael Baumgartner, Fabian Isensee, Klaus H. Maier-Hein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15718">Mitigating False Predictions In Unreasonable Body Regions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite considerable strides in developing deep learning models for 3D medical image segmentation, the challenge of effectively generalizing across diverse image distributions persists. While domain generalization is acknowledged as vital for robust application in clinical settings, the challenges stemming from training with a limited Field of View (FOV) remain unaddressed. This limitation leads to false predictions when applied to body regions beyond the FOV of the training data. In response to this problem, we propose a novel loss function that penalizes predictions in implausible body regions, applicable in both single-dataset and multi-dataset training schemes. It is realized with a Body Part Regression model that generates axial slice positional scores. Through comprehensive evaluation using a test set featuring varying FOVs, our approach demonstrates remarkable improvements in generalization capabilities. It effectively mitigates false positive tumor predictions up to 85% and significantly enhances overall segmentation performance.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2303.06679.pdf' target='_blank'>https://arxiv.org/pdf/2303.06679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Zhang, Zifeng Zhuang, Zhitao Wang, Donglin Wang, Wenbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06679">RotoGBML: Towards Out-of-Distribution Generalization for Gradient-Based Meta-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gradient-based meta-learning (GBML) algorithms are able to fast adapt to new tasks by transferring the learned meta-knowledge, while assuming that all tasks come from the same distribution (in-distribution, ID). However, in the real world, they often suffer from an out-of-distribution (OOD) generalization problem, where tasks come from different distributions. OOD exacerbates inconsistencies in magnitudes and directions of task gradients, which brings challenges for GBML to optimize the meta-knowledge by minimizing the sum of task gradients in each minibatch. To address this problem, we propose RotoGBML, a novel approach to homogenize OOD task gradients. RotoGBML uses reweighted vectors to dynamically balance diverse magnitudes to a common scale and uses rotation matrixes to rotate conflicting directions close to each other. To reduce overhead, we homogenize gradients with the features rather than the network parameters. On this basis, to avoid the intervention of non-causal features (e.g., backgrounds), we also propose an invariant self-information (ISI) module to extract invariant causal features (e.g., the outlines of objects). Finally, task gradients are homogenized based on these invariant causal features. Experiments show that RotoGBML outperforms other state-of-the-art methods on various few-shot image classification benchmarks.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2209.09809.pdf' target='_blank'>https://arxiv.org/pdf/2209.09809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lidia Garrucho, Kaisar Kushibar, Richard Osuala, Oliver Diaz, Alessandro Catanese, Javier del Riego, Maciej Bobowicz, Fredrik Strand, Laura Igual, Karim Lekadir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.09809">High-resolution synthesis of high-density breast mammograms: Application to improved fairness in deep learning based mass detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-aided detection systems based on deep learning have shown good performance in breast cancer detection. However, high-density breasts show poorer detection performance since dense tissues can mask or even simulate masses. Therefore, the sensitivity of mammography for breast cancer detection can be reduced by more than 20% in dense breasts. Additionally, extremely dense cases reported an increased risk of cancer compared to low-density breasts. This study aims to improve the mass detection performance in highdensity breasts using synthetic high-density full-field digital mammograms (FFDM) as data augmentation during breast mass detection model training. To this end, a total of five cycle-consistent GAN (CycleGAN) models using three FFDM datasets were trained for low-to-high-density image translation in highresolution mammograms. The training images were split by breast density BIRADS categories, being BI-RADS A almost entirely fatty and BI-RADS D extremely dense breasts. Our results showed that the proposed data augmentation technique improved the sensitivity and precision of mass detection in models trained with small datasets and improved the domain generalization of the models trained with large databases. In addition, the clinical realism of the synthetic images was evaluated in a reader study involving two expert radiologists and one surgical oncologist.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2208.09414.pdf' target='_blank'>https://arxiv.org/pdf/2208.09414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zdravko Marinov, Alina Roitberg, David Schneider, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.09414">ModSelect: Automatic Modality Selection for Synthetic-to-Real Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modality selection is an important step when designing multimodal systems, especially in the case of cross-domain activity recognition as certain modalities are more robust to domain shift than others. However, selecting only the modalities which have a positive contribution requires a systematic approach. We tackle this problem by proposing an unsupervised modality selection method (ModSelect), which does not require any ground-truth labels. We determine the correlation between the predictions of multiple unimodal classifiers and the domain discrepancy between their embeddings. Then, we systematically compute modality selection thresholds, which select only modalities with a high correlation and low domain discrepancy. We show in our experiments that our method ModSelect chooses only modalities with positive contributions and consistently improves the performance on a Synthetic-to-Real domain adaptation benchmark, narrowing the domain gap.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2510.04243.pdf' target='_blank'>https://arxiv.org/pdf/2510.04243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jincan Lou, Jingkun Chen, Haoquan Li, Hang Li, Wenjian Huang, Weihua Chen, Fan Wang, Jianguo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04243">The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate liver segmentation from contrast-enhanced MRI is essential for diagnosis, treatment planning, and disease monitoring. However, it remains challenging due to limited annotated data, heterogeneous enhancement protocols, and significant domain shifts across scanners and institutions. Traditional image-to-image translation frameworks have made great progress in domain generalization, but their application is not straightforward. For example, Pix2Pix requires image registration, and cycle-GAN cannot be integrated seamlessly into segmentation pipelines. Meanwhile, these methods are originally used to deal with cross-modality scenarios, and often introduce structural distortions and suffer from unstable training, which may pose drawbacks in our single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised mean teacher scheme to exploit large amounts of unlabeled volumes. A domain adaptation module, incorporating a randomized histogram-based style appearance transfer function and a trainable contrast-aware network, enriches domain diversity and mitigates cross-center variability. Furthermore, a continual test-time adaptation strategy is employed to improve robustness during inference. Extensive experiments demonstrate that our framework consistently outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance while exhibiting strong generalization to unseen domains under low-annotation conditions.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2510.03161.pdf' target='_blank'>https://arxiv.org/pdf/2510.03161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Huang, Zhipei Xu, Xuanyu Zhang, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03161">UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2509.22812.pdf' target='_blank'>https://arxiv.org/pdf/2509.22812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhang, Christopher Malon, Lichao Sun, Martin Renqiang Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22812">EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiology report generation requires advanced medical image analysis, effective temporal reasoning, and accurate text generation. Although recent innovations, particularly multimodal large language models (MLLMs), have shown improved performance, their supervised fine-tuning (SFT) objective is not explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO, a mixed-policy reinforcement learning (RL) algorithm designed specifically to optimize the generation through clinically motivated rewards. EditGRPO integrates on-policy exploration with off-policy guidance by injecting sentence-level detailed corrections during training rollouts. This mixed-policy approach addresses the exploration dilemma and sampling efficiency issues typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO baselines, achieving an average improvement of 3.4% in CheXbert, GREEN, Radgraph, and RATEScore metrics across four major chest X-ray report generation datasets. Notably, EditGRPO also demonstrates superior out-of-domain generalization, with an average performance gain of 5.9% on unseen datasets.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2509.07531.pdf' target='_blank'>https://arxiv.org/pdf/2509.07531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Dou, Deqing Wang, Fuzhen Zhuang, Jian Ren, Yanlin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07531">FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific document representation learning provides powerful embeddings for various tasks, while current methods face challenges across three approaches. 1) Contrastive training with citation-structural signals underutilizes citation information and still generates single-vector representations. 2) Fine-grained representation learning, which generates multiple vectors at the sentence or aspect level, requires costly integration and lacks domain generalization. 3) Task-aware learning depends on manually predefined task categorization, overlooking nuanced task distinctions and requiring extra training data for task-specific modules. To address these problems, we propose a new method that unifies the three approaches for better representations, namely FLeW. Specifically, we introduce a novel triplet sampling method that leverages citation intent and frequency to enhance citation-structural signals for training. Citation intents (background, method, result), aligned with the general structure of scientific writing, facilitate a domain-generalized facet partition for fine-grained representation learning. Then, we adopt a simple weight search to adaptively integrate three facet-level embeddings into a task-specific document embedding without task-aware fine-tuning. Experiments show the applicability and robustness of FLeW across multiple scientific tasks and fields, compared to prior models.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2506.06566.pdf' target='_blank'>https://arxiv.org/pdf/2506.06566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Bao, Chuanbing Huo, Qinyu Chen, Chang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06566">AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny, tailored for low-resource deployment on edge devices. Our approach introduces a hybrid training strategy that systematically combines standard and aphasic speech at varying ratios, enabling robust generalization, and a GPT-4-based reference enhancement method that refines noisy aphasic transcripts, improving supervision quality. We conduct extensive experiments across multiple data mixing configurations and evaluation settings. Results show that our fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30% while preserving performance on standard speech. The proposed framework offers a scalable, efficient solution for real-world disordered speech recognition.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2505.15447.pdf' target='_blank'>https://arxiv.org/pdf/2505.15447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, Chong Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15447">ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding is inherently intention-driven-humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15\% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2505.09484.pdf' target='_blank'>https://arxiv.org/pdf/2505.09484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Ma, Xun Lin, Zitong Yu, Xin Liu, Xiaochen Yuan, Weicheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09484">Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Anti-Spoofing (FAS) is essential for the security of facial recognition systems in diverse scenarios such as payment processing and surveillance. Current multimodal FAS methods often struggle with effective generalization, mainly due to modality-specific biases and domain shifts. To address these challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot generalization capability of CLIP, the MMDA framework effectively suppresses noise in multimodal data through denoising and alignment mechanisms, thereby significantly enhancing the generalization performance of cross-modal alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential \textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the impacts of domain and modality noise by refining the attention mechanism based on extracted common noise features. Furthermore, the \textbf{R}epresentation \textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the pre-trained CLIP model to align multi-domain multimodal data into a generalized representation space in a flexible manner, preserving intricate representations and enhancing the model's adaptability to various unseen conditions. We also design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation (\textbf{U-DSA}) module to enhance the adaptability of representations while maintaining generalization performance. These improvements not only enhance the framework's generalization capabilities but also boost its ability to represent complex representations. Our experimental results on four benchmark datasets under different evaluation protocols demonstrate that the MMDA framework outperforms existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy. The code will be released soon.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2503.21210.pdf' target='_blank'>https://arxiv.org/pdf/2503.21210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueying Gao, Dongliang Chang, Bingyao Yu, Haotian Qin, Muxi Diao, Lei Chen, Kongming Liang, Zhanyu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21210">Towards Generalizable Forgery Detection and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and interpretable detection of AI-generated images is essential for mitigating risks associated with AI misuse. However, the substantial domain gap among generative models makes it challenging to develop a generalizable forgery detection model. Moreover, since every pixel in an AI-generated image is synthesized, traditional saliency-based forgery explanation methods are not well suited for this task. To address these challenges, we formulate detection and explanation as a unified Forgery Detection and Reasoning task (FDR-Task), leveraging Multi-Modal Large Language Models (MLLMs) to provide accurate detection through reliable reasoning over forgery attributes. To facilitate this task, we introduce the Multi-Modal Forgery Reasoning dataset (MMFR-Dataset), a large-scale dataset containing 120K images across 10 generative models, with 378K reasoning annotations on forgery attributes, enabling comprehensive evaluation of the FDR-Task. Furthermore, we propose FakeReasoning, a forgery detection and reasoning framework with three key components: 1) a dual-branch visual encoder that integrates CLIP and DINO to capture both high-level semantics and low-level artifacts; 2) a Forgery-Aware Feature Fusion Module that leverages DINO's attention maps and cross-attention mechanisms to guide MLLMs toward forgery-related clues; 3) a Classification Probability Mapper that couples language modeling and forgery detection, enhancing overall performance. Experiments across multiple generative models demonstrate that FakeReasoning not only achieves robust generalization but also outperforms state-of-the-art methods on both detection and reasoning tasks.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2502.08975.pdf' target='_blank'>https://arxiv.org/pdf/2502.08975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Li, Yida Xiong, Hongzhi Zhang, Xiantao Cai, Jia Wu, Bo Du, Wenbin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08975">Graph-structured Small Molecule Drug Discovery Through Deep Learning: Progress, Challenges, and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to their excellent drug-like and pharmacokinetic properties, small molecule drugs are widely used to treat various diseases, making them a critical component of drug discovery. In recent years, with the rapid development of deep learning (DL) techniques, DL-based small molecule drug discovery methods have achieved excellent performance in prediction accuracy, speed, and complex molecular relationship modeling compared to traditional machine learning approaches. These advancements enhance drug screening efficiency and optimization and provide more precise and effective solutions for various drug discovery tasks. Contributing to this field's development, this paper aims to systematically summarize and generalize the recent key tasks and representative techniques in graph-structured small molecule drug discovery in recent years. Specifically, we provide an overview of the major tasks in small molecule drug discovery and their interrelationships. Next, we analyze the six core tasks, summarizing the related methods, commonly used datasets, and technological development trends. Finally, we discuss key challenges, such as interpretability and out-of-distribution generalization, and offer our insights into future research directions for small molecule drug discovery.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2411.09837.pdf' target='_blank'>https://arxiv.org/pdf/2411.09837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kirill Vasilevski, Dayi Lin, Ahmed E. Hassan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09837">Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To balance the quality and inference cost of a Foundation Model (FM, such as large language models (LLMs)) powered software, people often opt to train a routing model that routes requests to FMs with different sizes and capabilities. Existing routing models rely on learning the optimal routing decision from carefully curated data, require complex computations to be updated, and do not consider the potential evolution of weaker FMs. In this paper, we propose Real-time Adaptive Routing (RAR), an approach to continuously adapt FM routing decisions while using guided in-context learning to enhance the capabilities of weaker FM. The goal is to reduce reliance on stronger, more expensive FMs. We evaluate our approach on different subsets of the popular MMLU benchmark. Over time, our approach routes 50.2% fewer requests to computationally expensive models while maintaining around 90.5% of the general response quality. In addition, the guides generated from stronger models have shown intra-domain generalization and led to a better quality of responses compared to an equivalent approach with a standalone weaker FM.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2410.16732.pdf' target='_blank'>https://arxiv.org/pdf/2410.16732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runpu Wei, Zijin Yin, Kongming Liang, Min Min, Chengwei Pan, Gang Yu, Haonan Huang, Yan Liu, Zhanyu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16732">Polyp-E: Benchmarking the Robustness of Deep Segmentation Models via Polyp Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic polyp segmentation is helpful to assist clinical diagnosis and treatment. In daily clinical practice, clinicians exhibit robustness in identifying polyps with both location and size variations. It is uncertain if deep segmentation models can achieve comparable robustness in automated colonoscopic analysis. To benchmark the model robustness, we focus on evaluating the robustness of segmentation models on the polyps with various attributes (e.g. location and size) and healthy samples. Based on the Latent Diffusion Model, we perform attribute editing on real polyps and build a new dataset named Polyp-E. Our synthetic dataset boasts exceptional realism, to the extent that clinical experts find it challenging to discern them from real data. We evaluate several existing polyp segmentation models on the proposed benchmark. The results reveal most of the models are highly sensitive to attribute variations. As a novel data augmentation technique, the proposed editing pipeline can improve both in-distribution and out-of-distribution generalization ability. The code and datasets will be released.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2410.14974.pdf' target='_blank'>https://arxiv.org/pdf/2410.14974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangning Xia, Hongjie Fang, Cewu Lu, Hao-Shu Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14974">CAGE: Causal Attention Enables Data-Efficient Generalizable Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization in robotic manipulation remains a critical challenge, particularly when scaling to new environments with limited demonstrations. This paper introduces CAGE, a novel robotic manipulation policy designed to overcome these generalization barriers by integrating a causal attention mechanism. CAGE utilizes the powerful feature extraction capabilities of the vision foundation model DINOv2, combined with LoRA fine-tuning for robust environment understanding. The policy further employs a causal Perceiver for effective token compression and a diffusion-based action prediction head with attention mechanisms to enhance task-specific fine-grained conditioning. With as few as 50 demonstrations from a single training environment, CAGE achieves robust generalization across diverse visual changes in objects, backgrounds, and viewpoints. Extensive experiments validate that CAGE significantly outperforms existing state-of-the-art RGB/RGB-D approaches in various manipulation tasks, especially under large distribution shifts. In similar environments, CAGE offers an average of 42% increase in task completion rate. While all baselines fail to execute the task in unseen environments, CAGE manages to obtain a 43% completion rate and a 51% success rate in average, making a huge step towards practical deployment of robots in real-world settings. Project website: cage-policy.github.io.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2407.14872.pdf' target='_blank'>https://arxiv.org/pdf/2407.14872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanting Yang, Minghao Chen, Qibo Qiu, Jiahao Wu, Wenxiao Wang, Binbin Lin, Ziyu Guan, Xiaofei He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14872">Adapt2Reward: Adapting Video-Language Models to Generalizable Robotic Rewards via Failure Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For a general-purpose robot to operate in reality, executing a broad range of instructions across various environments is imperative. Central to the reinforcement learning and planning for such robotic agents is a generalizable reward function. Recent advances in vision-language models, such as CLIP, have shown remarkable performance in the domain of deep learning, paving the way for open-domain visual recognition. However, collecting data on robots executing various language instructions across multiple environments remains a challenge. This paper aims to transfer video-language models with robust generalization into a generalizable language-conditioned reward function, only utilizing robot video data from a minimal amount of tasks in a singular environment. Unlike common robotic datasets used for training reward functions, human video-language datasets rarely contain trivial failure videos. To enhance the model's ability to distinguish between successful and failed robot executions, we cluster failure video features to enable the model to identify patterns within. For each cluster, we integrate a newly trained failure prompt into the text encoder to represent the corresponding failure mode. Our language-conditioned reward function shows outstanding generalization to new environments and new instructions for robot planning and reinforcement learning.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2406.15156.pdf' target='_blank'>https://arxiv.org/pdf/2406.15156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steve Azzolin, Antonio Longa, Stefano Teso, Andrea Passerini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15156">Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Graph Neural Networks (GNNs) become more pervasive, it becomes paramount to build reliable tools for explaining their predictions. A core desideratum is that explanations are \textit{faithful}, \ie that they portray an accurate picture of the GNN's reasoning process. However, a number of different faithfulness metrics exist, begging the question of what is faithfulness exactly and how to achieve it. We make three key contributions. We begin by showing that \textit{existing metrics are not interchangeable} -- \ie explanations attaining high faithfulness according to one metric may be unfaithful according to others -- and can systematically ignore important properties of explanations. We proceed to show that, surprisingly, \textit{optimizing for faithfulness is not always a sensible design goal}. Specifically, we prove that for injective regular GNN architectures, perfectly faithful explanations are completely uninformative. This does not apply to modular GNNs, such as self-explainable and domain-invariant architectures, prompting us to study the relationship between architectural choices and faithfulness. Finally, we show that \textit{faithfulness is tightly linked to out-of-distribution generalization}, in that simply ensuring that a GNN can correctly recognize the domain-invariant subgraph, as prescribed by the literature, does not guarantee that it is invariant unless this subgraph is also faithful.The code is publicly available on GitHub
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2405.04741.pdf' target='_blank'>https://arxiv.org/pdf/2405.04741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>He Li, Mang Ye, Ming Zhang, Bo Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04741">All in One Framework for Multimodal Re-identification in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Re-identification (ReID), recent advancements yield noteworthy progress in both unimodal and cross-modal retrieval tasks. However, the challenge persists in developing a unified framework that could effectively handle varying multimodal data, including RGB, infrared, sketches, and textual information. Additionally, the emergence of large-scale models shows promising performance in various vision tasks but the foundation model in ReID is still blank. In response to these challenges, a novel multimodal learning paradigm for ReID is introduced, referred to as All-in-One (AIO), which harnesses a frozen pre-trained big model as an encoder, enabling effective multimodal retrieval without additional fine-tuning. The diverse multimodal data in AIO are seamlessly tokenized into a unified space, allowing the modality-shared frozen encoder to extract identity-consistent features comprehensively across all modalities. Furthermore, a meticulously crafted ensemble of cross-modality heads is designed to guide the learning trajectory. AIO is the \textbf{first} framework to perform all-in-one ReID, encompassing four commonly used modalities. Experiments on cross-modal and multimodal ReID reveal that AIO not only adeptly handles various modal data but also excels in challenging contexts, showcasing exceptional performance in zero-shot and domain generalization scenarios.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2403.19390.pdf' target='_blank'>https://arxiv.org/pdf/2403.19390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19390">Checkpoint Merging via Bayesian Optimization in LLM Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2403.17592.pdf' target='_blank'>https://arxiv.org/pdf/2403.17592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Hao, Yong Lin, Difan Zou, Tong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17592">On the Benefits of Over-parameterization for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, machine learning models have achieved success based on the independently and identically distributed assumption. However, this assumption can be easily violated in real-world applications, leading to the Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient. Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings. To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions. Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess in-distribution (ID) loss. We demonstrate that in this scenario, further increasing the model's parameterization can significantly reduce the OOD loss. Intuitively, the variance term of ID loss remains low due to orthogonality of long-tail features, meaning overfitting noise during training generally doesn't raise testing loss. However, in OOD cases, distributional shift increases the variance term. Thankfully, the inherent shift is unrelated to individual x, maintaining the orthogonality of long-tail features. Expanding the hidden dimension can additionally improve this orthogonality by mapping the features into higher-dimensional spaces, thereby reducing the variance term. We further show that model ensembles also improve OOD loss, akin to increasing model capacity. These insights explain the empirical phenomenon of enhanced OOD generalization through model ensembles, supported by consistent simulations with theoretical results.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2403.15690.pdf' target='_blank'>https://arxiv.org/pdf/2403.15690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15690">EAGLE: A Domain Generalization Framework for AI-generated Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models. While supervised AI-generated text detectors perform well on text generated by older LLMs, with the frequent release of new LLMs, building supervised detectors for identifying text from such new models would require new labeled training data, which is infeasible in practice. In this work, we tackle this problem and propose a domain generalization framework for the detection of AI-generated text from unseen target generators. Our proposed framework, EAGLE, leverages the labeled data that is available so far from older language models and learns features invariant across these generators, in order to detect text generated by an unknown target generator. EAGLE learns such domain-invariant features by combining the representational power of self-supervised contrastive learning with domain adversarial training. Through our experiments we demonstrate how EAGLE effectively achieves impressive performance in detecting text generated by unseen target generators, including recent state-of-the-art ones such as GPT-4 and Claude, reaching detection scores of within 4.7% of a fully supervised detector.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2402.04609.pdf' target='_blank'>https://arxiv.org/pdf/2402.04609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Li, Levon Haroutunian, Raj Tumuluri, Philip Cohen, Gholamreza Haffari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04609">Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text-generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs when editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5's performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2401.08464.pdf' target='_blank'>https://arxiv.org/pdf/2401.08464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binghui Xie, Yongqiang Chen, Jiaqi Wang, Kaiwen Zhou, Bo Han, Wei Meng, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08464">Enhancing Evolving Domain Generalization through Dynamic Latent Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization is a critical challenge for machine learning systems. Prior domain generalization methods focus on extracting domain-invariant features across several stationary domains to enable generalization to new domains. However, in non-stationary tasks where new domains evolve in an underlying continuous structure, such as time, merely extracting the invariant features is insufficient for generalization to the evolving new domains. Nevertheless, it is non-trivial to learn both evolving and invariant features within a single model due to their conflicts. To bridge this gap, we build causal models to characterize the distribution shifts concerning the two patterns, and propose to learn both dynamic and invariant features via a new framework called Mutual Information-Based Sequential Autoencoders (MISTS). MISTS adopts information theoretic constraints onto sequential autoencoders to disentangle the dynamic and invariant features, and leverage a domain adaptive classifier to make predictions based on both evolving and invariant information. Our experimental results on both synthetic and real-world datasets demonstrate that MISTS succeeds in capturing both evolving and invariant information, and present promising results in evolving domain generalization tasks.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2401.02044.pdf' target='_blank'>https://arxiv.org/pdf/2401.02044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Hong-Yu Zhou, Jiarun Liu, Weijian Huang, Cheng Li, Zhihuan Li, Yuanxu Gao, Qiegen Liu, Yong Liang, Qi Yang, Song Wu, Tao Tan, Hairong Zheng, Kang Zhang, Shanshan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02044">Multi-modal vision-language model for generalizable annotation-free pathology localization and clinical diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Defining pathologies automatically from medical images aids the understanding of the emergence and progression of diseases, and such an ability is crucial in clinical diagnostics. However, existing deep learning models heavily rely on expert annotations and lack generalization capabilities in open clinical environments. In this study, we present a generalizable vision-language model for Annotation-Free pathology Localization (AFLoc). The core strength of AFLoc lies in its extensive multi-level semantic structure-based contrastive learning, which comprehensively aligns multi-granularity medical concepts from reports with abundant image features, to adapt to the diverse expressions of pathologies and unseen pathologies without the reliance on image annotations from experts. We conducted primary experiments on a dataset of 220K pairs of image-report chest X-ray images, and performed extensive validation across 8 external datasets encompassing 34 types of chest pathologies. The results demonstrate that AFLoc outperforms state-of-the-art methods in both annotation-free localization and classification tasks. Additionally, we assessed the generalizability of AFLoc on other modalities, including histopathology and retinal fundus images. Extensive experiments show that AFLoc exhibits robust generalization capabilities, even surpassing human benchmarks in localizing five different types of pathological images. These results highlight the potential of AFLoc in reducing annotation requirements and its applicability in complex clinical environments.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2311.02787.pdf' target='_blank'>https://arxiv.org/pdf/2311.02787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang You, Bokui Shen, Congyue Deng, Haoran Geng, Songlin Wei, He Wang, Leonidas Guibas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02787">Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable Manipulation with Tools</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deformable object manipulation stands as one of the most captivating yet formidable challenges in robotics. While previous techniques have predominantly relied on learning latent dynamics through demonstrations, typically represented as either particles or images, there exists a pertinent limitation: acquiring suitable demonstrations, especially for long-horizon tasks, can be elusive. Moreover, basing learning entirely on demonstrations can hamper the model's ability to generalize beyond the demonstrated tasks. In this work, we introduce a demonstration-free hierarchical planning approach capable of tackling intricate long-horizon tasks without necessitating any training. We employ large language models (LLMs) to articulate a high-level, stage-by-stage plan corresponding to a specified task. For every individual stage, the LLM provides both the tool's name and the Python code to craft intermediate subgoal point clouds. With the tool and subgoal for a particular stage at our disposal, we present a granular closed-loop model predictive control strategy. This leverages Differentiable Physics with Point-to-Point correspondence (DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied iteratively. Experimental findings affirm that our technique surpasses multiple benchmarks in dough manipulation, spanning both short and long horizons. Remarkably, our model demonstrates robust generalization capabilities to novel and previously unencountered complex tasks without any preliminary demonstrations. We further substantiate our approach with experimental trials on real-world robotic platforms. Our project page: https://qq456cvb.github.io/projects/donut.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2310.19035.pdf' target='_blank'>https://arxiv.org/pdf/2310.19035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19035">Does Invariant Graph Learning via Environment Augmentation Learn Invariance?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invariant graph representation learning aims to learn the invariance among data from different environments for out-of-distribution generalization on graphs. As the graph environment partitions are usually expensive to obtain, augmenting the environment information has become the de facto approach. However, the usefulness of the augmented environment information has never been verified. In this work, we find that it is fundamentally impossible to learn invariant graph representations via environment augmentation without additional assumptions. Therefore, we develop a set of minimal assumptions, including variation sufficiency and variation consistency, for feasible invariant graph learning. We then propose a new framework Graph invAriant Learning Assistant (GALA). GALA incorporates an assistant model that needs to be sensitive to graph environment changes or distribution shifts. The correctness of the proxy predictions by the assistant model hence can differentiate the variations in spurious subgraphs. We show that extracting the maximally invariant subgraph to the proxy predictions provably identifies the underlying invariant subgraph for successful OOD generalization under the established minimal assumptions. Extensive experiments on datasets including DrugOOD with various graph distribution shifts confirm the effectiveness of GALA.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2309.16286.pdf' target='_blank'>https://arxiv.org/pdf/2309.16286.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenke Huang, Mang Ye, Zekun Shi, Bo Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16286">Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning is an important privacy-preserving multi-party learning paradigm, involving collaborative learning with others and local updating on private data. Model heterogeneity and catastrophic forgetting are two crucial challenges, which greatly limit the applicability and generalizability. This paper presents a novel FCCL+, federated correlation and similarity learning with non-target distillation, facilitating the both intra-domain discriminability and inter-domain generalization. For heterogeneity issue, we leverage irrelevant unlabeled public data for communication between the heterogeneous participants. We construct cross-correlation matrix and align instance similarity distribution on both logits and feature levels, which effectively overcomes the communication barrier and improves the generalizable ability. For catastrophic forgetting in local updating stage, FCCL+ introduces Federated Non Target Distillation, which retains inter-domain knowledge while avoiding the optimization conflict issue, fulling distilling privileged inter-domain information through depicting posterior classes relation. Considering that there is no standard benchmark for evaluating existing heterogeneous federated learning under the same setting, we present a comprehensive benchmark with extensive representative methods under four domain shift scenarios, supporting both heterogeneous and homogeneous federated settings. Empirical results demonstrate the superiority of our method and the efficiency of modules on various scenarios.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2306.09001.pdf' target='_blank'>https://arxiv.org/pdf/2306.09001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Li, Sihang Li, Xinhao Liu, Moonjun Gong, Kenan Li, Nuo Chen, Zijun Wang, Zhiheng Li, Tao Jiang, Fisher Yu, Yue Wang, Hang Zhao, Zhiding Yu, Chen Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09001">SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular scene understanding is a foundational component of autonomous systems. Within the spectrum of monocular perception topics, one crucial and useful task for holistic 3D scene understanding is semantic scene completion (SSC), which jointly completes semantic information and geometric details from RGB input. However, progress in SSC, particularly in large-scale street views, is hindered by the scarcity of high-quality datasets. To address this issue, we introduce SSCBench, a comprehensive benchmark that integrates scenes from widely used automotive datasets (e.g., KITTI-360, nuScenes, and Waymo). SSCBench follows an established setup and format in the community, facilitating the easy exploration of SSC methods in various street views. We benchmark models using monocular, trinocular, and point cloud input to assess the performance gap resulting from sensor coverage and modality. Moreover, we have unified semantic labels across diverse datasets to simplify cross-domain generalization testing. We commit to including more datasets and SSC models to drive further advancements in this field.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2305.16314.pdf' target='_blank'>https://arxiv.org/pdf/2305.16314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congyue Deng, Jiahui Lei, Bokui Shen, Kostas Daniilidis, Leonidas Guibas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16314">Banana: Banach Fixed-Point Network for Pointcloud Segmentation with Inter-Part Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equivariance has gained strong interest as a desirable network property that inherently ensures robust generalization. However, when dealing with complex systems such as articulated objects or multi-object scenes, effectively capturing inter-part transformations poses a challenge, as it becomes entangled with the overall structure and local transformations. The interdependence of part assignment and per-part group action necessitates a novel equivariance formulation that allows for their co-evolution. In this paper, we present Banana, a Banach fixed-point network for equivariant segmentation with inter-part equivariance by construction. Our key insight is to iteratively solve a fixed-point problem, where point-part assignment labels and per-part SE(3)-equivariance co-evolve simultaneously. We provide theoretical derivations of both per-step equivariance and global convergence, which induces an equivariant final convergent state. Our formulation naturally provides a strict definition of inter-part equivariance that generalizes to unseen inter-part configurations. Through experiments conducted on both articulated objects and multi-object scans, we demonstrate the efficacy of our approach in achieving strong generalization under inter-part transformations, even when confronted with substantial changes in pointcloud geometry and topology.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2304.11327.pdf' target='_blank'>https://arxiv.org/pdf/2304.11327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11327">Understanding and Improving Feature Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A common explanation for the failure of out-of-distribution (OOD) generalization is that the model trained with empirical risk minimization (ERM) learns spurious features instead of invariant features. However, several recent studies challenged this explanation and found that deep networks may have already learned sufficiently good features for OOD generalization. Despite the contradictions at first glance, we theoretically show that ERM essentially learns both spurious and invariant features, while ERM tends to learn spurious features faster if the spurious correlation is stronger. Moreover, when fed the ERM learned features to the OOD objectives, the invariant feature learning quality significantly affects the final OOD performance, as OOD objectives rarely learn new features. Therefore, ERM feature learning can be a bottleneck to OOD generalization. To alleviate the reliance, we propose Feature Augmented Training (FeAT), to enforce the model to learn richer features ready for OOD generalization. FeAT iteratively augments the model to learn new features while retaining the already learned features. In each round, the retention and augmentation operations are performed on different subsets of the training data that capture distinct features. Extensive experiments show that FeAT effectively learns richer features thus boosting the performance of various OOD objectives.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2210.08323.pdf' target='_blank'>https://arxiv.org/pdf/2210.08323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Xu, Li Jiang, Jianxiong Li, Xianyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.08323">A Policy-Guided Imitation Approach for Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution generalization but suffer from erroneous off-policy evaluation. Imitation-based methods avoid off-policy evaluation but are too conservative to surpass the dataset. In this study, we propose an alternative approach, inheriting the training stability of imitation-style methods while still allowing logical out-of-distribution generalization. We decompose the conventional reward-maximizing policy in offline RL into a guide-policy and an execute-policy. During training, the guide-poicy and execute-policy are learned using only data from the dataset, in a supervised and decoupled manner. During evaluation, the guide-policy guides the execute-policy by telling where it should go so that the reward can be maximized, serving as the \textit{Prophet}. By doing so, our algorithm allows \textit{state-compositionality} from the dataset, rather than \textit{action-compositionality} conducted in prior imitation-style methods. We dumb this new approach Policy-guided Offline RL (\texttt{POR}). \texttt{POR} demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline RL. We also highlight the benefits of \texttt{POR} in terms of improving with supplementary suboptimal data and easily adapting to new tasks by only changing the guide-poicy.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2206.07766.pdf' target='_blank'>https://arxiv.org/pdf/2206.07766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongqiang Chen, Kaiwen Zhou, Yatao Bian, Binghui Xie, Bingzhe Wu, Yonggang Zhang, Kaili Ma, Han Yang, Peilin Zhao, Bo Han, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.07766">Pareto Invariant Risk Minimization: Towards Mitigating the Optimization Dilemma in Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been a growing surge of interest in enabling machine learning systems to generalize well to Out-of-Distribution (OOD) data. Most efforts are devoted to advancing optimization objectives that regularize models to capture the underlying invariance; however, there often are compromises in the optimization process of these OOD objectives: i) Many OOD objectives have to be relaxed as penalty terms of Empirical Risk Minimization (ERM) for the ease of optimization, while the relaxed forms can weaken the robustness of the original objective; ii) The penalty terms also require careful tuning of the penalty weights due to the intrinsic conflicts between ERM and OOD objectives. Consequently, these compromises could easily lead to suboptimal performance of either the ERM or OOD objective. To address these issues, we introduce a multi-objective optimization (MOO) perspective to understand the OOD optimization process, and propose a new optimization scheme called PAreto Invariant Risk Minimization (PAIR). PAIR improves the robustness of OOD objectives by cooperatively optimizing with other OOD objectives, thereby bridging the gaps caused by the relaxations. Then PAIR approaches a Pareto optimal solution that trades off the ERM and OOD objectives properly. Extensive experiments on challenging benchmarks, WILDS, show that PAIR alleviates the compromises and yields top OOD performances.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2206.05263.pdf' target='_blank'>https://arxiv.org/pdf/2206.05263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Wang, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.05263">Causal Balancing for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While machine learning models rapidly advance the state-of-the-art on various real-world tasks, out-of-domain (OOD) generalization remains a challenging problem given the vulnerability of these models to spurious correlations. We propose a balanced mini-batch sampling strategy to transform a biased data distribution into a spurious-free balanced distribution, based on the invariance of the underlying causal mechanisms for the data generation process. We argue that the Bayes optimal classifiers trained on such balanced distribution are minimax optimal across a diverse enough environment space. We also provide an identifiability guarantee of the latent variable model of the proposed data generation process, when utilizing enough train environments. Experiments are conducted on DomainBed, demonstrating empirically that our method obtains the best performance across 20 baselines reported on the benchmark.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2109.05826.pdf' target='_blank'>https://arxiv.org/pdf/2109.05826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Wang, Haoliang Li, Hao Cheng, Bihan Wen, Lap-Pui Chau, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.05826">Variational Disentanglement for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn an invariant model that can generalize well to the unseen target domain. In this paper, we propose to tackle the problem of domain generalization by delivering an effective framework named Variational Disentanglement Network (VDN), which is capable of disentangling the domain-specific features and task-specific features, where the task-specific features are expected to be better generalized to unseen but related test data. We further show the rationale of our proposed method by proving that our proposed framework is equivalent to minimize the evidence upper bound of the divergence between the distribution of task-specific features and its invariant ground truth derived from variational inference. We conduct extensive experiments to verify our method on three benchmarks, and both quantitative and qualitative results illustrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2510.04861.pdf' target='_blank'>https://arxiv.org/pdf/2510.04861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing Chu, Xinke Zhang, Xueyi Zheng, Ke Zheng, Xiaobo Wen, Jiabo Ma, Yihui Wang, Jiewei Chen, Chengyou Zheng, Jiangyu Zhang, Yongqin Wen, Jiajia Meng, Ziqi Zeng, Xiaoqing Li, Jing Li, Dan Xie, Yaping Ye, Yu Wang, Hao Chen, Muyan Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04861">A Clinical-grade Universal Foundation Model for Intraoperative Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intraoperative pathology is pivotal to precision surgery, yet its clinical impact is constrained by diagnostic complexity and the limited availability of high-quality frozen-section data. While computational pathology has made significant strides, the lack of large-scale, prospective validation has impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a clinical-grade foundation model developed on over 100,000 frozen sections from eight medical centers, specifically designed to provide Clinical-grade Robust Intraoperative Support for Pathology (CRISP). CRISP was comprehensively evaluated on more than 15,000 intraoperative slides across nearly 100 retrospective diagnostic tasks, including benign-malignant discrimination, key intraoperative decision-making, and pan-cancer detection, etc. The model demonstrated robust generalization across diverse institutions, tumor types, and anatomical sites-including previously unseen sites and rare cancers. In a prospective cohort of over 2,000 patients, CRISP sustained high diagnostic accuracy under real-world conditions, directly informing surgical decisions in 92.6% of cases. Human-AI collaboration further reduced diagnostic workload by 35%, avoided 105 ancillary tests and enhanced detection of micrometastases with 87.5% accuracy. Together, these findings position CRISP as a clinical-grade paradigm for AI-driven intraoperative pathology, bridging computational advances with surgical precision and accelerating the translation of artificial intelligence into routine clinical practice.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2509.12747.pdf' target='_blank'>https://arxiv.org/pdf/2509.12747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Botao He, Amir Hossein Shahidzadeh, Yu Chen, Jiayi Wu, Tianrui Guan, Guofei Chen, Howie Choset, Dinesh Manocha, Glen Chou, Cornelia Fermuller, Yiannis Aloimonos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12747">NavMoE: Hybrid Model- and Learning-based Traversability Estimation for Local Navigation via Mixture of Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores traversability estimation for robot navigation. A key bottleneck in traversability estimation lies in efficiently achieving reliable and robust predictions while accurately encoding both geometric and semantic information across diverse environments. We introduce Navigation via Mixture of Experts (NAVMOE), a hierarchical and modular approach for traversability estimation and local navigation. NAVMOE combines multiple specialized models for specific terrain types, each of which can be either a classical model-based or a learning-based approach that predicts traversability for specific terrain types. NAVMOE dynamically weights the contributions of different models based on the input environment through a gating network. Overall, our approach offers three advantages: First, NAVMOE enables traversability estimation to adaptively leverage specialized approaches for different terrains, which enhances generalization across diverse and unseen environments. Second, our approach significantly improves efficiency with negligible cost of solution quality by introducing a training-free lazy gating mechanism, which is designed to minimize the number of activated experts during inference. Third, our approach uses a two-stage training strategy that enables the training for the gating networks within the hybrid MoE method that contains nondifferentiable modules. Extensive experiments show that NAVMOE delivers a better efficiency and performance balance than any individual expert or full ensemble across different domains, improving cross-domain generalization and reducing average computational cost by 81.2% via lazy gating, with less than a 2% loss in path quality.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2508.07917.pdf' target='_blank'>https://arxiv.org/pdf/2508.07917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07917">MolmoAct: Action Reasoning Models that can Reason in Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of robotic foundation models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1.5; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2505.04979.pdf' target='_blank'>https://arxiv.org/pdf/2505.04979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Qi, Sijin Zhou, Lei Meng, Han Hu, Han Yu, Xiangxu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04979">Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attribute bias in federated learning (FL) typically leads local models to optimize inconsistently due to the learning of non-causal associations, resulting degraded performance. Existing methods either use data augmentation for increasing sample diversity or knowledge distillation for learning invariant representations to address this problem. However, they lack a comprehensive analysis of the inference paths, and the interference from confounding factors limits their performance. To address these limitations, we propose the \underline{Fed}erated \underline{D}econfounding and \underline{D}ebiasing \underline{L}earning (FedDDL) method. It constructs a structured causal graph to analyze the model inference process, and performs backdoor adjustment to eliminate confounding paths. Specifically, we design an intra-client deconfounding learning module for computer vision tasks to decouple background and objects, generating counterfactual samples that establish a connection between the background and any label, which stops the model from using the background to infer the label. Moreover, we design an inter-client debiasing learning module to construct causal prototypes to reduce the proportion of the background in prototype components. Notably, it bridges the gap between heterogeneous representations via causal prototypical regularization. Extensive experiments on 2 benchmarking datasets demonstrate that \methodname{} significantly enhances the model capability to focus on main objects in unseen data, leading to 4.5\% higher Top-1 Accuracy on average over 9 state-of-the-art existing methods.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2504.00850.pdf' target='_blank'>https://arxiv.org/pdf/2504.00850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Qi, Runhui Zhang, Lei Meng, Wei Wu, Yachong Zhang, Xiangxu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00850">Global Intervention and Distillation for Federated Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attribute skew in federated learning leads local models to focus on learning non-causal associations, guiding them towards inconsistent optimization directions, which inevitably results in performance degradation and unstable convergence. Existing methods typically leverage data augmentation to enhance sample diversity or employ knowledge distillation to learn invariant representations. However, the instability in the quality of generated data and the lack of domain information limit their performance on unseen samples. To address these issues, this paper presents a global intervention and distillation method, termed FedGID, which utilizes diverse attribute features for backdoor adjustment to break the spurious association between background and label. It includes two main modules, where the global intervention module adaptively decouples objects and backgrounds in images, injects background information into random samples to intervene in the sample distribution, which links backgrounds to all categories to prevent the model from treating background-label associations as causal. The global distillation module leverages a unified knowledge base to guide the representation learning of client models, preventing local models from overfitting to client-specific attributes. Experimental results on three datasets demonstrate that FedGID enhances the model's ability to focus on the main subjects in unseen data and outperforms existing methods in collaborative modeling.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2503.08180.pdf' target='_blank'>https://arxiv.org/pdf/2503.08180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyue Dai, Ke Fan, Bin Ji, Haoran Xu, Haoyu Zhao, Junting Dong, Jingbo Wang, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08180">Towards Synthesized and Editable Motion In-Betweening Through Part-Wise Phase Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Styled motion in-betweening is crucial for computer animation and gaming. However, existing methods typically encode motion styles by modeling whole-body motions, often overlooking the representation of individual body parts. This limitation reduces the flexibility of infilled motion, particularly in adjusting the motion styles of specific limbs independently. To overcome this challenge, we propose a novel framework that models motion styles at the body-part level, enhancing both the diversity and controllability of infilled motions. Our approach enables more nuanced and expressive animations by allowing precise modifications to individual limb motions while maintaining overall motion coherence. Leveraging phase-related insights, our framework employs periodic autoencoders to automatically extract the phase of each body part, capturing distinctive local style features. Additionally, we effectively decouple the motion source from synthesis control by integrating motion manifold learning and conditional generation techniques from both image and motion domains. This allows the motion source to generate high-quality motions across various styles, with extracted motion and style features readily available for controlled synthesis in subsequent tasks. Comprehensive evaluations demonstrate that our method achieves superior speed, robust generalization, and effective generation of extended motion sequences.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2502.20790.pdf' target='_blank'>https://arxiv.org/pdf/2502.20790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawei Zhu, Xiyu Wei, Guangxiang Zhao, Wenhao Wu, Haosheng Zou, Junfeng Ran, Xun Wang, Lin Sun, Xiangzheng Zhang, Sujian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20790">Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have highlighted the challenge of handling long-context tasks, where models need to reason over extensive input contexts to aggregate target information. While Chain-of-Thought (CoT) prompting has shown promise for multi-step reasoning, its effectiveness for long-context scenarios remains underexplored. Through systematic investigation across diverse tasks, we demonstrate that CoT's benefits generalize across most long-context scenarios and amplify with increasing context length. Motivated by this critical observation, we propose LongRePS, a process-supervised framework that teaches models to generate high-quality reasoning paths for enhanced long-context performance. Our framework incorporates a self-sampling mechanism to bootstrap reasoning paths and a novel quality assessment protocol specifically designed for long-context scenarios. Experimental results on various long-context benchmarks demonstrate the effectiveness of our approach, achieving significant improvements over outcome supervision baselines on both in-domain tasks (+13.6/+3.8 points for LLaMA/Qwen on MuSiQue) and cross-domain generalization (+9.3/+8.1 points on average across diverse QA tasks). Our code, data and trained models are made public to facilitate future research.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2502.01117.pdf' target='_blank'>https://arxiv.org/pdf/2502.01117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Jenq-Neng Hwang, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01117">Learning to Learn Weight Generation via Local Consistency Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based algorithms have emerged as promising techniques for weight generation. However, existing solutions are limited by two challenges: generalizability and local target assignment. The former arises from the inherent lack of cross-task transferability in existing single-level optimization methods, limiting the model's performance on new tasks. The latter lies in existing research modeling only global optimal weights, neglecting the supervision signals in local target weights. Moreover, naively assigning local target weights causes local-global inconsistency. To address these issues, we propose Mc-Di, which integrates the diffusion algorithm with meta-learning for better generalizability. Furthermore, we extend the vanilla diffusion into a local consistency diffusion algorithm. Our theory and experiments demonstrate that it can learn from local targets while maintaining consistency with the global optima. We validate Mc-Di's superior accuracy and inference efficiency in tasks that require frequent weight updates, including transfer learning, few-shot learning, domain generalization, and large language model adaptation.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2412.11542.pdf' target='_blank'>https://arxiv.org/pdf/2412.11542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Chen, Yiwen Ye, Feilong Tang, Yongsheng Pan, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11542">Meta Curvature-Aware Minimization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to enhance the ability of models trained on source domains to generalize effectively to unseen domains. Recently, Sharpness-Aware Minimization (SAM) has shown promise in this area by reducing the sharpness of the loss landscape to obtain more generalized models. However, SAM and its variants sometimes fail to guide the model toward a flat minimum, and their training processes exhibit limitations, hindering further improvements in model generalization. In this paper, we first propose an improved model training process aimed at encouraging the model to converge to a flat minima. To achieve this, we design a curvature metric that has a minimal effect when the model is far from convergence but becomes increasingly influential in indicating the curvature of the minima as the model approaches a local minimum. Then we derive a novel algorithm from this metric, called Meta Curvature-Aware Minimization (MeCAM), to minimize the curvature around the local minima. Specifically, the optimization objective of MeCAM simultaneously minimizes the regular training loss, the surrogate gap of SAM, and the surrogate gap of meta-learning. We provide theoretical analysis on MeCAM's generalization error and convergence rate, and demonstrate its superiority over existing DG methods through extensive experiments on five benchmark DG datasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code will be available on GitHub.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2411.10136.pdf' target='_blank'>https://arxiv.org/pdf/2411.10136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihang Fu, Ziyang Chen, Yiwen Ye, Xingliang Lei, Zhisong Wang, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10136">CoSAM: Self-Correcting SAM for Domain Generalization in 2D Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical images often exhibit distribution shifts due to variations in imaging protocols and scanners across different medical centers. Domain Generalization (DG) methods aim to train models on source domains that can generalize to unseen target domains. Recently, the segment anything model (SAM) has demonstrated strong generalization capabilities due to its prompt-based design, and has gained significant attention in image segmentation tasks. Existing SAM-based approaches attempt to address the need for manual prompts by introducing prompt generators that automatically generate these prompts. However, we argue that auto-generated prompts may not be sufficiently accurate under distribution shifts, potentially leading to incorrect predictions that still require manual verification and correction by clinicians. To address this challenge, we propose a method for 2D medical image segmentation called Self-Correcting SAM (CoSAM). Our approach begins by generating coarse masks using SAM in a prompt-free manner, providing prior prompts for the subsequent stages, and eliminating the need for prompt generators. To automatically refine these coarse masks, we introduce a generalized error decoder that simulates the correction process typically performed by clinicians. Furthermore, we generate diverse prompts as feedback based on the corrected masks, which are used to iteratively refine the predictions within a self-correcting loop, enhancing the generalization performance of our model. Extensive experiments on two medical image segmentation benchmarks across multiple scenarios demonstrate the superiority of CoSAM over state-of-the-art SAM-based methods.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2407.00291.pdf' target='_blank'>https://arxiv.org/pdf/2407.00291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Xiao, Han Yin, Jisheng Bai, Rohan Kumar Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00291">FMSG-JLESS Submission for DCASE 2024 Task4 on Sound Event Detection with Heterogeneous Training Dataset and Potentially Missing Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents the systems developed and submitted by Fortemedia Singapore (FMSG) and Joint Laboratory of Environmental Sound Sensing (JLESS) for DCASE 2024 Task 4. The task focuses on recognizing event classes and their time boundaries, given that multiple events can be present and may overlap in an audio recording. The novelty this year is a dataset with two sources, making it challenging to achieve good performance without knowing the source of the audio clips during evaluation. To address this, we propose a sound event detection method using domain generalization. Our approach integrates features from bidirectional encoder representations from audio transformers and a convolutional recurrent neural network. We focus on three main strategies to improve our method. First, we apply mixstyle to the frequency dimension to adapt the mel-spectrograms from different domains. Second, we consider training loss of our model specific to each datasets for their corresponding classes. This independent learning framework helps the model extract domain-specific features effectively. Lastly, we use the sound event bounding boxes method for post-processing. Our proposed method shows superior macro-average pAUC and polyphonic SED score performance on the DCASE 2024 Challenge Task 4 validation dataset and public evaluation dataset.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2406.09130.pdf' target='_blank'>https://arxiv.org/pdf/2406.09130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxin Liu, Harshavardhan Kamarthi, Lingkai Kong, Zhiyuan Zhao, Chao Zhang, B. Aditya Prakash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09130">Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time-series forecasting (TSF) finds broad applications in real-world scenarios. Due to the dynamic nature of time-series data, it is crucial to equip TSF models with out-of-distribution (OOD) generalization abilities, as historical training data and future test data can have different distributions. In this paper, we aim to alleviate the inherent OOD problem in TSF via invariant learning. We identify fundamental challenges of invariant learning for TSF. First, the target variables in TSF may not be sufficiently determined by the input due to unobserved core variables in TSF, breaking the conventional assumption of invariant learning. Second, time-series datasets lack adequate environment labels, while existing environmental inference methods are not suitable for TSF.
  To address these challenges, we propose FOIL, a model-agnostic framework that enables timeseries Forecasting for Out-of-distribution generalization via Invariant Learning. FOIL employs a novel surrogate loss to mitigate the impact of unobserved variables. Further, FOIL implements a joint optimization by alternately inferring environments effectively with a multi-head network while preserving the temporal adjacency structure, and learning invariant representations across inferred environments for OOD generalized TSF. We demonstrate that the proposed FOIL significantly improves the performance of various TSF models, achieving gains of up to 85%.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2406.01432.pdf' target='_blank'>https://arxiv.org/pdf/2406.01432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh-Dat Truong, Xin Li, Bhiksha Raj, Jackson Cothren, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01432">ED-SAM: An Efficient Diffusion Sampling Approach to Domain Generalization in Vision-Language Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Vision-Language Foundation Model has recently shown outstanding performance in various perception learning tasks. The outstanding performance of the vision-language model mainly relies on large-scale pre-training datasets and different data augmentation techniques. However, the domain generalization problem of the vision-language foundation model needs to be addressed. This problem has limited the generalizability of the vision-language foundation model to unknown data distributions. In this paper, we introduce a new simple but efficient Diffusion Sampling approach to Domain Generalization (ED-SAM) to improve the generalizability of the vision-language foundation model. Our theoretical analysis in this work reveals the critical role and relation of the diffusion model to domain generalization in the vision-language foundation model. Then, based on the insightful analysis, we introduce a new simple yet effective Transport Transformation to diffusion sampling method. It can effectively generate adversarial samples to improve the generalizability of the foundation model against unknown data distributions. The experimental results on different scales of vision-language pre-training datasets, including CC3M, CC12M, and LAION400M, have consistently shown State-of-the-Art performance and scalability of the proposed ED-SAM approach compared to the other recent methods.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2405.20046.pdf' target='_blank'>https://arxiv.org/pdf/2405.20046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Qi, Lei Meng, Ruohan Zhang, Yu Wang, Xin Qi, Xiangxu Meng, Han Yu, Qiang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20046">Federated Cross-Training Learners for Robust Generalization under Data Heterogeneity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning benefits from cross-training strategies, which enables models to train on data from distinct sources to improve generalization capability. However, due to inherent differences in data distributions, the optimization goals of local models remain misaligned, and this mismatch continues to manifest as feature space heterogeneity even after cross-training. We argue that knowledge distillation from the personalized view preserves client-specific characteristics and expands the local knowledge base, while distillation from the global view provides consistent semantic anchors that facilitate feature alignment across clients. To achieve this goal, this paper presents a cross-training scheme, termed FedCT, includes three main modules, where the consistency-aware knowledge broadcasting module aims to optimize model assignment strategies, which enhances collaborative advantages between clients and achieves an efficient federated learning process. The multi-view knowledge-guided representation learning module leverages fused prototypical knowledge from both global and local views to enhance the preservation of local knowledge before and after model exchange, as well as to ensure consistency between local and global knowledge. The mixup-based feature augmentation module aggregates rich information to further increase the diversity of feature spaces, which enables the model to better discriminate complex samples. Extensive experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study. The results demonstrated that FedCT alleviates knowledge forgetting from both local and global views, which enables it outperform state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2403.07261.pdf' target='_blank'>https://arxiv.org/pdf/2403.07261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengxing Jia, Fuxiang Zhang, Yi-Chen Li, Chen-Xiao Gao, Xu-Hui Liu, Lei Yuan, Zongzhang Zhang, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07261">Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representations from previous OMRL methods tend to correlate spuriously with the behavior policy instead of reflecting the essential characteristics of the task, resulting in unfavorable out-of-distribution generalization. To alleviate this issue, we introduce a novel algorithm to disentangle the impact of behavior policy from task representation learning through a process called adversarial data augmentation. Specifically, the objective of adversarial data augmentation is not merely to generate data analogous to offline data distribution; instead, it aims to create adversarial examples designed to confound learned task representations and lead to incorrect task identification. Our experiments show that learning from such adversarial samples significantly enhances the robustness and effectiveness of the task identification process and realizes satisfactory out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2310.19351.pdf' target='_blank'>https://arxiv.org/pdf/2310.19351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryosuke Furuta, Yoichi Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19351">Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detectors do not work well when domains largely differ between training and testing data. To overcome this domain gap in object detection without requiring expensive annotations, we consider two problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain generalization for object detection that requires labeled data from multiple domains, SS-DGOD and WS-DGOD require labeled data only from one domain and unlabeled or weakly-labeled data from multiple domains for training. In this paper, we show that object detectors can be effectively trained on the two settings with the same Mean Teacher learning framework, where a student network is trained with pseudo-labels output from a teacher on the unlabeled or weakly-labeled data. We provide novel interpretations of why the Mean Teacher learning framework works well on the two settings in terms of the relationships between the generalization gap and flat minima in parameter space. On the basis of the interpretations, we also show that incorporating a simple regularization method into the Mean Teacher learning framework leads to flatter minima. The experimental results demonstrate that the regularization leads to flatter minima and boosts the performance of the detectors trained with the Mean Teacher learning framework on the two settings.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2310.03534.pdf' target='_blank'>https://arxiv.org/pdf/2310.03534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Zhao, Tong Zhang, Mathieu Salzmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03534">3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior methods that tackle the problem of generalizable object pose estimation highly rely on having dense views of the unseen object. By contrast, we address the scenario where only a single reference view of the object is available. Our goal then is to estimate the relative object pose between this reference view and a query image that depicts the object in a different pose. In this scenario, robust generalization is imperative due to the presence of unseen objects during testing and the large-scale object pose variation between the reference and the query. To this end, we present a new hypothesis-and-verification framework, in which we generate and evaluate multiple pose hypotheses, ultimately selecting the most reliable one as the relative object pose. To measure reliability, we introduce a 3D-aware verification that explicitly applies 3D transformations to the 3D object representations learned from the two input images. Our comprehensive experiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior accuracy of our approach in relative pose estimation and its robustness in large-scale pose variations, when dealing with unseen objects.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2309.15589.pdf' target='_blank'>https://arxiv.org/pdf/2309.15589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Aubreville, Nikolas Stathonikos, Taryn A. Donovan, Robert Klopfleisch, Jonathan Ganz, Jonas Ammeling, Frauke Wilm, Mitko Veta, Samir Jabari, Markus Eckstein, Jonas Annuscheit, Christian Krumnow, Engin Bozaba, Sercan Cayir, Hongyan Gu, Xiang 'Anthony' Chen, Mostafa Jahanifar, Adam Shephard, Satoshi Kondo, Satoshi Kasai, Sujatha Kotte, VG Saipradeep, Maxime W. Lafarge, Viktor H. Koelzer, Ziyue Wang, Yongbing Zhang, Sen Yang, Xiyue Wang, Katharina Breininger, Christof A. Bertram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15589">Domain generalization across tumor types, laboratories, and species -- insights from the 2022 edition of the Mitosis Domain Generalization Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognition of mitotic figures in histologic tumor specimens is highly relevant to patient outcome assessment. This task is challenging for algorithms and human experts alike, with deterioration of algorithmic performance under shifts in image representations. Considerable covariate shifts occur when assessment is performed on different tumor types, images are acquired using different digitization devices, or specimens are produced in different laboratories. This observation motivated the inception of the 2022 challenge on MItosis Domain Generalization (MIDOG 2022). The challenge provided annotated histologic tumor images from six different domains and evaluated the algorithmic approaches for mitotic figure detection provided by nine challenge participants on ten independent domains. Ground truth for mitotic figure detection was established in two ways: a three-expert consensus and an independent, immunohistochemistry-assisted set of labels. This work represents an overview of the challenge tasks, the algorithmic strategies employed by the participants, and potential factors contributing to their success. With an $F_1$ score of 0.764 for the top-performing team, we summarize that domain generalization across various tumor domains is possible with today's deep learning-based recognition pipelines. However, we also found that domain characteristics not present in the training set (feline as new species, spindle cell shape as new morphology and a new scanner) led to small but significant decreases in performance. When assessed against the immunohistochemistry-assisted reference standard, all methods resulted in reduced recall scores, but with only minor changes in the order of participants in the ranking.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2309.12530.pdf' target='_blank'>https://arxiv.org/pdf/2309.12530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Huang, Andy Zhou, Zijian Lin, Mu Cai, Haohan Wang, Yong Jae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12530">A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization studies the problem of training a model with samples from several domains (or distributions) and then testing the model with samples from a new, unseen domain. In this paper, we propose a novel approach for domain generalization that leverages recent advances in large vision-language models, specifically a CLIP teacher model, to train a smaller model that generalizes to unseen domains. The key technical contribution is a new type of regularization that requires the student's learned image representations to be close to the teacher's learned text representations obtained from encoding the corresponding text descriptions of images. We introduce two designs of the loss function, absolute and relative distance, which provide specific guidance on how the training process of the student model should be regularized. We evaluate our proposed method, dubbed RISE (Regularized Invariance with Semantic Embeddings), on various benchmark datasets and show that it outperforms several state-of-the-art domain generalization methods. To our knowledge, our work is the first to leverage knowledge distillation using a large vision-language model for domain generalization. By incorporating text-based information, RISE improves the generalization capability of machine learning models.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2307.10224.pdf' target='_blank'>https://arxiv.org/pdf/2307.10224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhecheng Yuan, Sizhe Yang, Pu Hua, Can Chang, Kaizhe Hu, Huazhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10224">RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Reinforcement Learning (Visual RL), coupled with high-dimensional observations, has consistently confronted the long-standing challenge of out-of-distribution generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents' visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel Reinforcement Learning Benchmark for Visual Generalization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclusions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspiration is that RL-ViGen will serve as a catalyst in this area, and lay a foundation for the future creation of universal visual generalization RL agents suitable for real-world scenarios. Access to our code and implemented algorithms is provided at https://gemcollector.github.io/RL-ViGen/.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2305.17721.pdf' target='_blank'>https://arxiv.org/pdf/2305.17721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongqiu Wu, Shaohua Zhang, Yuchen Zhang, Hai Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17721">Rethinking Masked Language Modeling for Chinese Spelling Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns. Given that BERT is the backbone of most CSC models, this phenomenon has a significant negative impact. To address this issue, we are releasing a multi-domain benchmark LEMON, with higher quality and diversity than existing benchmarks, to allow a comprehensive assessment of the open domain generalization of CSC models. Then, we demonstrate that a very simple strategy, randomly masking 20\% non-error tokens from the input sequence during fine-tuning is sufficient for learning a much better language model without sacrificing the error model. This technique can be applied to any model architecture and achieves new state-of-the-art results on SIGHAN, ECSpell, and LEMON.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2304.02991.pdf' target='_blank'>https://arxiv.org/pdf/2304.02991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adriano Cardace, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02991">Exploiting the Complementarity of 2D and 3D Networks to Address Domain-Shift in 3D Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D semantic segmentation is a critical task in many real-world applications, such as autonomous driving, robotics, and mixed reality. However, the task is extremely challenging due to ambiguities coming from the unstructured, sparse, and uncolored nature of the 3D point clouds. A possible solution is to combine the 3D information with others coming from sensors featuring a different modality, such as RGB cameras. Recent multi-modal 3D semantic segmentation networks exploit these modalities relying on two branches that process the 2D and 3D information independently, striving to maintain the strength of each modality. In this work, we first explain why this design choice is effective and then show how it can be improved to make the multi-modal semantic segmentation more robust to domain shift. Our surprisingly simple contribution achieves state-of-the-art performances on four popular multi-modal unsupervised domain adaptation benchmarks, as well as better results in a domain generalization scenario.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2303.01686.pdf' target='_blank'>https://arxiv.org/pdf/2303.01686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Wang, Xinhai Zhao, Hai-Ming Xu, Zehui Chen, Dameng Yu, Jiahao Chang, Zhen Yang, Feng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01686">Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view 3D object detection (MV3D-Det) in Bird-Eye-View (BEV) has drawn extensive attention due to its low cost and high efficiency. Although new algorithms for camera-only 3D object detection have been continuously proposed, most of them may risk drastic performance degradation when the domain of input images differs from that of training. In this paper, we first analyze the causes of the domain gap for the MV3D-Det task. Based on the covariate shift assumption, we find that the gap mainly attributes to the feature distribution of BEV, which is determined by the quality of both depth estimation and 2D image's feature representation. To acquire a robust depth prediction, we propose to decouple the depth estimation from the intrinsic parameters of the camera (i.e. the focal length) through converting the prediction of metric depth to that of scale-invariant depth and perform dynamic perspective augmentation to increase the diversity of the extrinsic parameters (i.e. the camera poses) by utilizing homography. Moreover, we modify the focal length values to create multiple pseudo-domains and construct an adversarial training loss to encourage the feature representation to be more domain-agnostic. Without bells and whistles, our approach, namely DG-BEV, successfully alleviates the performance drop on the unseen target domain without impairing the accuracy of the source domain. Extensive experiments on various public datasets, including Waymo, nuScenes, and Lyft, demonstrate the generalization and effectiveness of our approach. To the best of our knowledge, this is the first systematic study to explore a domain generalization method for MV3D-Det.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2301.04423.pdf' target='_blank'>https://arxiv.org/pdf/2301.04423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frauke Wilm, Marco Fragoso, Christof A. Bertram, Nikolas Stathonikos, Mathias Ãttl, Jingna Qiu, Robert Klopfleisch, Andreas Maier, Katharina Breininger, Marc Aubreville
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04423">Multi-Scanner Canine Cutaneous Squamous Cell Carcinoma Histopathology Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In histopathology, scanner-induced domain shifts are known to impede the performance of trained neural networks when tested on unseen data. Multi-domain pre-training or dedicated domain-generalization techniques can help to develop domain-agnostic algorithms. For this, multi-scanner datasets with a high variety of slide scanning systems are highly desirable. We present a publicly available multi-scanner dataset of canine cutaneous squamous cell carcinoma histopathology images, composed of 44 samples digitized with five slide scanners. This dataset provides local correspondences between images and thereby isolates the scanner-induced domain shift from other inherent, e.g. morphology-induced domain shifts. To highlight scanner differences, we present a detailed evaluation of color distributions, sharpness, and contrast of the individual scanner subsets. Additionally, to quantify the inherent scanner-induced domain shift, we train a tumor segmentation network on each scanner subset and evaluate the performance both in- and cross-domain. We achieve a class-averaged in-domain intersection over union coefficient of up to 0.86 and observe a cross-domain performance decrease of up to 0.38, which confirms the inherent domain shift of the presented dataset and its negative impact on the performance of deep neural networks.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2211.05272.pdf' target='_blank'>https://arxiv.org/pdf/2211.05272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi, Siyuan Huang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.05272">GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For years, researchers have been devoted to generalizable object perception and manipulation, where cross-category generalizability is highly desired yet underexplored. In this work, we propose to learn such cross-category skills via Generalizable and Actionable Parts (GAParts). By identifying and defining 9 GAPart classes (lids, handles, etc.) in 27 object categories, we construct a large-scale part-centric interactive dataset, GAPartNet, where we provide rich, part-level annotations (semantics, poses) for 8,489 part instances on 1,166 objects. Based on GAPartNet, we investigate three cross-category tasks: part segmentation, part pose estimation, and part-based object manipulation. Given the significant domain gaps between seen and unseen object categories, we propose a robust 3D segmentation method from the perspective of domain generalization by integrating adversarial learning techniques. Our method outperforms all existing methods by a large margin, no matter on seen or unseen categories. Furthermore, with part segmentation and pose estimation results, we leverage the GAPart pose definition to design part-based manipulation heuristics that can generalize well to unseen object categories in both the simulator and the real world. Our dataset, code, and demos are available on our project page.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2207.09944.pdf' target='_blank'>https://arxiv.org/pdf/2207.09944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cian Eastwood, Alexander Robey, Shashank Singh, Julius von KÃ¼gelgen, Hamed Hassani, George J. Pappas, Bernhard SchÃ¶lkopf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.09944">Probable Domain Generalization via Quantile Risk Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging data drawn from multiple related training distributions or domains. To achieve this, DG is commonly formulated as an average- or worst-case problem over the set of possible domains. However, predictors that perform well on average lack robustness while predictors that perform well in the worst case tend to be overly-conservative. To address this, we propose a new probabilistic framework for DG where the goal is to learn predictors that perform well with high probability. Our key idea is that distribution shifts seen during training should inform us of probable shifts at test time, which we realize by explicitly relating training and test domains as draws from the same underlying meta-distribution. To achieve probable DG, we propose a new optimization problem called Quantile Risk Minimization (QRM). By minimizing the $Î±$-quantile of predictor's risk distribution over domains, QRM seeks predictors that perform well with probability $Î±$. To solve QRM in practice, we propose the Empirical QRM (EQRM) algorithm and provide: (i) a generalization bound for EQRM; and (ii) the conditions under which EQRM recovers the causal predictor as $Î±\to 1$. In our experiments, we introduce a more holistic quantile-focused evaluation protocol for DG and demonstrate that EQRM outperforms state-of-the-art baselines on datasets from WILDS and DomainBed.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2510.06005.pdf' target='_blank'>https://arxiv.org/pdf/2510.06005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qin Dong, Yuntian Tang, Heming Jia, Yunhang Shen, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie, Shaohui Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06005">MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-Rank Adaptation (LoRA) has emerged as a dominant method in Parameter-Efficient Fine-Tuning (PEFT) for large language models, which augments the transformer layer with one down-projection $A$ and one up-projection $B$. However, LoRA's reliance on a single down-projection matrix ($A$) creates a representational bottleneck, as this solitary feature extractor is inherently insufficient for capturing the diverse signals required by complex tasks. This motivates our architectural shift to focus on enriching the feature adaptation to improve the downstream task adaptation ability. We propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is asymmetrically shared across layers to ensure parameter efficiency. In MASA, these specialized experts capture diverse features, which are then integrated by a single, layer-specific $B$-matrix. The effectiveness and versatility of our method are validated through a comprehensive suite of experiments spanning multi-domain generalization, single-domain specialization, and multi-task reasoning. For example, on the MMLU benchmark, MASA achieves an average accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative improvement of 1.84%) with comparable learnable parameters of 0.52%.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2509.13956.pdf' target='_blank'>https://arxiv.org/pdf/2509.13956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Yang, Zengqi Peng, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13956">SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous parking is a critical component for achieving safe and efficient urban autonomous driving. However, unstructured environments and dynamic interactions pose significant challenges to autonomous parking tasks. To address this problem, we propose SEG-Parking, a novel end-to-end offline reinforcement learning (RL) framework to achieve interaction-aware autonomous parking. Notably, a specialized parking dataset is constructed for parking scenarios, which include those without interference from the opposite vehicle (OV) and complex ones involving interactions with the OV. Based on this dataset, a goal-conditioned state encoder is pretrained to map the fused perception information into the latent space. Then, an offline RL policy is optimized with a conservative regularizer that penalizes out-of-distribution actions. Extensive closed-loop experiments are conducted in the high-fidelity CARLA simulator. Comparative results demonstrate the superior performance of our framework with the highest success rate and robust generalization to out-of-distribution parking scenarios. The related dataset and source code will be made publicly available after the paper is accepted.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2508.19705.pdf' target='_blank'>https://arxiv.org/pdf/2508.19705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Hu, Ying Zhou, Gepeng Ji, Nick Barnes, Qiang Li, Zhiwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19705">FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing video polyp segmentation (VPS) paradigms usually struggle to balance between spatiotemporal modeling and domain generalization, limiting their applicability in real clinical scenarios. To embrace this challenge, we recast the VPS task as a track-by-detect paradigm that leverages the spatial contexts captured by the image polyp segmentation (IPS) model while integrating the temporal modeling capabilities of segment anything model 2 (SAM2). However, during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error accumulation, resulting in a snowball effect that compromises segmentation stability. We mitigate this issue by repurposing SAM2 as a video polyp segmenter with two training-free modules. In particular, the intra-association filtering module eliminates spatial inaccuracies originating from the detecting stage, reducing false positives. The inter-association refinement module adaptively updates the memory bank to prevent error propagation over time, enhancing temporal coherence. Both modules work synergistically to stabilize SAM2, achieving cutting-edge performance in both in-domain and out-of-domain scenarios. Furthermore, we demonstrate the robust tracking capabilities of FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential reliable clinical analysis.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2508.11502.pdf' target='_blank'>https://arxiv.org/pdf/2508.11502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eyad Alshami, Shashank Agnihotri, Bernt Schiele, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11502">AIM: Amending Inherent Interpretability via Self-Supervised Masking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has been observed that deep neural networks (DNNs) often use both genuine as well as spurious features. In this work, we propose "Amending Inherent Interpretability via Self-Supervised Masking" (AIM), a simple yet interestingly effective method that promotes the network's utilization of genuine features over spurious alternatives without requiring additional annotations. In particular, AIM uses features at multiple encoding stages to guide a self-supervised, sample-specific feature-masking process. As a result, AIM enables the training of well-performing and inherently interpretable models that faithfully summarize the decision process. We validate AIM across a diverse range of challenging datasets that test both out-of-distribution generalization and fine-grained visual understanding. These include general-purpose classification benchmarks such as ImageNet100, HardImageNet, and ImageWoof, as well as fine-grained classification datasets such as Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual benefits: interpretability improvements, as measured by the Energy Pointing Game (EPG) score, and accuracy gains over strong baselines. These consistent gains across domains and architectures provide compelling evidence that AIM promotes the use of genuine and meaningful features that directly contribute to improved generalization and human-aligned interpretability.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2507.05677.pdf' target='_blank'>https://arxiv.org/pdf/2507.05677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Wang, Qin Xu, Bo Jiang, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05677">Integrated Structural Prompt Learning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning methods have significantly extended the transferability of pre-trained Vision-Language Models (VLMs) like CLIP for various downstream tasks. These methods adopt handcraft templates or learnable vectors to provide text or image instructions in fine-tuning VLMs. However, most existing works ignore the structural relationships between learnable prompts and tokens within and between modalities. Moreover, balancing the performance of base and new classes remains a significant challenge. In this paper, we propose an Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of information representations between the text and image branches. ISP introduces self-structural and cross-structural prompt modules to model the structural relationships between learnable prompts and frozen tokens within and across modalities. This enables efficient information transfer while preserving feature stability. Additionally, we propose a sample probing module that dynamically adjusts loss coefficients based on sample difficulty, preventing the mode from overfitting to simple samples and improving generalization ability to new classes. Extensive experiments on three widely used settings: base-to-new generalization, cross-dataset evaluation, and domain generalization demonstrate that the proposed ISP achieves competitive performance against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2507.05668.pdf' target='_blank'>https://arxiv.org/pdf/2507.05668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Wang, Qin Xu, Bo Jiang, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05668">Dynamic Rank Adaptation for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained large vision-language models (VLMs) like CLIP demonstrate impressive generalization ability. Existing prompt-based and adapter-based works have made significant progress in fine-tuning VLMs but still face the challenges of maintaining strong generalization abilities, particularly towards unseen new classes. This limitation partly arises from these methods treating all tokens of the image and text encoder equally, which can lead to overfitting on less informative features (e.g., background noise, template words) and degrade the general representations that are crucial for novel concept recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a novel adapter variant method, designed specifically to enhance new class generalization. DRA dynamically allocates adaptation ranks based on the importance of features during training to preserve general knowledge. DRA first employs token importance grouping, using sequence attention to evaluate and group tokens by their importance. Then, we adopt rank adaptation according to the importance of each token group dynamically by assigning higher feature ranks to the more important tokens. Also, we design a new channel response mechanism to prioritize the preservation and adaptation of feature channels identified as the most informative for each instance. In addition, a L1 regularization term is introduced to stabilize the training. Extensive experiments demonstrate the effectiveness and superiority of our proposed DRA over existing works, especially on enhancing the performance of new classes on various benchmarks, including base-new classes, cross-datasets evaluation and domain generalization. The source code will be published after the paper is received.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2506.23726.pdf' target='_blank'>https://arxiv.org/pdf/2506.23726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bartlomiej Sobieski, Matthew Tivnan, Yuang Wang, Siyeop Yoon, Pengfei Jin, Dufan Wu, Quanzheng Li, Przemyslaw Biecek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23726">System-Embedded Diffusion Bridge Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solving inverse problems -- recovering signals from incomplete or noisy measurements -- is fundamental in science and engineering. Score-based generative models (SGMs) have recently emerged as a powerful framework for this task. Two main paradigms have formed: unsupervised approaches that adapt pretrained generative models to inverse problems, and supervised bridge methods that train stochastic processes conditioned on paired clean and corrupted data. While the former typically assume knowledge of the measurement model, the latter have largely overlooked this structural information. We introduce System embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge methods that explicitly embed the known linear measurement system into the coefficients of a matrix-valued SDE. This principled integration yields consistent improvements across diverse linear inverse problems and demonstrates robust generalization under system misspecification between training and deployment, offering a promising solution to real-world applications.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2506.16160.pdf' target='_blank'>https://arxiv.org/pdf/2506.16160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyao Wang, Xiao Yang, Hao Lu, Dengbo He, Kaishun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16160">Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-source synsemantic domain generalization (MSSDG) for multi-task remote physiological measurement seeks to enhance the generalizability of these metrics and attracts increasing attention. However, challenges like partial labeling and environmental noise may disrupt task-specific accuracy. Meanwhile, given that real-time adaptation is necessary for personalized products, the test-time personalized adaptation (TTPA) after MSSDG is also worth exploring, while the gap between previous generalization and personalization methods is significant and hard to fuse. Thus, we proposed a unified framework for MSSD\textbf{G} and TTP\textbf{A} employing \textbf{P}riors (\textbf{GAP}) in biometrics and remote photoplethysmography (rPPG). We first disentangled information from face videos into invariant semantics, individual bias, and noise. Then, multiple modules incorporating priors and our observations were applied in different stages and for different facial information. Then, based on the different principles of achieving generalization and personalization, our framework could simultaneously address MSSDG and TTPA under multi-task remote physiological estimation with minimal adjustments. We expanded the MSSDG benchmark to the TTPA protocol on six publicly available datasets and introduced a new real-world driving dataset with complete labeling. Extensive experiments that validated our approach, and the codes along with the new dataset will be released.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2506.14317.pdf' target='_blank'>https://arxiv.org/pdf/2506.14317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyuan Chen, Qiyang Yan, Yuanpei Chen, Tianhao Wu, Jiyao Zhang, Zihan Ding, Jinzhou Li, Yaodong Yang, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14317">ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a geometry and spatially-embedded scene representation and a novel comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target-oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts. More details and videos are available at https://clutterdexgrasp.github.io/.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2506.09446.pdf' target='_blank'>https://arxiv.org/pdf/2506.09446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhe Ding, Jian Liang, Bo Jiang, Zi Wang, Aihua Zheng, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09446">Harmonizing and Merging Source Models for CLIP-based Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CLIP-based domain generalization aims to improve model generalization to unseen domains by leveraging the powerful zero-shot classification capabilities of CLIP and multiple source datasets. Existing methods typically train a single model across multiple source domains to capture domain-shared information. However, this paradigm inherently suffers from two types of conflicts: 1) sample conflicts, arising from noisy samples and extreme domain shifts among sources; and 2) optimization conflicts, stemming from competition and trade-offs during multi-source training. Both hinder the generalization and lead to suboptimal solutions. Recent studies have shown that model merging can effectively mitigate the competition of multi-objective optimization and improve generalization performance. Inspired by these findings, we propose Harmonizing and Merging (HAM), a novel source model merging framework for CLIP-based domain generalization. During the training process of the source models, HAM enriches the source samples without conflicting samples, and harmonizes the update directions of all models. Then, a redundancy-aware historical model merging method is introduced to effectively integrate knowledge across all source models. HAM comprehensively consolidates source domain information while enabling mutual enhancement among source models, ultimately yielding a final model with optimal generalization capabilities. Extensive experiments on five widely used benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2504.11914.pdf' target='_blank'>https://arxiv.org/pdf/2504.11914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Chao, Jie Liu, Jie Tang, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11914">AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial Anomaly Detection (IAD) poses a formidable challenge due to the scarcity of defective samples, making it imperative to deploy models capable of robust generalization to detect unseen anomalies effectively. Traditional approaches, often constrained by hand-crafted features or domain-specific expert models, struggle to address this limitation, underscoring the need for a paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional generalization and interpretability, to revolutionize IAD. By integrating MLLM with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution that autonomously processes inputs of image and domain knowledge, reasons through analysis, and generates precise anomaly localizations and masks. Based on the latest multimodal IAD benchmark, our compact 3-billion-parameter model outperforms existing methods, establishing state-of-the-art results. As MLLM capabilities continue to advance, this study is the first to deliver an end-to-end VLM-based IAD solution that demonstrates the transformative potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking cornerstone for next-generation intelligent anomaly detection systems in industrial applications with limited defective data.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2503.15435.pdf' target='_blank'>https://arxiv.org/pdf/2503.15435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baolu Li, Zongzhe Xu, Jinlong Li, Xinyu Liu, Jianwu Fang, Xiaopeng Li, Hongkai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15435">V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has demonstrated its impact on the safety and effectiveness of autonomous driving. Since current cooperative perception algorithms are trained and tested on the same dataset, the generalization ability of cooperative perception systems remains underexplored. This paper is the first work to study the Domain Generalization problem of LiDAR-based V2X cooperative perception (V2X-DG) for 3D detection based on four widely-used open source datasets: OPV2V, V2XSet, V2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only within the source domain but also across other unseen domains, achieved solely through training on source domain. To this end, we propose Cooperative Mixup Augmentation based Generalization (CMAG) to improve the model generalization capability by simulating the unseen cooperation, which is designed compactly for the domain gaps in cooperative perception. Furthermore, we propose a constraint for the regularization of the robust generalized feature representation learning: Cooperation Feature Consistency (CFC), which aligns the intermediately fused features of the generalized cooperation by CMAG and the early fused features of the original cooperation in source domain. Extensive experiments demonstrate that our approach achieves significant performance gains when generalizing to other unseen datasets while it also maintains strong performance on the source dataset.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2501.05057.pdf' target='_blank'>https://arxiv.org/pdf/2501.05057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengqi Peng, Yubin Wang, Xu Han, Lei Zheng, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05057">LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in reinforcement learning (RL) demonstrate the significant potential in autonomous driving. Despite this promise, challenges such as the manual design of reward functions and low sample efficiency in complex environments continue to impede the development of safe and effective driving policies. To tackle these issues, we introduce LearningFlow, an innovative automated policy learning workflow tailored to urban driving. This framework leverages the collaboration of multiple large language model (LLM) agents throughout the RL training process. LearningFlow includes a curriculum sequence generation process and a reward generation process, which work in tandem to guide the RL policy by generating tailored training curricula and reward functions. Particularly, each process is supported by an analysis agent that evaluates training progress and provides critical insights to the generation agent. Through the collaborative efforts of these LLM agents, LearningFlow automates policy learning across a series of complex driving tasks, and it significantly reduces the reliance on manual reward function design while enhancing sample efficiency. Comprehensive experiments are conducted in the high-fidelity CARLA simulator, along with comparisons with other existing methods, to demonstrate the efficacy of our proposed approach. The results demonstrate that LearningFlow excels in generating rewards and curricula. It also achieves superior performance and robust generalization across various driving tasks, as well as commendable adaptation to different RL algorithms.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2412.12448.pdf' target='_blank'>https://arxiv.org/pdf/2412.12448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Cheng, Ran Tao, Yuliang Gu, Shenlong Wang, Xiaofeng Wang, Naira Hovakimyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12448">Task-Parameter Nexus: Task-Specific Parameter Learning for Model-Based Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the Task-Parameter Nexus (TPN), a learning-based approach for online determination of the (near-)optimal control parameters of model-based controllers (MBCs) for tracking tasks. In TPN, a deep neural network is introduced to predict the control parameters for any given tracking task at runtime, especially when optimal parameters for new tasks are not immediately available. To train this network, we constructed a trajectory bank with various speeds and curvatures that represent different motion characteristics. Then, for each trajectory in the bank, we auto-tune the optimal control parameters offline and use them as the corresponding ground truth. With this dataset, the TPN is trained by supervised learning. We evaluated the TPN on the quadrotor platform. In simulation experiments, it is shown that the TPN can predict near-optimal control parameters for a spectrum of tracking tasks, demonstrating its robust generalization capabilities to unseen tasks.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2412.01115.pdf' target='_blank'>https://arxiv.org/pdf/2412.01115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wu, Zhihang Zhong, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01115">DIR: Retrieval-Augmented Image Captioning with Comprehensive Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image captioning models often suffer from performance degradation when applied to novel datasets, as they are typically trained on domain-specific data. To enhance generalization in out-of-domain scenarios, retrieval-augmented approaches have garnered increasing attention. However, current methods face two key challenges: (1) image features used for retrieval are often optimized based on ground-truth (GT) captions, which represent the image from a specific perspective and are influenced by annotator biases, and (2) they underutilize the full potential of retrieved text, typically relying on raw captions or parsed objects, which fail to capture the full semantic richness of the data. In this paper, we propose Dive Into Retrieval (DIR), a method designed to enhance both the image-to-text retrieval process and the utilization of retrieved text to achieve a more comprehensive understanding of the visual content. Our approach introduces two key innovations: (1) diffusion-guided retrieval enhancement, where a pretrained diffusion model guides image feature learning by reconstructing noisy images, allowing the model to capture more comprehensive and fine-grained visual information beyond standard annotated captions; and (2) a high-quality retrieval database, which provides comprehensive semantic information to enhance caption generation, especially in out-of-domain scenarios. Extensive experiments demonstrate that DIR not only maintains competitive in-domain performance but also significantly improves out-of-domain generalization, all without increasing inference costs.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2407.08027.pdf' target='_blank'>https://arxiv.org/pdf/2407.08027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazi Sajeed Mehrab, M. Maruf, Arka Daw, Abhilash Neog, Harish Babu Manogaran, Mridul Khurana, Zhenyang Feng, Bahadir Altintas, Yasin Bakis, Elizabeth G Campolongo, Matthew J Thompson, Xiaojun Wang, Hilmar Lapp, Tanya Berger-Wolf, Paula Mabee, Henry Bart, Wei-Lun Chao, Wasila M Dahdul, Anuj Karpatne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08027">Fish-Vista: A Multi-Purpose Dataset for Understanding & Identification of Traits from Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Fish-Visual Trait Analysis (Fish-Vista), the first organismal image dataset designed for the analysis of visual traits of aquatic species directly from images using problem formulations in computer vision. Fish-Vista contains 69,126 annotated images spanning 4,154 fish species, curated and organized to serve three downstream tasks of species classification, trait identification, and trait segmentation. Our work makes two key contributions. First, we perform a fully reproducible data processing pipeline to process images sourced from various museum collections. We annotate these images with carefully curated labels from biological databases and manual annotations to create an AI-ready dataset of visual traits, contributing to the advancement of AI in biodiversity science. Second, our proposed downstream tasks offer fertile grounds for novel computer vision research in addressing a variety of challenges such as long-tailed distributions, out-of-distribution generalization, learning with weak labels, explainable AI, and segmenting small objects. We benchmark the performance of several existing methods for our proposed tasks to expose future research opportunities in AI for biodiversity science problems involving visual traits.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2405.14132.pdf' target='_blank'>https://arxiv.org/pdf/2405.14132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zexi Li, Lingzhi Gao, Chao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14132">Text-to-Model: Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative artificial intelligence (GenAI) has made significant progress in understanding world knowledge and generating content from human languages across various modalities, like text-to-text large language models, text-to-image stable diffusion, and text-to-video Sora. While in this paper, we investigate the capability of GenAI for text-to-model generation, to see whether GenAI can comprehend hyper-level knowledge embedded within AI itself parameters. Specifically, we study a practical scenario termed train-once-for-all personalization, aiming to generate personalized models for diverse end-users and tasks using text prompts. Inspired by the recent emergence of neural network diffusion, we present Tina, a text-conditioned neural network diffusion for train-once-for-all personalization. Tina leverages a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. Despite the astronomical number of potential personalized tasks (e.g., $1.73\times10^{13}$), by our design, Tina demonstrates remarkable in-distribution and out-of-distribution generalization even trained on small datasets ($\sim 1000$). We further verify whether and how \Tina understands world knowledge by analyzing its capabilities under zero-shot/few-shot image prompts, different numbers of personalized classes, prompts of natural language descriptions, and predicting unseen entities.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2405.06201.pdf' target='_blank'>https://arxiv.org/pdf/2405.06201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyao Wang, Hao Lu, Ange Wang, Xiao Yang, Yingcong Chen, Dengbo He, Kaishun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06201">PhysMLE: Generalizable and Priors-Inclusive Multi-task Remote Physiological Measurement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote photoplethysmography (rPPG) has been widely applied to measure heart rate from face videos. To increase the generalizability of the algorithms, domain generalization (DG) attracted increasing attention in rPPG. However, when rPPG is extended to simultaneously measure more vital signs (e.g., respiration and blood oxygen saturation), achieving generalizability brings new challenges. Although partial features shared among different physiological signals can benefit multi-task learning, the sparse and imbalanced target label space brings the seesaw effect over task-specific feature learning. To resolve this problem, we designed an end-to-end Mixture of Low-rank Experts for multi-task remote Physiological measurement (PhysMLE), which is based on multiple low-rank experts with a novel router mechanism, thereby enabling the model to adeptly handle both specifications and correlations within tasks. Additionally, we introduced prior knowledge from physiology among tasks to overcome the imbalance of label space under real-world multi-task physiological measurement. For fair and comprehensive evaluations, this paper proposed a large-scale multi-task generalization benchmark, named Multi-Source Synsemantic Domain Generalization (MSSDG) protocol. Extensive experiments with MSSDG and intra-dataset have shown the effectiveness and efficiency of PhysMLE. In addition, a new dataset was collected and made publicly available to meet the needs of the MSSDG.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2403.06392.pdf' target='_blank'>https://arxiv.org/pdf/2403.06392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingtian Zou, Kenji Kawaguchi, Yingnan Liu, Jiashuo Liu, Mong-Li Lee, Wynne Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06392">Towards Robust Out-of-Distribution Generalization Bounds via Sharpness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by "robustness" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for "flat minima leads to better OOD generalization". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees. Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2402.02696.pdf' target='_blank'>https://arxiv.org/pdf/2402.02696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raha Moraffah, Paras Sheth, Saketh Vishnubhatla, Huan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02696">Causal Feature Selection for Responsible Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine Learning (ML) has become an integral aspect of many real-world applications. As a result, the need for responsible machine learning has emerged, focusing on aligning ML models to ethical and social values, while enhancing their reliability and trustworthiness. Responsible ML involves many issues. This survey addresses four main issues: interpretability, fairness, adversarial robustness, and domain generalization. Feature selection plays a pivotal role in the responsible ML tasks. However, building upon statistical correlations between variables can lead to spurious patterns with biases and compromised performance. This survey focuses on the current study of causal feature selection: what it is and how it can reinforce the four aspects of responsible ML. By identifying features with causal impacts on outcomes and distinguishing causality from correlation, causal feature selection is posited as a unique approach to ensuring ML models to be ethically and socially responsible in high-stakes applications.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2401.03002.pdf' target='_blank'>https://arxiv.org/pdf/2401.03002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Yan, Chi Liu, Zhen Yu, Lie Ju, Dwarikanath Mahapatra, Brigid Betz-Stablein, Victoria Mar, Monika Janda, Peter Soyer, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03002">Prompt-driven Latent Domain Generalization for Medical Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models for medical image analysis easily suffer from distribution shifts caused by dataset artifacts bias, camera variations, differences in the imaging station, etc., leading to unreliable diagnoses in real-world clinical settings. Domain generalization (DG) methods, which aim to train models on multiple domains to perform well on unseen domains, offer a promising direction to solve the problem. However, existing DG methods assume domain labels of each image are available and accurate, which is typically feasible for only a limited number of medical datasets. To address these challenges, we propose a novel DG framework for medical image classification without relying on domain labels, called Prompt-driven Latent Domain Generalization (PLDG). PLDG consists of unsupervised domain discovery and prompt learning. This framework first discovers pseudo domain labels by clustering the bias-associated style features, then leverages collaborative domain prompts to guide a Vision Transformer to learn knowledge from discovered diverse domains. To facilitate cross-domain knowledge learning between different prompts, we introduce a domain prompt generator that enables knowledge sharing between domain prompts and a shared prompt. A domain mixup strategy is additionally employed for more flexible decision margins and mitigates the risk of incorrect domain assignments. Extensive experiments on three medical image classification tasks and one debiasing task demonstrate that our method can achieve comparable or even superior performance than conventional DG algorithms without relying on domain labels. Our code will be publicly available upon the paper is accepted.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2401.00608.pdf' target='_blank'>https://arxiv.org/pdf/2401.00608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vardaan Pahuja, Weidi Luo, Yu Gu, Cheng-Hao Tu, Hong-You Chen, Tanya Berger-Wolf, Charles Stewart, Song Gao, Wei-Lun Chao, Yu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00608">Reviving the Context: Camera Trap Species Classification as Link Prediction on Multimodal Knowledge Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camera traps are important tools in animal ecology for biodiversity monitoring and conservation. However, their practical application is limited by issues such as poor generalization to new and unseen locations. Images are typically associated with diverse forms of context, which may exist in different modalities. In this work, we exploit the structured context linked to camera trap images to boost out-of-distribution generalization for species classification tasks in camera traps. For instance, a picture of a wild animal could be linked to details about the time and place it was captured, as well as structured biological knowledge about the animal species. While often overlooked by existing studies, incorporating such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively incorporating such heterogeneous context into the visual domain is a challenging problem. To address this, we propose a novel framework that transforms species classification as link prediction in a multimodal knowledge graph (KG). This framework enables the seamless integration of diverse multimodal contexts for visual recognition. We apply this framework for out-of-distribution species classification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets and achieve competitive performance with state-of-the-art approaches. Furthermore, our framework enhances sample efficiency for recognizing under-represented species.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2312.14394.pdf' target='_blank'>https://arxiv.org/pdf/2312.14394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tangwen Qian, Yile Chen, Gao Cong, Yongjun Xu, Fei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14394">AdapTraj: A Multi-Source Domain Generalization Framework for Multi-Agent Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent trajectory prediction, as a critical task in modeling complex interactions of objects in dynamic systems, has attracted significant research attention in recent years. Despite the promising advances, existing studies all follow the assumption that data distribution observed during model learning matches that encountered in real-world deployments. However, this assumption often does not hold in practice, as inherent distribution shifts might exist in the mobility patterns for deployment environments, thus leading to poor domain generalization and performance degradation. Consequently, it is appealing to leverage trajectories from multiple source domains to mitigate such discrepancies for multi-agent trajectory prediction task. However, the development of multi-source domain generalization in this task presents two notable issues: (1) negative transfer; (2) inadequate modeling for external factors. To address these issues, we propose a new causal formulation to explicitly model four types of features: domain-invariant and domain-specific features for both the focal agent and neighboring agents. Building upon the new formulation, we propose AdapTraj, a multi-source domain generalization framework specifically tailored for multi-agent trajectory prediction. AdapTraj serves as a plug-and-play module that is adaptable to a variety of models. Extensive experiments on four datasets with different domains demonstrate that AdapTraj consistently outperforms other baselines by a substantial margin.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2311.00807.pdf' target='_blank'>https://arxiv.org/pdf/2311.00807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suraj Jyothi Unni, Raha Moraffah, Huan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00807">VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing domain generalization datasets for VQA exhibit a unilateral focus on textual shifts while VQA being a multi-modal task contains shifts across both visual and textual domains. We propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift induced pipeline. Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing methods to joint multi-modal distribution shifts. validating that comprehensive multi-modal shifts are critical for robust VQA generalization. Models trained on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming the value of VQA-GEN. Further, we analyze the importance of each shift technique of our pipeline contributing to the generalization of the model.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2309.13005.pdf' target='_blank'>https://arxiv.org/pdf/2309.13005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Lin, Chen Zhao, Minglai Shao, Baoluo Meng, Xujiang Zhao, Haifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13005">Towards Counterfactual Fairness-aware Domain Generalization in Changing Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges related to unfair classification. Our strategy is rooted in the principles of causal inference to tackle these dual issues. To examine the intricate relationship between semantic information, sensitive attributes, and environmental cues, we systematically categorize exogenous uncertainty factors into four latent variables: 1) semantic information influenced by sensitive attributes, 2) semantic information unaffected by sensitive attributes, 3) environmental cues influenced by sensitive attributes, and 4) environmental cues unaffected by sensitive attributes. By incorporating fairness regularization, we exclusively employ semantic information for classification purposes. Empirical validation on synthetic and real-world datasets substantiates the effectiveness of our approach, demonstrating improved accuracy levels while ensuring the preservation of fairness in the evolving landscape of continuous domains.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2307.13856.pdf' target='_blank'>https://arxiv.org/pdf/2307.13856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Agnihotri, Kanchana Vaishnavi Gandikota, Julia Grabinski, Paramanand Chandramouli, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13856">On the unreasonable vulnerability of transformers for image restoration -- and an easy fix</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration. As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration. We consider the recently proposed Restormer model, as well as NAFNet and the "Baseline network" which are both simplified versions of a Restormer. We use Projected Gradient Descent (PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise prediction tasks for our robustness evaluation. Our experiments are performed on real-world images from the GoPro dataset for image deblurring. Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks. We attempt to improve their robustness through adversarial training. While this yields a significant increase in robustness for Restormer, results on other networks are less promising. Interestingly, the design choices in NAFNet and Baselines, which were based on iid performance, and not on robust generalization, seem to be at odds with the model robustness. Thus, we investigate this further and find a fix.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2306.08076.pdf' target='_blank'>https://arxiv.org/pdf/2306.08076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiner Li, Shurui Gui, Youzhi Luo, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08076">Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates both structure and feature spaces to generate OOD graph data. Our design tailors OOD samples for specific shifts without corrupting underlying causal mechanisms. Theoretical analysis and empirical results evidence the effectiveness of our method in solving target shifts, showing substantial and constant improvements across various graph OOD tasks.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2304.14369.pdf' target='_blank'>https://arxiv.org/pdf/2304.14369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingchuan Ma, Peter Yichen Chen, Bolei Deng, Joshua B. Tenenbaum, Tao Du, Chuang Gan, Wojciech Matusik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14369">Learning Neural Constitutive Laws From Motion Observations for Generalizable PDE Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a hybrid neural network (NN) and PDE approach for learning generalizable PDE dynamics from motion observations. Many NN approaches learn an end-to-end model that implicitly models both the governing PDE and constitutive models (or material models). Without explicit PDE knowledge, these approaches cannot guarantee physical correctness and have limited generalizability. We argue that the governing PDEs are often well-known and should be explicitly enforced rather than learned. Instead, constitutive models are particularly suitable for learning due to their data-fitting nature. To this end, we introduce a new framework termed "Neural Constitutive Laws" (NCLaw), which utilizes a network architecture that strictly guarantees standard constitutive priors, including rotation equivariance and undeformed state equilibrium. We embed this network inside a differentiable simulation and train the model by minimizing a loss function based on the difference between the simulation and the motion observation. We validate NCLaw on various large-deformation dynamical systems, ranging from solids to fluids. After training on a single motion trajectory, our method generalizes to new geometries, initial/boundary conditions, temporal ranges, and even multi-physics systems. On these extremely out-of-distribution generalization tasks, NCLaw is orders-of-magnitude more accurate than previous NN approaches. Real-world experiments demonstrate our method's ability to learn constitutive laws from videos.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2304.11697.pdf' target='_blank'>https://arxiv.org/pdf/2304.11697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Zhiwei Li, Zhenhong Zou, Xin Gao, Yijin Xiong, Dafeng Jin, Jun Li, Huaping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11697">Informative Data Selection with Uncertainty for Multi-modal Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Noise has always been nonnegligible trouble in object detection by creating confusion in model reasoning, thereby reducing the informativeness of the data. It can lead to inaccurate recognition due to the shift in the observed pattern, that requires a robust generalization of the models. To implement a general vision model, we need to develop deep learning models that can adaptively select valid information from multi-modal data. This is mainly based on two reasons. Multi-modal learning can break through the inherent defects of single-modal data, and adaptive information selection can reduce chaos in multi-modal data. To tackle this problem, we propose a universal uncertainty-aware multi-modal fusion model. It adopts a multi-pipeline loosely coupled architecture to combine the features and results from point clouds and images. To quantify the correlation in multi-modal information, we model the uncertainty, as the inverse of data information, in different modalities and embed it in the bounding box generation. In this way, our model reduces the randomness in fusion and generates reliable output. Moreover, we conducted a completed investigation on the KITTI 2D object detection dataset and its derived dirty data. Our fusion model is proven to resist severe noise interference like Gaussian, motion blur, and frost, with only slight degradation. The experiment results demonstrate the benefits of our adaptive fusion. Our analysis on the robustness of multi-modal fusion will provide further insights for future research.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2302.09795.pdf' target='_blank'>https://arxiv.org/pdf/2302.09795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lilian Ngweta, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09795">Simple Disentanglement of Style and Content in Visual Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning visual representations with interpretable features, i.e., disentangled representations, remains a challenging problem. Existing methods demonstrate some success but are hard to apply to large-scale vision datasets like ImageNet. In this work, we propose a simple post-processing framework to disentangle content and style in learned representations from pre-trained vision models. We model the pre-trained features probabilistically as linearly entangled combinations of the latent content and style factors and develop a simple disentanglement algorithm based on the probabilistic model. We show that the method provably disentangles content and style features and verify its efficacy empirically. Our post-processed features yield significant domain generalization performance improvements when the distribution shift occurs due to style changes or style-related spurious correlations.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2201.11620.pdf' target='_blank'>https://arxiv.org/pdf/2201.11620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lidia Garrucho, Kaisar Kushibar, Socayna Jouide, Oliver Diaz, Laura Igual, Karim Lekadir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.11620">Domain generalization in deep learning-based mass detection in mammography: A large-scale multi-center study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-aided detection systems based on deep learning have shown great potential in breast cancer detection. However, the lack of domain generalization of artificial neural networks is an important obstacle to their deployment in changing clinical environments. In this work, we explore the domain generalization of deep learning methods for mass detection in digital mammography and analyze in-depth the sources of domain shift in a large-scale multi-center setting. To this end, we compare the performance of eight state-of-the-art detection methods, including Transformer-based models, trained in a single domain and tested in five unseen domains. Moreover, a single-source mass detection training pipeline is designed to improve the domain generalization without requiring images from the new domain. The results show that our workflow generalizes better than state-of-the-art transfer learning-based approaches in four out of five domains while reducing the domain shift caused by the different acquisition protocols and scanner manufacturers. Subsequently, an extensive analysis is performed to identify the covariate shifts with bigger effects on the detection performance, such as due to differences in patient age, breast density, mass size, and mass malignancy. Ultimately, this comprehensive study provides key insights and best practices for future research on domain generalization in deep learning-based breast cancer detection.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2109.05742.pdf' target='_blank'>https://arxiv.org/pdf/2109.05742.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijun Yang, Shujun Wang, Lei Zhu, Lequan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.05742">HCDG: A Hierarchical Consistency Framework for Domain Generalization on Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern deep neural networks struggle to transfer knowledge and generalize across diverse domains when deployed to real-world applications. Currently, domain generalization (DG) is introduced to learn a universal representation from multiple domains to improve the network generalization ability on unseen domains. However, previous DG methods only focus on the data-level consistency scheme without considering the synergistic regularization among different consistency schemes. In this paper, we present a novel Hierarchical Consistency framework for Domain Generalization (HCDG) by integrating Extrinsic Consistency and Intrinsic Consistency synergistically. Particularly, for the Extrinsic Consistency, we leverage the knowledge across multiple source domains to enforce data-level consistency. To better enhance such consistency, we design a novel Amplitude Gaussian-mixing strategy into Fourier-based data augmentation called DomainUp. For the Intrinsic Consistency, we perform task-level consistency for the same instance under the dual-task scenario. We evaluate the proposed HCDG framework on two medical image segmentation tasks, i.e., optic cup/disc segmentation on fundus images and prostate MRI segmentation. Extensive experimental results manifest the effectiveness and versatility of our HCDG framework.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2510.01248.pdf' target='_blank'>https://arxiv.org/pdf/2510.01248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruyue Liu, Rong Yin, Xiangzhen Bo, Xiaoshuai Hao, Yong Liu, Jinwen Zhong, Can Ma, Weiping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01248">SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large scale pretrained models have revolutionized Natural Language Processing (NLP) and Computer Vision (CV), showcasing remarkable cross domain generalization abilities. However, in graph learning, models are typically trained on individual graph datasets, limiting their capacity to transfer knowledge across different graphs and tasks. This approach also heavily relies on large volumes of annotated data, which presents a significant challenge in resource-constrained settings. Unlike NLP and CV, graph structured data presents unique challenges due to its inherent heterogeneity, including domain specific feature spaces and structural diversity across various applications. To address these challenges, we propose a novel structure aware self supervised learning method for Text Attributed Graphs (SSTAG). By leveraging text as a unified representation medium for graph learning, SSTAG bridges the gap between the semantic reasoning of Large Language Models (LLMs) and the structural modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces a dual knowledge distillation framework that co-distills both LLMs and GNNs into structure-aware multilayer perceptrons (MLPs), enhancing the scalability of large-scale TAGs. Additionally, we introduce an in-memory mechanism that stores typical graph representations, aligning them with memory anchors in an in-memory repository to integrate invariant knowledge, thereby improving the model's generalization ability. Extensive experiments demonstrate that SSTAG outperforms state-of-the-art models on cross-domain transfer learning tasks, achieves exceptional scalability, and reduces inference costs while maintaining competitive performance.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2509.12728.pdf' target='_blank'>https://arxiv.org/pdf/2509.12728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongsol Kim, Chanseok Lee, Jongin You, Jong Chul Ye, Mooseok Jang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12728">Generalizable Holographic Reconstruction via Amplitude-Only Diffusion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Phase retrieval in inline holography is a fundamental yet ill-posed inverse problem due to the nonlinear coupling between amplitude and phase in coherent imaging. We present a novel off-the-shelf solution that leverages a diffusion model trained solely on object amplitude to recover both amplitude and phase from diffraction intensities. Using a predictor-corrector sampling framework with separate likelihood gradients for amplitude and phase, our method enables complex field reconstruction without requiring ground-truth phase data for training. We validate the proposed approach through extensive simulations and experiments, demonstrating robust generalization across diverse object shapes, imaging system configurations, and modalities, including lensless setups. Notably, a diffusion prior trained on simple amplitude data (e.g., polystyrene beads) successfully reconstructs complex biological tissue structures, highlighting the method's adaptability. This framework provides a cost-effective, generalizable solution for nonlinear inverse problems in computational imaging, and establishes a foundation for broader coherent imaging applications beyond holography.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2506.08011.pdf' target='_blank'>https://arxiv.org/pdf/2506.08011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08011">Play to Generalize: Learning to Reason Through Game Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2505.12684.pdf' target='_blank'>https://arxiv.org/pdf/2505.12684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinlin Zhu, Xunkai Li, Jishuo Jia, Miao Hu, Di Wu, Meikang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12684">Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2503.16106.pdf' target='_blank'>https://arxiv.org/pdf/2503.16106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad Hassan N C, Divyam Gupta, Mainak Singha, Sai Bhargav Rongali, Ankit Jha, Muhammad Haris Khan, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16106">OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel paradigm unifying low-shot learning with open-set domain generalization (ODG). While prompt-based methods using models like CLIP have advanced DG, they falter in low-data regimes (e.g., 1-shot) and lack precision in detecting open-set samples with fine-grained semantics related to training classes. To address these challenges, we propose OSLOPROMPT, an advanced prompt-learning framework for CLIP with two core innovations. First, to manage limited supervision across source domains and improve DG, we introduce a domain-agnostic prompt-learning mechanism that integrates adaptable domain-specific cues and visually guided semantic attributes through a novel cross-attention module, besides being supported by learnable domain- and class-generic visual prompts to enhance cross-modal adaptability. Second, to improve outlier rejection during inference, we classify unfamiliar samples as "unknown" and train specialized prompts with systematically synthesized pseudo-open samples that maintain fine-grained relationships to known classes, generated through a targeted query strategy with off-the-shelf foundation models. This strategy enhances feature learning, enabling our model to detect open samples with varied granularity more effectively. Extensive evaluations across five benchmarks demonstrate that OSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly outperforming existing methods.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2501.18564.pdf' target='_blank'>https://arxiv.org/pdf/2501.18564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoquan Fang, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox, Ranjay Krishna, Jiafei Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18564">SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation systems operating in diverse, dynamic environments must exhibit three critical abilities: multitask interaction, generalization to unseen scenarios, and spatial memory. While significant progress has been made in robotic manipulation, existing approaches often fall short in generalization to complex environmental variations and addressing memory-dependent tasks. To bridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based policy that leverages multi-resolution upsampling with visual representations from large-scale foundation model. SAM2Act achieves a state-of-the-art average success rate of 86.8% across 18 tasks in the RLBench benchmark, and demonstrates robust generalization on The Colosseum benchmark, with only a 4.3% performance gap under diverse environmental perturbations. Building on this foundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2, which incorporates a memory bank, an encoder, and an attention mechanism to enhance spatial memory. To address the need for evaluating memory-dependent tasks, we introduce MemoryBench, a novel benchmark designed to assess spatial memory and action recall in robotic manipulation. SAM2Act+ achieves an average success rate of 94.3% on memory-based tasks in MemoryBench, significantly outperforming existing approaches and pushing the boundaries of memory-based robotic systems. Project page: sam2act.github.io.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2412.14401.pdf' target='_blank'>https://arxiv.org/pdf/2412.14401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ainaz Eftekhar, Rose Hendrix, Luca Weihs, Jiafei Duan, Ege Caglar, Jordi Salvador, Alvaro Herrasti, Winson Han, Eli VanderBil, Aniruddha Kembhavi, Ali Farhadi, Ranjay Krishna, Kiana Ehsani, Kuo-Hao Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14401">The One RING: a Robotic Indoor Navigation Generalist</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern robots vary significantly in shape, size, and sensor configurations used to perceive and interact with their environments. However, most navigation policies are embodiment-specific--a policy trained on one robot typically fails to generalize to another, even with minor changes in body size or camera viewpoint. As custom hardware becomes increasingly common, there is a growing need for a single policy that generalizes across embodiments, eliminating the need to retrain for each specific robot. In this paper, we introduce RING (Robotic Indoor Navigation Generalist), an embodiment-agnostic policy that turns any mobile robot into an effective indoor semantic navigator. Trained entirely in simulation, RING leverages large-scale randomization over robot embodiments to enable robust generalization to many real-world platforms. To support this, we augment the AI2-THOR simulator to instantiate robots with controllable configurations, varying in body size, rotation pivot point, and camera parameters. On the visual object-goal navigation task, RING achieves strong cross-embodiment (XE) generalization--72.1% average success rate across five simulated embodiments (a 16.7% absolute improvement on the Chores-S benchmark) and 78.9% across four real-world platforms, including Stretch RE-1, LoCoBot, and Unitree Go1--matching or even surpassing embodiment-specific policies. We further deploy RING on the RB-Y1 wheeled humanoid in a real-world kitchen environment, showcasing its out-of-the-box potential for mobile manipulation platforms. (Project website: https://one-ring-policy.allen.ai)
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2411.17798.pdf' target='_blank'>https://arxiv.org/pdf/2411.17798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbin Zheng, Qianhui Xu, Ruichen Xia, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17798">DapPep: Domain Adaptive Peptide-agnostic Learning for Universal T-cell Receptor-antigen Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying T-cell receptors (TCRs) that interact with antigenic peptides provides the technical basis for developing vaccines and immunotherapies. The emergent deep learning methods excel at learning antigen binding patterns from known TCRs but struggle with novel or sparsely represented antigens. However, binding specificity for unseen antigens or exogenous peptides is critical. We introduce a domain-adaptive peptide-agnostic learning framework DapPep for universal TCR-antigen binding affinity prediction to address this challenge. The lightweight self-attention architecture combines a pre-trained protein language model with an inner-loop self-supervised regime to enable robust TCR-peptide representations. Extensive experiments on various benchmarks demonstrate that DapPep consistently outperforms existing tools, showcasing robust generalization capability, especially for data-scarce settings and unseen peptides. Moreover, DapPep proves effective in challenging clinical tasks such as sorting reactive T cells in tumor neoantigen therapy and identifying key positions in 3D structures.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2410.23156.pdf' target='_blank'>https://arxiv.org/pdf/2410.23156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, JoÃ£o F. Henriques, Kevin Ellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23156">VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2408.14950.pdf' target='_blank'>https://arxiv.org/pdf/2408.14950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangchen Zhao, Changde Du, Hui Li, Huiguang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14950">NeuralOOD: Improving Out-of-Distribution Generalization Performance with Brain-machine Fusion Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Neural Networks (DNNs) have demonstrated exceptional recognition capabilities in traditional computer vision (CV) tasks. However, existing CV models often suffer a significant decrease in accuracy when confronted with out-of-distribution (OOD) data. In contrast to these DNN models, human can maintain a consistently low error rate when facing OOD scenes, partly attributed to the rich prior cognitive knowledge stored in the human brain. Previous OOD generalization researches only focus on the single modal, overlooking the advantages of multimodal learning method. In this paper, we utilize the multimodal learning method to improve the OOD generalization and propose a novel Brain-machine Fusion Learning (BMFL) framework. We adopt the cross-attention mechanism to fuse the visual knowledge from CV model and prior cognitive knowledge from the human brain. Specially, we employ a pre-trained visual neural encoding model to predict the functional Magnetic Resonance Imaging (fMRI) from visual features which eliminates the need for the fMRI data collection and pre-processing, effectively reduces the workload associated with conventional BMFL methods. Furthermore, we construct a brain transformer to facilitate the extraction of knowledge inside the fMRI data. Moreover, we introduce the Pearson correlation coefficient maximization regularization method into the training process, which improves the fusion capability with better constrains. Our model outperforms the DINOv2 and baseline models on the ImageNet-1k validation dataset as well as six curated OOD datasets, showcasing its superior performance in diverse scenarios.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2408.09937.pdf' target='_blank'>https://arxiv.org/pdf/2408.09937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaining Zhang, Junyu Liu, Liu Liu, Liang Jiang, Min-Hsiu Hsieh, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09937">The curse of random quantum data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum machine learning, which involves running machine learning algorithms on quantum devices, may be one of the most significant flagship applications for these devices. Unlike its classical counterparts, the role of data in quantum machine learning has not been fully understood. In this work, we quantify the performances of quantum machine learning in the landscape of quantum data. Provided that the encoding of quantum data is sufficiently random, the performance, we find that the training efficiency and generalization capabilities in quantum machine learning will be exponentially suppressed with the increase in the number of qubits, which we call "the curse of random quantum data". Our findings apply to both the quantum kernel method and the large-width limit of quantum neural networks. Conversely, we highlight that through meticulous design of quantum datasets, it is possible to avoid these curses, thereby achieving efficient convergence and robust generalization. Our conclusions are corroborated by extensive numerical simulations.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2407.12996.pdf' target='_blank'>https://arxiv.org/pdf/2407.12996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiquan Lu, Xiaotian Liu, Yefan Zhou, Qunli Li, Kurt Keutzer, Michael W. Mahoney, Yujun Yan, Huanrui Yang, Yaoqing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12996">Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on deep ensembles have identified the sharpness of the local minima of individual learners and the diversity of the ensemble members as key factors in improving test-time performance. Building on this, our study investigates the interplay between sharpness and diversity within deep ensembles, illustrating their crucial role in robust generalization to both in-distribution (ID) and out-of-distribution (OOD) data. We discover a trade-off between sharpness and diversity: minimizing the sharpness in the loss landscape tends to diminish the diversity of individual members within the ensemble, adversely affecting the ensemble's improvement. The trade-off is justified through our theoretical analysis and verified empirically through extensive experiments. To address the issue of reduced diversity, we introduce SharpBalance, a novel training approach that balances sharpness and diversity within ensembles. Theoretically, we show that our training strategy achieves a better sharpness-diversity trade-off. Empirically, we conducted comprehensive evaluations in various data sets (CIFAR-10, CIFAR-100, TinyImageNet) and showed that SharpBalance not only effectively improves the sharpness-diversity trade-off, but also significantly improves ensemble performance in ID and OOD scenarios.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2406.01066.pdf' target='_blank'>https://arxiv.org/pdf/2406.01066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihuang Zheng, Jiashuo Liu, Jiaxing Li, Jiayun Wu, Peng Cui, Youyong Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01066">Topology-Aware Dynamic Reweighting for Distribution Shifts on Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) are widely used for node classification tasks but often fail to generalize when training and test nodes come from different distributions, limiting their practicality. To overcome this, recent approaches adopt invariant learning techniques from the out-of-distribution (OOD) generalization field, which seek to establish stable prediction methods across environments. However, the applicability of these invariant assumptions to graph data remains unverified, and such methods often lack solid theoretical support. In this work, we introduce the Topology-Aware Dynamic Reweighting (TAR) framework, which dynamically adjusts sample weights through gradient flow in the geometric Wasserstein space during training. Instead of relying on strict invariance assumptions, we prove that our method is able to provide distributional robustness, thereby enhancing the out-of-distribution generalization performance on graph data. By leveraging the inherent graph structure, TAR effectively addresses distribution shifts. Our framework's superiority is demonstrated through standard testing on four graph OOD datasets and three class-imbalanced node classification datasets, exhibiting marked improvements over existing methods.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2406.00661.pdf' target='_blank'>https://arxiv.org/pdf/2406.00661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayun Wu, Jiashuo Liu, Peng Cui, Zhiwei Steven Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00661">Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We establish a new model-agnostic optimization framework for out-of-distribution generalization via multicalibration, a criterion that ensures a predictor is calibrated across a family of overlapping groups. Multicalibration is shown to be associated with robustness of statistical inference under covariate shift. We further establish a link between multicalibration and robustness for prediction tasks both under and beyond covariate shift. We accomplish this by extending multicalibration to incorporate grouping functions that consider covariates and labels jointly. This leads to an equivalence of the extended multicalibration and invariance, an objective for robust learning in existence of concept shift. We show a linear structure of the grouping function class spanned by density ratios, resulting in a unifying framework for robust learning by designing specific grouping functions. We propose MC-Pseudolabel, a post-processing algorithm to achieve both extended multicalibration and out-of-distribution generalization. The algorithm, with lightweight hyperparameters and optimization through a series of supervised regression steps, achieves superior performance on real-world datasets with distribution shift.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2404.10312.pdf' target='_blank'>https://arxiv.org/pdf/2404.10312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyi Li, Xuhan Sheng, Weiqi Li, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10312">OmniSSR: Zero-shot Omnidirectional Image Super-Resolution using Stable Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Omnidirectional images (ODIs) are commonly used in real-world visual tasks, and high-resolution ODIs help improve the performance of related visual tasks. Most existing super-resolution methods for ODIs use end-to-end learning strategies, resulting in inferior realness of generated images and a lack of effective out-of-domain generalization capabilities in training methods. Image generation methods represented by diffusion model provide strong priors for visual tasks and have been proven to be effectively applied to image restoration tasks. Leveraging the image priors of the Stable Diffusion (SD) model, we achieve omnidirectional image super-resolution with both fidelity and realness, dubbed as OmniSSR. Firstly, we transform the equirectangular projection (ERP) images into tangent projection (TP) images, whose distribution approximates the planar image domain. Then, we use SD to iteratively sample initial high-resolution results. At each denoising iteration, we further correct and update the initial results using the proposed Octadecaplex Tangent Information Interaction (OTII) and Gradient Decomposition (GD) technique to ensure better consistency. Finally, the TP images are transformed back to obtain the final high-resolution results. Our method is zero-shot, requiring no training or fine-tuning. Experiments of our method on two benchmark datasets demonstrate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2403.01874.pdf' target='_blank'>https://arxiv.org/pdf/2403.01874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Yu, Jiashuo Liu, Xingxuan Zhang, Jiayun Wu, Peng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01874">A Survey on Evaluation of Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models, while progressively advanced, rely heavily on the IID assumption, which is often unfulfilled in practice due to inevitable distribution shifts. This renders them susceptible and untrustworthy for deployment in risk-sensitive applications. Such a significant problem has consequently spawned various branches of works dedicated to developing algorithms capable of Out-of-Distribution (OOD) generalization. Despite these efforts, much less attention has been paid to the evaluation of OOD generalization, which is also a complex and fundamental problem. Its goal is not only to assess whether a model's OOD generalization capability is strong or not, but also to evaluate where a model generalizes well or poorly. This entails characterizing the types of distribution shifts that a model can effectively address, and identifying the safe and risky input regions given a model. This paper serves as the first effort to conduct a comprehensive review of OOD evaluation. We categorize existing research into three paradigms: OOD performance testing, OOD performance prediction, and OOD intrinsic property characterization, according to the availability of test data. Additionally, we briefly discuss OOD evaluation in the context of pretrained models. In closing, we propose several promising directions for future research in OOD evaluation.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2402.07485.pdf' target='_blank'>https://arxiv.org/pdf/2402.07485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Zhao, Yifei Xin, Zhesong Yu, Bilei Zhu, Lu Lu, Zejun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07485">MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of audio-language pre-training (ALP), the challenge of achieving cross-modal alignment is significant. Moreover, the integration of audio inputs with diverse distributions and task variations poses challenges in developing generic audio-language models. In this study, we present MINT, a novel ALP framework boosting audio-language models through multi-target pre-training and instruction tuning. MINT leverages the strength of frozen pre-trained audio encoders and large language models (LLM) to improve audio-language pre-training, enabling effective transferablility to both audio-text understanding and generation tasks. To address the modality gap, we introduce Bridge-Net, a trainable module that enhances cross-modality alignment and the model's ability to follow instructions for a variety of audio-text tasks. Bridge-Net is pivotal within MINT, initially enhancing audio-language representation learning through a multi-target pre-training approach. Subsequently, Bridge-Net further boosts audio-to-language generative learning by integrating a frozen language model with instruction tuning. This integration empowers MINT to extract features in a flexible and effective manner, specifically tailored to the provided instructions for diverse tasks. Experimental results demonstrate that MINT attains superior performance across various audio-language understanding and generation tasks, highlighting its robust generalization capabilities even in zero-shot scenarios.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2402.06599.pdf' target='_blank'>https://arxiv.org/pdf/2402.06599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, Peng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06599">On the Out-Of-Distribution Generalization of Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the generalization boundaries of current Multimodal Large Language Models (MLLMs) via comprehensive evaluation under out-of-distribution scenarios and domain-specific tasks. We evaluate their zero-shot generalization across synthetic images, real-world distributional shifts, and specialized datasets like medical and molecular imagery. Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation. To understand the cause of unreliable performance, we analyze three hypotheses: semantic misinterpretation, visual feature extraction insufficiency, and mapping deficiency. Results identify mapping deficiency as the primary hurdle. To address this problem, we show that in-context learning (ICL) can significantly enhance MLLMs' generalization, opening new avenues for overcoming generalization barriers. We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples and test data.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2311.02599.pdf' target='_blank'>https://arxiv.org/pdf/2311.02599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prathmesh Bele, Valay Bundele, Avigyan Bhattacharya, Ankit Jha, Gemma Roig, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02599">Learning Class and Domain Augmentations for Single-Source Open-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source open-domain generalization (SS-ODG) addresses the challenge of labeled source domains with supervision during training and unlabeled novel target domains during testing. The target domain includes both known classes from the source domain and samples from previously unseen classes. Existing techniques for SS-ODG primarily focus on calibrating source-domain classifiers to identify open samples in the target domain. However, these methods struggle with visually fine-grained open-closed data, often misclassifying open samples as closed-set classes. Moreover, relying solely on a single source domain restricts the model's ability to generalize. To overcome these limitations, we propose a novel framework called SODG-Net that simultaneously synthesizes novel domains and generates pseudo-open samples using a learning-based objective, in contrast to the ad-hoc mixing strategies commonly found in the literature. Our approach enhances generalization by diversifying the styles of known class samples using a novel metric criterion and generates diverse pseudo-open samples to train a unified and confident multi-class classifier capable of handling both open and closed-set data. Extensive experimental evaluations conducted on multiple benchmarks consistently demonstrate the superior performance of SODG-Net compared to the literature.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2309.09380.pdf' target='_blank'>https://arxiv.org/pdf/2309.09380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zirui He, Huiqi Deng, Haiyan Zhao, Ninghao Liu, Mengnan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09380">Mitigating Shortcuts in Language Models with Soft Label Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has shown that large language models rely on spurious correlations in the data for natural language understanding (NLU) tasks. In this work, we aim to answer the following research question: Can we reduce spurious correlations by modifying the ground truth labels of the training data? Specifically, we propose a simple yet effective debiasing framework, named Soft Label Encoding (SoftLE). We first train a teacher model with hard labels to determine each sample's degree of relying on shortcuts. We then add one dummy class to encode the shortcut degree, which is used to smooth other dimensions in the ground truth label to generate soft labels. This new ground truth label is used to train a more robust student model. Extensive experiments on two NLU benchmark tasks demonstrate that SoftLE significantly improves out-of-distribution generalization while maintaining satisfactory in-distribution accuracy.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2307.07181.pdf' target='_blank'>https://arxiv.org/pdf/2307.07181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chia-Yuan Chang, Yu-Neng Chuang, Guanchu Wang, Mengnan Du, Na Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07181">DISPEL: Domain Generalization via Domain-Specific Liberating</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fine-tuned models. We derive a generalization error bound to guarantee the generalization performance by optimizing a designed objective loss. The experimental results on five benchmarks demonstrate DISPEL outperforms existing methods and can further generalize various algorithms.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2305.15644.pdf' target='_blank'>https://arxiv.org/pdf/2305.15644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyan Shen, Han Yu, Peng Cui, Jiashuo Liu, Xingxuan Zhang, Linjun Zhou, Furui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15644">Meta Adaptive Task Sampling for Few-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To ensure the out-of-distribution (OOD) generalization performance, traditional domain generalization (DG) methods resort to training on data from multiple sources with different underlying distributions. And the success of those DG methods largely depends on the fact that there are diverse training distributions. However, it usually needs great efforts to obtain enough heterogeneous data due to the high expenses, privacy issues or the scarcity of data. Thus an interesting yet seldom investigated problem arises: how to improve the OOD generalization performance when the perceived heterogeneity is limited. In this paper, we instantiate a new framework called few-domain generalization (FDG), which aims to learn a generalizable model from very few domains of novel tasks with the knowledge acquired from previous learning experiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling (MATS) procedure to differentiate base tasks according to their semantic and domain-shift similarity to the novel task. Empirically, we show that the newly introduced FDG framework can substantially improve the OOD generalization performance on the novel task and further combining MATS with episodic training could outperform several state-of-the-art DG baselines on widely used benchmarks like PACS and DomainNet.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2305.15253.pdf' target='_blank'>https://arxiv.org/pdf/2305.15253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Yu, Xingxuan Zhang, Renzhe Xu, Jiashuo Liu, Yue He, Peng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15253">Rethinking the Evaluation Protocol of Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is required that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the risks of test data information leakage from two aspects of the current evaluation protocol: supervised pretraining on ImageNet and oracle model selection. We propose modifications to the current protocol that we should employ self-supervised pretraining or train from scratch instead of employing the current supervised pretraining, and we should use multiple test domains. These would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the modified protocol and introduce new leaderboards to encourage future research in domain generalization with a fairer comparison.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2305.06221.pdf' target='_blank'>https://arxiv.org/pdf/2305.06221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Tian, Yiqi Wang, Xianda Guo, Zheng Zhu, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06221">Multi-Prompt with Depth Partitioned Cross-Modal Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, soft prompt learning methods have been proposed to fine-tune large-scale vision-language pre-trained models for various downstream tasks. These methods typically combine learnable textual tokens with class tokens as input for models with frozen parameters. However, they often employ a single prompt to describe class contexts, failing to capture categories' diverse attributes adequately. This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi-modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts. Our method divides the visual encoder depths and connects learnable prompts to the separated visual depths, enabling different prompts to capture the hierarchical contextual depths of visual representations. Furthermore, to maximize the advantages of multi-prompt learning, we incorporate prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabilities of our approach. We evaluate the effectiveness of our approach on three challenging tasks: new class generalization, cross-dataset evaluation, and domain generalization. For instance, our method achieves a $79.28$ harmonic mean, averaged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp), demonstrating significant competitiveness compared to state-of-the-art prompting methods.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2304.06280.pdf' target='_blank'>https://arxiv.org/pdf/2304.06280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Liu, Zhaoxuan Tan, Heng Wang, Shangbin Feng, Qinghua Zheng, Minnan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06280">BotMoE: Twitter Bot Detection with Community-Aware Mixtures of Modal-Specific Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Twitter bot detection has become a crucial task in efforts to combat online misinformation, mitigate election interference, and curb malicious propaganda. However, advanced Twitter bots often attempt to mimic the characteristics of genuine users through feature manipulation and disguise themselves to fit in diverse user communities, posing challenges for existing Twitter bot detection models. To this end, we propose BotMoE, a Twitter bot detection framework that jointly utilizes multiple user information modalities (metadata, textual content, network structure) to improve the detection of deceptive bots. Furthermore, BotMoE incorporates a community-aware Mixture-of-Experts (MoE) layer to improve domain generalization and adapt to different Twitter communities. Specifically, BotMoE constructs modal-specific encoders for metadata features, textual content, and graphical structure, which jointly model Twitter users from three modal-specific perspectives. We then employ a community-aware MoE layer to automatically assign users to different communities and leverage the corresponding expert networks. Finally, user representations from metadata, text, and graph perspectives are fused with an expert fusion layer, combining all three modalities while measuring the consistency of user information. Extensive experiments demonstrate that BotMoE significantly advances the state-of-the-art on three Twitter bot detection benchmarks. Studies also confirm that BotMoE captures advanced and evasive bots, alleviates the reliance on training data, and better generalizes to new and previously unseen user communities.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2304.00305.pdf' target='_blank'>https://arxiv.org/pdf/2304.00305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiashuo Liu, Jiayun Wu, Bo Li, Peng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00305">Predictive Heterogeneity: Measures and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As an intrinsic and fundamental property of big data, data heterogeneity exists in a variety of real-world applications, such as precision medicine, autonomous driving, financial applications, etc. For machine learning algorithms, the ignorance of data heterogeneity will greatly hurt the generalization performance and the algorithmic fairness, since the prediction mechanisms among different sub-populations are likely to differ from each other. In this work, we focus on the data heterogeneity that affects the prediction of machine learning models, and firstly propose the \emph{usable predictive heterogeneity}, which takes into account the model capacity and computational constraints. We prove that it can be reliably estimated from finite data with probably approximately correct (PAC) bounds. Additionally, we design a bi-level optimization algorithm to explore the usable predictive heterogeneity from data. Empirically, the explored heterogeneity provides insights for sub-population divisions in income prediction, crop yield prediction and image classification tasks, and leveraging such heterogeneity benefits the out-of-distribution generalization performance.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2302.09251.pdf' target='_blank'>https://arxiv.org/pdf/2302.09251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shirsha Bose, Ankit Jha, Enrico Fini, Mainak Singha, Elisa Ricci, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09251">StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale foundation models, such as CLIP, have demonstrated impressive zero-shot generalization performance on downstream tasks, leveraging well-designed language prompts. However, these prompt learning techniques often struggle with domain shift, limiting their generalization capabilities. In our study, we tackle this issue by proposing StyLIP, a novel approach for Domain Generalization (DG) that enhances CLIP's classification performance across domains. Our method focuses on a domain-agnostic prompt learning strategy, aiming to disentangle the visual style and content information embedded in CLIP's pre-trained vision encoder, enabling effortless adaptation to novel domains during inference. To achieve this, we introduce a set of style projectors that directly learn the domain-specific prompt tokens from the extracted multi-scale style features. These generated prompt embeddings are subsequently combined with the multi-scale visual content features learned by a content projector. The projectors are trained in a contrastive manner, utilizing CLIP's fixed vision and text backbones. Through extensive experiments conducted in five different DG settings on multiple benchmark datasets, we consistently demonstrate that StyLIP outperforms the current state-of-the-art (SOTA) methods.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2301.07845.pdf' target='_blank'>https://arxiv.org/pdf/2301.07845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuhao Zeng, Wei Wang, Fan Zhou, Charles Ling, Boyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07845">Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-stationary Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing domain generalization aims to learn a generalizable model to perform well even on unseen domains. For many real-world machine learning applications, the data distribution often shifts gradually along domain indices. For example, a self-driving car with a vision system drives from dawn to dusk, with the sky darkening gradually. Therefore, the system must be able to adapt to changes in ambient illumination and continue to drive safely on the road. In this paper, we formulate such problems as Evolving Domain Generalization, where a model aims to generalize well on a target domain by discovering and leveraging the evolving pattern of the environment. We then propose Directional Domain Augmentation (DDA), which simulates the unseen target features by mapping source data as augmentations through a domain transformer. Specifically, we formulate DDA as a bi-level optimization problem and solve it through a novel meta-learning approach in the representation space. We evaluate the proposed method on both synthetic datasets and realworld datasets, and empirical results show that our approach can outperform other existing methods.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2206.12444.pdf' target='_blank'>https://arxiv.org/pdf/2206.12444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon FÃ¶ll, Alina Dubatovka, Eugen Ernst, Siu Lun Chau, Martin Maritsch, Patrik Okanovic, Gudrun ThÃ¤ter, Joachim M. Buhmann, Felix Wortmann, Krikamol Muandet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.12444">Gated Domain Units for Multi-source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the dataset at training time, which can significantly impair the performance of a machine learning model in practical settings due to a lack of knowledge about the data's distribution at test time. To address this problem, we postulate that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. This assumption implies an invariant structure in the solution space that enables knowledge transfer to unseen domains. To exploit this property for domain generalization, we introduce a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution. During inference, a weighted ensemble of learning machines can be created by comparing new observations with the representations of each elementary distribution. Our flexible framework also accommodates scenarios where explicit domain information is not present. Extensive experiments on image, text, and graph data show consistent performance improvement on out-of-training target domains. These findings support the practicality of the I.E.D assumption and the effectiveness of GDUs for domain generalisation.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2108.13624.pdf' target='_blank'>https://arxiv.org/pdf/2108.13624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, Peng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.13624">Towards Out-Of-Distribution Generalization: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed ($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our discussion begins with a precise, formal characterization of the OOD generalization problem. Following that, we categorize existing methodologies into three segments: unsupervised representation learning, supervised model learning, and optimization, according to their positions within the overarching learning process. We provide an in-depth discussion on representative methodologies for each category, further elucidating the theoretical links between them. Subsequently, we outline the prevailing benchmark datasets employed in OOD generalization studies. To conclude, we overview the existing body of work in this domain and suggest potential avenues for future research on OOD generalization. A summary of the OOD generalization methodologies surveyed in this paper can be accessed at http://out-of-distribution-generalization.com.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2509.25747.pdf' target='_blank'>https://arxiv.org/pdf/2509.25747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialei Huang, Zhaoheng Yin, Yingdong Hu, Shuo Wang, Xingyu Lin, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25747">Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sim-to-real transfer remains a fundamental challenge in robot manipulation due to the entanglement of perception and control in end-to-end learning. We present a decoupled framework that learns each component where it is most reliable: control policies are trained in simulation with privileged state to master spatial layouts and manipulation dynamics, while perception is adapted only at deployment to bridge real observations to the frozen control policy. Our key insight is that control strategies and action patterns are universal across environments and can be learned in simulation through systematic randomization, while perception is inherently domain-specific and must be learned where visual observations are authentic. Unlike existing end-to-end approaches that require extensive real-world data, our method achieves strong performance with only 10-20 real demonstrations by reducing the complex sim-to-real problem to a structured perception alignment task. We validate our approach on tabletop manipulation tasks, demonstrating superior data efficiency and out-of-distribution generalization compared to end-to-end baselines. The learned policies successfully handle object positions and scales beyond the training distribution, confirming that decoupling perception from control fundamentally improves sim-to-real transfer.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2509.21207.pdf' target='_blank'>https://arxiv.org/pdf/2509.21207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olga Fink, Ismail Nejjar, Vinay Sharma, Keivan Faghih Niresi, Han Sun, Hao Dong, Chenghao Xu, Amaury Wei, Arthur Bizzi, Raffael Theiler, Yuan Tian, Leandro Von Krannichfeldt, Zhan Ma, Sergei Garmaev, Zepeng Zhang, Mengjie Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21207">From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prognostics and Health Management ensures the reliability, safety, and efficiency of complex engineered systems by enabling fault detection, anticipating equipment failures, and optimizing maintenance activities throughout an asset lifecycle. However, real-world PHM presents persistent challenges: sensor data is often noisy or incomplete, available labels are limited, and degradation behaviors and system interdependencies can be highly complex and nonlinear. Physics-informed machine learning has emerged as a promising approach to address these limitations by embedding physical knowledge into data-driven models. This review examines how incorporating learning and observational biases through physics-informed modeling and data strategies can guide models toward physically consistent and reliable predictions. Learning biases embed physical constraints into model training through physics-informed loss functions and governing equations, or by incorporating properties like monotonicity. Observational biases influence data selection and synthesis to ensure models capture realistic system behavior through virtual sensing for estimating unmeasured states, physics-based simulation for data augmentation, and multi-sensor fusion strategies. The review then examines how these approaches enable the transition from passive prediction to active decision-making through reinforcement learning, which allows agents to learn maintenance policies that respect physical constraints while optimizing operational objectives. This closes the loop between model-based predictions, simulation, and actual system operation, empowering adaptive decision-making. Finally, the review addresses the critical challenge of scaling PHM solutions from individual assets to fleet-wide deployment. Fast adaptation methods including meta-learning and few-shot learning are reviewed alongside domain generalization techniques ...
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2509.15791.pdf' target='_blank'>https://arxiv.org/pdf/2509.15791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan Pan, Kaiyu Guo, Dongli Xu, Zhaorui Tan, Chen Jiang, Deshu Chen, Xin Guo, Brian C. Lovell, Limei Han, Yuan Cheng, Mahsa Baktashmotlagh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15791">Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization ability of deep learning has been extensively studied in supervised settings, yet it remains less explored in unsupervised scenarios. Recently, the Unsupervised Domain Generalization (UDG) task has been proposed to enhance the generalization of models trained with prevalent unsupervised learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the challenge of distinguishing semantics from variations without category labels. Although some recent methods have employed domain labels to tackle this issue, such domain labels are often unavailable in real-world contexts. In this paper, we address these limitations by formalizing UDG as the task of learning a Minimal Sufficient Semantic Representation: a representation that (i) preserves all semantic information shared across augmented views (sufficiency), and (ii) maximally removes information irrelevant to semantics (minimality). We theoretically ground these objectives from the perspective of information theory, demonstrating that optimizing representations to achieve sufficiency and minimality directly reduces out-of-distribution risk. Practically, we implement this optimization through Minimal-Sufficient UDG (MS-UDG), a learnable model by integrating (a) an InfoNCE-based objective to achieve sufficiency; (b) two complementary components to promote minimality: a novel semantic-variation disentanglement loss and a reconstruction-based mechanism for capturing adequate variation. Empirically, MS-UDG sets a new state-of-the-art on popular unsupervised domain-generalization benchmarks, consistently outperforming existing SSL and UDG methods, without category or domain labels during representation learning.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2509.02597.pdf' target='_blank'>https://arxiv.org/pdf/2509.02597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuting Xu, Runtong Liu, Zhixuan Chen, Junlin Hou, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02597">Solutions for Mitotic Figure Detection and Atypical Classification in MIDOG 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has driven significant advances in mitotic figure analysis within computational pathology. In this paper, we present our approach to the Mitosis Domain Generalization (MIDOG) 2025 Challenge, which consists of two distinct tasks, i.e., mitotic figure detection and atypical mitosis classification. For the mitotic figure detection task, we propose a two-stage detection-classification framework that first localizes candidate mitotic figures and subsequently refines the predictions using a dedicated classification module. For the atypical mitosis classification task, we employ an ensemble strategy that integrates predictions from multiple state-of-the-art deep learning architectures to improve robustness and accuracy. Extensive experiments demonstrate the effectiveness of our proposed methods across both tasks.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2506.07631.pdf' target='_blank'>https://arxiv.org/pdf/2506.07631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brian Gordon, Yonatan Bitton, Andreea Marzoca, Yasumasa Onoe, Xiao Wang, Daniel Cohen-Or, Idan Szpektor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07631">Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (VLMs) now generate highly detailed, paragraphlength image captions, yet evaluating their factual accuracy remains challenging. Current methods often miss fine-grained errors, being designed for shorter texts or lacking datasets with verified inaccuracies. We introduce DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100 images, 14 VLMs) featuring over 10,216 sentence-level human annotations of factual correctness and explanatory rationales for errors, all within paragraph context. Building on this, we develop VNLI-Critique, a model for automated sentence-level factuality classification and critique generation. We highlight three key applications: (1) VNLI-Critique demonstrates robust generalization, validated by state-of-the-art performance on the M-HalDetect benchmark and strong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven AutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent alignment with human factuality judgments (e.g., 0.98 Spearman). (3) An innovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide LLM-based corrections, achieves substantial improvements in caption factuality (e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark alongside practical tools, designed to significantly elevate the standards for fine-grained evaluation and foster the improvement of VLM image understanding. Project page: https://google.github.io/unblocking-detail-caption
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2505.08464.pdf' target='_blank'>https://arxiv.org/pdf/2505.08464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lata Pangtey, Anukriti Bhatnagar, Shubhi Bansal, Shahid Shafi Dar, Nagendra Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08464">Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2505.03261.pdf' target='_blank'>https://arxiv.org/pdf/2505.03261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Ting Chen, Yu-Jiet Vong, Yi-Tsung Lee, Sy-Yen Kuo, Qiang Gao, Sizhuo Ma, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03261">DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Quality Assessment (VQA) aims to evaluate video quality based on perceptual distortions and human preferences. Despite the promising performance of existing methods using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), they often struggle to align closely with human perceptions, particularly in diverse real-world scenarios. This challenge is exacerbated by the limited scale and diversity of available datasets. To address this limitation, we introduce a novel VQA framework, DiffVQA, which harnesses the robust generalization capabilities of diffusion models pre-trained on extensive datasets. Our framework adapts these models to reconstruct identical input frames through a control module. The adapted diffusion model is then used to extract semantic and distortion features from a resizing branch and a cropping branch, respectively. To enhance the model's ability to handle long-term temporal dynamics, a parallel Mamba module is introduced, which extracts temporal coherence augmented features that are merged with the diffusion features to predict the final score. Experiments across multiple datasets demonstrate DiffVQA's superior performance on intra-dataset evaluations and its exceptional generalization across datasets. These results confirm that leveraging a diffusion model as a feature extractor can offer enhanced VQA performance compared to CNN and ViT backbones.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2502.20900.pdf' target='_blank'>https://arxiv.org/pdf/2502.20900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Zhang Chen, Tianrui Guan, Fanlian Zeng, Ka Num Lui, Yuyao Ye, Yitao Liang, Yaodong Yang, Yuanpei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20900">DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on restrictive assumptions, such as single-object settings or limited environments, showing constrained generalization. We present DexGraspVLA, a hierarchical framework for robust generalization in language-guided general dexterous grasping and beyond. It utilizes a pre-trained Vision-Language model as the high-level planner and learns a diffusion-based low-level Action controller. The key insight to achieve generalization lies in iteratively transforming diverse language and visual inputs into domain-invariant representations via foundation models, where imitation learning can be effectively applied due to the alleviation of domain shift. Notably, our method achieves a 90+% dexterous grasping success rate under thousands of challenging unseen cluttered scenes. Empirical analysis confirms the consistency of internal model behavior across environmental variations, validating our design. DexGraspVLA also, for the first time, simultaneously demonstrates free-form long-horizon prompt execution, robustness to adversarial objects and human disturbance, and failure recovery. Extended application to nonprehensile grasping further proves its generality. Project website: https://dexgraspvla.github.io.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2502.02247.pdf' target='_blank'>https://arxiv.org/pdf/2502.02247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bangzhen Liu, Chenxi Zheng, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02247">Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vulnerability of 3D point cloud analysis to unpredictable rotations poses an open yet challenging problem: orientation-aware 3D domain generalization. Cross-domain robustness and adaptability of 3D representations are crucial but not easily achieved through rotation augmentation. Motivated by the inherent advantages of intricate orientations in enhancing generalizability, we propose an innovative rotation-adaptive domain generalization framework for 3D point cloud analysis. Our approach aims to alleviate orientational shifts by leveraging intricate samples in an iterative learning process. Specifically, we identify the most challenging rotation for each point cloud and construct an intricate orientation set by optimizing intricate orientations. Subsequently, we employ an orientation-aware contrastive learning framework that incorporates an orientation consistency loss and a margin separation loss, enabling effective learning of categorically discriminative and generalizable features with rotation consistency. Extensive experiments and ablations conducted on 3D cross-domain benchmarks firmly establish the state-of-the-art performance of our proposed approach in the context of orientation-aware 3D domain generalization.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2501.02464.pdf' target='_blank'>https://arxiv.org/pdf/2501.02464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Guo, Sparsh Garg, S. Mahdi H. Miangoleh, Xinyu Huang, Liu Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02464">Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent depth foundation models exhibit strong zero-shot generalization, achieving accurate metric depth across diverse camera types-particularly those with large fields of view (FoV) such as fisheye and 360-degree cameras-remains a significant challenge. This paper presents Depth Any Camera (DAC), a powerful zero-shot metric depth estimation framework that extends a perspective-trained model to effectively handle cameras with varying FoVs. The framework is designed to ensure that all existing 3D data can be leveraged, regardless of the specific camera types used in new applications. Remarkably, DAC is trained exclusively on perspective images but generalizes seamlessly to fisheye and 360-degree cameras without the need for specialized training data. DAC employs Equi-Rectangular Projection (ERP) as a unified image representation, enabling consistent processing of images with diverse FoVs. Its core components include pitch-aware Image-to-ERP conversion with efficient online augmentation to simulate distorted ERP patches from undistorted inputs, FoV alignment operations to enable effective training across a wide range of FoVs, and multi-resolution data augmentation to further address resolution disparities between training and testing. DAC achieves state-of-the-art zero-shot metric depth estimation, improving $Î´_1$ accuracy by up to 50% on multiple fisheye and 360-degree datasets compared to prior metric depth foundation models, demonstrating robust generalization across camera types.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2501.01109.pdf' target='_blank'>https://arxiv.org/pdf/2501.01109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiusheng Xu, Lei Qi, Jingyang Zhou, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01109">BatStyler: Advancing Multi-category Style Generation for Source-free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Source-Free Domain Generalization (SFDG) aims to develop a model that performs on unseen domains without relying on any source domains. However, the implementation remains constrained due to the unavailability of training data. Research on SFDG focus on knowledge transfer of multi-modal models and style synthesis based on joint space of multiple modalities, thus eliminating the dependency on source domain images. However, existing works primarily work for multi-domain and less-category configuration, but performance on multi-domain and multi-category configuration is relatively poor. In addition, the efficiency of style synthesis also deteriorates in multi-category scenarios. How to efficiently synthesize sufficiently diverse data and apply it to multi-category configuration is a direction with greater practical value. In this paper, we propose a method called BatStyler, which is utilized to improve the capability of style synthesis in multi-category scenarios. BatStyler consists of two modules: Coarse Semantic Generation and Uniform Style Generation modules. The Coarse Semantic Generation module extracts coarse-grained semantics to prevent the compression of space for style diversity learning in multi-category configuration, while the Uniform Style Generation module provides a template of styles that are uniformly distributed in space and implements parallel training. Extensive experiments demonstrate that our method exhibits comparable performance on less-category datasets, while surpassing state-of-the-art methods on multi-category datasets.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2409.10281.pdf' target='_blank'>https://arxiv.org/pdf/2409.10281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fa-Ting Hong, Yunfei Liu, Yu Li, Changyin Zhou, Fei Yu, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10281">DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head synthesis strives to generate lifelike video portraits from provided audio. The diffusion model, recognized for its superior quality and robust generalization, has been explored for this task. However, establishing a robust correspondence between temporal audio cues and corresponding spatial facial expressions with diffusion models remains a significant challenge in talking head generation. To bridge this gap, we present DreamHead, a hierarchical diffusion framework that learns spatial-temporal correspondences in talking head synthesis without compromising the model's intrinsic quality and adaptability.~DreamHead learns to predict dense facial landmarks from audios as intermediate signals to model the spatial and temporal correspondences.~Specifically, a first hierarchy of audio-to-landmark diffusion is first designed to predict temporally smooth and accurate landmark sequences given audio sequence signals. Then, a second hierarchy of landmark-to-image diffusion is further proposed to produce spatially consistent facial portrait videos, by modeling spatial correspondences between the dense facial landmark and appearance. Extensive experiments show that proposed DreamHead can effectively learn spatial-temporal consistency with the designed hierarchical diffusion and produce high-fidelity audio-driven talking head videos for multiple identities.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2409.08557.pdf' target='_blank'>https://arxiv.org/pdf/2409.08557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaowei Miao, Yawei Luo, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08557">DICS: Find Domain-Invariant and Class-Specific Features for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep neural networks have made remarkable progress in various vision tasks, their performance typically deteriorates when tested in out-of-distribution (OOD) scenarios. Many OOD methods focus on extracting domain-invariant features but neglect whether these features are unique to each class. Even if some features are domain-invariant, they cannot serve as key classification criteria if shared across different classes. In OOD tasks, both domain-related and class-shared features act as confounders that hinder generalization. In this paper, we propose a DICS model to extract Domain-Invariant and Class-Specific features, including Domain Invariance Testing (DIT) and Class Specificity Testing (CST), which mitigate the effects of spurious correlations introduced by confounders. DIT learns domain-related features of each source domain and removes them from inputs to isolate domain-invariant class-related features. DIT ensures domain invariance by aligning same-class features across different domains. Then, CST calculates soft labels for those features by comparing them with features learned in previous steps. We optimize the cross-entropy between the soft labels and their true labels, which enhances same-class similarity and different-class distinctiveness, thereby reinforcing class specificity. Extensive experiments on widely-used benchmarks demonstrate the effectiveness of our proposed algorithm. Additional visualizations further demonstrate that DICS effectively identifies the key features of each class in target domains.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2408.09846.pdf' target='_blank'>https://arxiv.org/pdf/2408.09846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Feng, Bo Liu, Xiaoyu Dong, Zexin Lu, Li-Ming Zhan, Albert Y. S. Lam, Xiao-Ming Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09846">Continual Dialogue State Tracking via Reason-of-Select Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An ideal dialogue system requires continuous skill acquisition and adaptation to new tasks while retaining prior knowledge. Dialogue State Tracking (DST), vital in these systems, often involves learning new services and confronting catastrophic forgetting, along with a critical capability loss termed the "Value Selection Quandary." To address these challenges, we introduce the Reason-of-Select (RoS) distillation method by enhancing smaller models with a novel 'meta-reasoning' capability. Meta-reasoning employs an enhanced multi-domain perspective, combining fragments of meta-knowledge from domain-specific dialogues during continual learning. This transcends traditional single-perspective reasoning. The domain bootstrapping process enhances the model's ability to dissect intricate dialogues from multiple possible values. Its domain-agnostic property aligns data distribution across different domains, effectively mitigating forgetting. Additionally, two novel improvements, "multi-value resolution" strategy and Semantic Contrastive Reasoning Selection method, significantly enhance RoS by generating DST-specific selection chains and mitigating hallucinations in teachers' reasoning, ensuring effective and reliable knowledge transfer. Extensive experiments validate the exceptional performance and robust generalization capabilities of our method. The source code is provided for reproducibility.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2407.14562.pdf' target='_blank'>https://arxiv.org/pdf/2407.14562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Tan, Yongxin Deng, Xihe Qiu, Weidi Xu, Chao Qu, Wei Chu, Yinghui Xu, Yuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14562">Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Thought</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have shown exceptional performance as general-purpose assistants, excelling across a variety of reasoning tasks. This achievement represents a significant step toward achieving artificial general intelligence (AGI). Despite these advancements, the effectiveness of LLMs often hinges on the specific prompting strategies employed, and there remains a lack of a robust framework to facilitate learning and generalization across diverse reasoning tasks. To address these challenges, we introduce a novel learning framework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to imitate the Chain-of-Thought (CoT) process which is verified and translated from reasoning trajectories generated by a symbolic Prolog logic engine. This framework proceeds in a self-driven manner, that enables LLMs to formulate rules and statements from given instructions and leverage the symbolic Prolog engine to derive results. Subsequently, LLMs convert Prolog-derived successive reasoning trajectories into natural language CoT for imitation learning. Our empirical findings indicate that our proposed approach substantially enhances the reasoning abilities of LLMs and demonstrates robust generalization across out-of-distribution reasoning tasks.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2407.10197.pdf' target='_blank'>https://arxiv.org/pdf/2407.10197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Trinh, Ali Anwar, Siegfried Mercelis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10197">Multiple data sources and domain generalization learning method for road surface defect classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Roads are an essential mode of transportation, and maintaining them is critical to economic growth and citizen well-being. With the continued advancement of AI, road surface inspection based on camera images has recently been extensively researched and can be performed automatically. However, because almost all of the deep learning methods for detecting road surface defects were optimized for a specific dataset, they are difficult to apply to a new, previously unseen dataset. Furthermore, there is a lack of research on training an efficient model using multiple data sources. In this paper, we propose a method for classifying road surface defects using camera images. In our method, we propose a scheme for dealing with the invariance of multiple data sources while training a model on multiple data sources. Furthermore, we present a domain generalization training algorithm for developing a generalized model that can work with new, completely unseen data sources without requiring model updates. We validate our method using an experiment with six data sources corresponding to six countries from the RDD2022 dataset. The results show that our method can efficiently classify road surface defects on previously unseen data.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2403.16697.pdf' target='_blank'>https://arxiv.org/pdf/2403.16697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunlong Tang, Yuxuan Wan, Lei Qi, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16697">DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Source-Free Domain Generalization (SFDG) aims to develop a model that works for unseen target domains without relying on any source domain. Research in SFDG primarily bulids upon the existing knowledge of large-scale vision-language models and utilizes the pre-trained model's joint vision-language space to simulate style transfer across domains, thus eliminating the dependency on source domain images. However, how to efficiently simulate rich and diverse styles using text prompts, and how to extract domain-invariant information useful for classification from features that contain both semantic and style information after the encoder, are directions that merit improvement. In this paper, we introduce Dynamic PromptStyler (DPStyler), comprising Style Generation and Style Removal modules to address these issues. The Style Generation module refreshes all styles at every training epoch, while the Style Removal module eliminates variations in the encoder's output features caused by input styles. Moreover, since the Style Generation module, responsible for generating style word vectors using random sampling or style mixing, makes the model sensitive to input text prompts, we introduce a model ensemble method to mitigate this sensitivity. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on benchmark datasets.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2403.12167.pdf' target='_blank'>https://arxiv.org/pdf/2403.12167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarah Matta, Mathieu Lamard, Philippe Zhang, Alexandre Le Guilcher, Laurent Borderie, BÃ©atrice Cochener, GwenolÃ© Quellec
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12167">A Systematic Review of Generalization Research in Medical Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous Deep Learning (DL) classification models have been developed for a large spectrum of medical image analysis applications, which promises to reshape various facets of medical practice. Despite early advances in DL model validation and implementation, which encourage healthcare institutions to adopt them, a fundamental questions remain: how can these models effectively handle domain shift? This question is crucial to limit DL models performance degradation. Medical data are dynamic and prone to domain shift, due to multiple factors. Two main shift types can occur over time: 1) covariate shift mainly arising due to updates to medical equipment and 2) concept shift caused by inter-grader variability. To mitigate the problem of domain shift, existing surveys mainly focus on domain adaptation techniques, with an emphasis on covariate shift. More generally, no work has reviewed the state-of-the-art solutions while focusing on the shift types. This paper aims to explore existing domain generalization methods for DL-based classification models through a systematic review of literature. It proposes a taxonomy based on the shift type they aim to solve. Papers were searched and gathered on Scopus till 10 April 2023, and after the eligibility screening and quality evaluation, 77 articles were identified. Exclusion criteria included: lack of methodological novelty (e.g., reviews, benchmarks), experiments conducted on a single mono-center dataset, or articles not written in English. The results of this paper show that learning based methods are emerging, for both shift types. Finally, we discuss future challenges, including the need for improved evaluation protocols and benchmarks, and envisioned future developments to achieve robust, generalized models for medical image classification.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2403.07705.pdf' target='_blank'>https://arxiv.org/pdf/2403.07705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Zhang, Jiahe Li, Lei Huang, Xiaohan Yu, Lin Gu, Jin Zheng, Xiao Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07705">Robust Synthetic-to-Real Transfer for Stereo Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With advancements in domain generalized stereo matching networks, models pre-trained on synthetic data demonstrate strong robustness to unseen domains. However, few studies have investigated the robustness after fine-tuning them in real-world scenarios, during which the domain generalization ability can be seriously degraded. In this paper, we explore fine-tuning stereo matching networks without compromising their robustness to unseen domains. Our motivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for fine-tuning: GT degrades, but PL preserves the domain generalization ability. Empirically, we find the difference between GT and PL implies valuable information that can regularize networks during fine-tuning. We also propose a framework to utilize this difference for fine-tuning, consisting of a frozen Teacher, an exponential moving average (EMA) Teacher, and a Student network. The core idea is to utilize the EMA Teacher to measure what the Student has learned and dynamically improve GT and PL for fine-tuning. We integrate our framework with state-of-the-art networks and evaluate its effectiveness on several real-world datasets. Extensive experiments show that our method effectively preserves the domain generalization ability during fine-tuning.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2402.19145.pdf' target='_blank'>https://arxiv.org/pdf/2402.19145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Li, Lei Qi, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19145">A SAM-guided Two-stream Lightweight Model for Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial anomaly detection, model efficiency and mobile-friendliness become the primary concerns in real-world applications. Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns. In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for unsupervised anomaly detection (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM. We employ two lightweight image encoders, i.e., our two-stream lightweight module, guided by SAM's knowledge. To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions. Furthermore, we employ a shared mask decoder and a feature aggregation module to generate anomaly maps. Our experiments conducted on MVTec AD benchmark show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more difficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2309.04063.pdf' target='_blank'>https://arxiv.org/pdf/2309.04063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Yu, Huan-Hsin Tseng, Shinjae Yoo, Haibin Ling, Yuewei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04063">INSURE: An Information Theory Inspired Disentanglement and Purification Model for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to learn a generalizable model on the unseen target domain by only training on the multiple observed source domains. Although a variety of DG methods have focused on extracting domain-invariant features, the domain-specific class-relevant features have attracted attention and been argued to benefit generalization to the unseen target domain. To take into account the class-relevant domain-specific information, in this paper we propose an Information theory iNspired diSentanglement and pURification modEl (INSURE) to explicitly disentangle the latent features to obtain sufficient and compact (necessary) class-relevant feature for generalization to the unseen domain. Specifically, we first propose an information theory inspired loss function to ensure the disentangled class-relevant features contain sufficient class label information and the other disentangled auxiliary feature has sufficient domain information. We further propose a paired purification loss function to let the auxiliary feature discard all the class-relevant information and thus the class-relevant feature will contain sufficient and compact (necessary) class-relevant information. Moreover, instead of using multiple encoders, we propose to use a learnable binary mask as our disentangler to make the disentanglement more efficient and make the disentangled features complementary to each other. We conduct extensive experiments on four widely used DG benchmark datasets including PACS, OfficeHome, TerraIncognita, and DomainNet. The proposed INSURE outperforms the state-of-art methods. We also empirically show that domain-specific class-relevant features are beneficial for domain generalization.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2306.11541.pdf' target='_blank'>https://arxiv.org/pdf/2306.11541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liying Lu, Tianke Zhang, Yunfei Liu, Xuangeng Chu, Yu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11541">Audio-Driven 3D Facial Animation from In-the-Wild Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given an arbitrary audio clip, audio-driven 3D facial animation aims to generate lifelike lip motions and facial expressions for a 3D head. Existing methods typically rely on training their models using limited public 3D datasets that contain a restricted number of audio-3D scan pairs. Consequently, their generalization capability remains limited. In this paper, we propose a novel method that leverages in-the-wild 2D talking-head videos to train our 3D facial animation model. The abundance of easily accessible 2D talking-head videos equips our model with a robust generalization capability. By combining these videos with existing 3D face reconstruction methods, our model excels in generating consistent and high-fidelity lip synchronization. Additionally, our model proficiently captures the speaking styles of different individuals, allowing it to generate 3D talking-heads with distinct personal styles. Extensive qualitative and quantitative experimental results demonstrate the superiority of our method.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2306.02595.pdf' target='_blank'>https://arxiv.org/pdf/2306.02595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yimeng Chen, Tianyang Hu, Fengwei Zhou, Zhenguo Li, Zhiming Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02595">Explore and Exploit the Diverse Knowledge in Model Zoo for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of pretrained models, as a result of advancements in pretraining techniques, has led to the emergence of a vast zoo of publicly available models. Effectively utilizing these resources to obtain models with robust out-of-distribution generalization capabilities for downstream tasks has become a crucial area of research. Previous research has primarily focused on identifying the most powerful models within the model zoo, neglecting to fully leverage the diverse inductive biases contained within. This paper argues that the knowledge contained in weaker models is valuable and presents a method for leveraging the diversity within the model zoo to improve out-of-distribution generalization capabilities. Specifically, we investigate the behaviors of various pretrained models across different domains of downstream tasks by characterizing the variations in their encoded representations in terms of two dimensions: diversity shift and correlation shift. This characterization enables us to propose a new algorithm for integrating diverse pretrained models, not limited to the strongest models, in order to achieve enhanced out-of-distribution generalization performance. Our proposed method demonstrates state-of-the-art empirical results on a variety of datasets, thus validating the benefits of utilizing diverse knowledge.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2306.01334.pdf' target='_blank'>https://arxiv.org/pdf/2306.01334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Li, Xingwei Wang, Rongfei Zeng, Praveen Kumar Donta, Ilir Murturi, Min Huang, Schahram Dustdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01334">Federated Domain Generalization: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research area so far. This paper presents the first survey of recent advances in this area. Initially, we discuss the development process from traditional machine learning to domain adaptation and domain generalization, leading to FDG as well as provide the corresponding formal definition. Then, we categorize recent methodologies into four classes: federated domain alignment, data manipulation, learning strategies, and aggregation optimization, and present suitable algorithms in detail for each category. Next, we introduce commonly used datasets, applications, evaluations, and benchmarks. Finally, we conclude this survey by providing some potential research topics for the future.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2304.13976.pdf' target='_blank'>https://arxiv.org/pdf/2304.13976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Dai, Yonggang Zhang, Zhen Fang, Bo Han, Xinmei Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13976">Moderately Distributional Exploration for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to tackle the distribution shift between training domains and unknown target domains. Generating new domains is one of the most effective approaches, yet its performance gain depends on the distribution discrepancy between the generated and target domains. Distributionally robust optimization is promising to tackle distribution discrepancy by exploring domains in an uncertainty set. However, the uncertainty set may be overwhelmingly large, leading to low-confidence prediction in DG. It is because a large uncertainty set could introduce domains containing semantically different factors from training domains. To address this issue, we propose to perform a $\textbf{mo}$derately $\textbf{d}$istributional $\textbf{e}$xploration (MODE) for domain generalization. Specifically, MODE performs distribution exploration in an uncertainty $\textit{subset}$ that shares the same semantic factors with the training domains. We show that MODE can endow models with provable generalization performance on unknown target domains. The experimental results show that MODE achieves competitive performance compared to state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2303.05955.pdf' target='_blank'>https://arxiv.org/pdf/2303.05955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Lu, Zitong Yu, Xuesong Niu, Yingcong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05955">Neuron Structure Modeling for Generalizable Remote Physiological Measurement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote photoplethysmography (rPPG) technology has drawn increasing attention in recent years. It can extract Blood Volume Pulse (BVP) from facial videos, making many applications like health monitoring and emotional analysis more accessible. However, as the BVP signal is easily affected by environmental changes, existing methods struggle to generalize well for unseen domains. In this paper, we systematically address the domain shift problem in the rPPG measurement task. We show that most domain generalization methods do not work well in this problem, as domain labels are ambiguous in complicated environmental changes. In light of this, we propose a domain-label-free approach called NEuron STructure modeling (NEST). NEST improves the generalization capacity by maximizing the coverage of feature space during training, which reduces the chance for under-optimized feature activation during inference. Besides, NEST can also enrich and enhance domain invariant features across multi-domain. We create and benchmark a large-scale domain generalization protocol for the rPPG measurement task. Extensive experiments show that our approach outperforms the state-of-the-art methods on both cross-dataset and intra-dataset settings.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2211.14238.pdf' target='_blank'>https://arxiv.org/pdf/2211.14238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei Koh, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14238">Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distribution shift occurs when the test distribution differs from the training distribution, and it can considerably degrade performance of machine learning models deployed in the real world. Temporal shifts -- distribution shifts arising from the passage of time -- often occur gradually and have the additional structure of timestamp metadata. By leveraging timestamp metadata, models can potentially learn from trends in past distribution shifts and extrapolate into the future. While recent works have studied distribution shifts, temporal shifts remain underexplored. To address this gap, we curate Wild-Time, a benchmark of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including patient prognosis and news classification. On these datasets, we systematically benchmark 13 prior approaches, including methods in domain generalization, continual learning, self-supervised learning, and ensemble learning. We use two evaluation strategies: evaluation with a fixed time split (Eval-Fix) and evaluation with a data stream (Eval-Stream). Eval-Fix, our primary evaluation strategy, aims to provide a simple evaluation protocol, while Eval-Stream is more realistic for certain real-world applications. Under both evaluation strategies, we observe an average performance drop of 20% from in-distribution to out-of-distribution data. Existing methods are unable to close this gap. Code is available at https://wild-time.github.io/.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2211.06843.pdf' target='_blank'>https://arxiv.org/pdf/2211.06843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibing Liu, Chris Xing Tian, Haoliang Li, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.06843">Generalization Beyond Feature Alignment: Concept Activation-Guided Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning invariant representations via contrastive learning has seen state-of-the-art performance in domain generalization (DG). Despite such success, in this paper, we find that its core learning strategy -- feature alignment -- could heavily hinder model generalization. Drawing insights in neuron interpretability, we characterize this problem from a neuron activation view. Specifically, by treating feature elements as neuron activation states, we show that conventional alignment methods tend to deteriorate the diversity of learned invariant features, as they indiscriminately minimize all neuron activation differences. This instead ignores rich relations among neurons -- many of them often identify the same visual concepts despite differing activation patterns. With this finding, we present a simple yet effective approach, Concept Contrast (CoCo), which relaxes element-wise feature alignments by contrasting high-level concepts encoded in neurons. Our CoCo performs in a plug-and-play fashion, thus it can be integrated into any contrastive method in DG. We evaluate CoCo over four canonical contrastive methods, showing that CoCo promotes the diversity of feature representations and consistently improves model generalization capability. By decoupling this success through neuron coverage analysis, we further find that CoCo potentially invokes more meaningful neurons during training, thereby improving model learning.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2206.05658.pdf' target='_blank'>https://arxiv.org/pdf/2206.05658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, Jiebo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.05658">Improving Pre-trained Language Model Fine-tuning with Noise Stability Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of large-scale pre-trained language models has contributed greatly to the recent progress in natural language processing. Many state-of-the-art language models are first trained on a large text corpus and then fine-tuned on downstream tasks. Despite its recent success and wide adoption, fine-tuning a pre-trained language model often suffers from overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks. To address this problem, we propose a novel and effective fine-tuning framework, named Layerwise Noise Stability Regularization (LNSR). Specifically, we propose to inject the standard Gaussian noise or In-manifold noise and regularize hidden representations of the fine-tuned model. We first provide theoretical analyses to support the efficacy of our method. We then demonstrate the advantages of the proposed method over other state-of-the-art algorithms including L2-SP, Mixout and SMART. While these previous works only verify the effectiveness of their methods on relatively simple text classification tasks, we also verify the effectiveness of our method on question answering tasks, where the target problem is much more difficult and more training examples are available. Furthermore, extensive experimental results indicate that the proposed algorithm can not only enhance the in-domain performance of the language models but also improve the domain generalization performance on out-of-domain data.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2105.08511.pdf' target='_blank'>https://arxiv.org/pdf/2105.08511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chris Xing Tian, Haoliang Li, Yufei Wang, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.08511">Privacy-Preserving Constrained Domain Generalization via Gradient Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNN) have demonstrated unprecedented success for medical imaging applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for patient privacy protection, the broad applications of medical imaging classification driven by DNN with large-scale training data have been largely hindered. For example, when training the DNN from one domain (e.g., with data only from one hospital), the generalization capability to another domain (e.g., data from another hospital) could be largely lacking. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, We propose to improve the information aggregation process on the centralized server-side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the "unseen" but related medical images. The rationale and effectiveness of our proposed method can be explained by connecting our proposed method with the Maximum Mean Discrepancy (MMD) which has been widely adopted as the distribution distance measurement. Experimental results on two challenging medical imaging classification tasks indicate that our method can achieve better cross-domain generalization capability compared to the state-of-the-art federated learning methods.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2510.06638.pdf' target='_blank'>https://arxiv.org/pdf/2510.06638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Wen, Wenkang Wei, Yuan Fang, Xingtong Yu, Hui Zhang, Weicheng Zhu, Xin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06638">StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge-based Visual Question Answering (KVQA) requires models to ground entities in images and reason over factual knowledge. We study its implicit-knowledge variant, IK-KVQA, where a multimodal large language model (MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs lack explicit reasoning supervision and produce inconsistent justifications, and generalize poorly after standard supervised fine-tuning (SFT). We present StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises structured traces - dual symbolic relation paths plus path-grounded natural-language explanations - so that reasoning becomes transparent and verifiable. With one open-source MLLM, StaR-KVQA constructs and selects path-grounded reasoning traces to form a trace-enriched dataset, then fine-tunes via structured self-distillation to align generation with supervision; no external retrievers, verifiers, or curated knowledge bases (KBs) are used, traces are built offline, and inference is a single autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over the strongest baseline while exhibiting robust cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2509.22050.pdf' target='_blank'>https://arxiv.org/pdf/2509.22050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Ding, Muyun Jiang, Weibang Jiang, Shuailei Zhang, Xinliang Zhou, Chenyu Liu, Shanglin Li, Yong Li, Cuntai Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22050">BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography (EEG) is a non-invasive technique for recording brain electrical activity, widely used in brain-computer interface (BCI) and healthcare. Recent EEG foundation models trained on large-scale datasets have shown improved performance and generalizability over traditional decoding methods, yet significant challenges remain. Existing models often fail to explicitly capture channel-to-channel and region-to-region interactions, which are critical sources of information inherently encoded in EEG signals. Due to varying channel configurations across datasets, they either approximate spatial structure with self-attention or restrict training to a limited set of common channels, sacrificing flexibility and effectiveness. Moreover, although EEG datasets reflect diverse brain states such as emotion, motor, and others, current models rarely learn state-aware representations during self-supervised pre-training. To address these gaps, we propose BrainPro, a large EEG model that introduces a retrieval-based spatial learning block to flexibly capture channel- and region-level interactions across varying electrode layouts, and a brain state-decoupling block that enables state-aware representation learning through parallel encoders with decoupling and region-aware reconstruction losses. This design allows BrainPro to adapt seamlessly to diverse tasks and hardware settings. Pre-trained on an extensive EEG corpus, BrainPro achieves state-of-the-art performance and robust generalization across nine public BCI datasets. Our codes and the pre-trained weights will be released.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2509.15099.pdf' target='_blank'>https://arxiv.org/pdf/2509.15099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, Kei Sakaguchi, Walid Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15099">Digital Twin-based Cooperative Autonomous Driving in Smart Intersections: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsignalized intersections pose safety and efficiency challenges due to complex traffic flows and blind spots. In this paper, a digital twin (DT)-based cooperative driving system with roadside unit (RSU)-centric architecture is proposed for enhancing safety and efficiency at unsignalized intersections. The system leverages comprehensive bird-eye-view (BEV) perception to eliminate blind spots and employs a hybrid reinforcement learning (RL) framework combining offline pre-training with online fine-tuning. Specifically, driving policies are initially trained using conservative Q-learning (CQL) with behavior cloning (BC) on real datasets, then fine-tuned using multi-agent proximal policy optimization (MAPPO) with self-attention mechanisms to handle dynamic multi-agent coordination. The RSU implements real-time commands via vehicle-to-infrastructure (V2I) communications. Experimental results show that the proposed method yields failure rates below 0.03\% coordinating up to three connected autonomous vehicles (CAVs), significantly outperforming traditional methods. In addition, the system exhibits sub-linear computational scaling with inference times under 40 ms. Furthermore, it demonstrates robust generalization across diverse unsignalized intersection scenarios, indicating its practicality and readiness for real-world deployment.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2507.16795.pdf' target='_blank'>https://arxiv.org/pdf/2507.16795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16795">Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2507.08218.pdf' target='_blank'>https://arxiv.org/pdf/2507.08218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atticus Wang, Joshua Engels, Oliver Clive-Griffin, Senthooran Rajamanoharan, Neel Nanda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08218">Simple Mechanistic Explanations for Out-Of-Context Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs exhibit surprisingly deep out-of-distribution generalization. Rather than learning shallow heuristics, they implicitly internalize and act on the consequences of observations scattered throughout the fine-tuning data. In this work, we investigate this phenomenon mechanistically and find that many instances of OOCR in the literature have a simple explanation: the LoRA fine-tuning essentially adds a constant steering vector, steering the model towards a general concept. This improves performance on the fine-tuning task and in many other concept-related domains, causing the surprising generalization. Moreover, we can directly train steering vectors for these tasks from scratch, which also induces OOCR. We find that our results hold even for a task that seems like it must involve conditional behavior (model backdoors); it turns out that unconditionally adding a steering vector is sufficient. Overall, our work presents one explanation of what gets learned during fine-tuning for OOCR tasks, contributing to the key question of why LLMs can reason out of context, an advanced capability that is highly relevant to their safe and reliable deployment.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2507.07370.pdf' target='_blank'>https://arxiv.org/pdf/2507.07370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanhong Jiang, Dylan Shah, Hsin-Jung Yang, Soumik Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07370">Data-driven Kinematic Modeling in Soft Robots: System Identification and Uncertainty Quantification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise kinematic modeling is critical in calibration and controller design for soft robots, yet remains a challenging issue due to their highly nonlinear and complex behaviors. To tackle the issue, numerous data-driven machine learning approaches have been proposed for modeling nonlinear dynamics. However, these models suffer from prediction uncertainty that can negatively affect modeling accuracy, and uncertainty quantification for kinematic modeling in soft robots is underexplored. In this work, using limited simulation and real-world data, we first investigate multiple linear and nonlinear machine learning models commonly used for kinematic modeling of soft robots. The results reveal that nonlinear ensemble methods exhibit the most robust generalization performance. We then develop a conformal kinematic modeling framework for soft robots by utilizing split conformal prediction to quantify predictive position uncertainty, ensuring distribution-free prediction intervals with a theoretical guarantee.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2506.21387.pdf' target='_blank'>https://arxiv.org/pdf/2506.21387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaris KÃ¼ken, Lennart Purucker, Frank Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21387">Early Stopping Tabular In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tabular foundation models have shown strong performance across various tabular learning tasks via in-context learning, offering robust generalization without any downstream finetuning. However, their inference-time costs remain high, particularly for larger datasets. To address this, we propose early-stopping the in-context learning process. We achieve this by dynamically evaluating whether to stop in-context learning after each Transformer encoder layer. Once stopped, we decode the embedding using a pre-trained layer-wise decoder. Experiments across 34 small classification tasks size show that early stopping in-context learning accelerates inference by up to x1.3 with negligible degradation in predictive performance. To assess scalability, we further evaluate our method on five larger classification tasks, achieving speedups of up to x2.2. Our results demonstrate the potential of early exiting as an effective and practical strategy for improving the efficiency of tabular in-context learning.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2506.07309.pdf' target='_blank'>https://arxiv.org/pdf/2506.07309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Huang, Yifan Ethan Xu, Kai Sun, Vera Yan, Alicia Sun, Haidar Khan, Jimmy Nguyen, Mohammad Kachuee, Zhaojiang Lin, Yue Liu, Aaron Colak, Anuj Kumar, Wen-tau Yih, Xin Luna Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07309">ConfQA: Answer Only If You Are Confident</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit "I am unsure". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt "answer only if you are confident" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA's confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2506.07309.pdf' target='_blank'>https://arxiv.org/pdf/2506.07309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Huang, Yifan Ethan Xu, Kai Sun, Vera Yan, Alicia Sun, Haidar Khan, Jimmy Nguyen, Jingxiang Chen, Mohammad Kachuee, Zhaojiang Lin, Yue Liu, Aaron Colak, Anuj Kumar, Wen-tau Yih, Xin Luna Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07309">ConfRAG: Confidence-Guided Retrieval-Augmenting Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can Large Language Models (LLMs) be trained to avoid hallucinating factual statements, and can Retrieval-Augmented Generation (RAG) be triggered only when necessary to reduce retrieval and computation costs? In this work, we address both challenges simultaneously. We introduce ConfQA, a fine-tuning strategy that reduces hallucination rates from 20-40% to below 5% across multiple factuality benchmarks. The approach is simple: when the model answers correctly, it is trained to output the answer; otherwise, it is trained to respond with "I am unsure". Two design choices make this training effective: (1) a dampening prompt ("answer only if you are confident") that explicitly discourages overconfident hallucinations, and (2) training data drawn from atomic factual statements (e.g., knowledge graph attribute values), which calibrates model confidence and yields robust generalization across domains and question types. Building on ConfQA, we propose ConfRAG, a triggering strategy that invokes RAG only when the model responses with unsure. This framework achieves accuracy above 95% in ideal case while reducing unnecessary external retrievals by over 30%.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2505.24597.pdf' target='_blank'>https://arxiv.org/pdf/2505.24597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Liu, Ning Cao, Yile Chen, Yue Jiang, Gao Cong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24597">Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next location prediction plays a critical role in understanding human mobility patterns. However, existing approaches face two core limitations: (1) they fall short in capturing the complex, multi-functional semantics of real-world locations; and (2) they lack the capacity to model heterogeneous behavioral dynamics across diverse user groups. To tackle these challenges, we introduce NextLocMoE, a novel framework built upon large language models (LLMs) and structured around a dual-level Mixture-of-Experts (MoE) design. Our architecture comprises two specialized modules: a Location Semantics MoE that operates at the embedding level to encode rich functional semantics of locations, and a Personalized MoE embedded within the Transformer backbone to dynamically adapt to individual user mobility patterns. In addition, we incorporate a history-aware routing mechanism that leverages long-term trajectory data to enhance expert selection and ensure prediction stability. Empirical evaluations across several real-world urban datasets show that NextLocMoE achieves superior performance in terms of predictive accuracy, cross-domain generalization, and interpretability
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2505.18445.pdf' target='_blank'>https://arxiv.org/pdf/2505.18445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiren Song, Cheng Liu, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18445">OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose \textbf{OmniConsistency}, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2505.10223.pdf' target='_blank'>https://arxiv.org/pdf/2505.10223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10223">Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation models are often trained on curated datasets, leading to performance degradation when deployed in real-world clinical settings due to mismatches between training and test distributions. While data augmentation techniques are widely used to address these challenges, traditional visually consistent augmentation strategies lack the robustness needed for diverse real-world scenarios. In this work, we systematically evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary Fourier Augmentation. These methods mitigate the effects of multiple variations without explicitly targeting specific sources of distribution shifts. We demonstrate how these techniques significantly improve out-of-distribution generalization and robustness to imaging variations across a wide range of transformations in cardiac cine MRI and prostate MRI segmentation. We quantitatively find that these augmentation methods enhance learned feature representations by promoting separability and compactness. Additionally, we highlight how their integration into nnU-Net training pipelines provides an easy-to-implement, effective solution for enhancing the reliability of medical segmentation models in real-world applications.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2504.21066.pdf' target='_blank'>https://arxiv.org/pdf/2504.21066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Karathanasis, John Violos, Ioannis Kompatsiaris, Symeon Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21066">A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training and deploying deepfake detection models on edge devices offers the advantage of maintaining data privacy and confidentiality by processing it close to its source. However, this approach is constrained by the limited computational and memory resources available at the edge. To address this challenge, we explore compression techniques to reduce computational demands and inference time, alongside transfer learning methods to minimize training overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate the effectiveness of pruning, knowledge distillation (KD), quantization, fine-tuning, and adapter-based techniques. Our experimental results demonstrate that both compression and transfer learning can be effectively achieved, even with a high compression level of 90%, remaining at the same performance level when the training and validation data originate from the same DeepFake model. However, when the testing dataset is generated by DeepFake models not present in the training set, a domain generalization issue becomes evident.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2503.05638.pdf' target='_blank'>https://arxiv.org/pdf/2503.05638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark YU, Wenbo Hu, Jinbo Xing, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05638">TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2502.03393.pdf' target='_blank'>https://arxiv.org/pdf/2502.03393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewen Liu, Juntong Ni, Max S. Y. Lau, Wei Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03393">Pre-training Epidemic Time Series Forecasters with Compartmental Prototypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate epidemic forecasting is crucial for outbreak preparedness, but existing data-driven models are often brittle. Typically trained on a single pathogen, they struggle with data scarcity during new outbreaks and fail under distribution shifts caused by viral evolution or interventions. However, decades of surveillance data from diverse diseases offer an untapped source of transferable knowledge. To leverage the collective lessons from history, we propose CAPE, the first open-source pre-trained model for epidemic forecasting. Unlike existing time series foundation models that overlook epidemiological challenges, CAPE models epidemic dynamics as mixtures of latent population states, termed compartmental prototypes. It discovers a flexible dictionary of compartment prototypes directly from surveillance data, enabling each outbreak to be expressed as a time-varying mixture that links observed infections to latent population states. To promote robust generalization, CAPE combines self-supervised pre-training objectives with lightweight epidemic-aware regularizers that align the learned prototypes with epidemiological semantics. On a comprehensive benchmark spanning 17 diseases and 50+ regions, CAPE significantly outperforms strong baselines in zero-shot, few-shot, and full-shot forecasting. This work represents a principled step toward pre-trained epidemic models that are both transferable and epidemiologically grounded.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2502.03387.pdf' target='_blank'>https://arxiv.org/pdf/2502.03387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03387">LIMO: Less is More for Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We challenge the prevailing assumption that complex reasoning in large language models (LLMs) necessitates massive training data. We demonstrate that sophisticated mathematical reasoning can emerge with only a few examples. Specifically, through simple supervised fine-tuning, our model, LIMO, achieves 63.3\% accuracy on AIME24 and 95.6\% on MATH500, surpassing previous fine-tuned models (6.5\% on AIME24, 59.2\% on MATH500) while using only 1\% of the training data required by prior approaches. Furthermore, LIMO exhibits strong out-of-distribution generalization, achieving a 45.8\% absolute improvement across diverse benchmarks, outperforming models trained on 100x more data. Synthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning can emerge through minimal but strategically designed demonstrations of cognitive processes. This hypothesis suggests that the threshold for eliciting complex reasoning is not dictated by task complexity but rather by two key factors: (1) the completeness of the model's pre-trained knowledge base and (2) the effectiveness of post-training examples in serving as "cognitive templates" that guide reasoning.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2501.13967.pdf' target='_blank'>https://arxiv.org/pdf/2501.13967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Che, Yifei Wu, Haibo Jin, Yong Xia, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13967">FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization aims to train a global model from multiple source domains and ensure its generalization ability to unseen target domains. Due to the target domain being with unknown domain shifts, attempting to approximate these gaps by source domains may be the key to improving model generalization capability. Existing works mainly focus on sharing and recombining local domain-specific attributes to increase data diversity and simulate potential domain shifts. However, these methods may be insufficient since only the local attribute recombination can be hard to touch the out-of-distribution of global data. In this paper, we propose a simple-yet-efficient framework named Federated Domain Adversarial Generation (FedDAG). It aims to simulate the domain shift and improve the model generalization by adversarially generating novel domains different from local and global source domains. Specifically, it generates novel-style images by maximizing the instance-level feature discrepancy between original and generated images and trains a generalizable task model by minimizing their feature discrepancy. Further, we observed that FedDAG could cause different performance improvements for local models. It may be due to inherent data isolation and heterogeneity among clients, exacerbating the imbalance in their generalization contributions to the global model. Ignoring this imbalance can lead the global model's generalization ability to be sub-optimal, further limiting the novel domain generation procedure. Thus, to mitigate this imbalance, FedDAG hierarchically aggregates local models at the within-client and across-client levels by using the sharpness concept to evaluate client model generalization contributions. Extensive experiments across four medical benchmarks demonstrate FedDAG's ability to enhance generalization in federated medical scenarios.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2412.15544.pdf' target='_blank'>https://arxiv.org/pdf/2412.15544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, Sikai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15544">VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, reinforcement learning (RL)-based methods for learning driving policies have gained increasing attention in the autonomous driving community and have achieved remarkable progress in various driving scenarios. However, traditional RL approaches rely on manually engineered rewards, which require extensive human effort and often lack generalizability. To address these limitations, we propose \textbf{VLM-RL}, a unified framework that integrates pre-trained Vision-Language Models (VLMs) with RL to generate reward signals using image observation and natural language goals. The core of VLM-RL is the contrasting language goal (CLG)-as-reward paradigm, which uses positive and negative language goals to generate semantic rewards. We further introduce a hierarchical reward synthesis approach that combines CLG-based semantic rewards with vehicle state information, improving reward stability and offering a more comprehensive reward signal. Additionally, a batch-processing technique is employed to optimize computational efficiency during training. Extensive experiments in the CARLA simulator demonstrate that VLM-RL outperforms state-of-the-art baselines, achieving a 10.5\% reduction in collision rate, a 104.6\% increase in route completion rate, and robust generalization to unseen driving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any standard RL algorithms, potentially revolutionizing the existing RL paradigm that relies on manual reward engineering and enabling continuous performance improvements. The demo video and code can be accessed at: https://zilin-huang.github.io/VLM-RL-website.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2412.08393.pdf' target='_blank'>https://arxiv.org/pdf/2412.08393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyuan Chen, Jin Wang, Xuejie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08393">Learning to Reason via Self-Iterative Process Feedback for Small Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Small language models (SLMs) are more efficient, cost-effective, and customizable than large language models (LLMs), though they often underperform in specific areas like reasoning. Past methods for enhancing SLMs' reasoning, such as supervised fine-tuning and distillation, often depend on costly external signals, resulting in SLMs being overly confident with limited supervision signals, thus limiting their abilities. Therefore, this study enables SLMs to learn to reason from self-iterative feedback. By combining odds ratio preference optimization (ORPO), we fine-tune and align SLMs using positive and negative signals generated by themselves. Additionally, we introduce process supervision for rewards in preference alignment by sampling-based inference simulation and process reward models. Compared to Supervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B by 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed method also demonstrated superior out-of-domain generalization capabilities on MMLU_Math and HumanEval.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2412.06784.pdf' target='_blank'>https://arxiv.org/pdf/2412.06784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mara Levy, Siddhant Haldar, Lerrel Pinto, Abhinav Shirivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06784">P3-PO: Prescriptive Point Priors for Visuo-Spatial Generalization of Robot Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing generalizable robot policies that can robustly handle varied environmental conditions and object instances remains a fundamental challenge in robot learning. While considerable efforts have focused on collecting large robot datasets and developing policy architectures to learn from such data, naively learning from visual inputs often results in brittle policies that fail to transfer beyond the training data. This work presents Prescriptive Point Priors for Policies or P3-PO, a novel framework that constructs a unique state representation of the environment leveraging recent advances in computer vision and robot learning to achieve improved out-of-distribution generalization for robot manipulation. This representation is obtained through two steps. First, a human annotator prescribes a set of semantically meaningful points on a single demonstration frame. These points are then propagated through the dataset using off-the-shelf vision models. The derived points serve as an input to state-of-the-art policy architectures for policy learning. Our experiments across four real-world tasks demonstrate an overall 43% absolute improvement over prior methods when evaluated in identical settings as training. Further, P3-PO exhibits 58% and 80% gains across tasks for new object instances and more cluttered environments respectively. Videos illustrating the robot's performance are best viewed at point-priors.github.io.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2412.05208.pdf' target='_blank'>https://arxiv.org/pdf/2412.05208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditi Singh, Akash Shetty, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05208">A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-SQL systems facilitate smooth interaction with databases by translating natural language queries into Structured Query Language (SQL), bridging the gap between non-technical users and complex database management systems. This survey provides a comprehensive overview of the evolution of AI-driven text-to-SQL systems, highlighting their foundational components, advancements in large language model (LLM) architectures, and the critical role of datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine the applications of text-to-SQL in domains like healthcare, education, and finance, emphasizing their transformative potential for improving data accessibility. Additionally, we analyze persistent challenges, including domain generalization, query optimization, support for multi-turn conversational interactions, and the limited availability of datasets tailored for NoSQL databases and dynamic real-world scenarios. To address these challenges, we outline future research directions, such as extending text-to-SQL capabilities to support NoSQL databases, designing datasets for dynamic multi-turn interactions, and optimizing systems for real-world scalability and robustness. By surveying current advancements and identifying key gaps, this paper aims to guide the next generation of research and applications in LLM-based text-to-SQL systems.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2412.04296.pdf' target='_blank'>https://arxiv.org/pdf/2412.04296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Bao, Zhixin Zhou, Wen Jung Li, Rui Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04296">Structure-Aware Stylized Image Synthesis for Robust Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate medical image segmentation is essential for effective diagnosis and treatment planning but is often challenged by domain shifts caused by variations in imaging devices, acquisition conditions, and patient-specific attributes. Traditional domain generalization methods typically require inclusion of parts of the test domain within the training set, which is not always feasible in clinical settings with limited diverse data. Additionally, although diffusion models have demonstrated strong capabilities in image generation and style transfer, they often fail to preserve the critical structural information necessary for precise medical analysis. To address these issues, we propose a novel medical image segmentation method that combines diffusion models and Structure-Preserving Network for structure-aware one-shot image stylization. Our approach effectively mitigates domain shifts by transforming images from various sources into a consistent style while maintaining the location, size, and shape of lesions. This ensures robust and accurate segmentation even when the target domain is absent from the training data. Experimental evaluations on colonoscopy polyp segmentation and skin lesion segmentation datasets show that our method enhances the robustness and accuracy of segmentation models, achieving superior performance metrics compared to baseline models without style transfer. This structure-aware stylization framework offers a practical solution for improving medical image segmentation across diverse domains, facilitating more reliable clinical diagnoses.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2411.11939.pdf' target='_blank'>https://arxiv.org/pdf/2411.11939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Masroor, Tahir Hassan, Yu Tian, Kevin Wells, David Rosewarne, Thanh-Toan Do, Gustavo Carneiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11939">Fair Distillation: Teaching Fairness from Biased Teachers in Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has achieved remarkable success in image classification and segmentation tasks. However, fairness concerns persist, as models often exhibit biases that disproportionately affect demographic groups defined by sensitive attributes such as race, gender, or age. Existing bias-mitigation techniques, including Subgroup Re-balancing, Adversarial Training, and Domain Generalization, aim to balance accuracy across demographic groups, but often fail to simultaneously improve overall accuracy, group-specific accuracy, and fairness due to conflicts among these interdependent objectives. We propose the Fair Distillation (FairDi) method, a novel fairness approach that decomposes these objectives by leveraging biased ``teacher'' models, each optimized for a specific demographic group. These teacher models then guide the training of a unified ``student'' model, which distills their knowledge to maximize overall and group-specific accuracies, while minimizing inter-group disparities. Experiments on medical imaging datasets show that FairDi achieves significant gains in both overall and group-specific accuracy, along with improved fairness, compared to existing methods. FairDi is adaptable to various medical tasks, such as classification and segmentation, and provides an effective solution for equitable model performance.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2410.04968.pdf' target='_blank'>https://arxiv.org/pdf/2410.04968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Zhou, Yaoxin Wu, Zhiguang Cao, Wen Song, Jie Zhang, Zhiqi Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04968">Collaboration! Towards Robust Neural Methods for Routing Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite enjoying desirable efficiency and reduced reliance on domain expertise, existing neural methods for vehicle routing problems (VRPs) suffer from severe robustness issues -- their performance significantly deteriorates on clean instances with crafted perturbations. To enhance robustness, we propose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the defense of neural VRP methods, which is crucial yet underexplored in the literature. Given a neural VRP method, we adversarially train multiple models in a collaborative manner to synergistically promote robustness against attacks, while boosting standard generalization on clean instances. A neural router is designed to adeptly distribute training instances among models, enhancing overall load balancing and collaborative efficacy. Extensive experiments verify the effectiveness and versatility of CNF in defending against various attacks across different neural VRP methods. Notably, our approach also achieves impressive out-of-distribution generalization on benchmark instances.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2408.09040.pdf' target='_blank'>https://arxiv.org/pdf/2408.09040.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boning Li, Gunjan Verma, Timofey Efimov, Abhishek Kumar, Santiago Segarra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09040">GLANCE: Graph-based Learnable Digital Twin for Communication Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As digital twins (DTs) to physical communication systems, network simulators can aid the design and deployment of communication networks. However, time-consuming simulations must be run for every new set of network configurations. Learnable digital twins (LDTs), in contrast, can be trained offline to emulate simulation outcomes and serve as a more efficient alternative to simulation-based DTs at runtime. In this work, we propose GLANCE, a communication LDT that learns from the simulator ns-3. It can evaluate network key performance indicators (KPIs) and assist in network management with exceptional efficiency. Leveraging graph learning, we exploit network data characteristics and devise a specialized architecture to embed sequential and topological features of traffic flows within the network. In addition, multi-task learning (MTL) and transfer learning (TL) are leveraged to enhance GLANCE's generalizability to unseen inputs and efficacy across different tasks. Beyond end-to-end KPI prediction, GLANCE can be deployed within an optimization framework for network management. It serves as an efficient or differentiable evaluator in optimizing network configurations such as traffic loads and flow destinations. Through numerical experiments and benchmarking, we verify the effectiveness of the proposed LDT architecture, demonstrate its robust generalization to various inputs, and showcase its efficacy in network management applications.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2408.03608.pdf' target='_blank'>https://arxiv.org/pdf/2408.03608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Xinghao Ding, Yue Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03608">Mixstyle-Entropy: Domain Generalization with Causal Intervention and Perturbation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the considerable advancements achieved by deep neural networks, their performance tends to degenerate when the test environment diverges from the training ones. Domain generalization (DG) solves this issue by learning representations independent of domain-related information, thus facilitating extrapolation to unseen environments. Existing approaches typically focus on formulating tailored training objectives to extract shared features from the source data. However, the disjointed training and testing procedures may compromise robustness, particularly in the face of unforeseen variations during deployment. In this paper, we propose a novel and holistic framework based on causality, named InPer, designed to enhance model generalization by incorporating causal intervention during training and causal perturbation during testing. Specifically, during the training phase, we employ entropy-based causal intervention (EnIn) to refine the selection of causal variables. To identify samples with anti-interference causal variables from the target domain, we propose a novel metric, homeostatic score, through causal perturbation (HoPer) to construct a prototype classifier in test time. Experimental results across multiple cross-domain tasks confirm the efficacy of InPer.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2406.02024.pdf' target='_blank'>https://arxiv.org/pdf/2406.02024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guy Amir, Osher Maayan, Tom Zelazny, Guy Katz, Michael Schapira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02024">Verifying the Generalization of Deep Learning to Out-of-Distribution Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) play a crucial role in the field of machine learning, demonstrating state-of-the-art performance across various application domains. However, despite their success, DNN-based models may occasionally exhibit challenges with generalization, i.e., may fail to handle inputs that were not encountered during training. This limitation is a significant challenge when it comes to deploying deep learning for safety-critical tasks, as well as in real-world settings characterized by substantial variability. We introduce a novel approach for harnessing DNN verification technology to identify DNN-driven decision rules that exhibit robust generalization to previously unencountered input domains. Our method assesses generalization within an input domain by measuring the level of agreement between independently trained deep neural networks for inputs in this domain. We also efficiently realize our approach by using off-the-shelf DNN verification engines, and extensively evaluate it on both supervised and unsupervised DNN benchmarks, including a deep reinforcement learning (DRL) system for Internet congestion control -- demonstrating the applicability of our approach for real-world settings. Moreover, our research introduces a fresh objective for formal verification, offering the prospect of mitigating the challenges linked to deploying DNN-driven systems in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2402.17298.pdf' target='_blank'>https://arxiv.org/pdf/2402.17298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Xiaomin Yu, Gongyu Zhang, Zhen Zhu, Christos Bergeles, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17298">ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>"A data scientist is tasked with developing a low-cost surgical VQA system for a 2-month workshop. Due to data sensitivity, she collects 50 hours of surgical video from a hospital, requiring two months for privacy approvals. Privacy restrictions prevent uploading data to platforms like ChatGPT, so she assembles one annotator and a medical expert to manually create QA pairs. This process takes three weeks and costs over $10,000. The trained model provides accurate responses within the limited data scope but lacks broader generalizability, completing the project in 3 months."
  To simplify the challenges presented in the scenario above. In this paper, we replace the image input with text for Vision-language training. Inspired by prior noise injection methods to reduce modality gaps, we introduce Adaptive ranged cosine Similarity injected noise (ArcSin). First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity. Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale. This dual strategy effectively broadens the scope of the original domain while safeguarding content integrity. Our empirical results demonstrate that these models closely rival those trained on images in terms of performance. Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively. Additionally, we observe increases of 0.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2402.08831.pdf' target='_blank'>https://arxiv.org/pdf/2402.08831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, Xia Ning
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08831">eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM exhibits excellent generalizability to out-of-domain settings, including unseen products and unseen instructions, highlighting its superiority as a generalist e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective LLMs for e-commerce. ECInstruct and eCeLLM models are publicly accessible through https://ninglab.github.io/eCeLLM.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2402.08228.pdf' target='_blank'>https://arxiv.org/pdf/2402.08228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, Yi Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08228">Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism and the decoupled architecture contribute positively to graph OOD generalization. In contrast, we observe that the linear classification layer tends to compromise graph OOD generalization capability. Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries. These insights have empowered us to develop a novel GNN backbone model, DGAT, designed to harness the robust properties of both graph self-attention mechanism and the decoupled architecture. Extensive experimental results demonstrate the effectiveness of our model under graph OOD, exhibiting substantial and consistent enhancements across various training strategies.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2401.01220.pdf' target='_blank'>https://arxiv.org/pdf/2401.01220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Yao, Yuxiao Yi, Liangkai Hang, Weinan E, Weizong Wang, Yaoyu Zhang, Tianhan Zhang, Zhi-Qin John Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01220">Solving multiscale dynamical systems by deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiscale dynamical systems, modeled by high-dimensional stiff ordinary differential equations (ODEs) with wide-ranging characteristic timescales, arise across diverse fields of science and engineering, but their numerical solvers often encounter severe efficiency bottlenecks. This paper introduces a novel DeePODE method, which consists of an Evolutionary Monte Carlo Sampling method (EMCS) and an efficient end-to-end deep neural network (DNN) to predict multiscale dynamical systems. We validate this finding across dynamical systems from ecological systems to reactive flows, including a predator-prey model, a power system oscillation, a battery electrolyte thermal runaway, and turbulent reaction-diffusion systems with complex chemical kinetics. The method demonstrates robust generalization capabilities, allowing pre-trained DNN models to accurately predict the behavior in previously unseen scenarios, largely due to the delicately constructed dataset. While theoretical guarantees remain to be established, empirical evidence shows that DeePODE achieves the accuracy of implicit numerical schemes while maintaining the computational efficiency of explicit schemes. This work underscores the crucial relationship between training data distribution and neural network generalization performance. This work demonstrates the potential of deep learning approaches in modeling complex dynamical systems across scientific and engineering domains.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2312.06801.pdf' target='_blank'>https://arxiv.org/pdf/2312.06801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lyes Saad Saoud, Zhenwei Niu, Atif Sultan, Lakmal Seneviratne, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06801">ADOD: Adaptive Domain-Aware Object Detection with Residual Attention for Underwater Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research presents ADOD, a novel approach to address domain generalization in underwater object detection. Our method enhances the model's ability to generalize across diverse and unseen domains, ensuring robustness in various underwater environments. The first key contribution is Residual Attention YOLOv3, a novel variant of the YOLOv3 framework empowered by residual attention modules. These modules enable the model to focus on informative features while suppressing background noise, leading to improved detection accuracy and adaptability to different domains. The second contribution is the attention-based domain classification module, vital during training. This module helps the model identify domain-specific information, facilitating the learning of domain-invariant features. Consequently, ADOD can generalize effectively to underwater environments with distinct visual characteristics. Extensive experiments on diverse underwater datasets demonstrate ADOD's superior performance compared to state-of-the-art domain generalization methods, particularly in challenging scenarios. The proposed model achieves exceptional detection performance in both seen and unseen domains, showcasing its effectiveness in handling domain shifts in underwater object detection tasks. ADOD represents a significant advancement in adaptive object detection, providing a promising solution for real-world applications in underwater environments. With the prevalence of domain shifts in such settings, the model's strong generalization ability becomes a valuable asset for practical underwater surveillance and marine research endeavors.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2310.04724.pdf' target='_blank'>https://arxiv.org/pdf/2310.04724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoqi Chen, Luyao Tang, Leitian Tao, Hong-Yu Zhou, Yue Huang, Xiaoguang Han, Yizhou Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04724">Activate and Reject: Towards Safe Domain Generalization under Category Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Albeit the notable performance on in-domain test points, it is non-trivial for deep neural networks to attain satisfactory accuracy when deploying in the open world, where novel domains and object classes often occur. In this paper, we study a practical problem of Domain Generalization under Category Shift (DGCS), which aims to simultaneously detect unknown-class samples and classify known-class samples in the target domains. Compared to prior DG works, we face two new challenges: 1) how to learn the concept of ``unknown'' during training with only source known-class samples, and 2) how to adapt the source-trained model to unseen environments for safe model deployment. To this end, we propose a novel Activate and Reject (ART) framework to reshape the model's decision boundary to accommodate unknown classes and conduct post hoc modification to further discriminate known and unknown classes using unlabeled test data. Specifically, during training, we promote the response to the unknown by optimizing the unknown probability and then smoothing the overall output to mitigate the overconfidence issue. At test time, we introduce a step-wise online adaptation method that predicts the label by virtue of the cross-domain nearest neighbor and class prototype information without updating the network's parameters or using threshold-based mechanisms. Experiments reveal that ART consistently improves the generalization capability of deep networks on different vision tasks. For image classification, ART improves the H-score by 6.1% on average compared to the previous best method. For object detection and semantic segmentation, we establish new benchmarks and achieve competitive performance.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2307.11108.pdf' target='_blank'>https://arxiv.org/pdf/2307.11108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingxuan Zhang, Renzhe Xu, Han Yu, Yancheng Dong, Pengfei Tian, Peng Cu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11108">Flatness-Aware Minimization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-order flatness-aware optimization methods.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2307.04378.pdf' target='_blank'>https://arxiv.org/pdf/2307.04378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Che, Yuhan Cheng, Haibo Jin, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04378">Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic Retinopathy (DR) is a common complication of diabetes and a leading cause of blindness worldwide. Early and accurate grading of its severity is crucial for disease management. Although deep learning has shown great potential for automated DR grading, its real-world deployment is still challenging due to distribution shifts among source and target domains, known as the domain generalization problem. Existing works have mainly attributed the performance degradation to limited domain shifts caused by simple visual discrepancies, which cannot handle complex real-world scenarios. Instead, we present preliminary evidence suggesting the existence of three-fold generalization issues: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance. To tackle these issues, we propose a novel unified framework named Generalizable Diabetic Retinopathy Grading Network (GDRNet). GDRNet consists of three vital components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domain-class-aware re-balancing (DCR). FundusAug generates realistic augmented images via visual transformation and image degradation, while DahLoss jointly leverages pixel-level consistency and image-level semantics to capture the diverse diagnostic patterns and build generalizable feature representations. Moreover, DCR mitigates the data imbalance from a domain-class view and avoids undesired over-emphasis on rare domain-class pairs. Finally, we design a publicly available benchmark for fair evaluations. Extensive comparison experiments against advanced methods and exhaustive ablation studies demonstrate the effectiveness and generalization ability of GDRNet.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2306.05879.pdf' target='_blank'>https://arxiv.org/pdf/2306.05879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiming Zhuang, Lingjuan Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05879">FedWon: Triumphing Multi-domain Federated Learning Without Normalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, instead of label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while existing normalization techniques possess their own limitations. In order to address these issues, FedWon eliminates the normalization layers in FL and reparameterizes convolution layers with scaled weight standardization. Through extensive experimentation on five datasets and five models, our comprehensive experimental results demonstrate that FedWon surpasses both FedAvg and the current state-of-the-art method (FedBN) across all experimental setups, achieving notable accuracy improvements of more than 10% in certain domains. Furthermore, FedWon is versatile for both cross-silo and cross-device FL, exhibiting robust domain generalization capability, showcasing strong performance even with a batch size as small as 1, thereby catering to resource-constrained devices. Additionally, FedWon can also effectively tackle the challenge of skewed label distribution.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2302.06589.pdf' target='_blank'>https://arxiv.org/pdf/2302.06589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Minghan Li, Jimmy Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06589">Improving Out-of-Distribution Generalization of Neural Rerankers with Contextualized Late Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in information retrieval finds that embedding query and document representation into multi-vector yields a robust bi-encoder retriever on out-of-distribution datasets. In this paper, we explore whether late interaction, the simplest form of multi-vector, is also helpful to neural rerankers that only use the [CLS] vector to compute the similarity score. Although intuitively, the attention mechanism of rerankers at the previous layers already gathers the token-level information, we find adding late interaction still brings an extra 5% improvement in average on out-of-distribution datasets, with little increase in latency and no degradation in in-domain effectiveness. Through extensive experiments and analysis, we show that the finding is consistent across different model sizes and first-stage retrievers of diverse natures and that the improvement is more prominent on longer queries.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2510.05580.pdf' target='_blank'>https://arxiv.org/pdf/2510.05580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Li, Zhantao Yang, Han Zhang, Fangyi Chen, Chenchen Zhu, Anudeepsekhar Bolimera, Marios Savvides
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05580">MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2509.24797.pdf' target='_blank'>https://arxiv.org/pdf/2509.24797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zizhao Tong, Di Chen, Sicheng Hu, Hongwei Fan, Liliang Chen, Guanghui Ren, Hao Tang, Hao Dong, Ling Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24797">Fidelity-Aware Data Composition for Robust Robot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as $π_0$ and Diffusion Policy improves OOD success rates by over 54\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2509.24655.pdf' target='_blank'>https://arxiv.org/pdf/2509.24655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max van Spengler, Artem Moskalev, Tommaso Mansi, Mangal Prakash, Rui Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24655">HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models are increasingly applied to biological sequences like proteins and mRNA, yet their default Euclidean geometry may mismatch the hierarchical structures inherent to biological data. While hyperbolic geometry provides a better alternative for accommodating hierarchical data, it has yet to find a way into language modeling for mRNA sequences. In this work, we introduce HyperHELM, a framework that implements masked language model pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned representations with the biological hierarchy defined by the relationship between mRNA and amino acids. Across multiple multi-species datasets, it outperforms Euclidean baselines on 9 out of 10 tasks involving property prediction, with 10% improvement on average, and excels in out-of-distribution generalization to long and low-GC content sequences; for antibody region annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation accuracy. Our results highlight hyperbolic geometry as an effective inductive bias for hierarchical language modeling of mRNA sequences.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2509.00658.pdf' target='_blank'>https://arxiv.org/pdf/2509.00658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Lin, Dong Li, Xintao Wu, Minglai Shao, Xujiang Zhao, Zhong Chen, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00658">Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring fairness and robustness in machine learning models remains a challenge, particularly under domain shifts. We present Face4FairShifts, a large-scale facial image benchmark designed to systematically evaluate fairness-aware learning and domain generalization. The dataset includes 100,000 images across four visually distinct domains with 39 annotations within 14 attributes covering demographic and facial features. Through extensive experiments, we analyze model performance under distribution shifts and identify significant gaps. Our findings emphasize the limitations of existing related datasets and the need for more effective fairness-aware domain adaptation techniques. Face4FairShifts provides a comprehensive testbed for advancing equitable and reliable AI systems. The dataset is available online at https://meviuslab.github.io/Face4FairShifts/.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2508.04129.pdf' target='_blank'>https://arxiv.org/pdf/2508.04129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xun Lin, Xiaobao Guo, Taorui Wang, Yingjie Ma, Jiajian Huang, Jiayu Zhang, Junzhe Cao, Zitong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04129">SVC 2025: the First Multimodal Deception Detection Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deception detection is a critical task in real-world applications such as security screening, fraud prevention, and credibility assessment. While deep learning methods have shown promise in surpassing human-level performance, their effectiveness often depends on the availability of high-quality and diverse deception samples. Existing research predominantly focuses on single-domain scenarios, overlooking the significant performance degradation caused by domain shifts. To address this gap, we present the SVC 2025 Multimodal Deception Detection Challenge, a new benchmark designed to evaluate cross-domain generalization in audio-visual deception detection. Participants are required to develop models that not only perform well within individual domains but also generalize across multiple heterogeneous datasets. By leveraging multimodal data, including audio, video, and text, this challenge encourages the design of models capable of capturing subtle and implicit deceptive cues. Through this benchmark, we aim to foster the development of more adaptable, explainable, and practically deployable deception detection systems, advancing the broader field of multimodal learning. By the conclusion of the workshop competition, a total of 21 teams had submitted their final results. https://sites.google.com/view/svc-mm25 for more information.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2507.14141.pdf' target='_blank'>https://arxiv.org/pdf/2507.14141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danny Dongyeop Han, Ahhyun Lucy Lee, Taeyang Lee, Yonghyeon Gwon, Sebin Lee, Seongjin Lee, David Keetae Park, Shinjae Yoo, Jiook Cha, Chun Kee Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14141">DIVER-0 : A Fully Channel Equivariant EEG Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography (EEG) is a non-invasive technique widely used in brain-computer interfaces and clinical applications, yet existing EEG foundation models face limitations in modeling spatio-temporal brain dynamics and lack channel permutation equivariance, preventing robust generalization across diverse electrode configurations. To address these challenges, we propose DIVER-0, a novel EEG foundation model that demonstrates how full spatio-temporal attention-rather than segregated spatial or temporal processing-achieves superior performance when properly designed with Rotary Position Embedding (RoPE) for temporal relationships and binary attention biases for channel differentiation. We also introduce Sliding Temporal Conditional Positional Encoding (STCPE), which improves upon existing conditional positional encoding approaches by maintaining both temporal translation equivariance and channel permutation equivariance, enabling robust adaptation to arbitrary electrode configurations unseen during pretraining. Experimental results demonstrate that DIVER-0 achieves competitive performance with only 10% of pretraining data while maintaining consistent results across all channel permutation conditions, validating its effectiveness for cross-dataset generalization and establishing key design principles for handling the inherent heterogeneity of neural recording setups.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2507.03304.pdf' target='_blank'>https://arxiv.org/pdf/2507.03304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Huang, Yan Xia, Sashuai Zhou, Hanting Wang, Shulei Wang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03304">Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to enhance model robustness in unseen or distributionally shifted target domains through training exclusively on source domains. Although existing DG techniques, such as data manipulation, learning strategies, and representation learning, have shown significant progress, they predominantly address single-modal data. With the emergence of numerous multi-modal datasets and increasing demand for multi-modal tasks, a key challenge in Multi-modal Domain Generalization (MMDG) has emerged: enabling models trained on multi-modal sources to generalize to unseen target distributions within the same modality set. Due to the inherent differences between modalities, directly transferring methods from single-modal DG to MMDG typically yields sub-optimal results. These methods often exhibit randomness during generalization due to the invisibility of target domains and fail to consider inter-modal consistency. Applying these methods independently to each modality in the MMDG setting before combining them can lead to divergent generalization directions across different modalities, resulting in degraded generalization capabilities. To address these challenges, we propose a novel approach that leverages Unified Representations to map different paired modalities together, effectively adapting DG methods to MMDG by enabling synchronized multi-modal improvements within the unified space. Additionally, we introduce a supervised disentanglement framework that separates modal-general and modal-specific information, further enhancing the alignment of unified representations. Extensive experiments on benchmark datasets, including EPIC-Kitchens and Human-Animal-Cartoon, demonstrate the effectiveness and superiority of our method in enhancing multi-modal domain generalization.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2506.20743.pdf' target='_blank'>https://arxiv.org/pdf/2506.20743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Hao Van, Prateek Verma, Chen Zhao, Xintao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20743">A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\&A; atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2506.08516.pdf' target='_blank'>https://arxiv.org/pdf/2506.08516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mouadh Yagoubi, David Danan, Milad Leyli-Abadi, Ahmed Mazari, Jean-Patrick Brunet, Abbas Kabalan, Fabien Casenave, Yuxin Ma, Giovanni Catalani, Jean Fesquet, Jacob Helwig, Xuan Zhang, Haiyang Yu, Xavier Bertrand, Frederic Tost, Michael Baurheim, Joseph Morlier, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08516">NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of machine learning (ML) into the physical sciences is reshaping computational paradigms, offering the potential to accelerate demanding simulations such as computational fluid dynamics (CFD). Yet, persistent challenges in accuracy, generalization, and physical consistency hinder the practical deployment of ML models in scientific domains. To address these limitations and systematically benchmark progress, we organized the ML4CFD competition, centered on surrogate modeling for aerodynamic simulations over two-dimensional airfoils. The competition attracted over 240 teams, who were provided with a curated dataset generated via OpenFOAM and evaluated through a multi-criteria framework encompassing predictive accuracy, physical fidelity, computational efficiency, and out-of-distribution generalization. This retrospective analysis reviews the competition outcomes, highlighting several approaches that outperformed baselines under our global evaluation score. Notably, the top entry exceeded the performance of the original OpenFOAM solver on aggregate metrics, illustrating the promise of ML-based surrogates to outperform traditional solvers under tailored criteria. Drawing from these results, we analyze the key design principles of top submissions, assess the robustness of our evaluation framework, and offer guidance for future scientific ML challenges.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2505.12585.pdf' target='_blank'>https://arxiv.org/pdf/2505.12585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>En Yu, Jie Lu, Xiaoyu Yang, Guangquan Zhang, Zhen Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12585">Learning Robust Spectral Dynamics for Temporal Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern machine learning models struggle to maintain performance in dynamic environments where temporal distribution shifts, \emph{i.e., concept drift}, are prevalent. Temporal Domain Generalization (TDG) seeks to enable model generalization across evolving domains, yet existing approaches typically assume smooth incremental changes, struggling with complex real-world drifts involving long-term structure (incremental evolution/periodicity) and local uncertainties. To overcome these limitations, we introduce FreKoo, which tackles these challenges via a novel frequency-domain analysis of parameter trajectories. It leverages the Fourier transform to disentangle parameter evolution into distinct spectral bands. Specifically, low-frequency component with dominant dynamics are learned and extrapolated using the Koopman operator, robustly capturing diverse drift patterns including both incremental and periodicity. Simultaneously, potentially disruptive high-frequency variations are smoothed via targeted temporal regularization, preventing overfitting to transient noise and domain uncertainties. In addition, this dual spectral strategy is rigorously grounded through theoretical analysis, providing stability guarantees for the Koopman prediction, a principled Bayesian justification for the high-frequency regularization, and culminating in a multiscale generalization bound connecting spectral dynamics to improved generalization. Extensive experiments demonstrate FreKoo's significant superiority over SOTA TDG approaches, particularly excelling in real-world streaming scenarios with complex drifts and uncertainties.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2504.08019.pdf' target='_blank'>https://arxiv.org/pdf/2504.08019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Bi, Jingjun Yi, Hao Zheng, Haolan Zhan, Wei Ji, Yawen Huang, Yuexiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08019">DGFamba: Learning Flow Factorized State Space for Visual Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn a representation from the source domain, which can be generalized to arbitrary unseen target domains. A fundamental challenge for visual domain generalization is the domain gap caused by the dramatic style variation whereas the image content is stable. The realm of selective state space, exemplified by VMamba, demonstrates its global receptive field in representing the content. However, the way exploiting the domain-invariant property for selective state space is rarely explored. In this paper, we propose a novel Flow Factorized State Space model, dubbed as DG-Famba, for visual domain generalization. To maintain domain consistency, we innovatively map the style-augmented and the original state embeddings by flow factorization. In this latent flow space, each state embedding from a certain style is specified by a latent probability path. By aligning these probability paths in the latent space, the state embeddings are able to represent the same content distribution regardless of the style differences. Extensive experiments conducted on various visual domain generalization settings show its state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2503.22172.pdf' target='_blank'>https://arxiv.org/pdf/2503.22172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minho Park, Sunghyun Park, Jungsoo Lee, Hyojin Park, Kyuwoong Hwang, Fatih Porikli, Jaegul Choo, Sungha Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22172">Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of data scarcity in semantic segmentation by generating datasets through text-to-image (T2I) generation models, reducing image acquisition and labeling costs. Segmentation dataset generation faces two key challenges: 1) aligning generated samples with the target domain and 2) producing informative samples beyond the training data. Fine-tuning T2I models can help generate samples aligned with the target domain. However, it often overfits and memorizes training data, limiting their ability to generate diverse and well-aligned samples. To overcome these issues, we propose Concept-Aware LoRA (CA-LoRA), a novel fine-tuning approach that selectively identifies and updates only the weights associated with necessary concepts (e.g., style or viewpoint) for domain alignment while preserving the pretrained knowledge of the T2I model to produce informative samples. We demonstrate its effectiveness in generating datasets for urban-scene segmentation, outperforming baseline and state-of-the-art methods in in-domain (few-shot and fully-supervised) settings, as well as in domain generalization tasks, especially under challenging conditions such as adverse weather and varying illumination, further highlighting its superiority.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2503.16852.pdf' target='_blank'>https://arxiv.org/pdf/2503.16852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxi Li, Di Lin, Hao Chen, Hongying Liu, Liang Wan, Wei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16852">Casual Inference via Style Bias Deconfounding for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) often struggle with out-of-distribution data, limiting their reliability in diverse realworld applications. To address this issue, domain generalization methods have been developed to learn domain-invariant features from single or multiple training domains, enabling generalization to unseen testing domains. However, existing approaches usually overlook the impact of style frequency within the training set. This oversight predisposes models to capture spurious visual correlations caused by style confounding factors, rather than learning truly causal representations, thereby undermining inference reliability. In this work, we introduce Style Deconfounding Causal Learning (SDCL), a novel causal inference-based framework designed to explicitly address style as a confounding factor. Our approaches begins with constructing a structural causal model (SCM) tailored to the domain generalization problem and applies a backdoor adjustment strategy to account for style influence. Building on this foundation, we design a style-guided expert module (SGEM) to adaptively clusters style distributions during training, capturing the global confounding style. Additionally, a back-door causal learning module (BDCL) performs causal interventions during feature extraction, ensuring fair integration of global confounding styles into sample predictions, effectively reducing style bias. The SDCL framework is highly versatile and can be seamlessly integrated with state-of-the-art data augmentation techniques. Extensive experiments across diverse natural and medical image recognition tasks validate its efficacy, demonstrating superior performance in both multi-domain and the more challenging single-domain generalization scenarios.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2503.08407.pdf' target='_blank'>https://arxiv.org/pdf/2503.08407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yansong Guo, Jie Hu, Yansong Qu, Liujuan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08407">WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in interactive 3D segmentation from 2D images have demonstrated impressive performance. However, current models typically require extensive scene-specific training to accurately reconstruct and segment objects, which limits their applicability in real-time scenarios. In this paper, we introduce WildSeg3D, an efficient approach that enables the segmentation of arbitrary 3D objects across diverse environments using a feed-forward mechanism. A key challenge of this feed-forward approach lies in the accumulation of 3D alignment errors across multiple 2D views, which can lead to inaccurate 3D segmentation results. To address this issue, we propose Dynamic Global Aligning (DGA), a technique that improves the accuracy of global multi-view alignment by focusing on difficult-to-match 3D points across images, using a dynamic adjustment function. Additionally, for real-time interactive segmentation, we introduce Multi-view Group Mapping (MGM), a method that utilizes an object mask cache to integrate multi-view segmentations and respond rapidly to user prompts. WildSeg3D demonstrates robust generalization across arbitrary scenes, thereby eliminating the need for scene-specific training. Specifically, WildSeg3D not only attains the accuracy of state-of-the-art (SOTA) methods but also achieves a $40\times$ speedup compared to existing SOTA models. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2502.20237.pdf' target='_blank'>https://arxiv.org/pdf/2502.20237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Bencomo, Max Gupta, Ioana Marinescu, R. Thomas McCoy, Thomas L. Griffiths
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20237">Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial neural networks can acquire many aspects of human knowledge from data, making them promising as models of human learning. But what those networks can learn depends upon their inductive biases -- the factors other than the data that influence the solutions they discover -- and the inductive biases of neural networks remain poorly understood, limiting our ability to draw conclusions about human learning from the performance of these systems. Cognitive scientists and machine learning researchers often focus on the architecture of a neural network as a source of inductive bias. In this paper we explore the impact of another source of inductive bias -- the initial weights of the network -- using meta-learning as a tool for finding initial weights that are adapted for specific problems. We evaluate four widely-used architectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430 different models across three tasks requiring different biases and forms of generalization. We find that meta-learning can substantially reduce or entirely eliminate performance differences across architectures and data representations, suggesting that these factors may be less important as sources of inductive bias than is typically assumed. When differences are present, architectures and data representations that perform well without meta-learning tend to meta-train more effectively. Moreover, all architectures generalize poorly on problems that are far from their meta-training experience, underscoring the need for stronger inductive biases for robust generalization.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2502.15809.pdf' target='_blank'>https://arxiv.org/pdf/2502.15809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15809">Black Sheep in the Herd: Playing with Spuriously Correlated Attributes for Vision-Language Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot adaptation for Vision-Language Models (VLMs) presents a dilemma: balancing in-distribution accuracy with out-of-distribution generalization. Recent research has utilized low-level concepts such as visual attributes to enhance generalization. However, this study reveals that VLMs overly rely on a small subset of attributes on decision-making, which co-occur with the category but are not inherently part of it, termed spuriously correlated attributes. This biased nature of VLMs results in poor generalization. To address this, 1) we first propose Spurious Attribute Probing (SAP), identifying and filtering out these problematic attributes to significantly enhance the generalization of existing attribute-based methods; 2) We introduce Spurious Attribute Shielding (SAS), a plug-and-play module that mitigates the influence of these attributes on prediction, seamlessly integrating into various Parameter-Efficient Fine-Tuning (PEFT) methods. In experiments, SAP and SAS significantly enhance accuracy on distribution shifts across 11 datasets and 3 generalization tasks without compromising downstream performance, establishing a new state-of-the-art benchmark.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2501.14315.pdf' target='_blank'>https://arxiv.org/pdf/2501.14315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Shao-Hua Sun, Hung-yi Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14315">Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2501.14315.pdf' target='_blank'>https://arxiv.org/pdf/2501.14315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Shao-Hua Sun, Hung-yi Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14315">Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and three additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2412.11408.pdf' target='_blank'>https://arxiv.org/pdf/2412.11408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Soltany, Farhad Pourpanah, Mahdiyar Molahasani, Michael Greenspan, Ali Etemad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11408">Federated Domain Generalization with Label Smoothing and Balanced Decentralized Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel approach, Federated Domain Generalization with Label Smoothing and Balanced Decentralized Training (FedSB), to address the challenges of data heterogeneity within a federated learning framework. FedSB utilizes label smoothing at the client level to prevent overfitting to domain-specific features, thereby enhancing generalization capabilities across diverse domains when aggregating local models into a global model. Additionally, FedSB incorporates a decentralized budgeting mechanism which balances training among clients, which is shown to improve the performance of the aggregated global model. Extensive experiments on four commonly used multi-domain datasets, PACS, VLCS, OfficeHome, and TerraInc, demonstrate that FedSB outperforms competing methods, achieving state-of-the-art results on three out of four datasets, indicating the effectiveness of FedSB in addressing data heterogeneity.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2411.12913.pdf' target='_blank'>https://arxiv.org/pdf/2411.12913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qin Tian, Chen Zhao, Minglai Shao, Wenjun Wang, Yujie Lin, Dong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12913">MLDGG: Meta-Learning for Domain Generalization on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization on graphs aims to develop models with robust generalization capabilities, ensuring effective performance on the testing set despite disparities between testing and training distributions. However, existing methods often rely on static encoders directly applied to the target domain, constraining its flexible adaptability. In contrast to conventional methodologies, which concentrate on developing specific generalized models, our framework, MLDGG, endeavors to achieve adaptable generalization across diverse domains by integrating cross-multi-domain meta-learning with structure learning and semantic identification. Initially, it introduces a generalized structure learner to mitigate the adverse effects of task-unrelated edges, enhancing the comprehensiveness of representations learned by Graph Neural Networks (GNNs) while capturing shared structural information across domains. Subsequently, a representation learner is designed to disentangle domain-invariant semantic and domain-specific variation information in node embedding by leveraging causal reasoning for semantic identification, further enhancing generalization. In the context of meta-learning, meta-parameters for both learners are optimized to facilitate knowledge transfer and enable effective adaptation to graphs through fine-tuning within the target domains, where target graphs are inaccessible during training. Our empirical results demonstrate that MLDGG surpasses baseline methods, showcasing its effectiveness in three different distribution shift settings.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2410.15449.pdf' target='_blank'>https://arxiv.org/pdf/2410.15449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Zhao, Zhengqiu Zhu, Chen Gao, En Wang, Jincai Huang, Fei-Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15449">Heterogeneous Graph Reinforcement Learning for Dependency-aware Multi-task Allocation in Spatial Crowdsourcing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial Crowdsourcing (SC) is gaining traction in both academia and industry, with tasks on SC platforms becoming increasingly complex and requiring collaboration among workers with diverse skills. Recent research works address complex tasks by dividing them into subtasks with dependencies and assigning them to suitable workers. However, the dependencies among subtasks and their heterogeneous skill requirements, as well as the need for efficient utilization of workers' limited work time in the multi-task allocation mode, pose challenges in achieving an optimal task allocation scheme. Therefore, this paper formally investigates the problem of Dependency-aware Multi-task Allocation (DMA) and presents a well-designed framework to solve it, known as Heterogeneous Graph Reinforcement Learning-based Task Allocation (HGRL-TA). To address the challenges associated with representing and embedding diverse problem instances to ensure robust generalization, we propose a multi-relation graph model and a Compound-path-based Heterogeneous Graph Attention Network (CHANet) for effectively representing and capturing intricate relations among tasks and workers, as well as providing embedding of problem state. The task allocation decision is determined sequentially by a policy network, which undergoes simultaneous training with CHANet using the proximal policy optimization algorithm. Extensive experiment results demonstrate the effectiveness and generality of the proposed HGRL-TA in solving the DMA problem, leading to average profits that is 21.78% higher than those achieved using the metaheuristic methods.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2410.14375.pdf' target='_blank'>https://arxiv.org/pdf/2410.14375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialin Yu, Yuxiang Zhou, Yulan He, Nevin L. Zhang, Junchi Yu, Philip Torr, Ricardo Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14375">Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting to latent-confounded shifts remains a core challenge in modern AI. These shifts are propagated via latent variables that induce spurious, non-transportable correlations between inputs and labels. One practical failure mode arises when fine-tuning pre-trained foundation models on confounded data (e.g., where certain text tokens or image backgrounds spuriously correlate with the label), leaving models vulnerable at deployment. We frame causal fine-tuning as an identification problem and pose an explicit causal model that decomposes inputs into low-level spurious features and high-level causal representations. Under this family of models, we formalize the assumptions required for identification. Using pre-trained language models as a case study, we show how identifying and adjusting these components during causal fine-tuning enables automatic adaptation to latent-confounded shifts at test time. Experiments on semi-synthetic benchmarks derived from real-world problems demonstrate that our method outperforms black-box domain generalization baselines, illustrating the benefits of explicitly modeling causal structure.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2410.11723.pdf' target='_blank'>https://arxiv.org/pdf/2410.11723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Celestini, Amirhossein Afsharrad, Daniele Gammelli, Tommaso Guffanti, Gioele Zardini, Sanjay Lall, Elisa Capello, Simone D'Amico, Marco Pavone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11723">Generalizable Spacecraft Trajectory Generation via Multimodal Learning with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective trajectory generation is essential for reliable on-board spacecraft autonomy. Among other approaches, learning-based warm-starting represents an appealing paradigm for solving the trajectory generation problem, effectively combining the benefits of optimization- and data-driven methods. Current approaches for learning-based trajectory generation often focus on fixed, single-scenario environments, where key scene characteristics, such as obstacle positions or final-time requirements, remain constant across problem instances. However, practical trajectory generation requires the scenario to be frequently reconfigured, making the single-scenario approach a potentially impractical solution. To address this challenge, we present a novel trajectory generation framework that generalizes across diverse problem configurations, by leveraging high-capacity transformer neural networks capable of learning from multimodal data sources. Specifically, our approach integrates transformer-based neural network models into the trajectory optimization process, encoding both scene-level information (e.g., obstacle locations, initial and goal states) and trajectory-level constraints (e.g., time bounds, fuel consumption targets) via multimodal representations. The transformer network then generates near-optimal initial guesses for non-convex optimization problems, significantly enhancing convergence speed and performance. The framework is validated through extensive simulations and real-world experiments on a free-flyer platform, achieving up to 30% cost improvement and 80% reduction in infeasible cases with respect to traditional approaches, and demonstrating robust generalization across diverse scenario variations.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2408.09312.pdf' target='_blank'>https://arxiv.org/pdf/2408.09312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Li, Chen Zhao, Minglai Shao, Wenjun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09312">Learning Fair Invariant Representations under Covariate and Correlation Shifts Simultaneously</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving the generalization of an invariant classifier from training domains to shifted test domains while simultaneously considering model fairness is a substantial and complex challenge in machine learning. Existing methods address the problem of fairness-aware domain generalization, focusing on either covariate shift or correlation shift, but rarely consider both at the same time. In this paper, we introduce a novel approach that focuses on learning a fairness-aware domain-invariant predictor within a framework addressing both covariate and correlation shifts simultaneously, ensuring its generalization to unknown test domains inaccessible during training. In our approach, data are first disentangled into content and style factors in latent spaces. Furthermore, fairness-aware domain-invariant content representations can be learned by mitigating sensitive information and retaining as much other information as possible. Extensive empirical studies on benchmark datasets demonstrate that our approach surpasses state-of-the-art methods with respect to model accuracy as well as both group and individual fairness.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2407.07763.pdf' target='_blank'>https://arxiv.org/pdf/2407.07763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qixiang Zhang, Haonan Wang, Xiaomeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07763">S&D Messenger: Exchanging Semantic and Domain Knowledge for Generic Semi-Supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised medical image segmentation (SSMIS) has emerged as a promising solution to tackle the challenges of time-consuming manual labeling in the medical field. However, in practical scenarios, there are often domain variations within the datasets, leading to derivative scenarios like semi-supervised medical domain generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA). In this paper, we aim to develop a generic framework that masters all three tasks. We notice a critical shared challenge across three scenarios: the explicit semantic knowledge for segmentation performance and rich domain knowledge for generalizability exclusively exist in the labeled set and unlabeled set respectively. Such discrepancy hinders existing methods from effectively comprehending both types of knowledge under semi-supervised settings. To tackle this challenge, we develop a Semantic & Domain Knowledge Messenger (S&D Messenger) which facilitates direct knowledge delivery between the labeled and unlabeled set, and thus allowing the model to comprehend both of them in each individual learning flow. Equipped with our S&D Messenger, a naive pseudo-labeling method can achieve huge improvement on six benchmark datasets for SSMIS (+7.5%), UMDA (+5.6%), and Semi-MDG tasks (+1.14%), compared with state-of-the-art methods designed for specific tasks.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2407.04003.pdf' target='_blank'>https://arxiv.org/pdf/2407.04003.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mushui Liu, Bozheng Li, Yunlong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04003">Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt tuning, which involves training a small set of parameters, effectively enhances the pre-trained Vision-Language Models (VLMs) to downstream tasks. However, they often come at the cost of flexibility and adaptability when the tuned models are applied to different datasets or domains. In this paper, we explore capturing the task-specific information via meticulous refinement of entire VLMs, with minimal parameter adjustments. When fine-tuning the entire VLMs for specific tasks under limited supervision, overfitting and catastrophic forgetting become the defacto factors. To mitigate these issues, we propose a framework named CLIP-CITE via designing a discriminative visual-text task, further aligning the visual-text semantics in a supervision manner, and integrating knowledge distillation techniques to preserve the gained knowledge. Extensive experimental results under few-shot learning, base-to-new generalization, domain generalization, and cross-domain generalization settings, demonstrate that our method effectively enhances the performance on specific tasks under limited supervision while preserving the versatility of the VLMs on other datasets.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2406.09495.pdf' target='_blank'>https://arxiv.org/pdf/2406.09495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Lin, Dong Li, Minglai Shao, Guihong Wan, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09495">FADE: Towards Fairness-aware Generation for Domain Generalization via Classifier-Guided Score-based Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fairness-aware domain generalization (FairDG) has emerged as a critical challenge for deploying trustworthy AI systems, particularly in scenarios involving distribution shifts. Traditional methods for addressing fairness have failed in domain generalization due to their lack of consideration for distribution shifts. Although disentanglement has been used to tackle FairDG, it is limited by its strong assumptions. To overcome these limitations, we propose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as a novel approach to effectively address the FairDG issue. Specifically, we first pre-train a score-based diffusion model (SDM) and two classifiers to equip the model with strong generalization capabilities across different domains. Then, we guide the SDM using these pre-trained classifiers to effectively eliminate sensitive information from the generated data. Finally, the generated fair data is used to train downstream classifiers, ensuring robust performance under new data distributions. Extensive experiments on three real-world datasets demonstrate that FADE not only enhances fairness but also improves accuracy in the presence of distribution shifts. Additionally, FADE outperforms existing methods in achieving the best accuracy-fairness trade-offs.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2406.09166.pdf' target='_blank'>https://arxiv.org/pdf/2406.09166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenlong Yu, Dongyue Chen, Qilong Wang, Qinghua Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09166">Fine-Grained Domain Generalization with Feature Structuralization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained domain generalization (FGDG) is a more challenging task than traditional DG tasks due to its small inter-class variations and relatively large intra-class disparities. When domain distribution changes, the vulnerability of subtle features leads to a severe deterioration in model performance. Nevertheless, humans inherently demonstrate the capacity for generalizing to out-of-distribution data, leveraging structured multi-granularity knowledge that emerges from discerning the commonality and specificity within categories. Likewise, we propose a Feature Structuralized Domain Generalization (FSDG) model, wherein features experience structuralization into common, specific, and confounding segments, harmoniously aligned with their relevant semantic concepts, to elevate performance in FGDG. Specifically, feature structuralization (FS) is accomplished through joint optimization of five constraints: a decorrelation function applied to disentangled segments, three constraints ensuring common feature consistency and specific feature distinctiveness, and a prediction calibration term. By imposing these stipulations, FSDG is prompted to disentangle and align features based on multi-granularity knowledge, facilitating robust subtle distinctions among categories. Extensive experimentation on three benchmarks consistently validates the superiority of FSDG over state-of-the-art counterparts, with an average improvement of 6.2% in FGDG performance. Beyond that, the explainability analysis on explicit concept matching intensity between the shared concepts among categories and the model channels, along with experiments on various mainstream model architectures, substantiates the validity of FS.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2403.07601.pdf' target='_blank'>https://arxiv.org/pdf/2403.07601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Tang, Wenxin Su, Mao Ye, Jianwei Zhang, Xiatian Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07601">Unified Source-Free Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including Closed-set, Open-set, Partial-set, and Generalized settings. Existing methods, focusing on specific scenarios, not only address a limited subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. In this paper, we propose a novel approach latent Causal factors discovery for unified SFDA(CausalDA). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate CausalDA from a causality perspective. The objective is to uncover the causal relationships between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts. To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that CausalDA can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2401.03253.pdf' target='_blank'>https://arxiv.org/pdf/2401.03253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhao Chen, Yulong Zhang, Weisen Jiang, Jiangang Lu, Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03253">VLLaVO: Mitigating Visual Gap through LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To tackle this issue, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, disregarding the potential benefits of incorporating the text modality. In this work, we propose VLLaVO, combining Vision language models and Large Language models as Visual cross-dOmain learners. VLLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results under domain generalization and unsupervised domain adaptation settings demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2311.16494.pdf' target='_blank'>https://arxiv.org/pdf/2311.16494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Tian, Shu Zou, Zhaoyuan Yang, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16494">ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although soft prompt tuning is effective in efficiently adapting Vision-Language (V&L) models for downstream tasks, it shows limitations in dealing with distribution shifts. We address this issue with Attribute-Guided Prompt Tuning (ArGue), making three key contributions. 1) In contrast to the conventional approach of directly appending soft prompts preceding class names, we align the model with primitive visual attributes generated by Large Language Models (LLMs). We posit that a model's ability to express high confidence in these attributes signifies its capacity to discern the correct class rationales. 2) We introduce attribute sampling to eliminate disadvantageous attributes, thus only semantically meaningful attributes are preserved. 3) We propose negative prompting, explicitly enumerating class-agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features. In experiments, our method significantly outperforms current state-of-the-art prompt tuning methods on both novel class prediction and out-of-distribution generalization tasks.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2310.06666.pdf' target='_blank'>https://arxiv.org/pdf/2310.06666.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06666">Unlock the Potential of Counterfactually-Augmented Data in Out-Of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Counterfactually-Augmented Data (CAD) -- minimal editing of sentences to flip the corresponding labels -- has the potential to improve the Out-Of-Distribution (OOD) generalization capability of language models, as CAD induces language models to exploit domain-independent causal features and exclude spurious correlations. However, the empirical results of CAD's OOD generalization are not as efficient as anticipated. In this study, we attribute the inefficiency to the myopia phenomenon caused by CAD: language models only focus on causal features that are edited in the augmentation operation and exclude other non-edited causal features. Therefore, the potential of CAD is not fully exploited. To address this issue, we analyze the myopia phenomenon in feature space from the perspective of Fisher's Linear Discriminant, then we introduce two additional constraints based on CAD's structural properties (dataset-level and sentence-level) to help language models extract more complete causal features in CAD, thereby mitigating the myopia phenomenon and improving OOD generalization capability. We evaluate our method on two tasks: Sentiment Analysis and Natural Language Inference, and the experimental results demonstrate that our method could unlock the potential of CAD and improve the OOD generalization performance of language models by 1.0% to 5.9%.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2309.01155.pdf' target='_blank'>https://arxiv.org/pdf/2309.01155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Shi, Sibei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01155">LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt engineering is a powerful tool used to enhance the performance of pre-trained models on downstream tasks. For example, providing the prompt "Let's think step by step" improved GPT-3's reasoning accuracy to 63% on MutiArith while prompting "a photo of" filled with a class name enables CLIP to achieve $80$\% zero-shot accuracy on ImageNet. While previous research has explored prompt learning for the visual modality, analyzing what constitutes a good visual prompt specifically for image recognition is limited. In addition, existing visual prompt tuning methods' generalization ability is worse than text-only prompting tuning. This paper explores our key insight: synthetic text images are good visual prompts for vision-language models! To achieve that, we propose our LoGoPrompt, which reformulates the classification objective to the visual prompt selection and addresses the chicken-and-egg challenge of first adding synthetic text images as class-wise visual prompts or predicting the class first. Without any trainable visual prompt parameters, experimental results on 16 datasets demonstrate that our method consistently outperforms state-of-the-art methods in few-shot learning, base-to-new generalization, and domain generalization.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2308.02582.pdf' target='_blank'>https://arxiv.org/pdf/2308.02582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aseem Arora, Shabbirhussain Bhaisaheb, Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02582">Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to be performed one-time per new database with minimal human intervention. Our approach demonstrates superior performance on the KaggleDBQA dataset, designed to evaluate generalizability for the Text-to-SQL task. We further showcase consistent performance improvement of LTMP-DA-GP over GP, across LLMs and databases of KaggleDBQA, highlighting the efficacy and model agnostic benefits of our prompt based adapt and decompose approach.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2307.12502.pdf' target='_blank'>https://arxiv.org/pdf/2307.12502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenming Li, Daoan Zhang, Wenjian Huang, Jianguo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12502">Cross Contrasting Feature Perturbation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn a robust model from source domains that generalize well on unseen target domains. Recent studies focus on generating novel domain samples or features to diversify distributions complementary to source domains. Yet, these approaches can hardly deal with the restriction that the samples synthesized from various domains can cause semantic distortion. In this paper, we propose an online one-stage Cross Contrasting Feature Perturbation (CCFP) framework to simulate domain shift by generating perturbed features in the latent space while regularizing the model prediction against domain shift. Different from the previous fixed synthesizing strategy, we design modules with learnable feature perturbations and semantic consistency constraints. In contrast to prior work, our method does not use any generative-based models or domain labels. We conduct extensive experiments on a standard DomainBed benchmark with a strict evaluation protocol for a fair comparison. Comprehensive experiments show that our method outperforms the previous state-of-the-art, and quantitative analyses illustrate that our approach can alleviate the domain shift problem in out-of-distribution (OOD) scenarios.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2307.09520.pdf' target='_blank'>https://arxiv.org/pdf/2307.09520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Cheng, Tejas Gokhale, Yezhou Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09520">Adversarial Bayesian Augmentation for Single-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing to unseen image domains is a challenging problem primarily due to the lack of diverse training data, inaccessible target data, and the large domain shift that may exist in many real-world settings. As such data augmentation is a critical component of domain generalization methods that seek to address this problem. We present Adversarial Bayesian Augmentation (ABA), a novel algorithm that learns to generate image augmentations in the challenging single-source domain generalization setting. ABA draws on the strengths of adversarial learning and Bayesian neural networks to guide the generation of diverse data augmentations -- these synthesized image domains aid the classifier in generalizing to unseen domains. We demonstrate the strength of ABA on several types of domain shift including style shift, subpopulation shift, and shift in the medical imaging setting. ABA outperforms all previous state-of-the-art methods, including pre-specified augmentations, pixel-based and convolutional-based augmentations.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2307.08551.pdf' target='_blank'>https://arxiv.org/pdf/2307.08551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akshay Mehra, Yunbei Zhang, Bhavya Kailkhura, Jihun Hamm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08551">On the Fly Neural Style Smoothing for Risk-Averse Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving high accuracy on data from domains unseen during training is a fundamental challenge in domain generalization (DG). While state-of-the-art DG classifiers have demonstrated impressive performance across various tasks, they have shown a bias towards domain-dependent information, such as image styles, rather than domain-invariant information, such as image content. This bias renders them unreliable for deployment in risk-sensitive scenarios such as autonomous driving where a misclassification could lead to catastrophic consequences. To enable risk-averse predictions from a DG classifier, we propose a novel inference procedure, Test-Time Neural Style Smoothing (TT-NSS), that uses a "style-smoothed" version of the DG classifier for prediction at test time. Specifically, the style-smoothed classifier classifies a test image as the most probable class predicted by the DG classifier on random re-stylizations of the test image. TT-NSS uses a neural style transfer module to stylize a test image on the fly, requires only black-box access to the DG classifier, and crucially, abstains when predictions of the DG classifier on the stylized test images lack consensus. Additionally, we propose a neural style smoothing (NSS) based training procedure that can be seamlessly integrated with existing DG methods. This procedure enhances prediction consistency, improving the performance of TT-NSS on non-abstained samples. Our empirical results demonstrate the effectiveness of TT-NSS and NSS at producing and improving risk-averse predictions on unseen domains from DG classifiers trained with SOTA training methods on various benchmark datasets and their variations.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2306.15117.pdf' target='_blank'>https://arxiv.org/pdf/2306.15117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdiyar Molahasani, Ali Etemad, Michael Greenspan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15117">Continual Learning for Out-of-Distribution Pedestrian Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A continual learning solution is proposed to address the out-of-distribution generalization problem for pedestrian detection. While recent pedestrian detection models have achieved impressive performance on various datasets, they remain sensitive to shifts in the distribution of the inference data. Our method adopts and modifies Elastic Weight Consolidation to a backbone object detection network, in order to penalize the changes in the model weights based on their importance towards the initially learned task. We show that when trained with one dataset and fine-tuned on another, our solution learns the new distribution and maintains its performance on the previous one, avoiding catastrophic forgetting. We use two popular datasets, CrowdHuman and CityPersons for our cross-dataset experiments, and show considerable improvements over standard fine-tuning, with a 9% and 18% miss rate percent reduction improvement in the CrowdHuman and CityPersons datasets, respectively.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2306.02243.pdf' target='_blank'>https://arxiv.org/pdf/2306.02243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Rong, Hao Chen, Linlin Ou, Tianxiao Chen, Xinyi Yu, Yifan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02243">Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. Reprompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2305.13605.pdf' target='_blank'>https://arxiv.org/pdf/2305.13605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mei Wang, Weihong Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13605">Adaptive Face Recognition Using Adversarial Information Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many real-world applications, face recognition models often degenerate when training data (referred to as source domain) are different from testing data (referred to as target domain). To alleviate this mismatch caused by some factors like pose and skin tone, the utilization of pseudo-labels generated by clustering algorithms is an effective way in unsupervised domain adaptation. However, they always miss some hard positive samples. Supervision on pseudo-labeled samples attracts them towards their prototypes and would cause an intra-domain gap between pseudo-labeled samples and the remaining unlabeled samples within target domain, which results in the lack of discrimination in face recognition. In this paper, considering the particularity of face recognition, we propose a novel adversarial information network (AIN) to address it. First, a novel adversarial mutual information (MI) loss is proposed to alternately minimize MI with respect to the target classifier and maximize MI with respect to the feature extractor. By this min-max manner, the positions of target prototypes are adaptively modified which makes unlabeled images clustered more easily such that intra-domain gap can be mitigated. Second, to assist adversarial MI loss, we utilize a graph convolution network to predict linkage likelihoods between target data and generate pseudo-labels. It leverages valuable information in the context of nodes and can achieve more reliable results. The proposed method is evaluated under two scenarios, i.e., domain adaptation across poses and image conditions, and domain adaptation across faces with different skin tones. Extensive experiments show that AIN successfully improves cross-domain generalization and offers a new state-of-the-art on RFW dataset.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2305.11835.pdf' target='_blank'>https://arxiv.org/pdf/2305.11835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Xue, Sylvain Calinon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11835">Contact Optimization with Learning from Demonstration: Application in Long-term Non-prehensile Planar Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-term non-prehensile planar manipulation is a challenging task for planning and control, requiring determination of both continuous and discrete contact configurations, such as contact points and modes. This leads to the non-convexity and hybridness of contact optimization. To overcome these difficulties, we propose a novel approach that incorporates human demonstrations into trajectory optimization. We show that our approach effectively handles the hybrid combinatorial nature of the problem, mitigates the issues with local minima present in current state-of-the-art solvers, and requires only a small number of demonstrations while delivering robust generalization performance. We validate our results in simulation and demonstrate its applicability on a pusher-slider system with a real Franka Emika robot.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2304.10226.pdf' target='_blank'>https://arxiv.org/pdf/2304.10226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheren Li, Zhiming Cui, Lichi Zhang, Sheng Wang, Chenjin Lei, Xi Ouyang, Dongdong Chen, Xiangyu Zhao, Yajia Gu, Zaiyi Liu, Chunling Liu, Dinggang Shen, Jie-Zhi Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10226">Domain Generalization for Mammographic Image Analysis with Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deep learning technique has been shown to be effectively addressed several image analysis tasks in the computer-aided diagnosis scheme for mammography. The training of an efficacious deep learning model requires large data with diverse styles and qualities. The diversity of data often comes from the use of various scanners of vendors. But, in practice, it is impractical to collect a sufficient amount of diverse data for training. To this end, a novel contrastive learning is developed to equip the deep learning models with better style generalization capability. Specifically, the multi-style and multi-view unsupervised self-learning scheme is carried out to seek robust feature embedding against style diversity as a pretrained model. Afterward, the pretrained network is further fine-tuned to the downstream tasks, e.g., mass detection, matching, BI-RADS rating, and breast density classification. The proposed method has been evaluated extensively and rigorously with mammograms from various vendor style domains and several public datasets. The experimental results suggest that the proposed domain generalization method can effectively improve performance of four mammographic image tasks on the data from both seen and unseen domains, and outperform many state-of-the-art (SOTA) generalization methods.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2303.02660.pdf' target='_blank'>https://arxiv.org/pdf/2303.02660.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meiling Fang, Marco Huber, Naser Damer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02660">SynthASpoof: Developing Face Presentation Attack Detection Based on Privacy-friendly Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, significant progress has been made in face presentation attack detection (PAD), which aims to secure face recognition systems against presentation attacks, owing to the availability of several face PAD datasets. However, all available datasets are based on privacy and legally-sensitive authentic biometric data with a limited number of subjects. To target these legal and technical challenges, this work presents the first synthetic-based face PAD dataset, named SynthASpoof, as a large-scale PAD development dataset. The bona fide samples in SynthASpoof are synthetically generated and the attack samples are collected by presenting such synthetic data to capture systems in a real attack scenario. The experimental results demonstrate the feasibility of using SynthASpoof for the development of face PAD. Moreover, we boost the performance of such a solution by incorporating the domain generalization tool MixStyle into the PAD solutions. Additionally, we showed the viability of using synthetic data as a supplement to enrich the diversity of limited authentic training data and consistently enhance PAD performances. The SynthASpoof dataset, containing 25,000 bona fide and 78,800 attack samples, the implementation, and the pre-trained weights are made publicly available.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2302.09345.pdf' target='_blank'>https://arxiv.org/pdf/2302.09345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09345">Improving the Out-Of-Distribution Generalization Capability of Language Models: Counterfactually-Augmented Data is not Enough</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Counterfactually-Augmented Data (CAD) has the potential to improve language models' Out-Of-Distribution (OOD) generalization capability, as CAD induces language models to exploit causal features and exclude spurious correlations. However, the empirical results of OOD generalization on CAD are not as efficient as expected. In this paper, we attribute the inefficiency to Myopia Phenomenon caused by CAD: language models only focus on causal features that are edited in the augmentation and exclude other non-edited causal features. As a result, the potential of CAD is not fully exploited. Based on the structural properties of CAD, we design two additional constraints to help language models extract more complete causal features contained in CAD, thus improving the OOD generalization capability. We evaluate our method on two tasks: Sentiment Analysis and Natural Language Inference, and the experimental results demonstrate that our method could unlock CAD's potential and improve language models' OOD generalization capability.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2302.02609.pdf' target='_blank'>https://arxiv.org/pdf/2302.02609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaxiu Yao, Xinyu Yang, Xinyi Pan, Shengchao Liu, Pang Wei Koh, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02609">Improving Domain Generalization with Domain Relations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distribution shift presents a significant challenge in machine learning, where models often underperform during the test stage when faced with a different distribution than the one they were trained on. This paper focuses on domain shifts, which occur when the model is applied to new domains that are different from the ones it was trained on, and propose a new approach called D$^3$G. Unlike previous methods that aim to learn a single model that is domain invariant, D$^3$G leverages domain similarities based on domain metadata to learn domain-specific models. Concretely, D$^3$G learns a set of training-domain-specific functions during the training stage and reweights them based on domain relations during the test stage. These domain relations can be directly obtained and learned from domain metadata. Under mild assumptions, we theoretically prove that using domain relations to reweight training-domain-specific functions achieves stronger out-of-domain generalization compared to the conventional averaging approach. Empirically, we evaluate the effectiveness of D$^3$G using real-world datasets for tasks such as temperature regression, land use classification, and molecule-protein binding affinity prediction. Our results show that D$^3$G consistently outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2209.14905.pdf' target='_blank'>https://arxiv.org/pdf/2209.14905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>GrÃ©goire Mialon, Randall Balestriero, Yann LeCun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14905">Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE avoid collapse of their joint embedding architectures by constraining or regularizing the covariance matrix of their projector's output. This study highlights important properties of such strategy, which we coin Variance-Covariance regularization (VCReg). More precisely, we show that {\em VCReg combined to a MLP projector enforces pairwise independence between the features of the learned representation}. This result emerges by bridging VCReg applied on the projector's output to kernel independence criteria applied on the projector's input. We empirically validate our findings where (i) we put in evidence which projector's characteristics favor pairwise independence, (ii) we demonstrate pairwise independence to be beneficial for out-of-domain generalization, (iii) we demonstrate that the scope of VCReg goes beyond SSL by using it to solve Independent Component Analysis. This provides the first theoretical motivation and explanation of MLP projectors in SSL.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2507.09884.pdf' target='_blank'>https://arxiv.org/pdf/2507.09884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuzhao Li, Xuchen Li, Shiyu Hu, Yongzhen Guo, Wentao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09884">VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) increasingly rely on reinforcement learning (RL) to enhance their reasoning capabilities through feedback. A critical challenge is verifying the consistency of model-generated responses and reference answers, since these responses are often lengthy, diverse, and nuanced. Rule-based verifiers struggle with complexity, prompting the use of model-based verifiers. However, specialized verifiers lack flexibility, while general LLM judges can be inconsistent. Existing research primarily focuses on building better verifiers, yet a systematic evaluation of different types of verifiers' performance across domains remains lacking, severely constraining the reliable development of Reinforcement Learning with Verifiable Reward (RLVR). To address this, we propose VerifyBench--a cross-domain comprehensive benchmark for systematically evaluating verifiers. We construct 4,000 expert-level questions covering mathematics, physics, chemistry, and biology. Each question is equipped with reference answers and diverse responses. The reliability of the evaluation is ensured through a rigorous annotation process conducted by a multidisciplinary expert team. We design a four-dimensional experimental framework to comprehensively compare the performance boundaries of specialized verifiers and general LLMs under combined conditions of extracted answers vs. complete responses, and short vs. long outputs. Our evaluation uncovers fundamental trade-offs in verifiers: while specialized verifiers achieve leading accuracy, they exhibit deficiencies in recall; general models show stronger inclusivity but unstable precision. More importantly, we discover verifiers' high sensitivity to input structure and inherent limitations in cross-domain generalization, providing critical insights into the bottlenecks of current verifier technology.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2507.04391.pdf' target='_blank'>https://arxiv.org/pdf/2507.04391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochen Zhou, Minrui Xu, Shiqi Chen, Junteng Liu, Yunqi Li, Xinxin Lin, Zhengyu Chen, Junxian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04391">Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been a growing interest in enhancing the mathematical problem-solving (MPS) capabilities of large language models. While the majority of research efforts concentrate on creating specialized models to solve mathematical problems, it remains unknown how learning mathematical problem-solving generalizes to help develop other reasoning abilities. In this paper, we present an empirical investigation into the generalization potential of various MPS training approaches, such as continual pretraining, instruction tuning, and rule-based reinforcement learning across various data sources, including both short and long chain-of-thought (CoT) samples. Evaluation on 5 mathematical and 8 general reasoning benchmarks show that continual pretraining on math text is able to generalize to general reasoning tasks to some extent. In constrast, instruction tuning on conventional, short MPS samples provides limited benefits and, in many cases, even impairs generalization performance. Notably, training with long CoT responses for MPS samples and incorporating rule-based reinforcement learning on MPS queries exhibit distinct behavior, significantly enhancing generalization by extending the model's reasoning processes into other domains. These results suggest that traditional approaches to learning MPS with short reasoning chains largely fail to achieve robust generalization. However, the emerging paradigm of longer reasoning chains, coupled with self-reflection, offers a promising direction for improving generalized reasoning abilities through learning from specialized domains.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2506.17740.pdf' target='_blank'>https://arxiv.org/pdf/2506.17740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyu Han, Zeyi Liu, Shijin Chen, Dongliang Zou, Xiao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17740">Rethinking the Role of Operating Conditions for Learning-based Multi-condition Fault Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-condition fault diagnosis is prevalent in industrial systems and presents substantial challenges for conventional diagnostic approaches. The discrepancy in data distributions across different operating conditions degrades model performance when a model trained under one condition is applied to others. With the recent advancements in deep learning, transfer learning has been introduced to the fault diagnosis field as a paradigm for addressing multi-condition fault diagnosis. Among these methods, domain generalization approaches can handle complex scenarios by extracting condition-invariant fault features. Although many studies have considered fault diagnosis in specific multi-condition scenarios, the extent to which operating conditions affect fault information has been scarcely studied, which is crucial. However, the extent to which operating conditions affect fault information has been scarcely studied, which is crucial. When operating conditions have a significant impact on fault features, directly applying domain generalization methods may lead the model to learn condition-specific information, thereby reducing its overall generalization ability. This paper investigates the performance of existing end-to-end domain generalization methods under varying conditions, specifically in variable-speed and variable-load scenarios, using multiple experiments on a real-world gearbox. Additionally, a two-stage diagnostic framework is proposed, aiming to improve fault diagnosis performance under scenarios with significant operating condition impacts. By incorporating a domain-generalized encoder with a retraining strategy, the framework is able to extract condition-invariant fault features while simultaneously alleviating potential overfitting to the source domain. Several experiments on a real-world gearbox dataset are conducted to validate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2506.08672.pdf' target='_blank'>https://arxiv.org/pdf/2506.08672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Jiaqi Li, Zilong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08672">RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($Î$4.1% average points on eight ID tasks and $Î$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2506.00027.pdf' target='_blank'>https://arxiv.org/pdf/2506.00027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyu Chen, Yudong Wang, Teng Xiao, Ruochen Zhou, Xuesheng Yang, Wei Wang, Zhifang Sui, Jingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00027">From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in improving the reasoning capabilities of Large Language Models have underscored the efficacy of Process Reward Models (PRMs) in addressing intermediate errors through structured feedback mechanisms. This study analyzes PRMs from multiple perspectives, including training methodologies, scalability, and generalization capabilities. We investigate the interplay between pre-training and reward model training FLOPs to assess their influence on PRM efficiency and accuracy in complex reasoning tasks. Our analysis reveals a pattern of diminishing returns in performance with increasing PRM scale, highlighting the importance of balancing model size and computational cost. Furthermore, the diversity of training datasets significantly impacts PRM performance, emphasizing the importance of diverse data to enhance both accuracy and efficiency. We further examine test-time scaling strategies, identifying Monte Carlo Tree Search as the most effective method when computational resources are abundant, while Best-of-N Sampling serves as a practical alternative under resource-limited conditions. Notably, our findings indicate that PRMs trained on mathematical datasets exhibit performance comparable to those tailored for code generation, suggesting robust cross-domain generalization. Employing a gradient-based metric, we observe that PRMs exhibit a preference for selecting responses with similar underlying patterns, further informing their optimization.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2504.03185.pdf' target='_blank'>https://arxiv.org/pdf/2504.03185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaymari Chua, Chen Wang, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03185">Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2502.09507.pdf' target='_blank'>https://arxiv.org/pdf/2502.09507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elias Kempf, Simon Schrodi, Max Argus, Thomas Brox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09507">When and How Does CLIP Enable Domain and Compositional Generalization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric and mechanistic analyses, we find that successful generalization requires the learning of sufficiently shared representations in intermediate layers and circuits.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2412.14816.pdf' target='_blank'>https://arxiv.org/pdf/2412.14816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenfan Qu, Jian Liu, Haoxing Chen, Baihan Yu, Jingjing Liu, Weiqiang Wang, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14816">TextSleuth: Towards Explainable Tampered Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, tampered text detection has attracted increasing attention due to its essential role in information security. Although existing methods can detect the tampered text region, the interpretation of such detection remains unclear, making the prediction unreliable. To address this problem, we propose to explain the basis of tampered text detection with natural language via large multimodal models. To fill the data gap for this task, we propose a large-scale, comprehensive dataset, ETTD, which contains both pixel-level annotations for tampered text region and natural language annotations describing the anomaly of the tampered text. Multiple methods are employed to improve the quality of the proposed data. For example, elaborate queries are introduced to generate high-quality anomaly descriptions with GPT4o. A fused mask prompt is proposed to reduce confusion when querying GPT4o to generate anomaly descriptions. To automatically filter out low-quality annotations, we also propose to prompt GPT4o to recognize tampered texts before describing the anomaly, and to filter out the responses with low OCR accuracy. To further improve explainable tampered text detection, we propose a simple yet effective model called TextSleuth, which achieves improved fine-grained perception and cross-domain generalization by focusing on the suspected region, with a two-stage analysis paradigm and an auxiliary grounding prompt. Extensive experiments on both the ETTD dataset and the public dataset have verified the effectiveness of the proposed methods. In-depth analysis is also provided to inspire further research. Our dataset and code will be open-source.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2411.04847.pdf' target='_blank'>https://arxiv.org/pdf/2411.04847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04847">Prompt-Guided Internal States for Hallucination Detection of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of tasks in different domains. However, they sometimes generate responses that are logically coherent but factually incorrect or misleading, which is known as LLM hallucinations. Data-driven supervised methods train hallucination detectors by leveraging the internal states of LLMs, but detectors trained on specific domains often struggle to generalize well to other domains. In this paper, we aim to enhance the cross-domain performance of supervised detectors with only in-domain data. We propose a novel framework, prompt-guided internal states for hallucination detection of LLMs, namely PRISM. By utilizing appropriate prompts to guide changes to the structure related to text truthfulness in LLMs' internal states, we make this structure more salient and consistent across texts from different domains. We integrated our framework with existing hallucination detection methods and conducted experiments on datasets from different domains. The experimental results indicate that our framework significantly enhances the cross-domain generalization of existing hallucination detection methods.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2410.22461.pdf' target='_blank'>https://arxiv.org/pdf/2410.22461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyusam Chang, Jiwon Lee, Donghyun Kim, Jinkyu Kim, Dongwook Lee, Daehyun Ji, Sujin Jang, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22461">Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D object detection leveraging multi-view cameras have demonstrated their practical and economical value in various challenging vision tasks. However, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (\ie, direct transfer) due to the inevitable geometric misalignment between the source and target domains. In practice, we also encounter constraints on resources for training models and collecting annotations for the successful deployment of 3D object detectors. In this paper, we propose Unified Domain Generalization and Adaptation (UDGA), a practical solution to mitigate those drawbacks. We first propose Multi-view Overlap Depth Constraint that leverages the strong association between multi-view, significantly alleviating geometric gaps due to perspective view changes. Then, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (\ie, 1$\%$ and 5$\%)$, while preserving well-defined source knowledge for training efficiency. Overall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations. We demonstrate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, and Waymo, where our framework outperforms the current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2409.17993.pdf' target='_blank'>https://arxiv.org/pdf/2409.17993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchen Yu, Si-Yuan Cao, Runmin Zhang, Chenghao Zhang, Zhu Yu, Shujie Chen, Bailin Yang, Hui-liang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17993">SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel unsupervised cross-modal homography estimation learning framework, named Split Supervised Homography estimation Network (SSHNet). SSHNet reformulates the unsupervised cross-modal homography estimation into two supervised sub-problems, each addressed by its specialized network: a homography estimation network and a modality transfer network. To realize stable training, we introduce an effective split optimization strategy to train each network separately within its respective sub-problem. We also formulate an extra homography feature space supervision to enhance feature consistency, further boosting the estimation accuracy. Moreover, we employ a simple yet effective distillation training technique to reduce model parameters and improve cross-domain generalization ability while maintaining comparable performance. The training stability of SSHNet enables its cooperation with various homography estimation architectures. Experiments reveal that the SSHNet using IHN as homography estimation network, namely SSHNet-IHN, outperforms previous unsupervised approaches by a significant margin. Even compared to supervised approaches MHN and LocalTrans, SSHNet-IHN achieves 47.4% and 85.8% mean average corner errors (MACEs) reduction on the challenging OPT-SAR dataset.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2407.08582.pdf' target='_blank'>https://arxiv.org/pdf/2407.08582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junteng Liu, Shiqi Chen, Yu Cheng, Junxian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08582">On the Universal Truthfulness Hyperplane Inside LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LLMs) have demonstrated remarkable abilities across various fields, hallucination remains a significant challenge. Recent studies have explored hallucinations through the lens of internal representations, proposing mechanisms to decipher LLMs' adherence to facts. However, these approaches often fail to generalize to out-of-distribution data, leading to concerns about whether internal representation patterns reflect fundamental factual awareness, or only overfit spurious correlations on the specific datasets. In this work, we investigate whether a universal truthfulness hyperplane that distinguishes the model's factually correct and incorrect outputs exists within the model. To this end, we scale up the number of training datasets and conduct an extensive evaluation -- we train the truthfulness hyperplane on a diverse collection of over 40 datasets and examine its cross-task, cross-domain, and in-domain generalization. Our results indicate that increasing the diversity of the training datasets significantly enhances the performance in all scenarios, while the volume of data samples plays a less critical role. This finding supports the optimistic hypothesis that a universal truthfulness hyperplane may indeed exist within the model, offering promising directions for future research.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2407.06939.pdf' target='_blank'>https://arxiv.org/pdf/2407.06939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sriram Yenamandra, Arun Ramachandran, Mukul Khanna, Karmesh Yadav, Jay Vakil, Andrew Melnik, Michael BÃ¼ttner, Leon Harz, Lyon Brown, Gora Chand Nandi, Arjun PS, Gaurav Kumar Yadav, Rahul Kala, Robert Haschke, Yang Luo, Jinxin Zhu, Yansen Han, Bingyi Lu, Xuan Gu, Qinyuan Liu, Yaping Zhao, Qiting Ye, Chenxiao Dou, Yansong Chua, Volodymyr Kuzma, Vladyslav Humennyy, Ruslan Partsey, Jonathan Francis, Devendra Singh Chaplot, Gunjan Chhablani, Alexander Clegg, Theophile Gervet, Vidhi Jain, Ram Ramrakhya, Andrew Szot, Austin Wang, Tsung-Yen Yang, Aaron Edsinger, Charlie Kemp, Binit Shah, Zsolt Kira, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06939">Towards Open-World Mobile Manipulation in Homes: Lessons from the Neurips 2023 HomeRobot Open Vocabulary Mobile Manipulation Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to develop robots that can effectively serve as versatile and capable home assistants, it is crucial for them to reliably perceive and interact with a wide variety of objects across diverse environments. To this end, we proposed Open Vocabulary Mobile Manipulation as a key benchmark task for robotics: finding any object in a novel environment and placing it on any receptacle surface within that environment. We organized a NeurIPS 2023 competition featuring both simulation and real-world components to evaluate solutions to this task. Our baselines on the most challenging version of this task, using real perception in simulation, achieved only an 0.8% success rate; by the end of the competition, the best participants achieved an 10.8\% success rate, a 13x improvement. We observed that the most successful teams employed a variety of methods, yet two common threads emerged among the best solutions: enhancing error detection and recovery, and improving the integration of perception with decision-making processes. In this paper, we detail the results and methodologies used, both in simulation and real-world settings. We discuss the lessons learned and their implications for future research. Additionally, we compare performance in real and simulated environments, emphasizing the necessity for robust generalization to novel settings.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2406.09896.pdf' target='_blank'>https://arxiv.org/pdf/2406.09896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BrunÃ³ B. Englert, Fabrizio J. Piva, Tommie Kerssies, Daan de Geus, Gijs Dubbelman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09896">Exploring the Benefits of Vision Foundation Models for Unsupervised Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving robust generalization across diverse data domains remains a significant challenge in computer vision. This challenge is important in safety-critical applications, where deep-neural-network-based systems must perform reliably under various environmental conditions not seen during training. Our study investigates whether the generalization capabilities of Vision Foundation Models (VFMs) and Unsupervised Domain Adaptation (UDA) methods for the semantic segmentation task are complementary. Results show that combining VFMs with UDA has two main benefits: (a) it allows for better UDA performance while maintaining the out-of-distribution performance of VFMs, and (b) it makes certain time-consuming UDA components redundant, thus enabling significant inference speedups. Specifically, with equivalent model sizes, the resulting VFM-UDA method achieves an 8.4$\times$ speed increase over the prior non-VFM state of the art, while also improving performance by +1.2 mIoU in the UDA setting and by +6.1 mIoU in terms of out-of-distribution generalization. Moreover, when we use a VFM with 3.6$\times$ more parameters, the VFM-UDA approach maintains a 3.3$\times$ speed up, while improving the UDA performance by +3.1 mIoU and the out-of-distribution performance by +10.3 mIoU. These results underscore the significant benefits of combining VFMs with UDA, setting new standards and baselines for Unsupervised Domain Adaptation in semantic segmentation.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2406.05372.pdf' target='_blank'>https://arxiv.org/pdf/2406.05372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiancong Xiao, Ruoyu Sun, Qi Long, Weijie J. Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05372">Bridging the Gap: Rademacher Complexity in Robust and Standard Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training Deep Neural Networks (DNNs) with adversarial examples often results in poor generalization to test-time adversarial data. This paper investigates this issue, known as adversarially robust generalization, through the lens of Rademacher complexity. Building upon the studies by Khim and Loh (2018); Yin et al. (2019), numerous works have been dedicated to this problem, yet achieving a satisfactory bound remains an elusive goal. Existing works on DNNs either apply to a surrogate loss instead of the robust loss or yield bounds that are notably looser compared to their standard counterparts. In the latter case, the bounds have a higher dependency on the width $m$ of the DNNs or the dimension $d$ of the data, with an extra factor of at least $\mathcal{O}(\sqrt{m})$ or $\mathcal{O}(\sqrt{d})$.
  This paper presents upper bounds for adversarial Rademacher complexity of DNNs that match the best-known upper bounds in standard settings, as established in the work of Bartlett et al. (2017), with the dependency on width and dimension being $\mathcal{O}(\ln(dm))$. The central challenge addressed is calculating the covering number of adversarial function classes. We aim to construct a new cover that possesses two properties: 1) compatibility with adversarial examples, and 2) precision comparable to covers used in standard settings. To this end, we introduce a new variant of covering number called the \emph{uniform covering number}, specifically designed and proven to reconcile these two properties. Consequently, our method effectively bridges the gap between Rademacher complexity in robust and standard generalization.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2402.06974.pdf' target='_blank'>https://arxiv.org/pdf/2402.06974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Bartholet, Taehyeon Kim, Ami Beuret, Se-Young Yun, Joachim M. Buhmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06974">Hypernetwork-Driven Model Fusion for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) faces significant challenges with domain shifts in heterogeneous data, degrading performance. Traditional domain generalization aims to learn domain-invariant features, but the federated nature of model averaging often limits this due to its linear aggregation of local learning. To address this, we propose a robust framework, coined as hypernetwork-based Federated Fusion (hFedF), using hypernetworks for non-linear aggregation, facilitating generalization to unseen domains. Our method employs client-specific embeddings and gradient alignment techniques to manage domain generalization effectively. Evaluated in both zero-shot and few-shot settings, hFedF demonstrates superior performance in handling domain shifts. Comprehensive comparisons on PACS, Office-Home, and VLCS datasets show that hFedF consistently achieves the highest in-domain and out-of-domain accuracy with reliable predictions. Our study contributes significantly to the under-explored field of Federated Domain Generalization (FDG), setting a new benchmark for performance in this area.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2312.12444.pdf' target='_blank'>https://arxiv.org/pdf/2312.12444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaylee Burns, Zach Witzel, Jubayer Ibn Hamid, Tianhe Yu, Chelsea Finn, Karol Hausman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12444">What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the success of transfer learning in computer vision, roboticists have investigated visual pre-training as a means to improve the learning efficiency and generalization ability of policies learned from pixels. To that end, past work has favored large object interaction datasets, such as first-person videos of humans completing diverse tasks, in pursuit of manipulation-relevant features. Although this approach improves the efficiency of policy learning, it remains unclear how reliable these representations are in the presence of distribution shifts that arise commonly in robotic applications. Surprisingly, we find that visual representations designed for manipulation and control tasks do not necessarily generalize under subtle changes in lighting and scene texture or the introduction of distractor objects. To understand what properties do lead to robust representations, we compare the performance of 15 pre-trained vision models under different visual appearances. We find that emergent segmentation ability is a strong predictor of out-of-distribution generalization among ViT models. The rank order induced by this metric is more predictive than metrics that have previously guided generalization research within computer vision and machine learning, such as downstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by cue-conflict performance. We test this finding extensively on a suite of distribution shifts in ten tasks across two simulated manipulation environments. On the ALOHA setup, segmentation score predicts real-world performance after offline training with 50 demonstrations.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2312.03048.pdf' target='_blank'>https://arxiv.org/pdf/2312.03048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuru Jia, Lukas Hoyer, Shengyu Huang, Tianfu Wang, Luc Van Gool, Konrad Schindler, Anton Obukhov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03048">DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large, pretrained latent diffusion models (LDMs) have demonstrated an extraordinary ability to generate creative content, specialize to user data through few-shot fine-tuning, and condition their output on other modalities, such as semantic maps. However, are they usable as large-scale data generators, e.g., to improve tasks in the perception stack, like semantic segmentation? We investigate this question in the context of autonomous driving, and answer it with a resounding "yes". We propose an efficient data generation pipeline termed DGInStyle. First, we examine the problem of specializing a pretrained LDM to semantically-controlled generation within a narrow domain. Second, we propose a Style Swap technique to endow the rich generative prior with the learned semantic control. Third, we design a Multi-resolution Latent Fusion technique to overcome the bias of LDMs towards dominant objects. Using DGInStyle, we generate a diverse dataset of street scenes, train a domain-agnostic semantic segmentation model on it, and evaluate the model on multiple popular autonomous driving datasets. Our approach consistently increases the performance of several domain generalization methods compared to the previous state-of-the-art methods. The source code and the generated dataset are available at https://dginstyle.github.io.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2310.18807.pdf' target='_blank'>https://arxiv.org/pdf/2310.18807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rim Assouel, Pau Rodriguez, Perouz Taslakian, David Vazquez, Yoshua Bengio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18807">OC-NMN: Object-centric Compositional Neural Module Network for Generative Visual Analogical Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key aspect of human intelligence is the ability to imagine -- composing learned concepts in novel ways -- to make sense of new scenarios. Such capacity is not yet attained for machine learning systems. In this work, in the context of visual reasoning, we show how modularity can be leveraged to derive a compositional data augmentation framework inspired by imagination. Our method, denoted Object-centric Compositional Neural Module Network (OC-NMN), decomposes visual generative reasoning tasks into a series of primitives applied to objects without using a domain-specific language. We show that our modular architectural choices can be used to generate new training tasks that lead to better out-of-distribution generalization. We compare our model to existing and new baselines in proposed visual reasoning benchmark that consists of applying arithmetic operations to MNIST digits.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2310.02989.pdf' target='_blank'>https://arxiv.org/pdf/2310.02989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, Bruno RÃ©galdo-Saint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02989">xVal: A Continuous Numerical Tokenization for Scientific Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due in part to their discontinuous and discrete default encodings for numbers, Large Language Models (LLMs) have not yet been commonly used to process numerically-dense scientific datasets. Rendering datasets as text, however, could help aggregate diverse and multi-modal scientific data into a single training corpus, thereby potentially facilitating the development of foundation models for science. In this work, we introduce xVal, a strategy for continuously tokenizing numbers within language models that results in a more appropriate inductive bias for scientific applications. By training specially-modified language models from scratch on a variety of scientific datasets formatted as text, we find that xVal generally outperforms other common numerical tokenization strategies on metrics including out-of-distribution generalization and computational efficiency.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2308.14551.pdf' target='_blank'>https://arxiv.org/pdf/2308.14551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meiling Fang, Naser Damer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14551">Face Presentation Attack Detection by Excavating Causal Clues and Adapting Embedding Statistics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent face presentation attack detection (PAD) leverages domain adaptation (DA) and domain generalization (DG) techniques to address performance degradation on unknown domains. However, DA-based PAD methods require access to unlabeled target data, while most DG-based PAD solutions rely on a priori, i.e., known domain labels. Moreover, most DA-/DG-based methods are computationally intensive, demanding complex model architectures and/or multi-stage training processes. This paper proposes to model face PAD as a compound DG task from a causal perspective, linking it to model optimization. We excavate the causal factors hidden in the high-level representation via counterfactual intervention. Moreover, we introduce a class-guided MixStyle to enrich feature-level data distribution within classes instead of focusing on domain information. Both class-guided MixStyle and counterfactual intervention components introduce no extra trainable parameters and negligible computational resources. Extensive cross-dataset and analytic experiments demonstrate the effectiveness and efficiency of our method compared to state-of-the-art PADs. The implementation and the trained weights are publicly available.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2307.15460.pdf' target='_blank'>https://arxiv.org/pdf/2307.15460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhang, Ce Zhang, Yushun Tang, Zhihai He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15460">Cross-Modal Concept Learning and Inference for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP, establish the correlation between texts and images, achieving remarkable success on various downstream tasks with fine-tuning. In existing fine-tuning methods, the class-specific text description is matched against the whole image. We recognize that this whole image matching is not effective since images from the same class often contain a set of different semantic objects, and an object further consists of a set of semantic parts or concepts. Individual semantic parts or concepts may appear in image samples from different classes. To address this issue, in this paper, we develop a new method called cross-model concept learning and inference (CCLI). Using the powerful text-image correlation capability of CLIP, our method automatically learns a large set of distinctive visual concepts from images using a set of semantic text concepts. Based on these visual concepts, we construct a discriminative representation of images and learn a concept inference network to perform downstream image classification tasks, such as few-shot learning and domain generalization. Extensive experimental results demonstrate that our CCLI method is able to improve the performance upon the current state-of-the-art methods by large margins, for example, by up to 8.0% improvement on few-shot learning and by up to 1.3% for domain generalization.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2307.04726.pdf' target='_blank'>https://arxiv.org/pdf/2307.04726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suzan Ece Ada, Erhan Oztop, Emre Ugur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04726">Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. However, they face challenges handling distribution shifts due to the lack of online interaction during training. To this end, we propose a novel method named State Reconstruction for Diffusion Policies (SRDP) that incorporates state reconstruction feature learning in the recent class of diffusion policies to address the problem of out-of-distribution (OOD) generalization. Our method promotes learning of generalizable state representation to alleviate the distribution shift caused by OOD states. To illustrate the OOD generalization and faster convergence of SRDP, we design a novel 2D Multimodal Contextual Bandit environment and realize it on a 6-DoF real-world UR10 robot, as well as in simulation, and compare its performance with prior algorithms. In particular, we show the importance of the proposed state reconstruction via ablation studies. In addition, we assess the performance of our model on standard continuous control benchmarks (D4RL), namely the navigation of an 8-DoF ant and forward locomotion of half-cheetah, hopper, and walker2d, achieving state-of-the-art results. Finally, we demonstrate that our method can achieve 167% improvement over the competing baseline on a sparse continuous control navigation task where various regions of the state space are removed from the offline RL dataset, including the region encapsulating the goal.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2304.07896.pdf' target='_blank'>https://arxiv.org/pdf/2304.07896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Guo, Jonas Wildberger, Bernhard SchÃ¶lkopf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07896">Out-of-Variable Generalization for Discriminative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\textit{strong}$ or $\textit{out-of-distribution}$ generalization. However, merely considering differences in data distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring $\textit{subsets}$ of variables at any given time. Mathematically, $\textit{out-of-variable}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2303.08106.pdf' target='_blank'>https://arxiv.org/pdf/2303.08106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Akrout, Amal Feriani, Faouzi Bellili, Amine Mezghani, Ekram Hossain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08106">Domain Generalization in Machine Learning Models for Wireless Communications: Concepts, State-of-the-Art, and Open Issues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven machine learning (ML) is promoted as one potential technology to be used in next-generations wireless systems. This led to a large body of research work that applies ML techniques to solve problems in different layers of the wireless transmission link. However, most of these applications rely on supervised learning which assumes that the source (training) and target (test) data are independent and identically distributed (i.i.d). This assumption is often violated in the real world due to domain or distribution shifts between the source and the target data. Thus, it is important to ensure that these algorithms generalize to out-of-distribution (OOD) data. In this context, domain generalization (DG) tackles the OOD-related issues by learning models on different and distinct source domains/datasets with generalization capabilities to unseen new domains without additional finetuning. Motivated by the importance of DG requirements for wireless applications, we present a comprehensive overview of the recent developments in DG and the different sources of domain shift. We also summarize the existing DG methods and review their applications in selected wireless communication problems, and conclude with insights and open questions.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2303.01906.pdf' target='_blank'>https://arxiv.org/pdf/2303.01906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liwei Yang, Xiang Gu, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01906">Generalized Semantic Segmentation by Self-Supervised Source Domain Projection and Multi-Level Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep networks trained on the source domain show degraded performance when tested on unseen target domain data. To enhance the model's generalization ability, most existing domain generalization methods learn domain invariant features by suppressing domain sensitive features. Different from them, we propose a Domain Projection and Contrastive Learning (DPCL) approach for generalized semantic segmentation, which includes two modules: Self-supervised Source Domain Projection (SSDP) and Multi-level Contrastive Learning (MLCL). SSDP aims to reduce domain gap by projecting data to the source domain, while MLCL is a learning scheme to learn discriminative and generalizable features on the projected data. During test time, we first project the target data by SSDP to mitigate domain shift, then generate the segmentation results by the learned segmentation network based on MLCL. At test time, we can update the projected data by minimizing our proposed pixel-to-pixel contrastive loss to obtain better results. Extensive experiments for semantic segmentation demonstrate the favorable generalization capability of our method on benchmark datasets.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2303.00196.pdf' target='_blank'>https://arxiv.org/pdf/2303.00196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andong Wang, Chao Li, Mingyuan Bai, Zhong Jin, Guoxu Zhou, Qibin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00196">Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving efficient and robust multi-channel data learning is a challenging task in data science. By exploiting low-rankness in the transformed domain, i.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has achieved extensive success in multi-channel data representation and has recently been extended to function representation such as Neural Networks with t-product layers (t-NNs). However, it still remains unclear how t-SVD theoretically affects the learning behavior of t-NNs. This paper is the first to answer this question by deriving the upper bounds of the generalization error of both standard and adversarially trained t-NNs. It reveals that the t-NNs compressed by exact transformed low-rank parameterization can achieve a sharper adversarial generalization bound. In practice, although t-NNs rarely have exactly transformed low-rank weights, our analysis further shows that by adversarial training with gradient flow (GF), the over-parameterized t-NNs with ReLU activations are trained with implicit regularization towards transformed low-rank parameterization under certain conditions. We also establish adversarial generalization bounds for t-NNs with approximately transformed low-rank weights. Our analysis indicates that the transformed low-rank parameterization can promisingly enhance robust generalization for t-NNs.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2302.02350.pdf' target='_blank'>https://arxiv.org/pdf/2302.02350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daoan Zhang, Mingkai Chen, Chenming Li, Lingyun Huang, Jianguo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02350">Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) is a fundamental challenge for machine learning models, which aims to improve model generalization on various domains. Previous methods focus on generating domain invariant features from various source domains. However, we argue that the domain variantions also contain useful information, ie, classification-aware information, for downstream tasks, which has been largely ignored. Different from learning domain invariant features from source domains, we decouple the input images into Domain Expert Features and noise. The proposed domain expert features lie in a learned latent space where the images in each domain can be classified independently, enabling the implicit use of classification-aware domain variations. Based on the analysis, we proposed a novel paradigm called Domain Disentanglement Network (DDN) to disentangle the domain expert features from the source domain images and aggregate the source domain expert features for representing the target test domain. We also propound a new contrastive learning method to guide the domain expert features to form a more balanced and separable feature space. Experiments on the widely-used benchmarks of PACS, VLCS, OfficeHome, DomainNet, and TerraIncognita demonstrate the competitive performance of our method compared to the recently proposed alternatives.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2205.14230.pdf' target='_blank'>https://arxiv.org/pdf/2205.14230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochen Jiao, Xiangguo Liu, Takami Sato, Qi Alfred Chen, Qi Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.14230">Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the trajectories of surrounding objects is a critical task for self-driving vehicles and many other autonomous systems. Recent works demonstrate that adversarial attacks on trajectory prediction, where small crafted perturbations are introduced to history trajectories, may significantly mislead the prediction of future trajectories and induce unsafe planning. However, few works have addressed enhancing the robustness of this important safety-critical task.In this paper, we present a novel adversarial training method for trajectory prediction. Compared with typical adversarial training on image tasks, our work is challenged by more random input with rich context and a lack of class labels. To address these challenges, we propose a method based on a semi-supervised adversarial autoencoder, which models disentangled semantic features with domain knowledge and provides additional latent labels for the adversarial training. Extensive experiments with different types of attacks demonstrate that our Semisupervised Semantics-guided Adversarial Training (SSAT) method can effectively mitigate the impact of adversarial attacks by up to 73% and outperform other popular defense methods. In addition, experiments show that our method can significantly improve the system's robust generalization to unseen patterns of attacks. We believe that such semantics-guided architecture and advancement on robust generalization is an important step for developing robust prediction models and enabling safe decision-making.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2204.03916.pdf' target='_blank'>https://arxiv.org/pdf/2204.03916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephen Cha, Taehyeon Kim, Hayeon Lee, Se-Young Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.03916">A Survey of Supernet Optimization and its Applications: Spatial and Temporal Optimization for Neural Architecture Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This survey focuses on categorizing and evaluating the methods of supernet optimization in the field of Neural Architecture Search (NAS). Supernet optimization involves training a single, over-parameterized network that encompasses the search space of all possible network architectures. The survey analyses supernet optimization methods based on their approaches to spatial and temporal optimization. Spatial optimization relates to optimizing the architecture and parameters of the supernet and its subnets, while temporal optimization deals with improving the efficiency of selecting architectures from the supernet. The benefits, limitations, and potential applications of these methods in various tasks and settings, including transferability, domain generalization, and Transformer models, are also discussed.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2203.12198.pdf' target='_blank'>https://arxiv.org/pdf/2203.12198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Lin, Zhizheng Zhang, Zhipeng Huang, Yan Lu, Cuiling Lan, Peng Chu, Quanzeng You, Jiang Wang, Zicheng Liu, Amey Parulkar, Viraj Navkal, Zhibo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.12198">Deep Frequency Filtering for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving the generalization ability of Deep Neural Networks (DNNs) is critical for their practical uses, which has been a longstanding challenge. Some theoretical studies have uncovered that DNNs have preferences for some frequency components in the learning process and indicated that this may affect the robustness of learned features. In this paper, we propose Deep Frequency Filtering (DFF) for learning domain-generalizable features, which is the first endeavour to explicitly modulate the frequency components of different transfer difficulties across domains in the latent space during training. To achieve this, we perform Fast Fourier Transform (FFT) for the feature maps at different layers, then adopt a light-weight module to learn attention masks from the frequency representations after FFT to enhance transferable components while suppressing the components not conducive to generalization. Further, we empirically compare the effectiveness of adopting different types of attention designs for implementing DFF. Extensive experiments demonstrate the effectiveness of our proposed DFF and show that applying our DFF on a plain baseline outperforms the state-of-the-art methods on different domain generalization tasks, including close-set classification and open-set retrieval.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2112.15275.pdf' target='_blank'>https://arxiv.org/pdf/2112.15275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimberly Stachenfeld, Drummond B. Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, Alvaro Sanchez-Gonzalez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.15275">Learned Coarse Models for Efficient Turbulence Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Turbulence simulation with classical numerical solvers requires high-resolution grids to accurately resolve dynamics. Here we train learned simulators at low spatial and temporal resolutions to capture turbulent dynamics generated at high resolution. We show that our proposed model can simulate turbulent dynamics more accurately than classical numerical solvers at the comparably low resolutions across various scientifically relevant metrics. Our model is trained end-to-end from data and is capable of learning a range of challenging chaotic and turbulent dynamics at low resolution, including trajectories generated by the state-of-the-art Athena++ engine. We show that our simpler, general-purpose architecture outperforms various more specialized, turbulence-specific architectures from the learned turbulence simulation literature. In general, we see that learned simulators yield unstable trajectories; however, we show that tuning training noise and temporal downsampling solves this problem. We also find that while generalization beyond the training distribution is a challenge for learned models, training noise, added loss constraints, and dataset augmentation can help. Broadly, we conclude that our learned simulator outperforms traditional solvers run on coarser grids, and emphasize that simple design choices can offer stability and robust generalization.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2509.22695.pdf' target='_blank'>https://arxiv.org/pdf/2509.22695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitao Wang, Yanke Wang, Jiangtao Wen, Roberto Horowitz, Yuxing Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22695">ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation in unstructured environments requires the generation of robust and long-horizon trajectory-level policy with conditions of perceptual observations and benefits from the advantages of SE(3)-equivariant diffusion models that are data-efficient. However, these models suffer from the inference time costs. Inspired by the inference efficiency of rectified flows, we introduce the rectification to the SE(3)-diffusion models and propose the ReSeFlow, i.e., Rectifying SE(3)-Equivariant Policy Learning Flows, providing fast, geodesic-consistent, least-computational policy generation. Crucially, both components employ SE(3)-equivariant networks to preserve rotational and translational symmetry, enabling robust generalization under rigid-body motions. With the verification on the simulated benchmarks, we find that the proposed ReSeFlow with only one inference step can achieve better performance with lower geodesic distance than the baseline methods, achieving up to a 48.5% error reduction on the painting task and a 21.9% reduction on the rotating triangle task compared to the baseline's 100-step inference. This method takes advantages of both SE(3) equivariance and rectified flow and puts it forward for the real-world application of generative policy learning models with the data and inference efficiency.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2509.20684.pdf' target='_blank'>https://arxiv.org/pdf/2509.20684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Wang, Di Wang, Ke Li, Yifeng Wang, Chengjian Wang, Libin Sun, Zhihong Wu, Yiming Zhang, Quan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20684">Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view geo-localization (CVGL) aims to match images of the same location captured from drastically different viewpoints. Despite recent progress, existing methods still face two key challenges: (1) achieving robustness under severe appearance variations induced by diverse UAV orientations and fields of view, which hinders cross-domain generalization, and (2) establishing reliable correspondences that capture both global scene-level semantics and fine-grained local details. In this paper, we propose EGS, a novel CVGL framework designed to enhance cross-domain generalization. Specifically, we introduce an E(2)-Steerable CNN encoder to extract stable and reliable features under rotation and viewpoint shifts. Furthermore, we construct a graph with a virtual super-node that connects to all local nodes, enabling global semantics to be aggregated and redistributed to local regions, thereby enforcing global-local consistency. Extensive experiments on the University-1652 and SUES-200 benchmarks demonstrate that EGS consistently achieves substantial performance gains and establishes a new state of the art in cross-domain CVGL.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2509.11082.pdf' target='_blank'>https://arxiv.org/pdf/2509.11082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongwu Xie, Kaijie Yun, Yang Liu, Yiming Ji, Han Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11082">Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a robust multi-modal framework for predicting traversability costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce a bird's-eye-view (BEV) terrain costmap, trained self-supervised using IMU-derived labels. Key updates include a DINOv3-based image encoder, FiLM-based sensor fusion, and an optimization loss combining Huber and smoothness terms. Experimental ablations (removing image color, occluding inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry dominates the learned cost and the model is highly robust. We attribute the small performance differences to the IMU labeling primarily reflecting terrain geometry rather than semantics and to limited data diversity. Unlike prior work claiming large gains, we emphasize our contributions: (1) a high-fidelity, reproducible simulation environment; (2) a self-supervised IMU-based labeling pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss limitations and future work such as domain generalization and dataset expansion.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2508.21769.pdf' target='_blank'>https://arxiv.org/pdf/2508.21769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21769">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2508.21769.pdf' target='_blank'>https://arxiv.org/pdf/2508.21769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21769">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2508.21769.pdf' target='_blank'>https://arxiv.org/pdf/2508.21769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21769">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2508.16976.pdf' target='_blank'>https://arxiv.org/pdf/2508.16976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Pan, Shiyu Shen, Zongbin Wang, Zhenwei Shi, Xia Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16976">Preserving Domain Generalization in Fine-Tuning via Joint Parameter Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization seeks to develop models trained on a limited set of source domains that are capable of generalizing effectively to unseen target domains. While the predominant approach leverages large-scale pre-trained vision models as initialization, recent studies have highlighted that full fine-tuning can compromise the intrinsic generalization capabilities of these models. To address this limitation, parameter-efficient adaptation strategies have emerged, wherein only a subset of model parameters is selectively fine-tuned, thereby balancing task adaptation with the preservation of generalization. Motivated by this paradigm, we introduce Joint Parameter Selection (JPS), a novel method that restricts updates to a small, sparse subset of parameters, thereby retaining and harnessing the generalization strength of pre-trained models. Theoretically, we establish a generalization error bound that explicitly accounts for the sparsity of parameter updates, thereby providing a principled justification for selective fine-tuning. Practically, we design a selection mechanism employing dual operators to identify and update parameters exhibiting consistent and significant gradients across all source domains. Extensive benchmark experiments demonstrate that JPS achieves superior performance compared to state-of-the-art domain generalization methods, substantiating both the efficiency and efficacy of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2507.23326.pdf' target='_blank'>https://arxiv.org/pdf/2507.23326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingkai Wang, Yaoyao Zhu, Xiuding Cai, Yuhao Xiao, Haotian Wu, Yu Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23326">Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation plays a crucial role in clinical workflows, but domain shift often leads to performance degradation when models are applied to unseen clinical domains. This challenge arises due to variations in imaging conditions, scanner types, and acquisition protocols, limiting the practical deployment of segmentation models. Unlike natural images, medical images typically exhibit consistent anatomical structures across patients, with domain-specific variations mainly caused by imaging conditions. This unique characteristic makes medical image segmentation particularly challenging.
  To address this challenge, we propose a domain generalization framework tailored for medical image segmentation. Our approach improves robustness to domain-specific variations by introducing implicit feature perturbations guided by domain statistics. Specifically, we employ a learnable semantic direction selector and a covariance-based semantic intensity sampler to modulate domain-variant features while preserving task-relevant anatomical consistency. Furthermore, we design an adaptive consistency constraint that is selectively applied only when feature adjustment leads to degraded segmentation performance. This constraint encourages the adjusted features to align with the original predictions, thereby stabilizing feature selection and improving the reliability of the segmentation.
  Extensive experiments on two public multi-center benchmarks show that our framework consistently outperforms existing domain generalization approaches, achieving robust and generalizable segmentation performance across diverse clinical domains.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2507.12008.pdf' target='_blank'>https://arxiv.org/pdf/2507.12008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Wang, Yinda Chen, Xiaoyu Liu, Che Liu, Dong Liu, Jianqing Gao, Zhiwei Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12008">Dual form Complementary Masking for Domain-Adaptive Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have correlated Masked Image Modeling (MIM) with consistency regularization in Unsupervised Domain Adaptation (UDA). However, they merely treat masking as a special form of deformation on the input images and neglect the theoretical analysis, which leads to a superficial understanding of masked reconstruction and insufficient exploitation of its potential in enhancing feature extraction and representation learning. In this paper, we reframe masked reconstruction as a sparse signal reconstruction problem and theoretically prove that the dual form of complementary masks possesses superior capabilities in extracting domain-agnostic image features. Based on this compelling insight, we propose MaskTwins, a simple yet effective UDA framework that integrates masked reconstruction directly into the main training pipeline. MaskTwins uncovers intrinsic structural patterns that persist across disparate domains by enforcing consistency between predictions of images masked in complementary ways, enabling domain generalization in an end-to-end manner. Extensive experiments verify the superiority of MaskTwins over baseline methods in natural and biological image segmentation. These results demonstrate the significant advantages of MaskTwins in extracting domain-invariant features without the need for separate pre-training, offering a new paradigm for domain-adaptive segmentation.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2507.10961.pdf' target='_blank'>https://arxiv.org/pdf/2507.10961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joohwan Seo, Arvind Kruthiventy, Soomi Lee, Megan Teng, Xiang Zhang, Seoyeon Choi, Jongeun Choi, Roberto Horowitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10961">EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a framework for learning vision-based robotic policies for contact-rich manipulation tasks that generalize spatially across task configurations. We focus on achieving robust spatial generalization of the policy for the peg-in-hole (PiH) task trained from a small number of demonstrations. We propose EquiContact, a hierarchical policy composed of a high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF) and a novel low-level compliant visuomotor policy (Geometric Compliant ACT, G-CompACT). G-CompACT operates using only localized observations (geometrically consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB images) and produces actions defined in the end-effector frame. Through these design choices, we show that the entire EquiContact pipeline is SE(3)-equivariant, from perception to force control. We also outline three key components for spatially generalizable contact-rich policies: compliance, localized policies, and induced equivariance. Real-world experiments on PiH tasks demonstrate a near-perfect success rate and robust generalization to unseen spatial configurations, validating the proposed framework and principles. The experimental videos can be found on the project website: https://sites.google.com/berkeley.edu/equicontact
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2506.20841.pdf' target='_blank'>https://arxiv.org/pdf/2506.20841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Min Son, Shahbaz Rezaei, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20841">FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised domain generalization (SSDG) aims to solve the problem of generalizing to out-of-distribution data when only a few labels are available. Due to label scarcity, applying domain generalization methods often underperform. Consequently, existing SSDG methods combine semi-supervised learning methods with various regularization terms. However, these methods do not explicitly regularize to learn domains invariant representations across all domains, which is a key goal for domain generalization. To address this, we introduce FixCLR. Inspired by success in self-supervised learning, we change two crucial components to adapt contrastive learning for explicit domain invariance regularization: utilization of class information from pseudo-labels and using only a repelling term. FixCLR can also be added on top of most existing SSDG and semi-supervised methods for complementary performance improvements. Our research includes extensive experiments that have not been previously explored in SSDG studies. These experiments include benchmarking different improvements to semi-supervised methods, evaluating the performance of pretrained versus non-pretrained models, and testing on datasets with many domains. Overall, FixCLR proves to be an effective SSDG method, especially when combined with other semi-supervised methods.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2506.18304.pdf' target='_blank'>https://arxiv.org/pdf/2506.18304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchao Fan, Xuyang Lei, Xiaolin Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18304">Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) has emerged as a promising paradigm for autonomous driving. However, despite their advanced capabilities, DRL-based policies remain highly vulnerable to adversarial attacks, posing serious safety risks in real-world deployments. Investigating such attacks is crucial for revealing policy vulnerabilities and guiding the development of more robust autonomous systems. While prior attack methods have made notable progress, they still face several challenges: 1) they often rely on high-frequency attacks, yet critical attack opportunities are typically context-dependent and temporally sparse, resulting in inefficient attack patterns; 2) restricting attack frequency can improve efficiency but often results in unstable training due to the adversary's limited exploration. To address these challenges, we propose an adaptive expert-guided adversarial attack method that enhances both the stability and efficiency of attack policy training. Our method first derives an expert policy from successful attack demonstrations using imitation learning, strengthened by an ensemble Mixture-of-Experts architecture for robust generalization across scenarios. This expert policy then guides a DRL-based adversary through a KL-divergence regularization term. Due to the diversity of scenarios, expert policies may be imperfect. To address this, we further introduce a performance-aware annealing strategy that gradually reduces reliance on the expert as the adversary improves. Extensive experiments demonstrate that our method achieves outperforms existing approaches in terms of collision rate, attack efficiency, and training stability, especially in cases where the expert policy is sub-optimal.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2505.18770.pdf' target='_blank'>https://arxiv.org/pdf/2505.18770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuedi Zhang, Shuanghao Bai, Wanqi Zhou, Zhirong Luan, Badong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18770">Dual-Path Stable Soft Prompt Generation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn a model using data from one or multiple related but distinct source domains that can generalize well to unseen out-of-distribution target domains. Inspired by the success of large pre-trained vision-language models (VLMs), prompt tuning has emerged as an effective generalization strategy. However, it often struggles to capture domain-specific features due to its reliance on manually or fixed prompt inputs. Recently, some prompt generation methods have addressed this limitation by dynamically generating instance-specific and domain-specific prompts for each input, enriching domain information and demonstrating potential for enhanced generalization. Through further investigation, we identify a notable issue in existing prompt generation methods: the same input often yields significantly different and suboptimal prompts across different random seeds, a phenomenon we term Prompt Variability. To address this, we introduce negative learning into the prompt generation process and propose Dual-Path Stable Soft Prompt Generation (DPSPG), a transformer-based framework designed to improve both the stability and generalization of prompts. Specifically, DPSPG incorporates a complementary prompt generator to produce negative prompts, thereby reducing the risk of introducing misleading information. Both theoretical and empirical analyses demonstrate that negative learning leads to more robust and effective prompts by increasing the effective margin and reducing the upper bound of the gradient norm. Extensive experiments on five DG benchmark datasets show that DPSPG consistently outperforms state-of-the-art methods while maintaining prompt stability.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2505.14088.pdf' target='_blank'>https://arxiv.org/pdf/2505.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Chen, Shen Yan, Juelin Zhu, Chen Chen, Yu Liu, Maojun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14088">Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Land-MoE, a novel approach for multispectral land cover classification (MLCC). Spectral shift, which emerges from disparities in sensors and geospatial conditions, poses a significant challenge in this domain. Existing methods predominantly rely on domain adaptation and generalization strategies, often utilizing small-scale models that exhibit limited performance. In contrast, Land-MoE addresses these issues by hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts, to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner. Specifically, Land-MoE comprises two key modules: the mixture of low-rank token experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages rank-differentiated tokens to generate diverse feature adjustments for individual instances within multispectral images. By dynamically combining learnable low-rank token experts of varying ranks, it enhances the robustness against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on the refined features. This process enables the model to effectively capture frequency band information that is strongly correlated with semantic essence, while simultaneously suppressing frequency noise irrelevant to the task. Comprehensive experiments on MLCC tasks involving cross-sensor and cross-geospatial setups demonstrate that Land-MoE outperforms existing methods by a large margin. Additionally, the proposed approach has also achieved state-of-the-art performance in domain generalization semantic segmentation tasks of RGB remote sensing images.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2505.07209.pdf' target='_blank'>https://arxiv.org/pdf/2505.07209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Xie, Zequn Zeng, Hao Zhang, Yucheng Ding, Yi Wang, Zhengjue Wang, Bo Chen, Hongwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07209">Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Concept Bottleneck Models (CBMs) try to make the decision-making process transparent by exploring an intermediate concept space between the input image and the output prediction. Existing CBMs just learn coarse-grained relations between the whole image and the concepts, less considering local image information, leading to two main drawbacks: i) they often produce spurious visual-concept relations, hence decreasing model reliability; and ii) though CBMs could explain the importance of every concept to the final prediction, it is still challenging to tell which visual region produces the prediction. To solve these problems, this paper proposes a Disentangled Optimal Transport CBM (DOT-CBM) framework to explore fine-grained visual-concept relations between local image patches and concepts. Specifically, we model the concept prediction process as a transportation problem between the patches and concepts, thereby achieving explicit fine-grained feature alignment. We also incorporate orthogonal projection losses within the modality to enhance local feature disentanglement. To further address the shortcut issues caused by statistical biases in the data, we utilize the visual saliency map and concept label statistics as transportation priors. Thus, DOT-CBM can visualize inversion heatmaps, provide more reliable concept predictions, and produce more accurate class predictions. Comprehensive experiments demonstrate that our proposed DOT-CBM achieves SOTA performance on several tasks, including image classification, local part detection and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2505.01452.pdf' target='_blank'>https://arxiv.org/pdf/2505.01452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franco Maria Nardini, Thong Nguyen, Cosimo Rulli, Rossano Venturini, Andrew Yates
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01452">Effective Inference-Free Retrieval for Learned Sparse Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learned Sparse Retrieval (LSR) is an effective IR approach that exploits pre-trained language models for encoding text into a learned bag of words. Several efforts in the literature have shown that sparsity is key to enabling a good trade-off between the efficiency and effectiveness of the query processor. To induce the right degree of sparsity, researchers typically use regularization techniques when training LSR models. Recently, new efficient -- inverted index-based -- retrieval engines have been proposed, leading to a natural question: has the role of regularization changed in training LSR models? In this paper, we conduct an extended evaluation of regularization approaches for LSR where we discuss their effectiveness, efficiency, and out-of-domain generalization capabilities. We first show that regularization can be relaxed to produce more effective LSR encoders. We also show that query encoding is now the bottleneck limiting the overall query processor performance. To remove this bottleneck, we advance the state-of-the-art of inference-free LSR by proposing Learned Inference-free Retrieval (Li-LSR). At training time, Li-LSR learns a score for each token, casting the query encoding step into a seamless table lookup. Our approach yields state-of-the-art effectiveness for both in-domain and out-of-domain evaluation, surpassing Splade-v3-Doc by 1 point of mRR@10 on MS MARCO and 1.8 points of nDCG@10 on BEIR.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2504.06637.pdf' target='_blank'>https://arxiv.org/pdf/2504.06637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Ma, Haihong E., Junpeng Ding, Jun Zhang, Ziyan Ma, Huang Qing, Bofei Gao, Liang Chen, Yifan Zhu, Meina Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06637">SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex Multimodal Reasoning in Academic Areas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate impressive problem-solving skills in many tasks and domains. However, their ability to reason with complex images in academic domains has not been systematically investigated. To bridge this gap, we present SCI-Reason, a dataset for complex multimodel reasoning in academic areas. SCI-Reason aims to test and improve the reasoning ability of large multimodal models using real complex images in academic domains. The dataset contains 12,066 images and 12,626 question-answer pairs extracted from PubMed, divided into training, validation and test splits. Each question-answer pair also contains an accurate and efficient inference chain as a guide to improving the inference properties of the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8 well-known models. The best performing model, Claude-3.7-Sonnet, only achieved an accuracy of 55.19%. Error analysis shows that more than half of the model failures are due to breakdowns in multi-step inference chains rather than errors in primary visual feature extraction. This finding underscores the inherent limitations in reasoning capabilities exhibited by current multimodal models when processing complex image analysis tasks within authentic academic contexts. Experiments on open-source models show that SCI-Reason not only enhances reasoning ability but also demonstrates cross-domain generalization in VQA tasks. We also explore future applications of model inference capabilities in this domain, highlighting its potential for future research.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2503.13868.pdf' target='_blank'>https://arxiv.org/pdf/2503.13868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Wu, Fei Teng, Xingwang Li, Ji Zhang, Tianrui Li, Qiang Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13868">Out-of-Distribution Generalization in Time Series: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series frequently manifest distribution shifts, diverse latent features, and non-stationary learning dynamics, particularly in open and evolving environments. These characteristics pose significant challenges for out-of-distribution (OOD) generalization. While substantial progress has been made, a systematic synthesis of advancements remains lacking. To address this gap, we present the first comprehensive review of OOD generalization methodologies for time series, organized to delineate the field's evolutionary trajectory and contemporary research landscape. We organize our analysis across three foundational dimensions: data distribution, representation learning, and OOD evaluation. For each dimension, we present several popular algorithms in detail. Furthermore, we highlight key application scenarios, emphasizing their real-world impact. Finally, we identify persistent challenges and propose future research directions. A detailed summary of the methods reviewed for the generalization of OOD in time series can be accessed at https://tsood-generalization.com.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2503.08168.pdf' target='_blank'>https://arxiv.org/pdf/2503.08168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhang, Jun Yin, Pengyu Zeng, Yiqing Shen, Shuai Lu, Xueqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08168">TSCnet: A Text-driven Semantic-level Controllable Framework for Customized Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based image enhancement methods show significant advantages in reducing noise and improving visibility in low-light conditions. These methods are typically based on one-to-one mapping, where the model learns a direct transformation from low light to specific enhanced images. Therefore, these methods are inflexible as they do not allow highly personalized mapping, even though an individual's lighting preferences are inherently personalized. To overcome these limitations, we propose a new light enhancement task and a new framework that provides customized lighting control through prompt-driven, semantic-level, and quantitative brightness adjustments. The framework begins by leveraging a Large Language Model (LLM) to understand natural language prompts, enabling it to identify target objects for brightness adjustments. To localize these target objects, the Retinex-based Reasoning Segment (RRS) module generates precise target localization masks using reflection images. Subsequently, the Text-based Brightness Controllable (TBC) module adjusts brightness levels based on the generated illumination map. Finally, an Adaptive Contextual Compensation (ACC) module integrates multi-modal inputs and controls a conditional diffusion model to adjust the lighting, ensuring seamless and precise enhancements accurately. Experimental results on benchmark datasets demonstrate our framework's superior performance at increasing visibility, maintaining natural color balance, and amplifying fine details without creating artifacts. Furthermore, its robust generalization capabilities enable complex semantic-level lighting adjustments in diverse open-world environments through natural language interactions.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2503.02988.pdf' target='_blank'>https://arxiv.org/pdf/2503.02988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Xu, Bin Shi, Zhen Peng, Huixiang Liu, Bo Dong, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02988">Out-of-Distribution Generalization on Graphs via Progressive Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development and evaluation of graph neural networks (GNNs) generally follow the independent and identically distributed (i.i.d.) assumption. Yet this assumption is often untenable in practice due to the uncontrollable data generation mechanism. In particular, when the data distribution shows a significant shift, most GNNs would fail to produce reliable predictions and may even make decisions randomly. One of the most promising solutions to improve the model generalization is to pick out causal invariant parts in the input graph. Nonetheless, we observe a significant distribution gap between the causal parts learned by existing methods and the ground truth, leading to undesirable performance. In response to the above issues, this paper presents GPro, a model that learns graph causal invariance with progressive inference. Specifically, the complicated graph causal invariant learning is decomposed into multiple intermediate inference steps from easy to hard, and the perception of GPro is continuously strengthened through a progressive inference process to extract causal features that are stable to distribution shifts. We also enlarge the training distribution by creating counterfactual samples to enhance the capability of the GPro in capturing the causal invariant parts. Extensive experiments demonstrate that our proposed GPro outperforms the state-of-the-art methods by 4.91% on average. For datasets with more severe distribution shifts, the performance improvement can be up to 6.86%.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2502.05593.pdf' target='_blank'>https://arxiv.org/pdf/2502.05593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoyao Zhu, Xiuding Cai, Yingkai Wang, Yu Yao, Xu Luo, Zhongliang Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05593">Semantic Data Augmentation Enhanced Invariant Risk Minimization for Medical Image Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has achieved remarkable success in medical image classification. However, its clinical application is often hindered by data heterogeneity caused by variations in scanner vendors, imaging protocols, and operators. Approaches such as invariant risk minimization (IRM) aim to address this challenge of out-of-distribution generalization. For instance, VIRM improves upon IRM by tackling the issue of insufficient feature support overlap, demonstrating promising potential. Nonetheless, these methods face limitations in medical imaging due to the scarcity of annotated data and the inefficiency of augmentation strategies. To address these issues, we propose a novel domain-oriented direction selector to replace the random augmentation strategy used in VIRM. Our method leverages inter-domain covariance as a guider for augmentation direction, guiding data augmentation towards the target domain. This approach effectively reduces domain discrepancies and enhances generalization performance. Experiments on a multi-center diabetic retinopathy dataset demonstrate that our method outperforms state-of-the-art approaches, particularly under limited data conditions and significant domain heterogeneity.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2411.06842.pdf' target='_blank'>https://arxiv.org/pdf/2411.06842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vladyslav Zalevskyi, Thomas Sanchez, Margaux Roulet, HÃ©lÃ¨ne Lajous, Jordina Aviles Verdera, Roxane Licandro, Georg Langs, Gregor Kasprian, Jana Hutter, Hamza Kebiri, Meritxell Bach Cuadra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06842">DRIFTS: Optimizing Domain Randomization with Synthetic Data and Weight Interpolation for Fetal Brain Tissue Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fetal brain tissue segmentation in magnetic resonance imaging (MRI) is a crucial tool that supports understanding of neurodevelopment, yet it faces challenges due to the heterogeneity of data coming from different scanners and settings, as well as data scarcity. Recent approaches based on domain randomization, like SynthSeg, have shown great potential for single-source domain generalization by simulating images with randomized contrast and image resolution from the label maps. In this work, we investigate how to maximize the out-of-domain (OOD) generalization potential of SynthSegbased methods in fetal brain MRI. Specifically, we demonstrate that the simple Gaussian mixture models employed in FetalSynthSeg outperform physics-informed generation methods in terms of OOD generalization. We further show that incorporating intensity clustering significantly enhances generalization in settings with limited label classes by producing more realistic synthetic data. By combining synthetic pretraining with fine-tuning on real images and applying weight-space interpolation between the two models, we propose DRIFTS as an effective and practical solution for single-source domain generalization. DRIFTS consistently outperforms current state-of-the-art models across multiple benchmarks and is, to our knowledge, the first method to achieve accurate brain tissue segmentation on fetal T1-weighted images. We validate our approach on 308 subjects from four datasets acquired at three different sites, covering a range of scanner field strengths (0.55T to 3T) and both T1w and T2w modalities. We conclude with five practical recommendations to guide the development of SynthSeg-based methods for other organs and imaging modalities.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2411.01798.pdf' target='_blank'>https://arxiv.org/pdf/2411.01798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atoosa Chegini, Hamid Kazemi, Iman Mirzadeh, Dong Yin, Maxwell Horton, Moin Nabi, Mehrdad Farajtabar, Keivan Alizadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01798">SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2410.21698.pdf' target='_blank'>https://arxiv.org/pdf/2410.21698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, Sanjiv Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21698">On the Role of Depth and Looping for In-Context Learning with Task Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intriguing in-context learning (ICL) abilities of deep Transformer models have lately garnered significant attention. By studying in-context linear regression on unimodal Gaussian data, recent empirical and theoretical works have argued that ICL emerges from Transformers' abilities to simulate learning algorithms like gradient descent. However, these works fail to capture the remarkable ability of Transformers to learn multiple tasks in context. To this end, we study in-context learning for linear regression with diverse tasks, characterized by data covariance matrices with condition numbers ranging from $[1, Îº]$, and highlight the importance of depth in this setting. More specifically, (a) we show theoretical lower bounds of $\log(Îº)$ (or $\sqrtÎº$) linear attention layers in the unrestricted (or restricted) attention setting and, (b) we show that multilayer Transformers can indeed solve such tasks with a number of layers that matches the lower bounds. However, we show that this expressivity of multilayer Transformer comes at the price of robustness. In particular, multilayer Transformers are not robust to even distributional shifts as small as $O(e^{-L})$ in Wasserstein distance, where $L$ is the depth of the network. We then demonstrate that Looped Transformers -- a special class of multilayer Transformers with weight-sharing -- not only exhibit similar expressive power but are also provably robust under mild assumptions. Besides out-of-distribution generalization, we also show that Looped Transformers are the only models that exhibit a monotonic behavior of loss with respect to depth.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2410.13787.pdf' target='_blank'>https://arxiv.org/pdf/2410.13787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13787">Looking Inward: Language Models Can Learn About Themselves by Introspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data.
  We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).
  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2407.19795.pdf' target='_blank'>https://arxiv.org/pdf/2407.19795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juhwan Choi, Junehyoung Kwon, JungMin Yun, Seunguk Yu, YoungBin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19795">VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalizability is a crucial aspect of a deep learning model since it determines the capability of the model to perform well on data from unseen domains. However, research on the domain generalizability of deep learning models for vision-language tasks remains limited, primarily because of the lack of required datasets. To address these challenges, we propose VolDoGer: Vision-Language Dataset for Domain Generalization, a dedicated dataset designed for domain generalization that addresses three vision-language tasks: image captioning, visual question answering, and visual entailment. We constructed VolDoGer by extending LLM-based data annotation techniques to vision-language tasks, thereby alleviating the burden of recruiting human annotators. We evaluated the domain generalizability of various models, ranging from fine-tuned models to a recent multimodal large language model, through VolDoGer.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2407.08672.pdf' target='_blank'>https://arxiv.org/pdf/2407.08672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhang, Chun-Wun Cheng, Ke Yu, Zhihai He, Carola-Bibiane SchÃ¶nlieb, Angelica I. Aviles-Rivero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08672">NODE-Adapter: Neural Ordinary Differential Equations for Better Vision-Language Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we consider the problem of prototype-based vision-language reasoning problem. We observe that existing methods encounter three major challenges: 1) escalating resource demands and prolonging training times, 2) contending with excessive learnable parameters, and 3) fine-tuning based only on a single modality. These challenges will hinder their capability to adapt Vision-Language Models (VLMs) to downstream tasks. Motivated by this critical observation, we propose a novel method called NODE-Adapter, which utilizes Neural Ordinary Differential Equations for better vision-language reasoning. To fully leverage both visual and textual modalities and estimate class prototypes more effectively and accurately, we divide our method into two stages: cross-modal prototype construction and cross-modal prototype optimization using neural ordinary differential equations. Specifically, we exploit VLM to encode hand-crafted prompts into textual features and few-shot support images into visual features. Then, we estimate the textual prototype and visual prototype by averaging the textual features and visual features, respectively, and adaptively combine the textual prototype and visual prototype to construct the cross-modal prototype. To alleviate the prototype bias, we then model the prototype optimization process as an initial value problem with Neural ODEs to estimate the continuous gradient flow. Our extensive experimental results, which cover few-shot classification, domain generalization, and visual reasoning on human-object interaction, demonstrate that the proposed method significantly outperforms existing state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2407.04100.pdf' target='_blank'>https://arxiv.org/pdf/2407.04100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Gao, Bin Pan, Zhenwei Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04100">C$^3$DG: Conditional Domain Generalization for Hyperspectral Imagery Classification with Convergence and Constrained-risk Theories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral imagery (HSI) classification may suffer the challenge of hyperspectral-monospectra, where different classes present similar spectra. Joint spatial-spectral feature extraction is a popular solution for the problem, but this strategy tends to inflate accuracy since test pixels may exist in training patches. Domain generalization methods show promising potential, but they still fail to distinguish similar spectra across varying domains, in addition, the theoretical support is usually ignored. In this paper, we only rely on spectral information to solve the hyperspectral-monospectra problem, and propose a Convergence and Error-Constrained Conditional Domain Generalization method for Hyperspectral Imagery Classification (C$^3$DG). The major contributions of this paper include two aspects: the Conditional Revising Inference Block (CRIB), and the corresponding theories for model convergence and generalization errors. CRIB is the kernel structure of the proposed method, which employs a shared encoder and multi-branch decoders to fully leverage the conditional distribution during training, achieving a decoupling that aligns with the generation mechanisms of HSI. Moreover, to ensure model convergence and maintain controllable error, we propose the optimization convergence theorem and risk upper bound theorem. In the optimization convergence theorem, we ensure the model convergence by demonstrating that the gradients of the loss terms are not contradictory. In the risk upper bound theorem, our theoretical analysis explores the relationship between test-time training and recent related work to establish a concrete bound for error. Experimental results on three benchmark datasets indicate the superiority of C$^3$DG.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2407.03824.pdf' target='_blank'>https://arxiv.org/pdf/2407.03824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Wu, Ziyu Wang, Bhiksha Raj, Gus Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03824">Unsupervised Disentanglement of Content and Style via Variance-Invariance Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We contribute an unsupervised method that effectively learns disentangled content and style representations from sequences of observations. Unlike most disentanglement algorithms that rely on domain-specific labels or knowledge, our method is based on the insight of domain-general statistical differences between content and style -- content varies more among different fragments within a sample but maintains an invariant vocabulary across data samples, whereas style remains relatively invariant within a sample but exhibits more significant variation across different samples. We integrate such inductive bias into an encoder-decoder architecture and name our method after V3 (variance-versus-invariance). Experimental results show that V3 generalizes across multiple domains and modalities, successfully learning disentangled content and style representations, such as pitch and timbre from music audio, digit and color from images of hand-written digits, and action and character appearance from simple animations. V3 demonstrates strong disentanglement performance compared to existing unsupervised methods, along with superior out-of-distribution generalization under few-shot adaptation compared to supervised counterparts. Lastly, symbolic-level interpretability emerges in the learned content codebook, forging a near one-to-one alignment between machine representation and human knowledge.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2407.02350.pdf' target='_blank'>https://arxiv.org/pdf/2407.02350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhang, Ke Yu, Siqi Wu, Zhihai He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02350">Conceptual Codebook Learning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel fine-tuning method for vision-language models (VLMs) to address the challenge of improving the generalization capability of VLMs while fine-tuning them on downstream tasks in a few-shot setting. We recognize that visual concepts, such as textures, shapes, and colors are naturally transferable across domains and play a crucial role in generalization tasks. Motivated by this interesting finding, we learn a conceptual codebook consisting of visual concepts as keys and conceptual prompts as values, which serves as a link between the image encoder's outputs and the text encoder's inputs. Specifically, for a given image, we leverage the codebook to identify the most relevant conceptual prompts associated with the class embeddings to perform the classification. Additionally, we incorporate a handcrafted concept cache as a regularization to alleviate the overfitting issues in low-shot scenarios. We observe that this conceptual codebook learning method is able to achieve enhanced alignment between visual and linguistic modalities. Extensive experimental results demonstrate that our CoCoLe method remarkably outperforms the existing state-of-the-art methods across various evaluation settings, including base-to-new generalization, cross-dataset evaluation, and domain generalization tasks. Detailed ablation studies further confirm the efficacy of each component in CoCoLe.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2406.05628.pdf' target='_blank'>https://arxiv.org/pdf/2406.05628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongbin Wang, Bin Pan, Shiyu Shen, Tianyang Shi, Zhenwei Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05628">Domain Generalization Guided by Large-Scale Pre-Trained Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to train a model from limited source domains, allowing it to generalize to unknown target domains. Typically, DG models only employ large-scale pre-trained models during the initialization of fine-tuning. However, large-scale pre-trained models already possess the ability to resist domain shift. If we reference pre-trained models continuously during fine-tuning to maintain this ability, it could further enhance the generalization ability of the DG model. For this purpose, we introduce a new method called Fine-Tune with Large-scale pre-trained Priors (FT-LP), which incorporates the pre-trained model as a prior into the DG fine-tuning process, ensuring that the model refers to its pre-trained model at each optimization step. FT-LP comprises a theoretical framework and a simple implementation strategy. In theory, we verify the rationality of FT-LP by introducing a generalization error bound with the pre-trained priors for DG. In implementation, we utilize an encoder to simulate the model distribution, enabling the use of FT-LP when only pre-trained weights are available. In summary, we offer a new fine-tuning method for DG algorithms to utilize pre-trained models throughout the fine-tuning process. Through experiments on various datasets and DG models, our proposed method exhibits significant improvements, indicating its effectiveness.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2406.05616.pdf' target='_blank'>https://arxiv.org/pdf/2406.05616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongbin Wang, Bin Pan, Zhenwei Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05616">Domain Agnostic Conditional Invariant Predictions for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to develop a model that can perform well on unseen target domains by learning from multiple source domains. However, recent-proposed domain generalization models usually rely on domain labels, which may not be available in many real-world scenarios. To address this challenge, we propose a Discriminant Risk Minimization (DRM) theory and the corresponding algorithm to capture the invariant features without domain labels. In DRM theory, we prove that reducing the discrepancy of prediction distribution between overall source domain and any subset of it can contribute to obtaining invariant features. To apply the DRM theory, we develop an algorithm which is composed of Bayesian inference and a new penalty termed as Categorical Discriminant Risk (CDR). In Bayesian inference, we transform the output of the model into a probability distribution to align with our theoretical assumptions. We adopt sliding update approach to approximate the overall prediction distribution of the model, which enables us to obtain CDR penalty. We also indicate the effectiveness of these components in finding invariant features. We evaluate our algorithm against various domain generalization methods on multiple real-world datasets, providing empirical support for our theory.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2406.01435.pdf' target='_blank'>https://arxiv.org/pdf/2406.01435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan He, Mingzhen He, Lei Shi, Xiaolin Huang, Johan A. K. Suykens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01435">Learning Analysis of Kernel Ridgeless Regression with Asymmetric Kernel Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ridgeless regression has garnered attention among researchers, particularly in light of the ``Benign Overfitting'' phenomenon, where models interpolating noisy samples demonstrate robust generalization. However, kernel ridgeless regression does not always perform well due to the lack of flexibility. This paper enhances kernel ridgeless regression with Locally-Adaptive-Bandwidths (LAB) RBF kernels, incorporating kernel learning techniques to improve performance in both experiments and theory. For the first time, we demonstrate that functions learned from LAB RBF kernels belong to an integral space of Reproducible Kernel Hilbert Spaces (RKHSs). Despite the absence of explicit regularization in the proposed model, its optimization is equivalent to solving an $\ell_0$-regularized problem in the integral space of RKHSs, elucidating the origin of its generalization ability. Taking an approximation analysis viewpoint, we introduce an $l_q$-norm analysis technique (with $0<q<1$) to derive the learning rate for the proposed model under mild conditions. This result deepens our theoretical understanding, explaining that our algorithm's robust approximation ability arises from the large capacity of the integral space of RKHSs, while its generalization ability is ensured by sparsity, controlled by the number of support vectors. Experimental results on both synthetic and real datasets validate our theoretical conclusions.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2405.15203.pdf' target='_blank'>https://arxiv.org/pdf/2405.15203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyungtae Lee, Yan Zhang, Yi-Ting Shen, Heesung Kwon, Shuvra S. Bhattacharyya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15203">Exploring the Impact of Synthetic Data for Aerial-view Human Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial-view human detection has a large demand for large-scale data to capture more diverse human appearances compared to ground-view human detection. Therefore, synthetic data can be a good resource to expand data, but the domain gap with real-world data is the biggest obstacle to its use in training. As a common solution to deal with the domain gap, the sim2real transformation is used, and its quality is affected by three factors: i) the real data serving as a reference when calculating the domain gap, ii) the synthetic data chosen to avoid the transformation quality degradation, and iii) the synthetic data pool from which the synthetic data is selected. In this paper, we investigate the impact of these factors on maximizing the effectiveness of synthetic data in training in terms of improving learning performance and acquiring domain generalization ability--two main benefits expected of using synthetic data. As an evaluation metric for the second benefit, we introduce a method for measuring the distribution gap between two datasets, which is derived as the normalized sum of the Mahalanobis distances of all test data. As a result, we have discovered several important findings that have never been investigated or have been used previously without accurate understanding. We expect that these findings can break the current trend of either naively using or being hesitant to use synthetic data in machine learning due to the lack of understanding, leading to more appropriate use in future research.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2405.01022.pdf' target='_blank'>https://arxiv.org/pdf/2405.01022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juhwan Choi, Yeonghwa Kim, Seunguk Yu, JungMin Yun, YoungBin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01022">UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference. Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference. However, their applicability to various domains is limited because they tend to generate domain-specific datasets. In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain. This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm. Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2404.04538.pdf' target='_blank'>https://arxiv.org/pdf/2404.04538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Yang, Zuchao Li, Shuai Xie, Wei Yu, Shijun Li, Bo Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04538">Soft-Prompting with Graph-of-Thought for Multi-modal Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The chain-of-thought technique has been received well in multi-modal tasks. It is a step-by-step linear reasoning process that adjusts the length of the chain to improve the performance of generated prompts. However, human thought processes are predominantly non-linear, as they encompass multiple aspects simultaneously and employ dynamic adjustment and updating mechanisms. Therefore, we propose a novel Aggregation-Graph-of-Thought (AGoT) mechanism for soft-prompt tuning in multi-modal representation learning. The proposed AGoT models the human thought process not only as a chain but also models each step as a reasoning aggregation graph to cope with the overlooked multiple aspects of thinking in single-step reasoning. This turns the entire reasoning process into prompt aggregation and prompt flow operations. Experiments show that our multi-modal model enhanced with AGoT soft-prompting achieves good results in several tasks such as text-image retrieval, visual question answering, and image recognition. In addition, we demonstrate that it has good domain generalization performance due to better reasoning.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2403.09981.pdf' target='_blank'>https://arxiv.org/pdf/2403.09981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09981">Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While text-to-3D and image-to-3D generation tasks have received considerable attention, one important but under-explored field between them is controllable text-to-3D generation, which we mainly focus on in this work. To address this task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network architecture designed to enhance existing pre-trained multi-view diffusion models by integrating additional input conditions, such as edge, depth, normal, and scribble maps. Our innovation lies in the introduction of a conditioning module that controls the base diffusion model using both local and global embeddings, which are computed from the input condition images and camera poses. Once trained, MVControl is able to offer 3D diffusion guidance for optimization-based 3D generation. And, 2) we propose an efficient multi-stage 3D generation pipeline that leverages the benefits of recent large reconstruction models and score distillation algorithm. Building upon our MVControl architecture, we employ a unique hybrid diffusion guidance method to direct the optimization process. In pursuit of efficiency, we adopt 3D Gaussians as our representation instead of the commonly used implicit representations. We also pioneer the use of SuGaR, a hybrid representation that binds Gaussians to mesh triangle faces. This approach alleviates the issue of poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained geometry on the mesh. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content. Project page: https://lizhiqi49.github.io/MVControl/.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2312.16243.pdf' target='_blank'>https://arxiv.org/pdf/2312.16243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songming Zhang, Yuxiao Luo, Qizhou Wang, Haoang Chi, Xiaofeng Chen, Bo Han, Jinyan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16243">Mixture Data for Training Cannot Ensure Out-of-distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks often face generalization problems to handle out-of-distribution (OOD) data, and there remains a notable theoretical gap between the contributing factors and their respective impacts. Literature evidence from in-distribution data has suggested that generalization error can shrink if the size of mixture data for training increases. However, when it comes to OOD samples, this conventional understanding does not hold anymore -- Increasing the size of training data does not always lead to a reduction in the test generalization error. In fact, diverse trends of the errors have been found across various shifting scenarios including those decreasing trends under a power-law pattern, initial declines followed by increases, or continuous stable patterns. Previous work has approached OOD data qualitatively, treating them merely as samples unseen during training, which are hard to explain the complicated non-monotonic trends. In this work, we quantitatively redefine OOD data as those situated outside the convex hull of mixed training data and establish novel generalization error bounds to comprehend the counterintuitive observations better. Our proof of the new risk bound agrees that the efficacy of well-trained models can be guaranteed for unseen data within the convex hull; More interestingly, but for OOD data beyond this coverage, the generalization cannot be ensured, which aligns with our observations. Furthermore, we attempted various OOD techniques to underscore that our results not only explain insightful observations in recent OOD generalization work, such as the significance of diverse data and the sensitivity to unseen shifts of existing algorithms, but it also inspires a novel and effective data selection strategy.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2311.13816.pdf' target='_blank'>https://arxiv.org/pdf/2311.13816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Zhao, Kai Jiang, Xintao Wu, Haoliang Wang, Latifur Khan, Christan Grant, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13816">Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The endeavor to preserve the generalization of a fair and invariant classifier across domains, especially in the presence of distribution shifts, becomes a significant and intricate challenge in machine learning. In response to this challenge, numerous effective algorithms have been developed with a focus on addressing the problem of fairness-aware domain generalization. These algorithms are designed to navigate various types of distribution shifts, with a particular emphasis on covariate and dependence shifts. In this context, covariate shift pertains to changes in the marginal distribution of input features, while dependence shift involves alterations in the joint distribution of the label variable and sensitive attributes. In this paper, we introduce a simple but effective approach that aims to learn a fair and invariant classifier by simultaneously addressing both covariate and dependence shifts across domains. We assert the existence of an underlying transformation model can transform data from one domain to another, while preserving the semantics related to non-sensitive attributes and classes. By augmenting various synthetic data domains through the model, we learn a fair and invariant classifier in source domains. This classifier can then be generalized to unknown target domains, maintaining both model prediction and fairness concerns. Extensive empirical studies on four benchmark datasets demonstrate that our approach surpasses state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2310.16277.pdf' target='_blank'>https://arxiv.org/pdf/2310.16277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Shen, Bin Pan, Tianyang Shi, Tao Li, Zhenwei Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16277">Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain invariant learning aims to learn models that extract invariant features over various training domains, resulting in better generalization to unseen target domains. Recently, Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distributions, including the invariant posterior and the posteriors on training domains. Furthermore, we develop a lite version of PTG for widespread applications. PTG shows competitive performance on various domain generalization benchmarks on DomainBed. Additionally, PTG can use any existing domain generalization methods as its prior, and combined with previous state-of-the-art method the performance can be further improved. Code will be made public.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2310.11031.pdf' target='_blank'>https://arxiv.org/pdf/2310.11031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyuseong Lee, Wooseok Jang, Jinhyeon Kim, Jaewoo Jung, Seungryong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11031">Domain Generalization Using Large Pretrained Models with Mixture-of-Adapters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning robust vision models that perform well in out-of-distribution (OOD) situations is an important task for model deployment in real-world settings. Despite extensive research in this field, many proposed methods have only shown minor performance improvements compared to the simplest empirical risk minimization (ERM) approach, which was evaluated on a benchmark with a limited hyperparameter search space. Our focus in this study is on leveraging the knowledge of large pretrained models to improve handling of OOD scenarios and tackle domain generalization problems. However, prior research has revealed that naively fine-tuning a large pretrained model can impair OOD robustness. Thus, we employ parameter-efficient fine-tuning (PEFT) techniques to effectively preserve OOD robustness while working with large models. Our extensive experiments and analysis confirm that the most effective approaches involve ensembling diverse models and increasing the scale of pretraining. As a result, we achieve state-of-the-art performance in domain generalization tasks. Our code and project page are available at: https://cvlab-kaist.github.io/MoA
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2310.01508.pdf' target='_blank'>https://arxiv.org/pdf/2310.01508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chia-Yuan Chang, Yu-Neng Chuang, Zhimeng Jiang, Kwei-Herng Lai, Anxiao Jiang, Na Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01508">CODA: Temporal Domain Generalization via Concept Drift Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world applications, machine learning models often become obsolete due to shifts in the joint distribution arising from underlying temporal trends, a phenomenon known as the "concept drift". Existing works propose model-specific strategies to achieve temporal generalization in the near-future domain. However, the diverse characteristics of real-world datasets necessitate customized prediction model architectures. To this end, there is an urgent demand for a model-agnostic temporal domain generalization approach that maintains generality across diverse data modalities and architectures. In this work, we aim to address the concept drift problem from a data-centric perspective to bypass considering the interaction between data and model. Developing such a framework presents non-trivial challenges: (i) existing generative models struggle to generate out-of-distribution future data, and (ii) precisely capturing the temporal trends of joint distribution along chronological source domains is computationally infeasible. To tackle the challenges, we propose the COncept Drift simulAtor (CODA) framework incorporating a predicted feature correlation matrix to simulate future data for model training. Specifically, CODA leverages feature correlations to represent data characteristics at specific time points, thereby circumventing the daunting computational costs. Experimental results demonstrate that using CODA-generated data as training input effectively achieves temporal domain generalization across different model architectures.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2308.11507.pdf' target='_blank'>https://arxiv.org/pdf/2308.11507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhang, Ce Zhang, Xueting Hu, Zhihai He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11507">Unsupervised Prototype Adapter for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, large-scale pre-trained vision-language models (e.g. CLIP and ALIGN) have demonstrated remarkable effectiveness in acquiring transferable visual representations. To leverage the valuable knowledge encoded within these models for downstream tasks, several fine-tuning approaches, including prompt tuning methods and adapter-based methods, have been developed to adapt vision-language models effectively with supervision. However, these methods rely on the availability of annotated samples, which can be labor-intensive and time-consuming to acquire, thus limiting scalability. To address this issue, in this work, we design an unsupervised fine-tuning approach for vision-language models called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for the unannotated target datasets, we leverage the text-image aligning capability of CLIP to automatically select the most confident samples for each class. Utilizing these selected samples, we generate class prototypes, which serve as the initialization for the learnable prototype model. After fine-tuning, the prototype model prediction is combined with the original CLIP's prediction by a residual connection to perform downstream recognition tasks. Our extensive experimental results on image recognition and domain generalization show that the proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter, and also the state-of-the-art UPL method by large margins.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2306.09890.pdf' target='_blank'>https://arxiv.org/pdf/2306.09890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felipe del Rio, Julio Hurtado, Cristian Buc, Alvaro Soto, Vincenzo Lomonaco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09890">Studying Generalization on Memory-Based Methods in Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the objectives of Continual Learning is to learn new concepts continually over a stream of experiences and at the same time avoid catastrophic forgetting. To mitigate complete knowledge overwriting, memory-based methods store a percentage of previous data distributions to be used during training. Although these methods produce good results, few studies have tested their out-of-distribution generalization properties, as well as whether these methods overfit the replay memory. In this work, we show that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations. Using a controlled environment, the Synbol benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of out-of-distribution generalization mainly occurs in the linear classifier.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2306.08010.pdf' target='_blank'>https://arxiv.org/pdf/2306.08010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahed Masoudian, Khaled Koutini, Markus Schedl, Gerhard Widmer, Navid Rekabsaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08010">Domain Information Control at Inference Time for Acoustic Scene Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift is considered a challenge in machine learning as it causes significant degradation of model performance. In the Acoustic Scene Classification task (ASC), domain shift is mainly caused by different recording devices. Several studies have already targeted domain generalization to improve the performance of ASC models on unseen domains, such as new devices. Recently, the Controllable Gate Adapter ConGater has been proposed in Natural Language Processing to address the biased training data problem. ConGater allows controlling the debiasing process at inference time. ConGater's main advantage is the continuous and selective debiasing of a trained model, during inference. In this work, we adapt ConGater to the audio spectrogram transformer for an acoustic scene classification task. We show that ConGater can be used to selectively adapt the learned representations to be invariant to device domain shifts such as recording devices. Our analysis shows that ConGater can progressively remove device information from the learned representations and improve the model generalization, especially under domain shift conditions (e.g. unseen devices). We show that information removal can be extended to both device and location domain. Finally, we demonstrate ConGater's ability to enhance specific device performance without further training.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2306.01433.pdf' target='_blank'>https://arxiv.org/pdf/2306.01433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eloi Moliner, Filip Elvander, Vesa VÃ¤limÃ¤ki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01433">Blind Audio Bandwidth Extension: A Diffusion-Based Zero-Shot Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio bandwidth extension involves the realistic reconstruction of high-frequency spectra from bandlimited observations. In cases where the lowpass degradation is unknown, such as in restoring historical audio recordings, this becomes a blind problem. This paper introduces a novel method called BABE (Blind Audio Bandwidth Extension) that addresses the blind problem in a zero-shot setting, leveraging the generative priors of a pre-trained unconditional diffusion model. During the inference process, BABE utilizes a generalized version of diffusion posterior sampling, where the degradation operator is unknown but parametrized and inferred iteratively. The performance of the proposed method is evaluated using objective and subjective metrics, and the results show that BABE surpasses state-of-the-art blind bandwidth extension baselines and achieves competitive performance compared to informed methods when tested with synthetic data. Moreover, BABE exhibits robust generalization capabilities when enhancing real historical recordings, effectively reconstructing the missing high-frequency content while maintaining coherence with the original recording. Subjective preference tests confirm that BABE significantly improves the audio quality of historical music recordings. Examples of historical recordings restored with the proposed method are available on the companion webpage: (http://research.spa.aalto.fi/publications/papers/ieee-taslp-babe/)
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2303.14828.pdf' target='_blank'>https://arxiv.org/pdf/2303.14828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dina Bashkirova, Samarth Mishra, Diala Lteif, Piotr Teterwak, Donghyun Kim, Fadi Alladkani, James Akl, Berk Calli, Sarah Adel Bargal, Kate Saenko, Daehan Kim, Minseok Seo, YoungJin Jeon, Dong-Geol Choi, Shahaf Ettedgui, Raja Giryes, Shady Abu-Hussein, Binhui Xie, Shuang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14828">VisDA 2022 Challenge: Domain Adaptation for Industrial Waste Sorting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Label-efficient and reliable semantic segmentation is essential for many real-life applications, especially for industrial settings with high visual diversity, such as waste sorting. In industrial waste sorting, one of the biggest challenges is the extreme diversity of the input stream depending on factors like the location of the sorting facility, the equipment available in the facility, and the time of year, all of which significantly impact the composition and visual appearance of the waste stream. These changes in the data are called ``visual domains'', and label-efficient adaptation of models to such domains is needed for successful semantic segmentation of industrial waste. To test the abilities of computer vision models on this task, we present the VisDA 2022 Challenge on Domain Adaptation for Industrial Waste Sorting. Our challenge incorporates a fully-annotated waste sorting dataset, ZeroWaste, collected from two real material recovery facilities in different locations and seasons, as well as a novel procedurally generated synthetic waste sorting dataset, SynthWaste. In this competition, we aim to answer two questions: 1) can we leverage domain adaptation techniques to minimize the domain gap? and 2) can synthetic data augmentation improve performance on this task and help adapt to changing data distributions? The results of the competition show that industrial waste detection poses a real domain adaptation problem, that domain generalization techniques such as augmentations, ensembling, etc., improve the overall performance on the unlabeled target domain examples, and that leveraging synthetic data effectively remains an open problem. See https://ai.bu.edu/visda-2022/
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2303.09914.pdf' target='_blank'>https://arxiv.org/pdf/2303.09914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rizhao Cai, Yawen Cui, Zhi Li, Zitong Yu, Haoliang Li, Yongjian Hu, Alex Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09914">Rehearsal-Free Domain Continual Face Anti-Spoofing: Generalize More and Forget Less</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Anti-Spoofing (FAS) is recently studied under the continual learning setting, where the FAS models are expected to evolve after encountering the data from new domains. However, existing methods need extra replay buffers to store previous data for rehearsal, which becomes infeasible when previous data is unavailable because of privacy issues. In this paper, we propose the first rehearsal-free method for Domain Continual Learning (DCL) of FAS, which deals with catastrophic forgetting and unseen domain generalization problems simultaneously. For better generalization to unseen domains, we design the Dynamic Central Difference Convolutional Adapter (DCDCA) to adapt Vision Transformer (ViT) models during the continual learning sessions. To alleviate the forgetting of previous domains without using previous data, we propose the Proxy Prototype Contrastive Regularization (PPCR) to constrain the continual learning with previous domain knowledge from the proxy prototypes. Simulate practical DCL scenarios, we devise two new protocols which evaluate both generalization and anti-forgetting performance. Extensive experimental results show that our proposed method can improve the generalization performance in unseen domains and alleviate the catastrophic forgetting of the previous knowledge. The codes and protocols will be released soon.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2209.09485.pdf' target='_blank'>https://arxiv.org/pdf/2209.09485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sitong Zhou, Kevin Lybarger, Meliha Yetisgen, Mari Ostendorf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.09485">Generalizing through Forgetting -- Domain Generalization for Symptom Event Extraction in Clinical Notes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Symptom information is primarily documented in free-text clinical notes and is not directly accessible for downstream applications. To address this challenge, information extraction approaches that can handle clinical language variation across different institutions and specialties are needed. In this paper, we present domain generalization for symptom extraction using pretraining and fine-tuning data that differs from the target domain in terms of institution and/or specialty and patient population. We extract symptom events using a transformer-based joint entity and relation extraction method. To reduce reliance on domain-specific features, we propose a domain generalization method that dynamically masks frequent symptoms words in the source domain. Additionally, we pretrain the transformer language model (LM) on task-related unlabeled texts for better representation. Our experiments indicate that masking and adaptive pretraining methods can significantly improve performance when the source domain is more distant from the target domain.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2201.11986.pdf' target='_blank'>https://arxiv.org/pdf/2201.11986.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irene Tenison, Sai Aravind Sreeramadas, Vaikkunth Mugunthan, Edouard Oyallon, Irina Rish, Eugene Belilovsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.11986">Gradient Masked Averaging for Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) is an emerging paradigm that permits a large number of clients with heterogeneous data to coordinate learning of a unified global model without the need to share data amongst each other. A major challenge in federated learning is the heterogeneity of data across client, which can degrade the performance of standard FL algorithms. Standard FL algorithms involve averaging of model parameters or gradient updates to approximate the global model at the server. However, we argue that in heterogeneous settings, averaging can result in information loss and lead to poor generalization due to the bias induced by dominant client gradients. We hypothesize that to generalize better across non-i.i.d datasets, the algorithms should focus on learning the invariant mechanism that is constant while ignoring spurious mechanisms that differ across clients. Inspired from recent works in Out-of-Distribution generalization, we propose a gradient masked averaging approach for FL as an alternative to the standard averaging of client updates. This aggregation technique for client updates can be adapted as a drop-in replacement in most existing federated algorithms. We perform extensive experiments on multiple FL algorithms with in-distribution, real-world, feature-skewed out-of-distribution, and quantity imbalanced datasets and show that it provides consistent improvements, particularly in the case of heterogeneous clients.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2107.00520.pdf' target='_blank'>https://arxiv.org/pdf/2107.00520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aahlad Puli, Lily H. Zhang, Eric K. Oermann, Rajesh Ranganath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.00520">Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2509.25856.pdf' target='_blank'>https://arxiv.org/pdf/2509.25856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Han Huang, Jeng-Lin Li, Po-Hsuan Huang, Ming-Ching Chang, Wei-Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25856">PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection is increasingly relying on foundation models, aiming for strong out-of-distribution generalization and rapid adaptation in real-world deployments. Notably, past studies have primarily focused on textual prompt tuning, leaving the intrinsic visual counterpart fragmented into processing steps specific to each foundation model. We aim to address this limitation by proposing a unified patch-focused framework, Patch-Exclusive Anomaly Detection (PatchEAD), enabling training-free anomaly detection that is compatible with diverse foundation models. The framework constructs visual prompting techniques, including an alignment module and foreground masking. Our experiments show superior few-shot and batch zero-shot performance compared to prior work, despite the absence of textual features. Our study further examines how backbone structure and pretrained characteristics affect patch-similarity robustness, providing actionable guidance for selecting and configuring foundation models for real-world visual inspection. These results confirm that a well-unified patch-only framework can enable quick, calibration-light deployment without the need for carefully engineered textual prompts.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2509.24661.pdf' target='_blank'>https://arxiv.org/pdf/2509.24661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Wu, Rolandos Alexandros Potamias, Xuyang Zhang, Zhongqun Zhang, Jiankang Deng, Shan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24661">CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-embodiment dexterous grasp synthesis refers to adaptively generating and optimizing grasps for various robotic hands with different morphologies. This capability is crucial for achieving versatile robotic manipulation in diverse environments and requires substantial amounts of reliable and diverse grasp data for effective model training and robust generalization. However, existing approaches either rely on physics-based optimization that lacks human-like kinematic understanding or require extensive manual data collection processes that are limited to anthropomorphic structures. In this paper, we propose CEDex, a novel cross-embodiment dexterous grasp synthesis method at scale that bridges human grasping kinematics and robot kinematics by aligning robot kinematic models with generated human-like contact representations. Given an object's point cloud and an arbitrary robotic hand model, CEDex first generates human-like contact representations using a Conditional Variational Auto-encoder pretrained on human contact data. It then performs kinematic human contact alignment through topological merging to consolidate multiple human hand parts into unified robot components, followed by a signed distance field-based grasp optimization with physics-aware constraints. Using CEDex, we construct the largest cross-embodiment grasp dataset to date, comprising 500K objects across four gripper types with 20M total grasps. Extensive experiments show that CEDex outperforms state-of-the-art approaches and our dataset benefits cross-embodiment grasp learning with high-quality diverse grasps.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2509.18542.pdf' target='_blank'>https://arxiv.org/pdf/2509.18542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Wang, Hanyang Peng, Yue Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18542">Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2509.09595.pdf' target='_blank'>https://arxiv.org/pdf/2509.09595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikang Ding, Jiwen Liu, Wenyuan Zhang, Zekun Wang, Wentao Hu, Liyuan Cui, Mingming Lao, Yingchao Shao, Hui Liu, Xiaohan Li, Ming Chen, Xiaoqiang Liu, Yu-Shen Liu, Pengfei Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09595">Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2509.01944.pdf' target='_blank'>https://arxiv.org/pdf/2509.01944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenlong Yuan, Jing Tang, Jinguo Luo, Rui Chen, Chengxuan Qian, Lei Sun, Xiangxiang Chu, Yujun Cai, Dapeng Zhang, Shuo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01944">AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models in autonomous driving systems have recently demonstrated transformative potential by integrating multimodal perception with decision-making capabilities. However, the interpretability and coherence of the decision process and the plausibility of action sequences remain largely underexplored. To address these issues, we propose AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and self-reflection capabilities of autonomous driving systems through chain-of-thought (CoT) processing and reinforcement learning (RL). Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation. Moreover, to maximize both reasoning and self-reflection during the RL stage, we further employ the Group Relative Policy Optimization (GRPO) algorithm within a physics-grounded reward framework that incorporates spatial alignment, vehicle dynamic, and temporal smoothness criteria to ensure reliable and realistic trajectory planning. Extensive evaluation results across both nuScenes and Waymo datasets demonstrates the state-of-the-art performance and robust generalization capacity of our proposed method.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2509.01031.pdf' target='_blank'>https://arxiv.org/pdf/2509.01031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhou Ye, Kevin I-Kai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01031">Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) using wearable sensors is crucial for healthcare, fitness tracking, and smart environments, yet cross-user variability -- stemming from diverse motion patterns, sensor placements, and physiological traits -- hampers generalization in real-world settings. Conventional supervised learning methods often overfit to user-specific patterns, leading to poor performance on unseen users. Existing domain generalization approaches, while promising, frequently overlook temporal dependencies or depend on impractical domain-specific labels. We propose Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a novel framework that redefines feature extraction as a sequential decision-making process driven by reinforcement learning. TPRL-DG leverages a Transformer-based autoregressive generator to produce temporal tokens that capture user-invariant activity dynamics, optimized via a multi-objective reward function balancing class discrimination and cross-user invariance. Key innovations include: (1) an RL-driven approach for domain generalization, (2) autoregressive tokenization to preserve temporal coherence, and (3) a label-free reward design eliminating the need for target user annotations. Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses state-of-the-art methods in cross-user generalization, achieving superior accuracy without per-user calibration. By learning robust, user-invariant temporal patterns, TPRL-DG enables scalable HAR systems, facilitating advancements in personalized healthcare, adaptive fitness tracking, and context-aware environments.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2508.16514.pdf' target='_blank'>https://arxiv.org/pdf/2508.16514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parker Seegmiller, Kartik Mehta, Soumya Saha, Chenyang Tao, Shereen Oraby, Arpit Gupta, Tagyoung Chung, Mohit Bansal, Nanyun Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16514">FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2507.21783.pdf' target='_blank'>https://arxiv.org/pdf/2507.21783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malte Londschien, Manuel Burger, Gunnar RÃ¤tsch, Peter BÃ¼hlmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21783">Domain Generalization and Adaptation in Intensive Care with Anchor Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of predictive models in clinical settings often degrades when deployed in new hospitals due to distribution shifts. This paper presents a large-scale study of causality-inspired domain generalization on heterogeneous multi-center intensive care unit (ICU) data. We apply anchor regression and introduce anchor boosting, a novel, tree-based nonlinear extension, to a large dataset comprising 400,000 patients from nine distinct ICU databases. The anchor regularization consistently improves out-of-distribution performance, particularly for the most dissimilar target domains. The methods appear robust to violations of theoretical assumptions, such as anchor exogeneity. Furthermore, we propose a novel conceptual framework to quantify the utility of large external data datasets. By evaluating performance as a function of available target-domain data, we identify three regimes: (i) a domain generalization regime, where only the external model should be used, (ii) a domain adaptation regime, where refitting the external model is optimal, and (iii) a data-rich regime, where external data provides no additional value.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2507.21530.pdf' target='_blank'>https://arxiv.org/pdf/2507.21530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming-Hui Liu, Harry Cheng, Xin Luo, Xin-Shun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21530">Suppressing Gradient Conflict for Generalizable Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust deepfake detection models must be capable of generalizing to ever-evolving manipulation techniques beyond training data. A promising strategy is to augment the training data with online synthesized fake images containing broadly generalizable artifacts. However, in the context of deepfake detection, it is surprising that jointly training on both original and online synthesized forgeries may result in degraded performance. This contradicts the common belief that incorporating more source-domain data should enhance detection accuracy. Through empirical analysis, we trace this degradation to gradient conflicts during backpropagation which force a trade-off between source domain accuracy and target domain generalization. To overcome this issue, we propose a Conflict-Suppressed Deepfake Detection (CS-DFD) framework that explicitly mitigates the gradient conflict via two synergistic modules. First, an Update Vector Search (UVS) module searches for an alternative update vector near the initial gradient vector to reconcile the disparities of the original and online synthesized forgeries. By further transforming the search process into an extremum optimization problem, UVS yields the uniquely update vector, which maximizes the simultaneous loss reductions for each data type. Second, a Conflict Gradient Reduction (CGR) module enforces a low-conflict feature embedding space through a novel Conflict Descent Loss. This loss penalizes misaligned gradient directions and guides the learning of representations with aligned, non-conflicting gradients. The synergy of UVS and CGR alleviates gradient interference in both parameter optimization and representation learning. Experiments on multiple deepfake benchmarks demonstrate that CS-DFD achieves state-of-the-art performance in both in-domain detection accuracy and cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2507.09081.pdf' target='_blank'>https://arxiv.org/pdf/2507.09081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Yu, Mohd Yamani Idna Idris, Hua Wang, Pei Wang, Junyi Chen, Kun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09081">From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative remote sensing inversion aims to estimate continuous surface variables-such as biomass, vegetation indices, and evapotranspiration-from satellite observations, supporting applications in ecosystem monitoring, carbon accounting, and land management. With the evolution of remote sensing systems and artificial intelligence, traditional physics-based paradigms are giving way to data-driven and foundation model (FM)-based approaches. This paper systematically reviews the methodological evolution of inversion techniques, from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods (e.g., deep learning, multimodal fusion), and further to foundation models (e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application scenarios, and limitations of each paradigm, with emphasis on recent FM advances in self-supervised pretraining, multi-modal integration, and cross-task adaptation. We also highlight persistent challenges in physical interpretability, domain generalization, limited supervision, and uncertainty quantification. Finally, we envision the development of next-generation foundation models for remote sensing inversion, emphasizing unified modeling capacity, cross-domain generalization, and physical interpretability.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2506.18880.pdf' target='_blank'>https://arxiv.org/pdf/2506.18880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18880">OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2506.16029.pdf' target='_blank'>https://arxiv.org/pdf/2506.16029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenting Qi, Fan Nie, Alexandre Alahi, James Zou, Himabindu Lakkaraju, Yilun Du, Eric Xing, Sham Kakade, Hanlin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16029">EvoLM: In Search of Lost Language Model Training Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2506.15498.pdf' target='_blank'>https://arxiv.org/pdf/2506.15498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15498">SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Process or step-wise supervision has played a crucial role in advancing complex multi-step reasoning capabilities of Large Language Models (LLMs). However, efficient, high-quality automated process annotation remains a significant challenge. To address this, we introduce Single-Pass Annotation with Reference-Guided Evaluation (SPARE), a novel structured framework that enables efficient per-step annotation by jointly aligning solution steps to reference solutions and determine its accuracy with explicit reasoning in single generation. We demonstrate SPARE's effectiveness across four diverse datasets spanning mathematical reasoning (GSM8K, MATH), multi-hop question answering (MuSiQue-Ans), and spatial reasoning (SpaRP), showing consistent improvements in two applications: (1) training Process Reward Models (PRMs) for ranking and aggregating multiple generations, and (2) fine-tuning models via offline reinforcement learning for greedy decoding. On ProcessBench, SPARE demonstrates data-efficient out-of-distribution generalization, using only $\sim$16% of training samples compared to human-labeled and other synthetically trained baselines. Additionally, it achieves competitive performance with MCTS-based methods while offering 2.3$\times$ speedup in terms of total token count. Manual analysis reveals complementary precision-recall characteristics with MCTS approaches, suggesting potential for ensemble methods. These results establish SPARE as a practical and scalable solution for automatic process supervision in LLM reasoning.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2506.10805.pdf' target='_blank'>https://arxiv.org/pdf/2506.10805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex McKenzie, Urja Pawar, Phil Blandfort, William Bankes, David Krueger, Ekdeep Singh Lubana, Dmitrii Krasheninnikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10805">Detecting High-Stakes Interactions with Activation Probes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring is an important aspect of safely deploying Large Language Models (LLMs). This paper examines activation probes for detecting "high-stakes" interactions -- where the text indicates that the interaction might lead to significant harm -- as a critical, yet underexplored, target for such monitoring. We evaluate several probe architectures trained on synthetic data, and find them to exhibit robust generalization to diverse, out-of-distribution, real-world data. Probes' performance is comparable to that of prompted or finetuned medium-sized LLM monitors, while offering computational savings of six orders-of-magnitude. Our experiments also highlight the potential of building resource-aware hierarchical monitoring systems, where probes serve as an efficient initial filter and flag cases for more expensive downstream analysis. We release our novel synthetic dataset and codebase to encourage further study.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2506.01962.pdf' target='_blank'>https://arxiv.org/pdf/2506.01962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhou Ye, Kevin I-Kai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01962">Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-user variability poses a significant challenge in sensor-based Human Activity Recognition (HAR) systems, as traditional models struggle to generalize across users due to differences in behavior, sensor placement, and data distribution. To address this, we propose GNN-ADG (Graph Neural Network with Adversarial Domain Generalization), a novel method that leverages both the strength from both the Graph Neural Networks (GNNs) and adversarial learning to achieve robust cross-user generalization. GNN-ADG models spatial relationships between sensors on different anatomical body parts, extracting three types of Anatomical Units: (1) Interconnected Units, capturing inter-relations between neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or functionally similar body parts; and (3) Lateral Units, connecting sensors based on their position to capture region-specific coordination. These units information are fused into an unified graph structure with a cyclic training strategy, dynamically integrating spatial, functional, and lateral correlations to facilitate a holistic, user-invariant representation. Information fusion mechanism of GNN-ADG occurs by iteratively cycling through edge topologies during training, allowing the model to refine its understanding of inter-sensor relationships across diverse perspectives. By representing the spatial configuration of sensors as an unified graph and incorporating adversarial learning, Information Fusion GNN-ADG effectively learns features that generalize well to unseen users without requiring target user data during training, making it practical for real-world applications.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2505.22622.pdf' target='_blank'>https://arxiv.org/pdf/2505.22622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Ge, Amanda Wang, Shange Tang, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22622">Principled Out-of-Distribution Generalization via Simplicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern foundation models exhibit remarkable out-of-distribution (OOD) generalization, solving tasks far beyond the support of their training data. However, the theoretical principles underpinning this phenomenon remain elusive. This paper investigates this problem by examining the compositional generalization abilities of diffusion models in image generation. Our analysis reveals that while neural network architectures are expressive enough to represent a wide range of models -- including many with undesirable behavior on OOD inputs -- the true, generalizable model that aligns with human expectations typically corresponds to the simplest among those consistent with the training data.
  Motivated by this observation, we develop a theoretical framework for OOD generalization via simplicity, quantified using a predefined simplicity metric. We analyze two key regimes: (1) the constant-gap setting, where the true model is strictly simpler than all spurious alternatives by a fixed gap, and (2) the vanishing-gap setting, where the fixed gap is replaced by a smoothness condition ensuring that models close in simplicity to the true model yield similar predictions. For both regimes, we study the regularized maximum likelihood estimator and establish the first sharp sample complexity guarantees for learning the true, generalizable, simple model.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2505.06301.pdf' target='_blank'>https://arxiv.org/pdf/2505.06301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhou Ye, Kevin I-Kai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06301">Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-user variability in Human Activity Recognition (HAR) remains a critical challenge due to differences in sensor placement, body dynamics, and behavioral patterns. Traditional methods often fail to capture biomechanical invariants that persist across users, limiting their generalization capability. We propose an Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG) framework that integrates anatomical correlation knowledge into a unified graph neural network (GNN) architecture. By modeling three biomechanically motivated relationships together-Interconnected Units, Analogous Units, and Lateral Units-our method encodes domain-invariant features while addressing user-specific variability through Variational Edge Feature Extractor. A Gradient Reversal Layer (GRL) enforces adversarial domain generalization, ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and DSADS datasets demonstrate state-of-the-art performance. Our work bridges biomechanical principles with graph-based adversarial learning by integrating information fusion techniques. This fusion of information underpins our unified and generalized model for cross-user HAR.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2505.03414.pdf' target='_blank'>https://arxiv.org/pdf/2505.03414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03414">Enhancing Target-unspecific Tasks through a Features Matrix</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent developments in prompt learning of large Vision-Language Models (VLMs) have significantly improved performance in target-specific tasks. However, these prompting methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge. The general knowledge has a strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2) the FM significantly showcases its effectiveness in enhancing target-unspecific tasks (base-to-novel generalization, domain generalization, and cross-dataset generalization), achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2504.02411.pdf' target='_blank'>https://arxiv.org/pdf/2504.02411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Misrahi, Nadezhda Chirkova, Maxime Louis, Vassilina Nikoulina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02411">Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-Augmented Generation (RAG) enhances LLM factuality, but multi-domain applications face challenges like lack of diverse benchmarks and poor out-of-domain generalization. The first contribution of this work is to introduce a diverse benchmark comprising a variety of question-answering tasks from 8 sources and covering 13 domains. Our second contribution consists in systematically testing out-of-domain generalization for typical RAG tuning strategies. While our findings reveal that standard fine-tuning fails to generalize effectively, we show that sequence-level distillation with teacher-generated labels improves out-of-domain performance by providing more coherent supervision. Our findings highlight key strategies for improving multi-domain RAG robustness.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2503.17211.pdf' target='_blank'>https://arxiv.org/pdf/2503.17211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Dai, Lehong Wang, Fangzhou Lin, Yidong Wang, Zhigang Li, Kazunori D Yamada, Ziming Zhang, Wang Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17211">A Language Anchor-Guided Method for Robust Noisy Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world machine learning applications often struggle with two major challenges: distribution shift and label noise. Models tend to overfit by focusing on redundant and uninformative features in the training data, which makes it hard for them to generalize to the target domain. Noisy data worsens this problem by causing further overfitting to the noise, meaning that existing methods often fail to tell the difference between true, invariant features and misleading, spurious ones. To tackle these issues, we introduce Anchor Alignment and Adaptive Weighting (A3W). This new algorithm uses sample reweighting guided by natural language processing (NLP) anchors to extract more representative features. In simple terms, A3W leverages semantic representations from natural language models as a source of domain-invariant prior knowledge. Additionally, it employs a weighted loss function that adjusts each sample's contribution based on its similarity to the corresponding NLP anchor. This adjustment makes the model more robust to noisy labels. Extensive experiments on standard benchmark datasets show that A3W consistently outperforms state-of-the-art domain generalization methods, offering significant improvements in both accuracy and robustness across different datasets and noise levels.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2503.04315.pdf' target='_blank'>https://arxiv.org/pdf/2503.04315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuang Liu, Yihan Wang, Yifan Zhu, Yibo Miao, Xiao-Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04315">Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for adversarial noise via Wasserstein distance and statistical error via Kullback-Leibler divergence, called the Statistically Robust WDRO. We establish a robust generalization bound for the new optimization framework, implying that out-of-distribution adversarial performance is at least as good as the statistically robust training loss with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary, giving an optimal robust model in certain sense. Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2503.04111.pdf' target='_blank'>https://arxiv.org/pdf/2503.04111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijia Yu, Yibo Miao, Yifan Zhu, Xiao-Shan Gao, Lijun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04111">Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive Ability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The primary objective of learning methods is generalization. Classic uniform generalization bounds, which rely on VC-dimension or Rademacher complexity, fail to explain the significant attribute that over-parameterized models in deep learning exhibit nice generalizability. On the other hand, algorithm-dependent generalization bounds, like stability bounds, often rely on strict assumptions. To establish generalizability under less stringent assumptions, this paper investigates the generalizability of neural networks that minimize or approximately minimize empirical risk. We establish a lower bound for population accuracy based on the expressiveness of these networks, which indicates that with an adequate large number of training samples and network sizes, these networks, including over-parameterized ones, can generalize effectively. Additionally, we provide a necessary condition for generalization, demonstrating that, for certain data distributions, the quantity of training data required to ensure generalization exceeds the network size needed to represent the corresponding data distribution. Finally, we provide theoretical insights into several phenomena in deep learning, including robust generalization, importance of over-parameterization, and effect of loss function on generalization.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2503.03399.pdf' target='_blank'>https://arxiv.org/pdf/2503.03399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanyu Duan, Yi Yang, Ahmed Abbasi, Kar Yan Tam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03399">Predicting Practically? Domain Generalization for Predictive Analytics in Real-world Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive machine learning models are widely used in customer relationship management (CRM) to forecast customer behaviors and support decision-making. However, the dynamic nature of customer behaviors often results in significant distribution shifts between training data and serving data, leading to performance degradation in predictive models. Domain generalization, which aims to train models that can generalize to unseen environments without prior knowledge of their distributions, has become a critical area of research. In this work, we propose a novel domain generalization method tailored to handle complex distribution shifts, encompassing both covariate and concept shifts. Our method builds upon the Distributionally Robust Optimization framework, optimizing model performance over a set of hypothetical worst-case distributions rather than relying solely on the training data. Through simulation experiments, we demonstrate the working mechanism of the proposed method. We also conduct experiments on a real-world customer churn dataset, and validate its effectiveness in both temporal and spatial generalization settings. Finally, we discuss the broader implications of our method for advancing Information Systems (IS) design research, particularly in building robust predictive models for dynamic managerial environments.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2502.06593.pdf' target='_blank'>https://arxiv.org/pdf/2502.06593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paschalis Giakoumoglou, Dimitrios Karageorgiou, Symeon Papadopoulos, Panagiotis C. Petrantonakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06593">SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in generative AI have made text-guided image inpainting - adding, removing, or altering image regions using textual prompts - widely accessible. However, generating semantically correct photorealistic imagery, typically requires carefully-crafted prompts and iterative refinement by evaluating the realism of the generated content - tasks commonly performed by humans. To automate the generative process, we propose Semantically Aligned and Uncertainty Guided AI Image Inpainting (SAGI), a model-agnostic pipeline, to sample prompts from a distribution that closely aligns with human perception and to evaluate the generated content and discard instances that deviate from such a distribution, which we approximate using pretrained large language models and vision-language models. By applying this pipeline on multiple state-of-the-art inpainting models, we create the SAGI Dataset (SAGI-D), currently the largest and most diverse dataset of AI-generated inpaintings, comprising over 95k inpainted images and a human-evaluated subset. Our experiments show that semantic alignment significantly improves image quality and aesthetics, while uncertainty guidance effectively identifies realistic manipulations - human ability to distinguish inpainted images from real ones drops from 74% to 35% in terms of accuracy, after applying our pipeline. Moreover, using SAGI-D for training several image forensic approaches increases in-domain detection performance on average by 37.4% and out-of-domain generalization by 26.1% in terms of IoU, also demonstrating its utility in countering malicious exploitation of generative AI. Code and dataset are available at https://mever-team.github.io/SAGI/
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2501.14653.pdf' target='_blank'>https://arxiv.org/pdf/2501.14653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Trong-Binh Nguyen, Minh-Duong Nguyen, Jinsun Park, Quoc-Viet Pham, Won Joo Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14653">Federated Domain Generalization with Data-free On-server Matching Gradient</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. One of the key approaches in DG is training an encoder which generates domain-invariant representations. However, this approach is not applicable in Federated Domain Generalization (FDG), where data from various domains are distributed across different clients. In this paper, we introduce a novel approach, dubbed Federated Learning via On-server Matching Gradient (FedOMG), which can \emph{efficiently leverage domain information from distributed domains}. Specifically, we utilize the local gradients as information about the distributed models to find an invariant gradient direction across all domains through gradient inner product maximization. The advantages are two-fold: 1) FedOMG can aggregate the characteristics of distributed models on the centralized server without incurring any additional communication cost, and 2) FedOMG is orthogonal to many existing FL/FDG methods, allowing for additional performance improvements by being seamlessly integrated with them. Extensive experimental evaluations on various settings to demonstrate the robustness of FedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA baselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and CIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome).
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2501.13273.pdf' target='_blank'>https://arxiv.org/pdf/2501.13273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaojie Jin, Sihao Wu, Jiaxu Liu, Tianjin Huang, Ronghui Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13273">Enhancing Robust Fairness via Confusional Spectral Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has highlighted a critical issue known as ``robust fairness", where robust accuracy varies significantly across different classes, undermining the reliability of deep neural networks (DNNs). A common approach to address this has been to dynamically reweight classes during training, giving more weight to those with lower empirical robust performance. However, we find there is a divergence of class-wise robust performance between training set and testing set, which limits the effectiveness of these explicit reweighting methods, indicating the need for a principled alternative. In this work, we derive a robust generalization bound for the worst-class robust error within the PAC-Bayesian framework, accounting for unknown data distributions. Our analysis shows that the worst-class robust error is influenced by two main factors: the spectral norm of the empirical robust confusion matrix and the information embedded in the model and training set. While the latter has been extensively studied, we propose a novel regularization technique targeting the spectral norm of the robust confusion matrix to improve worst-class robust accuracy and enhance robust fairness. We validate our approach through comprehensive experiments on various datasets and models, demonstrating its effectiveness in enhancing robust fairness.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2501.13084.pdf' target='_blank'>https://arxiv.org/pdf/2501.13084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13084">Attention-Driven Hierarchical Reinforcement Learning with Particle Filtering for Source Localization in Dynamic Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many real-world scenarios, such as gas leak detection or environmental pollutant tracking, solving the Inverse Source Localization and Characterization problem involves navigating complex, dynamic fields with sparse and noisy observations. Traditional methods face significant challenges, including partial observability, temporal and spatial dynamics, out-of-distribution generalization, and reward sparsity. To address these issues, we propose a hierarchical framework that integrates Bayesian inference and reinforcement learning. The framework leverages an attention-enhanced particle filtering mechanism for efficient and accurate belief updates, and incorporates two complementary execution strategies: Attention Particle Filtering Planning and Attention Particle Filtering Reinforcement Learning. These approaches optimize exploration and adaptation under uncertainty. Theoretical analysis proves the convergence of the attention-enhanced particle filter, while extensive experiments across diverse scenarios validate the framework's superior accuracy, adaptability, and computational efficiency. Our results highlight the framework's potential for broad applications in dynamic field estimation tasks.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2412.16339.pdf' target='_blank'>https://arxiv.org/pdf/2412.16339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16339">Deliberative Alignment: Reasoning Enables Safer Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align OpenAI's o-series models, and achieved highly precise adherence to OpenAI's safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2412.14474.pdf' target='_blank'>https://arxiv.org/pdf/2412.14474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shange Tang, Jiayun Wu, Jianqing Fan, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14474">Benign Overfitting in Out-of-Distribution Generalization of Linear Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benign overfitting refers to the phenomenon where an over-parameterized model fits the training data perfectly, including noise in the data, but still generalizes well to the unseen test data. While prior work provides some theoretical understanding of this phenomenon under the in-distribution setup, modern machine learning often operates in a more challenging Out-of-Distribution (OOD) regime, where the target (test) distribution can be rather different from the source (training) distribution. In this work, we take an initial step towards understanding benign overfitting in the OOD regime by focusing on the basic setup of over-parameterized linear models under covariate shift. We provide non-asymptotic guarantees proving that benign overfitting occurs in standard ridge regression, even under the OOD regime when the target covariance satisfies certain structural conditions. We identify several vital quantities relating to source and target covariance, which govern the performance of OOD generalization. Our result is sharp, which provably recovers prior in-distribution benign overfitting guarantee [Tsigler and Bartlett, 2023], as well as under-parameterized OOD guarantee [Ge et al., 2024] when specializing to each setup. Moreover, we also present theoretical results for a more general family of target covariance matrix, where standard ridge regression only achieves a slow statistical rate of $O(1/\sqrt{n})$ for the excess risk, while Principal Component Regression (PCR) is guaranteed to achieve the fast rate $O(1/n)$, where $n$ is the number of samples.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2412.10061.pdf' target='_blank'>https://arxiv.org/pdf/2412.10061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuur Stuyck, Gene Wei-Chin Lin, Egor Larionov, Hsiao-yu Chen, Aljaz Bozic, Nikolaos Sarafianos, Doug Roble
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10061">Quaffure: Real-Time Quasi-Static Neural Hair Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic hair motion is crucial for high-quality avatars, but it is often limited by the computational resources available for real-time applications. To address this challenge, we propose a novel neural approach to predict physically plausible hair deformations that generalizes to various body poses, shapes, and hairstyles. Our model is trained using a self-supervised loss, eliminating the need for expensive data generation and storage. We demonstrate our method's effectiveness through numerous results across a wide range of pose and shape variations, showcasing its robust generalization capabilities and temporally smooth results. Our approach is highly suitable for real-time applications with an inference time of only a few milliseconds on consumer hardware and its ability to scale to predicting the drape of 1000 grooms in 0.3 seconds.
  Please see our project page here following https://tuurstuyck.github.io/quaffure/quaffure.html
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2412.00744.pdf' target='_blank'>https://arxiv.org/pdf/2412.00744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haowei Sun, Jinwu Hu, Zhirui Zhang, Haoyuan Tian, Xinze Xie, Yufeng Wang, Zhuliang Yu, Xiaohua Xie, Mingkui Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00744">A Cross-Scene Benchmark for Open-World Drone Active Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone Visual Active Tracking aims to autonomously follow a target object by controlling the motion system based on visual observations, providing a more practical solution for effective tracking in dynamic environments. However, accurate Drone Visual Active Tracking using reinforcement learning remains challenging due to the absence of a unified benchmark, the complexity of open-world environments with frequent interference, and the diverse motion behavior of dynamic targets. To address these issues, we propose a unified cross-scene cross-domain benchmark for open-world drone active tracking called DAT. The DAT benchmark provides 24 visually complex environments to assess the algorithms' cross-scene and cross-domain generalization abilities, and high-fidelity modeling of realistic robot dynamics. Additionally, we propose a reinforcement learning-based drone tracking method called R-VAT, which aims to improve the performance of drone tracking targets in complex scenarios. Specifically, inspired by curriculum learning, we introduce a Curriculum-Based Training strategy that progressively enhances the agent tracking performance in vast environments with complex interference. We design a goal-centered reward function to provide precise feedback to the drone agent, preventing targets farther from the center of view from receiving higher rewards than closer ones. This allows the drone to adapt to the diverse motion behavior of open-world targets. Experiments demonstrate that the R-VAT has about 400% improvement over the SOTA method in terms of the cumulative reward metric.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2411.19534.pdf' target='_blank'>https://arxiv.org/pdf/2411.19534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenfang Sun, Yingjun Du, Gaowen Liu, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19534">QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of quantifying the number of objects by a generative text-to-image model. Rather than retraining such a model for each new image domain of interest, which leads to high computational costs and limited scalability, we are the first to consider this problem from a domain-agnostic perspective. We propose QUOTA, an optimization framework for text-to-image models that enables effective object quantification across unseen domains without retraining. It leverages a dual-loop meta-learning strategy to optimize a domain-invariant prompt. Further, by integrating prompt learning with learnable counting and domain tokens, our method captures stylistic variations and maintains accuracy, even for object classes not encountered during training. For evaluation, we adopt a new benchmark specifically designed for object quantification in domain generalization, enabling rigorous assessment of object quantification accuracy and adaptability across unseen domains in text-to-image generation. Extensive experiments demonstrate that QUOTA outperforms conventional models in both object quantification accuracy and semantic consistency, setting a new benchmark for efficient and scalable text-to-image generation for any domain.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2411.17472.pdf' target='_blank'>https://arxiv.org/pdf/2411.17472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Hanchen Jiang, Yasi Zhang, Zhi Zhang, Yixin Wan, Andrew Lizarraga, Shufan Li, Ying Nian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17472">Unlocking the Potential of Text-to-Image Diffusion with PAC-Bayesian Theory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image (T2I) diffusion models have revolutionized generative modeling by producing high-fidelity, diverse, and visually realistic images from textual prompts. Despite these advances, existing models struggle with complex prompts involving multiple objects and attributes, often misaligning modifiers with their corresponding nouns or neglecting certain elements. Recent attention-based methods have improved object inclusion and linguistic binding, but still face challenges such as attribute misbinding and a lack of robust generalization guarantees. Leveraging the PAC-Bayes framework, we propose a Bayesian approach that designs custom priors over attention distributions to enforce desirable properties, including divergence between objects, alignment between modifiers and their corresponding nouns, minimal attention to irrelevant tokens, and regularization for better generalization. Our approach treats the attention mechanism as an interpretable component, enabling fine-grained control and improved attribute-object alignment. We demonstrate the effectiveness of our method on standard benchmarks, achieving state-of-the-art results across multiple metrics. By integrating custom priors into the denoising process, our method enhances image quality and addresses long-standing challenges in T2I diffusion models, paving the way for more reliable and interpretable generative models.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2411.16788.pdf' target='_blank'>https://arxiv.org/pdf/2411.16788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16788">TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of single-source domain generalization. Existing methods typically rely on extensive augmentations to synthetically cover diverse domains during training. However, they struggle with semantic shifts (e.g., background and viewpoint changes), as they often learn global features instead of local concepts that tend to be domain invariant. To address this gap, we propose an approach that compels models to leverage such local concepts during prediction. Given no suitable dataset with per-class concepts and localization maps exists, we first develop a novel pipeline to generate annotations by exploiting the rich features of diffusion and large-language models. Our next innovation is TIDE, a novel training scheme with a concept saliency alignment loss that ensures model focus on the right per-concept regions and a local concept contrastive loss that promotes learning domain-invariant concept representations. This not only gives a robust model but also can be visually interpreted using the predicted concept saliency maps. Given these maps at test time, our final contribution is a new correction algorithm that uses the corresponding local concept representations to iteratively refine the prediction until it aligns with prototypical concept representations that we store at the end of model training. We evaluate our approach extensively on four standard DG benchmark datasets and substantially outperform the current state-ofthe-art (12% improvement on average) while also demonstrating that our predictions can be visually interpreted
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2411.07392.pdf' target='_blank'>https://arxiv.org/pdf/2411.07392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoliang Wang, Chen Zhao, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07392">Feature-Space Semantic Invariance: Enhanced OOD Detection for Open-Set Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-set domain generalization addresses a real-world challenge: training a model to generalize across unseen domains (domain generalization) while also detecting samples from unknown classes not encountered during training (open-set recognition). However, most existing approaches tackle these issues separately, limiting their practical applicability. To overcome this limitation, we propose a unified framework for open-set domain generalization by introducing Feature-space Semantic Invariance (FSI). FSI maintains semantic consistency across different domains within the feature space, enabling more accurate detection of OOD instances in unseen domains. Additionally, we adopt a generative model to produce synthetic data with novel domain styles or class labels, enhancing model robustness. Initial experiments show that our method improves AUROC by 9.1% to 18.9% on ColoredMNIST, while also significantly increasing in-distribution classification accuracy.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2411.02444.pdf' target='_blank'>https://arxiv.org/pdf/2411.02444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoliang Wang, Chen Zhao, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02444">MADOD: Generalizing OOD Detection to Unseen Domains via G-Invariance Meta-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world machine learning applications often face simultaneous covariate and semantic shifts, challenging traditional domain generalization and out-of-distribution (OOD) detection methods. We introduce Meta-learned Across Domain Out-of-distribution Detection (MADOD), a novel framework designed to address both shifts concurrently. MADOD leverages meta-learning and G-invariance to enhance model generalizability and OOD detection in unseen domains. Our key innovation lies in task construction: we randomly designate in-distribution classes as pseudo-OODs within each meta-learning task, simulating OOD scenarios using existing data. This approach, combined with energy-based regularization, enables the learning of robust, domain-invariant features while calibrating decision boundaries for effective OOD detection. Operating in a test domain-agnostic setting, MADOD eliminates the need for adaptation during inference, making it suitable for scenarios where test data is unavailable. Extensive experiments on real-world and synthetic datasets demonstrate MADOD's superior performance in semantic OOD detection across unseen domains, achieving an AUPR improvement of 8.48% to 20.81%, while maintaining competitive in-distribution classification accuracy, representing a significant advancement in handling both covariate and semantic shifts.
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2411.01316.pdf' target='_blank'>https://arxiv.org/pdf/2411.01316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Jiang, Chen Zhao, Haoliang Wang, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01316">FEED: Fairness-Enhanced Meta-Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing to out-of-distribution data while being aware of model fairness is a significant and challenging problem in meta-learning. The goal of this problem is to find a set of fairness-aware invariant parameters of classifier that is trained using data drawn from a family of related training domains with distribution shift on non-sensitive features as well as different levels of dependence between model predictions and sensitive features so that the classifier can achieve good generalization performance on unknown but distinct test domains. To tackle this challenge, existing state-of-the-art methods either address the domain generalization problem but completely ignore learning with fairness or solely specify shifted domains with various fairness levels. This paper introduces an approach to fairness-aware meta-learning that significantly enhances domain generalization capabilities. Our framework, Fairness-Enhanced Meta-Learning for Domain Generalization (FEED), disentangles latent data representations into content, style, and sensitive vectors. This disentanglement facilitates the robust generalization of machine learning models across diverse domains while adhering to fairness constraints. Unlike traditional methods that focus primarily on domain invariance or sensitivity to shifts, our model integrates a fairness-aware invariance criterion directly into the meta-learning process. This integration ensures that the learned parameters uphold fairness consistently, even when domain characteristics vary widely. We validate our approach through extensive experiments across multiple benchmarks, demonstrating not only superior performance in maintaining high accuracy and fairness but also significant improvements over existing state-of-the-art methods in domain generalization tasks.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2410.20164.pdf' target='_blank'>https://arxiv.org/pdf/2410.20164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjun Du, Gaowen Liu, Yuzhang Shang, Yuguang Yao, Ramana Kompella, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20164">Prompt Diffusion Robustifies Any-Modality Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models enable prompt-based classifiers for zero-shot and few-shot learning. Nonetheless, the conventional method of employing fixed prompts suffers from distributional shifts that negatively impact generalizability to unseen samples. This paper introduces prompt diffusion, which uses a diffusion model to gradually refine the prompts to obtain a customized prompt for each sample. Specifically, we first optimize a collection of prompts to obtain over-fitted prompts per sample. Then, we propose a prompt diffusion model within the prompt space, enabling the training of a generative transition process from a random prompt to its overfitted prompt. As we cannot access the label of a test image during inference, our model gradually generates customized prompts solely from random prompts using our trained, prompt diffusion. Our prompt diffusion is generic, flexible, and modality-agnostic, making it a simple plug-and-play module seamlessly embedded into existing prompt learning methods for textual, visual, or multi-modal prompt learning. Our diffusion model uses a fast ODE-based sampling strategy to optimize test sample prompts in just five steps, offering a good trade-off between performance improvement and computational efficiency. For all prompt learning methods tested, adding prompt diffusion yields more robust results for base-to-new generalization, cross-dataset generalization, and domain generalization in classification tasks tested over 15 diverse datasets.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2410.08258.pdf' target='_blank'>https://arxiv.org/pdf/2410.08258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prasanna Mayilvahanan, Roland S. Zimmermann, ThaddÃ¤us Wiedemer, Evgenia Rusak, Attila Juhos, Matthias Bethge, Wieland Brendel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08258">In Search of Forgotten Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION -- LAION-Natural and LAION-Rendition -- that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale -- a crucial prerequisite for improving model robustness.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2410.02136.pdf' target='_blank'>https://arxiv.org/pdf/2410.02136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Liu, Lu Zhang, Tian Gao, Yue Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02136">Disentangled Representation Learning for Parametric Partial Differential Equations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural operators (NOs) have demonstrated remarkable success in learning mappings between function spaces, serving as efficient approximators for the forward solutions of complex physical systems governed by partial differential equations (PDEs). However, while effective as black-box solvers, they offer limited insight into the underlying physical mechanism, due to the lack of interpretable representations of the physical parameters that drive the system. To tackle this challenge, we propose a new paradigm for learning disentangled representations from neural operator parameters, thereby effectively solving an inverse problem. Specifically, we introduce DisentangO, a novel hyper-neural operator architecture designed to unveil and disentangle the latent physical factors of variation embedded within the black-box neural operator parameters. At the core of DisentangO is a multi-task neural operator architecture that distills the varying parameters of the governing PDE through a task-wise adaptive layer, coupled with a hierarchical variational autoencoder that disentangles these variations into identifiable latent factors. By learning these disentangled representations, our model not only enhances physical interpretability but also enables more robust generalization across diverse physical systems. Empirical evaluations across supervised, semi-supervised, and unsupervised learning contexts show that DisentangO effectively extracts meaningful and interpretable latent features, bridging the divide between predictive performance and physical understanding in neural operator frameworks.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2408.09310.pdf' target='_blank'>https://arxiv.org/pdf/2408.09310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gus Kristiansen, Mark Sandler, Andrey Zhmoginov, Nolan Miller, Anirudh Goyal, Jihwan Lee, Max Vladymyrov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09310">Narrowing the Focus: Learned Optimizers for Pretrained Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In modern deep learning, the models are learned by applying gradient updates using an optimizer, which transforms the updates based on various statistics. Optimizers are often hand-designed and tuning their hyperparameters is a big part of the training process. Learned optimizers have shown some initial promise, but are generally unsuccessful as a general optimization mechanism applicable to every problem. In this work we explore a different direction: instead of learning general optimizers, we instead specialize them to a specific training environment. We propose a novel optimizer technique that learns a layer-specific linear combination of update directions provided by a set of base optimizers, effectively adapting its strategy to the specific model and dataset. When evaluated on image classification tasks, this specialized optimizer significantly outperforms both traditional off-the-shelf methods such as Adam, as well as existing general learned optimizers. Moreover, it demonstrates robust generalization with respect to model initialization, evaluating on unseen datasets, and training durations beyond its meta-training horizon.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2406.02125.pdf' target='_blank'>https://arxiv.org/pdf/2406.02125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Chen, Hongrun Zhang, U Wang Chan, Rui Yin, Xiaofei Wang, Chao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02125">Domain Game: Disentangle Anatomical Feature for Single Domain Generalized Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization aims to address the challenge of out-of-distribution generalization problem with only one source domain available. Feature distanglement is a classic solution to this purpose, where the extracted task-related feature is presumed to be resilient to domain shift. However, the absence of references from other domains in a single-domain scenario poses significant uncertainty in feature disentanglement (ill-posedness). In this paper, we propose a new framework, named \textit{Domain Game}, to perform better feature distangling for medical image segmentation, based on the observation that diagnostic relevant features are more sensitive to geometric transformations, whilist domain-specific features probably will remain invariant to such operations. In domain game, a set of randomly transformed images derived from a singular source image is strategically encoded into two separate feature sets to represent diagnostic features and domain-specific features, respectively, and we apply forces to pull or repel them in the feature space, accordingly. Results from cross-site test domain evaluation showcase approximately an ~11.8% performance boost in prostate segmentation and around ~10.5% in brain tumor segmentation compared to the second-best method.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2405.14017.pdf' target='_blank'>https://arxiv.org/pdf/2405.14017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhang, Di Chang, Fang Li, Mohammad Soleymani, Narendra Ahuja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14017">MagicPose4D: Crafting Articulated Models with Appearance and Motion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the success of 2D and 3D visual generative models, there is growing interest in generating 4D content. Existing methods primarily rely on text prompts to produce 4D content, but they often fall short of accurately defining complex or rare motions. To address this limitation, we propose MagicPose4D, a novel framework for refined control over both appearance and motion in 4D generation. Unlike current 4D generation methods, MagicPose4D accepts monocular videos or mesh sequences as motion prompts, enabling precise and customizable motion control. MagicPose4D comprises two key modules: (i) Dual-Phase 4D Reconstruction Module, which operates in two phases. The first phase focuses on capturing the model's shape using accurate 2D supervision and less accurate but geometrically informative 3D pseudo-supervision without imposing skeleton constraints. The second phase extracts the 3D motion (skeleton poses) using more accurate pseudo-3D supervision, obtained in the first phase and introduces kinematic chain-based skeleton constraints to ensure physical plausibility. Additionally, we propose a Global-local Chamfer loss that aligns the overall distribution of predicted mesh vertices with the supervision while maintaining part-level alignment without extra annotations. (ii) Cross-category Motion Transfer Module, which leverages the extracted motion from the 4D reconstruction module and uses a kinematic-chain-based skeleton to achieve cross-category motion transfer. It ensures smooth transitions between frames through dynamic rigidity, facilitating robust generalization without additional training. Through extensive experiments, we demonstrate that MagicPose4D significantly improves the accuracy and consistency of 4D content generation, outperforming existing methods in various benchmarks.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2405.10633.pdf' target='_blank'>https://arxiv.org/pdf/2405.10633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongrong Ma, Guansong Pang, Ling Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10633">Harnessing Collective Structure Knowledge in Data Augmentation for Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural networks (GNNs) have achieved state-of-the-art performance in graph representation learning. Message passing neural networks, which learn representations through recursively aggregating information from each node and its neighbors, are among the most commonly-used GNNs. However, a wealth of structural information of individual nodes and full graphs is often ignored in such process, which restricts the expressive power of GNNs. Various graph data augmentation methods that enable the message passing with richer structure knowledge have been introduced as one main way to tackle this issue, but they are often focused on individual structure features and difficult to scale up with more structure features. In this work we propose a novel approach, namely collective structure knowledge-augmented graph neural network (CoS-GNN), in which a new message passing method is introduced to allow GNNs to harness a diverse set of node- and graph-level structure features, together with original node features/attributes, in augmented graphs. In doing so, our approach largely improves the structural knowledge modeling of GNNs in both node and graph levels, resulting in substantially improved graph representations. This is justified by extensive empirical results where CoS-GNN outperforms state-of-the-art models in various graph-level learning tasks, including graph classification, anomaly detection, and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2403.10782.pdf' target='_blank'>https://arxiv.org/pdf/2403.10782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Alehdaghi, Pourya Shamsolmoali, Rafael M. O. Cruz, Eric Granger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10782">Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key challenge in visible-infrared person re-identification (V-I ReID) is training a backbone model capable of effectively addressing the significant discrepancies across modalities. State-of-the-art methods that generate a single intermediate bridging domain are often less effective, as this generated domain may not adequately capture sufficient common discriminant information. This paper introduces Bidirectional Multi-step Domain Generalization (BMDG), a novel approach for unifying feature representations across diverse modalities. BMDG creates multiple virtual intermediate domains by learning and aligning body part features extracted from both I and V modalities. In particular, our method aims to minimize the cross-modal gap in two steps. First, BMDG aligns modalities in the feature space by learning shared and modality-invariant body part prototypes from V and I images. Then, it generalizes the feature representation by applying bidirectional multi-step learning, which progressively refines feature representations in each step and incorporates more prototypes from both modalities. Based on these prototypes, multiple bridging steps enhance the feature representation. Experiments conducted on V-I ReID datasets indicate that our BMDG approach can outperform state-of-the-art part-based and intermediate generation methods, and can be integrated into other part-based methods to enhance their V-I ReID performance. (Our code is available at:https:/alehdaghi.github.io/BMDG/ )
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2403.06315.pdf' target='_blank'>https://arxiv.org/pdf/2403.06315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Teresa Parreira, Sukruth Gowdru Lingaraju, Adolfo Ramirez-Aristizabal, Manaswi Saha, Michael Kuniavsky, Wendy Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06315">A Study on Domain Generalization for Failure Detection through Human Reactions in HRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models are commonly tested in-distribution (same dataset); performance almost always drops in out-of-distribution settings. For HRI research, the goal is often to develop generalized models. This makes domain generalization - retaining performance in different settings - a critical issue. In this study, we present a concise analysis of domain generalization in failure detection models trained on human facial expressions. Using two distinct datasets of humans reacting to videos where error occurs, one from a controlled lab setting and another collected online, we trained deep learning models on each dataset. When testing these models on the alternate dataset, we observed a significant performance drop. We reflect on the causes for the observed model behavior and leave recommendations. This work emphasizes the need for HRI research focusing on improving model robustness and real-life applicability.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2403.05848.pdf' target='_blank'>https://arxiv.org/pdf/2403.05848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Sur Richard Park, Siu Wun Cheung, Youngsoo Choi, Yeonjong Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05848">tLaSDI: Thermodynamics-informed latent space dynamics identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a latent space dynamics identification method, namely tLaSDI, that embeds the first and second principles of thermodynamics. The latent variables are learned through an autoencoder as a nonlinear dimension reduction model. The latent dynamics are constructed by a neural network-based model that precisely preserves certain structures for the thermodynamic laws through the GENERIC formalism. An abstract error estimate is established, which provides a new loss formulation involving the Jacobian computation of autoencoder. The autoencoder and the latent dynamics are simultaneously trained to minimize the new loss. Computational examples demonstrate the effectiveness of tLaSDI, which exhibits robust generalization ability, even in extrapolation. In addition, an intriguing correlation is empirically observed between a quantity from tLaSDI in the latent space and the behaviors of the full-state solution.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2403.05523.pdf' target='_blank'>https://arxiv.org/pdf/2403.05523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijiang Li, Sucheng Ren, Weipeng Deng, Yuzhi Xu, Ying Gao, Edith Ngai, Haohan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05523">Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization is a favorable yet challenging property for deep neural networks. The core challenges lie in the limited availability of source domains that help models learn an invariant representation from the spurious features. Various domain augmentation have been proposed but largely rely on interpolating existing domains and frequently face difficulties in creating truly "novel" domains. Humans, on the other hand, can easily extrapolate novel domains, thus, an intriguing question arises: How can neural networks extrapolate like humans and achieve OOD generalization?
  We introduce a novel approach to domain extrapolation that leverages reasoning ability and the extensive knowledge encapsulated within large language models (LLMs) to synthesize entirely new domains. Starting with the class of interest, we query the LLMs to extract relevant knowledge for these novel domains. We then bridge the gap between the text-centric knowledge derived from LLMs and the pixel input space of the model using text-to-image generation techniques. By augmenting the training set of domain generalization datasets with high-fidelity, photo-realistic images of these new domains, we achieve significant improvements over all existing methods, as demonstrated in both single and multi-domain generalization across various benchmarks.
  With the ability to extrapolate any domains for any class, our method has the potential to learn a generalized model for any task without any data. To illustrate, we put forth a much more difficult setting termed, data-free domain generalization, that aims to learn a generalized model in the absence of any collected data. Our empirical findings support the above argument and our methods exhibit commendable performance in this setting, even surpassing the supervised setting by approximately 1-2\% on datasets such as VLCS.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2403.02714.pdf' target='_blank'>https://arxiv.org/pdf/2403.02714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Hou, Jin Yuan, Ying Yang, Yang Liu, Yang Zhang, Cheng Zhong, Zhongchao Shi, Jianping Fan, Yong Rui, Zhiqiang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02714">DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data. With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains. With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2402.06150.pdf' target='_blank'>https://arxiv.org/pdf/2402.06150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kecheng Chen, Elena Gal, Hong Yan, Haoliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06150">Domain Generalization with Small Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose to tackle the problem of domain generalization in the context of \textit{insufficient samples}. Instead of extracting latent feature embeddings based on deterministic models, we propose to learn a domain-invariant representation based on the probabilistic framework by mapping each data point into probabilistic embeddings. Specifically, we first extend empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can measure the discrepancy between mixture distributions (i.e., source domains) consisting of a series of latent distributions rather than latent points. Moreover, instead of imposing the contrastive semantic alignment (CSA) loss based on pairs of latent points, a novel probabilistic CSA loss encourages positive probabilistic embedding pairs to be closer while pulling other negative ones apart. Benefiting from the learned representation captured by probabilistic models, our proposed method can marriage the measurement on the \textit{distribution over distributions} (i.e., the global perspective alignment) and the distribution-based contrastive semantic alignment (i.e., the local perspective alignment). Extensive experimental results on three challenging medical datasets show the effectiveness of our proposed method in the context of insufficient data compared with state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2311.15145.pdf' target='_blank'>https://arxiv.org/pdf/2311.15145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jixuan Leng, Yijiang Li, Haohan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15145">Choosing Wisely and Learning Deeply: Selective Cross-Modality Distillation via CLIP for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG), a crucial research area, seeks to train models across multiple domains and test them on unseen ones. In this paper, we introduce a novel approach, namely, Selective Cross-Modality Distillation for Domain Generalization (SCMD). SCMD leverages the capabilities of large vision-language models, specifically CLIP, to train a more efficient model, ensuring it acquires robust generalization capabilities across unseen domains. Our primary contribution is a unique selection framework strategically designed to identify hard-to-learn samples for distillation. In parallel, we introduce a novel cross-modality module that seamlessly combines the projected features of the student model with the text embeddings from CLIP, ensuring the alignment of similarity distributions. We assess SCMD's performance on various benchmarks, where it empowers a ResNet50 to deliver state-of-the-art performance, surpassing existing domain generalization methods. Furthermore, we provide a theoretical analysis of our selection strategy, offering deeper insight into its effectiveness and potential in the field of DG.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2311.01908.pdf' target='_blank'>https://arxiv.org/pdf/2311.01908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Yeona Cho, Ik Jae Lee, Jin Sung Kim, Jong Chul Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01908">LLM-driven Multimodal Target Volume Contouring in Radiation Oncology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Target volume contouring for radiation therapy is considered significantly more challenging than the normal organ segmentation tasks as it necessitates the utilization of both image and text-based clinical information. Inspired by the recent advancement of large language models (LLMs) that can facilitate the integration of the textural information and images, here we present a novel LLM-driven multimodal AI, namely LLMSeg, that utilizes the clinical text information and is applicable to the challenging task of target volume contouring for radiation therapy, and validate it within the context of breast cancer radiation therapy target volume contouring. Using external validation and data-insufficient environments, which attributes highly conducive to real-world applications, we demonstrate that the proposed model exhibits markedly improved performance compared to conventional unimodal AI models, particularly exhibiting robust generalization performance and data efficiency. To our best knowledge, this is the first LLM-driven multimodal AI model that integrates the clinical text information into target volume delineation for radiation oncology.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2310.09612.pdf' target='_blank'>https://arxiv.org/pdf/2310.09612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexa R. Tartaglini, Sheridan Feucht, Michael A. Lepori, Wai Keen Vong, Charles Lovering, Brenden M. Lake, Ellie Pavlick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09612">Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although deep neural networks can achieve human-level performance on many object recognition benchmarks, prior work suggests that these same models fail to learn simple abstract relations, such as determining whether two objects are the same or different. Much of this prior work focuses on training convolutional neural networks to classify images of two same or two different abstract shapes, testing generalization on within-distribution stimuli. In this article, we comprehensively study whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using a variety of architectures, forms of pretraining, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Furthermore, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn generalizable same-different visual relations.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2310.09562.pdf' target='_blank'>https://arxiv.org/pdf/2310.09562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prasanna Mayilvahanan, ThaddÃ¤us Wiedemer, Evgenia Rusak, Matthias Bethge, Wieland Brendel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09562">Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP's OOD performance, and other properties of the training data must drive CLIP to learn more generalizable representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION ($\frac{1}{4}$th of its original size) on which CLIP can be trained to match its original OOD performance.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2310.05755.pdf' target='_blank'>https://arxiv.org/pdf/2310.05755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yegor Klochkov, Jean-Francois Ton, Ruocheng Guo, Yang Liu, Hang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05755">Deep Concept Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of concept removal in deep neural networks, aiming to learn representations that do not encode certain specified concepts (e.g., gender etc.) We propose a novel method based on adversarial linear classifiers trained on a concept dataset, which helps to remove the targeted attribute while maintaining model performance. Our approach Deep Concept Removal incorporates adversarial probing classifiers at various layers of the network, effectively addressing concept entanglement and improving out-of-distribution generalization. We also introduce an implicit gradient-based technique to tackle the challenges associated with adversarial training using linear classifiers. We evaluate the ability to remove a concept on a set of popular distributionally robust optimization (DRO) benchmarks with spurious correlations, as well as out-of-distribution (OOD) generalization tasks.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2309.15493.pdf' target='_blank'>https://arxiv.org/pdf/2309.15493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wei, Peilun Shi, Juzheng Miao, Minqing Zhang, Guitao Bai, Jianing Qiu, Furui Liu, Wu Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15493">CauDR: A Causality-inspired Domain Generalization Framework for Fundus-based Diabetic Retinopathy Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic retinopathy (DR) is the most common diabetic complication, which usually leads to retinal damage, vision loss, and even blindness. A computer-aided DR grading system has a significant impact on helping ophthalmologists with rapid screening and diagnosis. Recent advances in fundus photography have precipitated the development of novel retinal imaging cameras and their subsequent implementation in clinical practice. However, most deep learning-based algorithms for DR grading demonstrate limited generalization across domains. This inferior performance stems from variance in imaging protocols and devices inducing domain shifts. We posit that declining model performance between domains arises from learning spurious correlations in the data. Incorporating do-operations from causality analysis into model architectures may mitigate this issue and improve generalizability. Specifically, a novel universal structural causal model (SCM) was proposed to analyze spurious correlations in fundus imaging. Building on this, a causality-inspired diabetic retinopathy grading framework named CauDR was developed to eliminate spurious correlations and achieve more generalizable DR diagnostics. Furthermore, existing datasets were reorganized into 4DR benchmark for DG scenario. Results demonstrate the effectiveness and the state-of-the-art (SOTA) performance of CauDR.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2309.13377.pdf' target='_blank'>https://arxiv.org/pdf/2309.13377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alan Q. Wang, Minh Nguyen, Mert R. Sabuncu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13377">Learning Invariant Representations with a Nonparametric Nadaraya-Watson Head</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models will often fail when deployed in an environment with a data distribution that is different than the training distribution. When multiple environments are available during training, many methods exist that learn representations which are invariant across the different distributions, with the hope that these representations will be transportable to unseen domains. In this work, we present a nonparametric strategy for learning invariant representations based on the recently-proposed Nadaraya-Watson (NW) head. The NW head makes a prediction by comparing the learned representations of the query to the elements of a support set that consists of labeled data. We demonstrate that by manipulating the support set, one can encode different causal assumptions. In particular, restricting the support set to a single environment encourages the model to learn invariant features that do not depend on the environment. We present a causally-motivated setup for our modeling and training strategy and validate on three challenging real-world domain generalization tasks in computer vision.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2309.10209.pdf' target='_blank'>https://arxiv.org/pdf/2309.10209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoliang Wang, Chen Zhao, Yunhui Guo, Kai Jiang, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10209">Towards Effective Semantic OOD Detection in Unseen Domains: A Domain Generalization Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two prevalent types of distributional shifts in machine learning are the covariate shift (as observed across different domains) and the semantic shift (as seen across different classes). Traditional OOD detection techniques typically address only one of these shifts. However, real-world testing environments often present a combination of both covariate and semantic shifts. In this study, we introduce a novel problem, semantic OOD detection across domains, which simultaneously addresses both distributional shifts. To this end, we introduce two regularization strategies: domain generalization regularization, which ensures semantic invariance across domains to counteract the covariate shift, and OOD detection regularization, designed to enhance OOD detection capabilities against the semantic shift through energy bounding. Through rigorous testing on three standard domain generalization benchmarks, our proposed framework showcases its superiority over conventional domain generalization approaches in terms of OOD detection performance. Moreover, it holds its ground by maintaining comparable InD classification accuracy.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2309.09888.pdf' target='_blank'>https://arxiv.org/pdf/2309.09888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharut Gupta, Stefanie Jegelka, David Lopez-Paz, Kartik Ahuja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09888">Context is Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context is environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context$\unicode{x2013}\unicode{x2013}$unlabeled examples as they arrive$\unicode{x2013}\unicode{x2013}$allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, leading to significant out-of-distribution performance improvements. From all of this, two messages are worth taking home. Researchers in domain generalization should consider environment as context, and harness the adaptive power of in-context learning. Researchers in LLMs should consider context as environment, to better structure data towards generalization.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2309.04038.pdf' target='_blank'>https://arxiv.org/pdf/2309.04038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rizhao Cai, Zitong Yu, Chenqi Kong, Haoliang Li, Changsheng Chen, Yongjian Hu, Alex Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04038">S-Adapter: Generalizing Vision Transformer for Face Anti-Spoofing with Statistical Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Anti-Spoofing (FAS) aims to detect malicious attempts to invade a face recognition system by presenting spoofed faces. State-of-the-art FAS techniques predominantly rely on deep learning models but their cross-domain generalization capabilities are often hindered by the domain shift problem, which arises due to different distributions between training and testing data. In this study, we develop a generalized FAS method under the Efficient Parameter Transfer Learning (EPTL) paradigm, where we adapt the pre-trained Vision Transformer models for the FAS task. During training, the adapter modules are inserted into the pre-trained ViT model, and the adapters are updated while other pre-trained parameters remain fixed. We find the limitations of previous vanilla adapters in that they are based on linear layers, which lack a spoofing-aware inductive bias and thus restrict the cross-domain generalization. To address this limitation and achieve cross-domain generalized FAS, we propose a novel Statistical Adapter (S-Adapter) that gathers local discriminative and statistical information from localized token histograms. To further improve the generalization of the statistical tokens, we propose a novel Token Style Regularization (TSR), which aims to reduce domain style variance by regularizing Gram matrices extracted from tokens across different domains. Our experimental results demonstrate that our proposed S-Adapter and TSR provide significant benefits in both zero-shot and few-shot cross-domain testing, outperforming state-of-the-art methods on several benchmark tests. We will release the source code upon acceptance.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2306.17008.pdf' target='_blank'>https://arxiv.org/pdf/2306.17008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fubao Zhu, Yanhui Tian, Chuang Han, Yanting Li, Jiaofen Nan, Ni Yao, Weihua Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17008">MLA-BIN: Model-level Attention and Batch-instance Style Normalization for Domain Generalization of Federated Learning on Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The privacy protection mechanism of federated learning (FL) offers an effective solution for cross-center medical collaboration and data sharing. In multi-site medical image segmentation, each medical site serves as a client of FL, and its data naturally forms a domain. FL supplies the possibility to improve the performance of seen domains model. However, there is a problem of domain generalization (DG) in the actual de-ployment, that is, the performance of the model trained by FL in unseen domains will decrease. Hence, MLA-BIN is proposed to solve the DG of FL in this study. Specifically, the model-level attention module (MLA) and batch-instance style normalization (BIN) block were designed. The MLA represents the unseen domain as a linear combination of seen domain models. The atten-tion mechanism is introduced for the weighting coefficient to obtain the optimal coefficient ac-cording to the similarity of inter-domain data features. MLA enables the global model to gen-eralize to unseen domain. In the BIN block, batch normalization (BN) and instance normalization (IN) are combined to perform the shallow layers of the segmentation network for style normali-zation, solving the influence of inter-domain image style differences on DG. The extensive experimental results of two medical image seg-mentation tasks demonstrate that the proposed MLA-BIN outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2306.02618.pdf' target='_blank'>https://arxiv.org/pdf/2306.02618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhui Sun, Sanchit Sinha, Aidong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02618">Enhance Diffusion to Improve Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch size. We further propose a novel approach, \emph{Diffusion Enhanced Adversarial Training} (DEAT), to manipulate the diffusion term to improve robust generalization with virtually no extra computational burden. We theoretically show that DEAT obtains a tighter generalization bound than PGD-AT. Our empirical investigation is extensive and firmly attests that DEAT universally outperforms PGD-AT by a significant margin.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2305.17378.pdf' target='_blank'>https://arxiv.org/pdf/2305.17378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daking Rai, Bailin Wang, Yilun Zhou, Ziyu Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17378">Improving Generalization in Language Model-Based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-Based Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs). In this study, we empirically investigate improving an LM's generalization in semantic parsing with two simple techniques: at the token level, we introduce a token preprocessing method to preserve the semantic boundaries of tokens produced by LM tokenizers; at the sequence level, we propose to use special tokens to mark the boundaries of components aligned between input and output. Our experimental results on two text-to-SQL semantic parsing datasets show that our token preprocessing, although simple, can substantially improve the LM performance on both types of generalization, and our component boundary marking method is particularly helpful for compositional generalization.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2304.11705.pdf' target='_blank'>https://arxiv.org/pdf/2304.11705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristiano Saltori, AljoÅ¡a OÅ¡ep, Elisa Ricci, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11705">Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain LiDAR semantic segmentation. However, do these methods generalize across domains? To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data, compared to $48.49$ mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains $34.88$ mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional decoder that learns to classify a birds-eye view of the point cloud. This simple auxiliary task encourages the 3D network to learn features that are robust to sensor placement shifts and resolution, and are transferable across domains. With this work, we aim to inspire the community to develop and evaluate future models in such cross-domain conditions.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2303.13662.pdf' target='_blank'>https://arxiv.org/pdf/2303.13662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyou Sun, Yaojie Liu, Xiaoming Liu, Yixuan Li, Wen-Sheng Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13662">Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work studies the generalization issue of face anti-spoofing (FAS) models on domain gaps, such as image resolution, blurriness and sensor variations. Most prior works regard domain-specific signals as a negative impact, and apply metric learning or adversarial losses to remove them from feature representation. Though learning a domain-invariant feature space is viable for the training data, we show that the feature shift still exists in an unseen test domain, which backfires on the generalizability of the classifier. In this work, instead of constructing a domain-invariant feature space, we encourage domain separability while aligning the live-to-spoof transition (i.e., the trajectory from live to spoof) to be the same for all domains. We formulate this FAS strategy of separability and alignment (SA-FAS) as a problem of invariant risk minimization (IRM), and learn domain-variant feature representation but domain-invariant classifier. We demonstrate the effectiveness of SA-FAS on challenging cross-domain FAS datasets and establish state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2301.12515.pdf' target='_blank'>https://arxiv.org/pdf/2301.12515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Fang, Dingfu Zhou, Jingjing Zhao, Chenming Wu, Chulin Tang, Cheng-Zhong Xu, Liangjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12515">LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past few years, there has been remarkable progress in research on 3D point clouds and their use in autonomous driving scenarios has become widespread. However, deep learning methods heavily rely on annotated data and often face domain generalization issues. Unlike 2D images whose domains usually pertain to the texture information present in them, the features derived from a 3D point cloud are affected by the distribution of the points. The lack of a 3D domain adaptation benchmark leads to the common practice of training a model on one benchmark (e.g. Waymo) and then assessing it on another dataset (e.g. KITTI). This setting results in two distinct domain gaps: scenarios and sensors, making it difficult to analyze and evaluate the method accurately. To tackle this problem, this paper presents LiDAR Dataset with Cross Sensors (LiDAR-CS Dataset), which contains large-scale annotated LiDAR point cloud under six groups of different sensors but with the same corresponding scenarios, captured from hybrid realistic LiDAR simulator. To our knowledge, LiDAR-CS Dataset is the first dataset that addresses the sensor-related gaps in the domain of 3D object detection in real traffic. Furthermore, we evaluate and analyze the performance using various baseline detectors and demonstrated its potential applications. Project page: https://opendriving.github.io/lidar-cs.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2301.10418.pdf' target='_blank'>https://arxiv.org/pdf/2301.10418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Liu, Lixu Wang, Lingjuan Lyu, Chen Sun, Xiao Wang, Qi Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10418">DEJA VU: Continual Model Generalization For Unseen Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation - in what we call the "Unfamiliar Period", especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models' target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a training-free data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA&DG in TDG, and achieves comparable TDA and FA capabilities.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2207.11971.pdf' target='_blank'>https://arxiv.org/pdf/2207.11971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingyi Chen, Xi Shen, Yahui Liu, Qinghua Tao, Johan A. K. Suykens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.11971">Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of Vision Transformer (ViT) in various computer vision tasks has promoted the ever-increasing prevalence of this convolution-free network. The fact that ViT works on image patches makes it potentially relevant to the problem of jigsaw puzzle solving, which is a classical self-supervised task aiming at reordering shuffled sequential image patches back to their natural form. Despite its simplicity, solving jigsaw puzzle has been demonstrated to be helpful for diverse tasks using Convolutional Neural Networks (CNNs), such as self-supervised feature representation learning, domain generalization, and fine-grained classification.
  In this paper, we explore solving jigsaw puzzle as a self-supervised auxiliary loss in ViT for image classification, named Jigsaw-ViT. We show two modifications that can make Jigsaw-ViT superior to standard ViT: discarding positional embeddings and masking patches randomly. Yet simple, we find that Jigsaw-ViT is able to improve both in generalization and robustness over the standard ViT, which is usually rather a trade-off. Experimentally, we show that adding the jigsaw puzzle branch provides better generalization than ViT on large-scale image classification on ImageNet. Moreover, the auxiliary task also improves robustness to noisy labels on Animal-10N, Food-101N, and Clothing1M as well as adversarial examples. Our implementation is available at https://yingyichen-cyy.github.io/Jigsaw-ViT/.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2204.10268.pdf' target='_blank'>https://arxiv.org/pdf/2204.10268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias C. Caro, Hsin-Yuan Huang, Nicholas Ezzell, Joe Gibbs, Andrew T. Sornborger, Lukasz Cincio, Patrick J. Coles, ZoÃ« Holmes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.10268">Out-of-distribution generalization for learning quantum dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization bounds are a critical tool to assess the training data requirements of Quantum Machine Learning (QML). Recent work has established guarantees for in-distribution generalization of quantum neural networks (QNNs), where training and testing data are drawn from the same data distribution. However, there are currently no results on out-of-distribution generalization in QML, where we require a trained model to perform well even on data drawn from a different distribution to the training distribution. Here, we prove out-of-distribution generalization for the task of learning an unknown unitary. In particular, we show that one can learn the action of a unitary on entangled states having trained only product states. Since product states can be prepared using only single-qubit gates, this advances the prospects of learning quantum dynamics on near term quantum hardware, and further opens up new methods for both the classical and quantum compilation of quantum circuits.
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2107.02378.pdf' target='_blank'>https://arxiv.org/pdf/2107.02378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Shu, Deyu Meng, Zongben Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.02378">Learning an Explicit Hyperparameter Prediction Function Conditioned on Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meta learning has attracted much attention recently in machine learning community. Contrary to conventional machine learning aiming to learn inherent prediction rules to predict labels for new query data, meta learning aims to learn the learning methodology for machine learning from observed tasks, so as to generalize to new query tasks by leveraging the meta-learned learning methodology. In this study, we interpret such learning methodology as learning an explicit hyper-parameter prediction function shared by all training tasks. Specifically, this function is represented as a parameterized function called meta-learner, mapping from a training/test task to its suitable hyper-parameter setting, extracted from a pre-specified function set called meta learning machine. Such setting guarantees that the meta-learned learning methodology is able to flexibly fit diverse query tasks, instead of only obtaining fixed hyper-parameters by many current meta learning methods, with less adaptability to query task's variations. Such understanding of meta learning also makes it easily succeed from traditional learning theory for analyzing its generalization bounds with general losses/tasks/models. The theory naturally leads to some feasible controlling strategies for ameliorating the quality of the extracted meta-learner, verified to be able to finely ameliorate its generalization capability in some typical meta learning applications, including few-shot regression, few-shot classification and domain generalization.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2510.06274.pdf' target='_blank'>https://arxiv.org/pdf/2510.06274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Mahdi Samiei Paqaleh, Arash Marioriyad, Arman Tahmasebi-Zadeh, Mohamadreza Fereydooni, Mahdi Ghaznavai, Mahdieh Soleymani Baghshah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06274">Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2510.02787.pdf' target='_blank'>https://arxiv.org/pdf/2510.02787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Zdenek, Wataru Shimoda, Kota Yamaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02787">OTR: Synthesizing Overlay Text Dataset for Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text removal is a crucial task in computer vision with applications such as privacy preservation, image editing, and media reuse. While existing research has primarily focused on scene text removal in natural images, limitations in current datasets hinder out-of-domain generalization or accurate evaluation. In particular, widely used benchmarks such as SCUT-EnsText suffer from ground truth artifacts due to manual editing, overly simplistic text backgrounds, and evaluation metrics that do not capture the quality of generated results. To address these issues, we introduce an approach to synthesizing a text removal benchmark applicable to domains other than scene texts. Our dataset features text rendered on complex backgrounds using object-aware placement and vision-language model-generated content, ensuring clean ground truth and challenging text removal scenarios. The dataset is available at https://huggingface.co/datasets/cyberagent/OTR .
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2509.24923.pdf' target='_blank'>https://arxiv.org/pdf/2509.24923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24923">When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2509.22115.pdf' target='_blank'>https://arxiv.org/pdf/2509.22115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Wang, Tao Yang, Hongtao Tian, Yunsheng Shi, Qiyao Ma, Xiaotao Liu, Ting Yao, Wenbo Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22115">Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling (D$^3$S)} framework that prioritizes the most informative samples and tokens across groups to improve the efficient of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\text{Var}(A)$). We theoretically proven that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance and generalization while requiring \textit{fewer} samples and tokens across diverse reasoning benchmarks. Our code is added in the supplementary materials and will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2509.15330.pdf' target='_blank'>https://arxiv.org/pdf/2509.15330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Zhang, Bo Jiang, Jie Zhou, Yimeng Liu, Xin Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15330">CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in pre-training vision-language models (VLMs), e.g., contrastive language-image pre-training (CLIP) methods, have shown great potential in learning out-of-distribution (OOD) representations. Despite showing competitive performance, the prompt-based CLIP methods still suffer from: i) inaccurate text descriptions, which leads to degraded accuracy and robustness, and poses a challenge for zero-shot CLIP methods. ii) limited vision-language embedding alignment, which significantly affects the generalization performance. To tackle the above issues, this paper proposes a novel Conditional Domain prompt Learning (CoDoL) method, which utilizes readily-available domain information to form prompts and improves the vision-language embedding alignment for improving OOD generalization. To capture both instance-specific and domain-specific information, we further propose a lightweight Domain Meta Network (DMN) to generate input-conditional tokens for images in each domain. Extensive experiments on four OOD benchmarks (PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed CoDoL in terms of improving the vision-language embedding alignment as well as the out-of-distribution generalization performance.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2509.14420.pdf' target='_blank'>https://arxiv.org/pdf/2509.14420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Lin, Xiaolin Wu, Xi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14420">Class-invariant Test-Time Augmentation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep models often suffer significant performance degradation under distribution shifts. Domain generalization (DG) seeks to mitigate this challenge by enabling models to generalize to unseen domains. Most prior approaches rely on multi-domain training or computationally intensive test-time adaptation. In contrast, we propose a complementary strategy: lightweight test-time augmentation. Specifically, we develop a novel Class-Invariant Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple variants of each input image through elastic and grid deformations that nevertheless belong to the same class as the original input. Their predictions are aggregated through a confidence-guided filtering scheme that remove unreliable outputs, ensuring the final decision relies on consistent and trustworthy cues. Extensive Experiments on PACS and Office-Home datasets demonstrate consistent gains across different DG algorithms and backbones, highlighting the effectiveness and generality of our approach.
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2509.00351.pdf' target='_blank'>https://arxiv.org/pdf/2509.00351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marzi Heidari, Yuhong Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00351">Target-Oriented Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep models trained on a single source domain often fail catastrophically under distribution shifts, a critical challenge in Single Domain Generalization (SDG). While existing methods focus on augmenting source data or learning invariant features, they neglect a readily available resource: textual descriptions of the target deployment environment. We propose Target-Oriented Single Domain Generalization (TO-SDG), a novel problem setup that leverages the textual description of the target domain, without requiring any target data, to guide model generalization. To address TO-SDG, we introduce Spectral TARget Alignment (STAR), a lightweight module that injects target semantics into source features by exploiting visual-language models (VLMs) such as CLIP. STAR uses a target-anchored subspace derived from the text embedding of the target description to recenter image features toward the deployment domain, then utilizes spectral projection to retain directions aligned with target cues while discarding source-specific noise. Moreover, we use a vision-language distillation to align backbone features with VLM's semantic geometry. STAR further employs feature-space Mixup to ensure smooth transitions between source and target-oriented representations. Experiments across various image classification and object detection benchmarks demonstrate STAR's superiority. This work establishes that minimal textual metadata, which is a practical and often overlooked resource, significantly enhances generalization under severe data constraints, opening new avenues for deploying robust models in target environments with unseen data.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2508.12798.pdf' target='_blank'>https://arxiv.org/pdf/2508.12798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Damian Machlanski, Stephanie Riley, Edward Moroshko, Kurt Butler, Panagiotis Dimitrakopoulos, Thomas Melistas, Akchunya Chanchal, Steven McDonagh, Ricardo Silva, Sotirios A. Tsaftaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12798">A Shift in Perspective on Causality in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promise that causal modelling can lead to robust AI generalization has been challenged in recent work on domain generalization (DG) benchmarks. We revisit the claims of the causality and DG literature, reconciling apparent contradictions and advocating for a more nuanced theory of the role of causality in generalization. We also provide an interactive demo at https://chai-uk.github.io/ukairs25-causal-predictors/.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2508.05135.pdf' target='_blank'>https://arxiv.org/pdf/2508.05135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thinh Nguyen, Trung Phan, Binh T. Nguyen, Khoa D Doan, Kok-Seng Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05135">HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) is a decentralized approach where multiple clients collaboratively train a shared global model without sharing their raw data. Despite its effectiveness, conventional FL faces scalability challenges due to excessive computational and communication demands placed on a single central server as the number of participating devices grows. Hierarchical Federated Learning (HFL) addresses these issues by distributing model aggregation tasks across intermediate nodes (stations), thereby enhancing system scalability and robustness against single points of failure. However, HFL still suffers from a critical yet often overlooked limitation: domain shift, where data distributions vary significantly across different clients and stations, reducing model performance on unseen target domains. While Federated Domain Generalization (FedDG) methods have emerged to improve robustness to domain shifts, their integration into HFL frameworks remains largely unexplored. In this paper, we formally introduce Hierarchical Federated Domain Generalization (HFedDG), a novel scenario designed to investigate domain shift within hierarchical architectures. Specifically, we propose HFedATM, a hierarchical aggregation method that first aligns the convolutional filters of models from different stations through Filter-wise Optimal Transport Alignment and subsequently merges aligned models using a Shrinkage-aware Regularized Mean Aggregation. Our extensive experimental evaluations demonstrate that HFedATM significantly boosts the performance of existing FedDG baselines across multiple datasets and maintains computational and communication efficiency. Moreover, theoretical analyses indicate that HFedATM achieves tighter generalization error bounds compared to standard hierarchical averaging, resulting in faster convergence and stable training behavior.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2508.02458.pdf' target='_blank'>https://arxiv.org/pdf/2508.02458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Yichao, Haoran Luo, Lang Feng, Shuai Zhao, Anh Tuan Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02458">From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models show promise in emotion understanding, social reasoning, and empathy, yet they struggle with psychologically grounded tasks that require inferring implicit mental states in context-rich, ambiguous settings. These limitations arise from the absence of theory-aligned supervision and the difficulty of capturing nuanced mental processes in real-world narratives. To address this gap, we leverage expert-labeled, psychologically rich scenarios and propose a trajectory-aware reinforcement learning framework that explicitly imitates expert psychological thought patterns. By integrating real-world stimuli with structured reasoning guidance, our approach enables compact models to internalize social-cognitive principles, perform nuanced psychological inference, and support continual self-improvement. Comprehensive experiments across multiple benchmarks further demonstrate that our models achieve expert-level interpretive capabilities, exhibiting strong out-of-distribution generalization and robust continual learning across diverse, challenging, and psychologically grounded tasks.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2508.01602.pdf' target='_blank'>https://arxiv.org/pdf/2508.01602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lubin Gan, Jing Zhang, Linhao Qu, Yijun Wang, Siying Wu, Xiaoyan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01602">Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fine-grained classification of brain tumor subtypes from histopathological whole slide images is highly challenging due to subtle morphological variations and the scarcity of annotated data. Although vision-language models have enabled promising zero-shot classification, their ability to capture fine-grained pathological features remains limited, resulting in suboptimal subtype discrimination. To address these challenges, we propose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot framework tailored for digital pathology. FG-PAN consists of two key modules: (1) a local feature refinement module that enhances patch-level visual features by modeling spatial relationships among representative patches, and (2) a fine-grained text description generation module that leverages large language models to produce pathology-aware, class-specific semantic prototypes. By aligning refined visual features with LLM-generated fine-grained descriptions, FG-PAN effectively increases class separability in both visual and semantic spaces. Extensive experiments on multiple public pathology datasets, including EBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance and robust generalization in zero-shot brain tumor subtype classification.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2507.21367.pdf' target='_blank'>https://arxiv.org/pdf/2507.21367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>I-Hsiang Chen, Hua-En Chang, Wei-Ting Chen, Jenq-Neng Hwang, Sy-Yen Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21367">Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging task, as domain shifts in unseen environments can severely compromise model performance. While recent studies enhance feature alignment by projecting features into the source domain, they often neglect intrinsic latent domain priors, leading to suboptimal results. In this paper, we introduce PDAF, a Probabilistic Diffusion Alignment Framework that enhances the generalization of existing segmentation networks through probabilistic diffusion modeling. PDAF introduces a Latent Domain Prior (LDP) to capture domain shifts and uses this prior as a conditioning factor to align both source and unseen target domains. To achieve this, PDAF integrates into a pre-trained segmentation model and utilizes paired source and pseudo-target images to simulate latent domain shifts, enabling LDP modeling. The framework comprises three modules: the Latent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the Domain Compensation Module (DCM) adjusts feature representations to mitigate domain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion process to estimate the LDP without requiring paired samples. This design enables PDAF to iteratively model domain shifts, progressively refining feature representations to enhance generalization under complex target conditions. Extensive experiments validate the effectiveness of PDAF across diverse and challenging urban scenes.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2507.17281.pdf' target='_blank'>https://arxiv.org/pdf/2507.17281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanli Zhuo, Leilei Ma, Haifeng Zhao, Shiwei Zhou, Dengdi Sun, Yanping Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17281">Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although SAM-based single-source domain generalization models for medical image segmentation can mitigate the impact of domain shift on the model in cross-domain scenarios, these models still face two major challenges. First, the segmentation of SAM is highly dependent on domain-specific expert-annotated prompts, which prevents SAM from achieving fully automated medical image segmentation and therefore limits its application in clinical settings. Second, providing poor prompts (such as bounding boxes that are too small or too large) to the SAM prompt encoder can mislead SAM into generating incorrect mask results. Therefore, we propose the FA-SAM, a single-source domain generalization framework for medical image segmentation that achieves fully automated SAM. FA-SAM introduces two key innovations: an Auto-prompted Generation Model (AGM) branch equipped with a Shallow Feature Uncertainty Modeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module integrated into the SAM mask decoder. Specifically, AGM models the uncertainty distribution of shallow features through the SUFM module to generate bounding box prompts for the target domain, enabling fully automated segmentation with SAM. The IPEF module integrates multiscale information from SAM image embeddings and prompt embeddings to capture global and local details of the target object, enabling SAM to mitigate the impact of poor prompts. Extensive experiments on publicly available prostate and fundus vessel datasets validate the effectiveness of FA-SAM and highlight its potential to address the above challenges.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2507.14227.pdf' target='_blank'>https://arxiv.org/pdf/2507.14227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khoi Do, Duong Nguyen, Nam-Khanh Le, Quoc-Viet Pham, Binh-Son Hua, Won-Joo Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14227">Domain Generalization via Pareto Optimal Gradient Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we address the gradient-based domain generalization problem, where predictors aim for consistent gradient directions across different domains. Existing methods have two main challenges. First, minimization of gradient empirical distance or gradient inner products (GIP) leads to gradient fluctuations among domains, thereby hindering straightforward learning. Second, the direct application of gradient learning to the joint loss function can incur high computation overheads due to second-order derivative approximation. To tackle these challenges, we propose a new Pareto Optimality Gradient Matching (POGM) method. In contrast to existing methods that add gradient matching as regularization, we leverage gradient trajectories as collected data and apply independent training at the meta-learner. In the meta-update, we maximize GIP while limiting the learned gradient from deviating too far from the empirical risk minimization gradient trajectory. By doing so, the aggregate gradient can incorporate knowledge from all domains without suffering gradient fluctuation towards any particular domain. Experimental evaluations on datasets from DomainBed demonstrate competitive results yielded by POGM against other baselines while achieving computational efficiency.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2507.07572.pdf' target='_blank'>https://arxiv.org/pdf/2507.07572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupu Liang, Yaping Zhang, Zhiyang Zhang, Yang Zhao, Lu Xiang, Chengqing Zong, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07572">Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Document Image Machine Translation (DIMT) aims to translate text within document images, facing generalization challenges due to limited training data and the complex interplay between visual and textual information. To address these challenges, we introduce M4Doc, a novel single-to-mix modality alignment framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an image-only encoder with the multimodal representations of an MLLM, pre-trained on large-scale document image datasets. This alignment enables a lightweight DIMT model to learn crucial visual-textual correlations during training. During inference, M4Doc bypasses the MLLM, maintaining computational efficiency while benefiting from its multimodal knowledge. Comprehensive experiments demonstrate substantial improvements in translation quality, especially in cross-domain generalization and challenging document image scenarios.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2507.05823.pdf' target='_blank'>https://arxiv.org/pdf/2507.05823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tangzheng Lian, Guanyu Hu, Dimitrios Kollias, Xinyu Yang, Oya Celiktutan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05823">Fair Domain Generalization: An Information-Theoretic View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) and algorithmic fairness are two critical challenges in machine learning. However, most DG methods focus only on minimizing expected risk in the unseen target domain without considering algorithmic fairness. Conversely, fairness methods typically do not account for domain shifts, so the fairness achieved during training may not generalize to unseen test domains. In this work, we bridge these gaps by studying the problem of Fair Domain Generalization (FairDG), which aims to minimize both expected risk and fairness violations in unseen target domains. We derive novel mutual information-based upper bounds for expected risk and fairness violations in multi-class classification tasks with multi-group sensitive attributes. These bounds provide key insights for algorithm design from an information-theoretic perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal Fairness for Domain Generalization), a practical framework that solves the FairDG problem and models the utility-fairness trade-off through Pareto optimization. Experiments on real-world vision and language datasets show that PAFDG achieves superior utility-fairness trade-offs compared to existing methods.
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2506.15211.pdf' target='_blank'>https://arxiv.org/pdf/2506.15211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng He, Zijun Chen, Xinnian Liang, Tingting Ma, Yunqi Qiu, Shuangzhi Wu, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15211">ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain generalization capabilities. However, the underlying mechanisms supporting such transfer remain poorly understood. We hypothesize that cross-domain generalization arises from shared abstract reasoning prototypes -- fundamental reasoning patterns that capture the essence of problems across domains. These prototypes minimize the nuances of the representation, revealing that seemingly diverse tasks are grounded in shared reasoning structures.Based on this hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning ability of LLMs by leveraging scalable and verifiable prototypical representations (Prolog for logical reasoning, PDDL for planning).ProtoReasoning features: (1) an automated prototype construction pipeline that transforms problems into corresponding prototype representations; (2) a comprehensive verification system providing reliable feedback through Prolog/PDDL interpreters; (3) the scalability to synthesize problems arbitrarily within prototype space while ensuring correctness. Extensive experiments show that ProtoReasoning achieves 4.7% improvement over baseline models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics (AIME24). Significantly, our ablation studies confirm that learning in prototype space also demonstrates enhanced generalization to structurally similar problems compared to training solely on natural language representations, validating our hypothesis that reasoning prototypes serve as the foundation for generalizable reasoning in large language models.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2506.11033.pdf' target='_blank'>https://arxiv.org/pdf/2506.11033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjae Kwon, Tyler Ingebrand, Ufuk Topcu, Lu Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11033">Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variations in hidden parameters, such as a robot's mass distribution or friction, pose safety risks during execution. We develop a runtime shielding mechanism for reinforcement learning, building on the formalism of constrained hidden-parameter Markov decision processes. Function encoders enable real-time inference of hidden parameters from observations, allowing the shield and the underlying policy to adapt online. The shield constrains the action space by forecasting future safety risks (such as obstacle proximity) and accounts for uncertainty via conformal prediction. We prove that the proposed mechanism satisfies probabilistic safety guarantees and yields optimal policies among the set of safety-compliant policies. Experiments across diverse environments with varying hidden parameters show that our method significantly reduces safety violations and achieves strong out-of-distribution generalization, while incurring minimal runtime overhead.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2506.07899.pdf' target='_blank'>https://arxiv.org/pdf/2506.07899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Wang, Yiming Qin, Nikolaos Dimitriadis, Alessandro Favero, Pascal Frossard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07899">MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably-without retraining or forgetting previous information-remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through sample-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks for LLaMA-3 and Mistral backbones demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting.
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2506.03709.pdf' target='_blank'>https://arxiv.org/pdf/2506.03709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aniruddh Sikdar, Aditya Gandhamal, Suresh Sundaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03709">AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary semantic segmentation (OVSS) involves assigning labels to each pixel in an image based on textual descriptions, leveraging world models like CLIP. However, they encounter significant challenges in cross-domain generalization, hindering their practical efficacy in real-world applications. Embodied AI systems are transforming autonomous navigation for ground vehicles and drones by enhancing their perception abilities, and in this study, we present AetherVision-Bench, a benchmark for multi-angle segmentation across aerial, and ground perspectives, which facilitates an extensive evaluation of performance across different viewing angles and sensor modalities. We assess state-of-the-art OVSS models on the proposed benchmark and investigate the key factors that impact the performance of zero-shot transfer models. Our work pioneers the creation of a robustness benchmark, offering valuable insights and establishing a foundation for future research.
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2506.00649.pdf' target='_blank'>https://arxiv.org/pdf/2506.00649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neil De La Fuente, Oscar Sainz, Iker GarcÃ­a-Ferrero, Eneko Agirre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00649">GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2505.21010.pdf' target='_blank'>https://arxiv.org/pdf/2505.21010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sabbir Ahmed, Mamshad Nayeem Rizve, Abdullah Al Arafat, Jacqueline Liu, Rahim Hossain, Mohaiminul Al Nahian, Adnan Siraj Rakin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21010">Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-Supervised Federated Learning (SSFL) is gaining popularity over conventional Federated Learning in many real-world applications. Due to the practical limitation of limited labeled data on the client side, SSFL considers that participating clients train with unlabeled data, and only the central server has the necessary resources to access limited labeled data, making it an ideal fit for real-world applications (e.g., healthcare). However, traditional SSFL assumes that the data distributions in the training phase and testing phase are the same. In practice, however, domain shifts frequently occur, making it essential for SSFL to incorporate generalization capabilities and enhance their practicality. The core challenge is improving model generalization to new, unseen domains while the client participate in SSFL. However, the decentralized setup of SSFL and unsupervised client training necessitates innovation to achieve improved generalization across domains. To achieve this, we propose a novel framework called the Unified Alignment Protocol (UAP), which consists of an alternating two-stage training process. The first stage involves training the server model to learn and align the features with a parametric distribution, which is subsequently communicated to clients without additional communication overhead. The second stage proposes a novel training algorithm that utilizes the server feature distribution to align client features accordingly. Our extensive experiments on standard domain generalization benchmark datasets across multiple model architectures reveal that proposed UAP successfully achieves SOTA generalization performance in SSFL setting.
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2505.19213.pdf' target='_blank'>https://arxiv.org/pdf/2505.19213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohao Rui, Kaitao Chen, Weijie Ma, Xiaosong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19213">Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in reinforcement learning with verifiable, rule-based rewards have greatly enhanced the reasoning capabilities and out-of-distribution generalization of VLMs/LLMs, obviating the need for manually crafted reasoning chains. Despite these promising developments in the general domain, their translation to medical imaging remains limited. Current medical reinforcement fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby restricting the model's ability to engage in world knowledge retrieval and flexible task adaptation. More critically, these methods fall short of addressing the critical clinical demand for open-ended, reasoning-intensive decision-making. To bridge this gap, we introduce \textbf{MedCCO}, the first multimodal reinforcement learning framework tailored for medical VQA that unifies close-ended and open-ended data within a curriculum-driven RFT paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of close-ended medical VQA tasks to establish domain-grounded reasoning capabilities, and is then progressively adapted to open-ended tasks to foster deeper knowledge enhancement and clinical interpretability. We validate MedCCO across eight challenging medical VQA benchmarks, spanning both close-ended and open-ended settings. Experimental results show that MedCCO consistently enhances performance and generalization, achieving a 11.4\% accuracy gain across three in-domain tasks, and a 5.7\% improvement on five out-of-domain benchmarks. These findings highlight the promise of curriculum-guided RL in advancing robust, clinically-relevant reasoning in medical multimodal language models.
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2505.14808.pdf' target='_blank'>https://arxiv.org/pdf/2505.14808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soo Min Kwon, Alec S. Xu, Can Yaras, Laura Balzano, Qing Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14808">Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to demystify the out-of-distribution (OOD) capabilities of in-context learning (ICL) by studying linear regression tasks parameterized with low-rank covariance matrices. With such a parameterization, we can model distribution shifts as a varying angle between the subspace of the training and testing covariance matrices. We prove that a single-layer linear attention model incurs a test risk with a non-negligible dependence on the angle, illustrating that ICL is not robust to such distribution shifts. However, using this framework, we also prove an interesting property of ICL: when trained on task vectors drawn from a union of low-dimensional subspaces, ICL can generalize to any subspace within their span, given sufficiently long prompt lengths. This suggests that the OOD generalization ability of Transformers may actually stem from the new task lying within the span of those encountered during training. We empirically show that our results also hold for models such as GPT-2, and conclude with (i) experiments on how our observations extend to nonlinear function classes and (ii) results on how LoRA has the ability to capture distribution shifts.
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2505.11164.pdf' target='_blank'>https://arxiv.org/pdf/2505.11164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Rudin, Junzhe He, Joshua Aurand, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11164">Parkour in the Wild: Learning a General and Extensible Agile Locomotion Policy Using Multi-expert Distillation and RL Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Legged robots are well-suited for navigating terrains inaccessible to wheeled robots, making them ideal for applications in search and rescue or space exploration. However, current control methods often struggle to generalize across diverse, unstructured environments. This paper introduces a novel framework for agile locomotion of legged robots by combining multi-expert distillation with reinforcement learning (RL) fine-tuning to achieve robust generalization. Initially, terrain-specific expert policies are trained to develop specialized locomotion skills. These policies are then distilled into a unified foundation policy via the DAgger algorithm. The distilled policy is subsequently fine-tuned using RL on a broader terrain set, including real-world 3D scans. The framework allows further adaptation to new terrains through repeated fine-tuning. The proposed policy leverages depth images as exteroceptive inputs, enabling robust navigation across diverse, unstructured terrains. Experimental results demonstrate significant performance improvements over existing methods in synthesizing multi-terrain skills into a single controller. Deployment on the ANYmal D robot validates the policy's ability to navigate complex environments with agility and robustness, setting a new benchmark for legged robot locomotion.
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2504.20568.pdf' target='_blank'>https://arxiv.org/pdf/2504.20568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danilo Avola, Federica Bruni, Gian Luca Foresti, Daniele Pannone, Amedeo Ranaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20568">Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze environments, enabling tasks such as tracking people, detecting intrusions, and recognizing gestures. The rise of this technology is driven by the IEEE 802.11bf standard and growing demand for tools that can ensure privacy and operate through obstacles. However, the performance of Wi-Fi sensing is heavily influenced by environmental conditions, especially when extracting spatial and temporal features from the surrounding scene. A key challenge is achieving robust generalization across domains, ensuring stable performance even when the sensing environment changes significantly. This paper introduces a novel deep learning model for cross-domain adaptation of Wi-Fi signals, inspired by physical signal shielding. The model uses a Relativistic average Generative Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM) architectures for both the generator and discriminator. To simulate physical shielding, an acrylic box lined with electromagnetic shielding fabric was constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from various materials both inside (domain-free) and outside (domain-dependent) the box to train the model. A multi-class Support Vector Machine (SVM) was trained on domain-free spectra and tested on signals denoised by the RaGAN. The system achieved 96% accuracy and demonstrated strong material discrimination capabilities, offering potential for use in security applications to identify concealed objects based on their composition.
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2503.22309.pdf' target='_blank'>https://arxiv.org/pdf/2503.22309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zakaria Laskar, Tomas Vojir, Matej Grcic, Iaroslav Melekhov, Shankar Gangisettye, Juho Kannala, Jiri Matas, Giorgos Tolias, C. V. Jawahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22309">A Dataset for Semantic Segmentation in the Presence of Unknowns</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Before deployment in the real-world deep neural networks require thorough evaluation of how they handle both knowns, inputs represented in the training data, and unknowns (anomalies). This is especially important for scene understanding tasks with safety critical applications, such as in autonomous driving. Existing datasets allow evaluation of only knowns or unknowns - but not both, which is required to establish "in the wild" suitability of deep neural network models. To bridge this gap, we propose a novel anomaly segmentation dataset, ISSU, that features a diverse set of anomaly inputs from cluttered real-world environments. The dataset is twice larger than existing anomaly segmentation datasets, and provides a training, validation and test set for controlled in-domain evaluation. The test set consists of a static and temporal part, with the latter comprised of videos. The dataset provides annotations for both closed-set (knowns) and anomalies, enabling closed-set and open-set evaluation. The dataset covers diverse conditions, such as domain and cross-sensor shift, illumination variation and allows ablation of anomaly detection methods with respect to these variations. Evaluation results of current state-of-the-art methods confirm the need for improvements especially in domain-generalization, small and large object segmentation.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2503.08271.pdf' target='_blank'>https://arxiv.org/pdf/2503.08271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhe Niu, Zongxia Xie, Yanru Sun, Wei He, Man Xu, Chao Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08271">LangTime: A Language-Guided Unified Model for Time Series Forecasting with Proximal Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has shown an increasing interest in utilizing pre-trained large language models (LLMs) for a variety of time series applications. However, there are three main challenges when using LLMs as foundational models for time series forecasting: (1) Cross-domain generalization. (2) Cross-modality alignment. (3) Error accumulation in autoregressive frameworks. To address these challenges, we proposed LangTime, a language-guided unified model for time series forecasting that incorporates cross-domain pre-training with reinforcement learning-based fine-tuning. Specifically, LangTime constructs Temporal Comprehension Prompts (TCPs), which include dataset-wise and channel-wise instructions, to facilitate domain adaptation and condense time series into a single token, enabling LLMs to understand better and align temporal data. To improve autoregressive forecasting, we introduce TimePPO, a reinforcement learning-based fine-tuning algorithm. TimePPO mitigates error accumulation by leveraging a multidimensional rewards function tailored for time series and a repeat-based value estimation strategy. Extensive experiments demonstrate that LangTime achieves state-of-the-art cross-domain forecasting performance, while TimePPO fine-tuning effectively enhances the stability and accuracy of autoregressive forecasting.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2503.06288.pdf' target='_blank'>https://arxiv.org/pdf/2503.06288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yan, Marzi Heidari, Yuhong Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06288">Single Domain Generalization with Adversarial Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to train models that can generalize to unseen testing domains by leveraging data from multiple training domains. However, traditional DG methods rely on the availability of multiple diverse training domains, limiting their applicability in data-constrained scenarios. Single Domain Generalization (SDG) addresses the more realistic and challenging setting by restricting the training data to a single domain distribution. The main challenges in SDG stem from the limited diversity of training data and the inaccessibility of unseen testing data distributions. To tackle these challenges, we propose a single domain generalization method that leverages an adversarial memory bank to augment training features. Our memory-based feature augmentation network maps both training and testing features into an invariant subspace spanned by diverse memory features, implicitly aligning the training and testing domains in the projected space. To maintain a diverse and representative feature memory bank, we introduce an adversarial feature generation method that creates features extending beyond the training domain distribution. Experimental results demonstrate that our approach achieves state-of-the-art performance on standard single domain generalization benchmarks.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2502.16366.pdf' target='_blank'>https://arxiv.org/pdf/2502.16366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Dobre, Mehrnaz Mofakhami, Sophie Xhonneux, Leo Schwinn, Gauthier Gidel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16366">A Generative Approach to LLM Harmfulness Mitigation with Red Flag Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many safety post-training methods for large language models (LLMs) are designed to modify the model's behaviour from producing unsafe answers to issuing refusals. However, such distribution shifts are often brittle and degrade performance on desirable tasks. To address these pitfalls, we propose augmenting the model's vocabulary with a special red flag token, and training the model to insert this token whenever harmful content is generated or imminent. This approach enables the model to explicitly learn the concept of harmfulness in its representations, with minimal impact on utility due to the marginal change in the generated distribution of natural language. Moreover, because the token is embedded in the model's vocabulary, we can naturally leverage the LLMs' generalization capabilities, such as in-context learning (ICL) and out-of-distribution generalization to languages that are not formally supported (e.g., Japanese for Llama3). In particular, we demonstrate that through ICL alone, the model can learn to initiate reflective reasoning upon generating the red flag token at inference, which steers the response away from harmful continuations or enables self-correction when the flag is raised falsely. This approach is orthogonal and complementary to existing safety technique (such as safety classifiers or standard safety training) and easier to evaluate in comparison to natural language refusals, as it does not require a human or automated judge to assess the harmlessness of the answers.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2502.16064.pdf' target='_blank'>https://arxiv.org/pdf/2502.16064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marzi Heidari, Yuhong Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16064">Single Domain Generalization with Model-aware Parametric Batch-wise Mixup</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Domain Generalization (SDG) remains a formidable challenge in the field of machine learning, particularly when models are deployed in environments that differ significantly from their training domains. In this paper, we propose a novel data augmentation approach, named as Model-aware Parametric Batch-wise Mixup (MPBM), to tackle the challenge of SDG. MPBM deploys adversarial queries generated with stochastic gradient Langevin dynamics, and produces model-aware augmenting instances with a parametric batch-wise mixup generator network that is carefully designed through an innovative attention mechanism. By exploiting inter-feature correlations, the parameterized mixup generator introduces additional versatility in combining features across a batch of instances, thereby enhancing the capacity to generate highly adaptive and informative synthetic instances for specific queries. The synthetic data produced by this adaptable generator network, guided by informative queries, is expected to significantly enrich the representation space covered by the original training dataset and subsequently enhance the prediction model's generalizability across diverse and previously unseen domains. To prevent excessive deviation from the training data, we further incorporate a real-data alignment-based adversarial loss into the learning process of MPBM, regularizing any tendencies toward undesirable expansions. We conduct extensive experiments on several benchmark datasets. The empirical results demonstrate that by augmenting the training set with informative synthesis data, our proposed MPBM method achieves the state-of-the-art performance for single domain generalization.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2501.15486.pdf' target='_blank'>https://arxiv.org/pdf/2501.15486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Gupta, Vinay Sutar, Varunav Singh, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15486">FedAlign: Federated Domain Generalization with Cross-Client Feature Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) offers a decentralized paradigm for collaborative model training without direct data sharing, yet it poses unique challenges for Domain Generalization (DG), including strict privacy constraints, non-i.i.d. local data, and limited domain diversity. We introduce FedAlign, a lightweight, privacy-preserving framework designed to enhance DG in federated settings by simultaneously increasing feature diversity and promoting domain invariance. First, a cross-client feature extension module broadens local domain representations through domain-invariant feature perturbation and selective cross-client feature transfer, allowing each client to safely access a richer domain space. Second, a dual-stage alignment module refines global feature learning by aligning both feature embeddings and predictions across clients, thereby distilling robust, domain-invariant features. By integrating these modules, our method achieves superior generalization to unseen domains while maintaining data privacy and operating with minimal computational and communication overhead.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2501.09688.pdf' target='_blank'>https://arxiv.org/pdf/2501.09688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiho Choi, Seonho Lee, Minhyun Lee, Seungho Lee, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09688">Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing fine-grained parts in unseen categories. We identify two primary challenges in OVPS: (1) the difficulty in aligning part-level image-text correspondence, and (2) the lack of structural understanding in segmenting object parts. To address these issues, we propose PartCATSeg, a novel framework that integrates object-aware part-level cost aggregation, compositional loss, and structural guidance from DINO. Our approach employs a disentangled cost aggregation strategy that handles object and part-level costs separately, enhancing the precision of part-level segmentation. We also introduce a compositional loss to better capture part-object relationships, compensating for the limited part annotations. Additionally, structural guidance from DINO features improves boundary delineation and inter-part understanding. Extensive experiments on Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that our method significantly outperforms state-of-the-art approaches, setting a new baseline for robust generalization to unseen part categories.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2501.03466.pdf' target='_blank'>https://arxiv.org/pdf/2501.03466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Liu, Yudong Zhang, Shuihua Wang, Siyue Li, Jin Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03466">DGSSA: Domain generalization with structural and stylistic augmentation for retinal vessel segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retinal vascular morphology is crucial for diagnosing diseases such as diabetes, glaucoma, and hypertension, making accurate segmentation of retinal vessels essential for early intervention. Traditional segmentation methods assume that training and testing data share similar distributions, which can lead to poor performance on unseen domains due to domain shifts caused by variations in imaging devices and patient demographics. This paper presents a novel approach, DGSSA, for retinal vessel image segmentation that enhances model generalization by combining structural and style augmentation strategies. We utilize a space colonization algorithm to generate diverse vascular-like structures that closely mimic actual retinal vessels, which are then used to generate pseudo-retinal images with an improved Pix2Pix model, allowing the segmentation model to learn a broader range of structure distributions. Additionally, we utilize PixMix to implement random photometric augmentations and introduce uncertainty perturbations, thereby enriching stylistic diversity and significantly enhancing the model's adaptability to varying imaging conditions. Our framework has been rigorously evaluated on four challenging datasets-DRIVE, CHASEDB, HRF, and STARE-demonstrating state-of-the-art performance that surpasses existing methods. This validates the effectiveness of our proposed approach, highlighting its potential for clinical application in automated retinal vessel analysis.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2412.12793.pdf' target='_blank'>https://arxiv.org/pdf/2412.12793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shizhuo Deng, Bowen Han, Jiaqi Chen, Hao Wang, Dongyue Chen, Tong Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12793">CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Noisy labels threaten the robustness of few-shot learning (FSL) due to the inexact features in a new domain. CLIP, a large-scale vision-language model, performs well in FSL on image-text embedding similarities, but it is susceptible to misclassification caused by noisy labels. How to enhance domain generalization of CLIP on noisy data within FSL tasks is a critical challenge. In this paper, we provide a novel view to mitigate the influence of noisy labels, CLIP-based Robust Few-shot learning (CRoF). CRoF is a general plug-in module for CLIP-based models. To avoid misclassification and confused label embedding, we design the few-shot task-oriented prompt generator to give more discriminative descriptions of each category. The proposed prompt achieves larger distances of inter-class textual embedding. Furthermore, rather than fully trusting zero-shot classification by CLIP, we fine-tune CLIP on noisy few-shot data in a new domain with a weighting strategy like label-smooth. The weights for multiple potentially correct labels consider the relationship between CLIP's prior knowledge and original label information to ensure reliability. Our multiple label loss function further supports robust training under this paradigm. Comprehensive experiments show that CRoF, as a plug-in, outperforms fine-tuned and vanilla CLIP models on different noise types and noise ratios.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2412.05269.pdf' target='_blank'>https://arxiv.org/pdf/2412.05269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krzysztof Maziarz, Guoqing Liu, Hubert Misztela, Austin Tripp, Junren Li, Aleksei Kornev, Piotr GaiÅski, Holger Hoefling, Mike Fortunato, Rishi Gupta, Marwin Segler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05269">Chemist-aligned retrosynthesis by ensembling diverse inductive bias models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chemical synthesis remains a critical bottleneck in the discovery and manufacture of functional small molecules. AI-based synthesis planning models could be a potential remedy to find effective syntheses, and have made progress in recent years. However, they still struggle with less frequent, yet critical reactions for synthetic strategy, as well as hallucinated, incorrect predictions. This hampers multi-step search algorithms that rely on models, and leads to misalignment with chemists' expectations. Here we propose RetroChimera: a frontier retrosynthesis model, built upon two newly developed components with complementary inductive biases, which we fuse together using a new framework for integrating predictions from multiple sources via a learning-based ensembling strategy. Through experiments across several orders of magnitude in data scale and splitting strategy, we show RetroChimera outperforms all major models by a large margin, demonstrating robustness outside the training data, as well as for the first time the ability to learn from even a very small number of examples per reaction class. Moreover, industrial organic chemists prefer predictions from RetroChimera over the reactions it was trained on in terms of quality, revealing high levels of alignment. Finally, we demonstrate zero-shot transfer to an internal dataset from a major pharmaceutical company, showing robust generalization under distribution shift. With the new dimension that our ensembling framework unlocks, we anticipate further acceleration in the development of even more accurate models.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2411.16142.pdf' target='_blank'>https://arxiv.org/pdf/2411.16142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaobin Mo, Qingyuan Liu, Baohua Yan, Longxiang Zhang, Xuan Di
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16142">Causal Adjacency Learning for Spatiotemporal Prediction Over Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatiotemporal prediction over graphs (STPG) is crucial for transportation systems. In existing STPG models, an adjacency matrix is an important component that captures the relations among nodes over graphs. However, most studies calculate the adjacency matrix by directly memorizing the data, such as distance- and correlation-based matrices. These adjacency matrices do not consider potential pattern shift for the test data, and may result in suboptimal performance if the test data has a different distribution from the training one. This issue is known as the Out-of-Distribution generalization problem. To address this issue, in this paper we propose a Causal Adjacency Learning (CAL) method to discover causal relations over graphs. The learned causal adjacency matrix is evaluated on a downstream spatiotemporal prediction task using real-world graph data. Results demonstrate that our proposed adjacency matrix can capture the causal relations, and using our learned adjacency matrix can enhance prediction performance on the OOD test data, even though causal learning is not conducted in the downstream task.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2411.08606.pdf' target='_blank'>https://arxiv.org/pdf/2411.08606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengwei Yin, Jingjing Wang, Guanzhong Zeng, Di Xie, Jiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08606">LG-Gaze: Learning Geometry-aware Continuous Prompts for Language-Guided Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability of gaze estimation models to generalize is often significantly hindered by various factors unrelated to gaze, especially when the training dataset is limited. Current strategies aim to address this challenge through different domain generalization techniques, yet they have had limited success due to the risk of overfitting when solely relying on value labels for regression. Recent progress in pre-trained vision-language models has motivated us to capitalize on the abundant semantic information available. We propose a novel approach in this paper, reframing the gaze estimation task as a vision-language alignment issue. Our proposed framework, named Language-Guided Gaze Estimation (LG-Gaze), learns continuous and geometry-sensitive features for gaze estimation benefit from the rich prior knowledges of vision-language models. Specifically, LG-Gaze aligns gaze features with continuous linguistic features through our proposed multimodal contrastive regression loss, which customizes adaptive weights for different negative samples. Furthermore, to better adapt to the labels for gaze estimation task, we propose a geometry-aware interpolation method to obtain more precise gaze embeddings. Through extensive experiments, we validate the efficacy of our framework in four different cross-domain evaluation tasks.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2410.12953.pdf' target='_blank'>https://arxiv.org/pdf/2410.12953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Agrawal, Aniruddh Sikdar, Rajini Makam, Suresh Sundaram, Suresh Kumar Besai, Mahesh Gopi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12953">Syn2Real Domain Generalization for Underwater Mine-like Object Detection Using Side-Scan Sonar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater mine detection with deep learning suffers from limitations due to the scarcity of real-world data.
  This scarcity leads to overfitting, where models perform well on training data but poorly on unseen data. This paper proposes a Syn2Real (Synthetic to Real) domain generalization approach using diffusion models to address this challenge. We demonstrate that synthetic data generated with noise by DDPM and DDIM models, even if not perfectly realistic, can effectively augment real-world samples for training. The residual noise in the final sampled images improves the model's ability to generalize to real-world data with inherent noise and high variation. The baseline Mask-RCNN model when trained on a combination of synthetic and original training datasets, exhibited approximately a 60% increase in Average Precision (AP) compared to being trained solely on the original training data. This significant improvement highlights the potential of Syn2Real domain generalization for underwater mine detection tasks.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2410.08165.pdf' target='_blank'>https://arxiv.org/pdf/2410.08165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryo Lotfi, Enrico Fini, Samy Bengio, Moin Nabi, Emmanuel Abbe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08165">Chain-of-Sketch: Enabling Global Visual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern vision models have achieved remarkable success in benchmarks where local features provide critical information about the target. There is now a growing interest in tackling tasks requiring more global reasoning, where local features do not provide significant information. Minsky and Papert put forward such tasks in 1969 with their connectivity study, exposing the limitations of the perceptron model. In this paper, we introduce an expanded set of global visual datasets involving graphs, strings, mazes, and image grids. We show that large vision models still struggle to learn these tasks efficiently. Similarly, state-of-the-art multi-modal LLMs perform poorly on these datasets. We explain this learning inefficiency by means of the 'globality degree' measure. To mitigate this, we propose a method called chain-of-sketch (CoS). Similar to the chain-of-thought and scratchpad techniques used in language models, CoS breaks the original task into intermediate visual steps to help learn a complex task. In addition, we show that not all CoS strategies perform equally well. Our key insight is to impose a Markovian structure on the CoS frames. This leads to the introduction of 'inductive CoS' which achieves better out-of-distribution generalization and performs well even with smaller models compared to non-inductive variants.
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2409.17087.pdf' target='_blank'>https://arxiv.org/pdf/2409.17087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luigi Russo, Francesco Mauro, Alessandro Sebastianelli, Paolo Gamba, Silvia Liberata Ullo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17087">SEN12-WATER: A New Dataset for Hydrological Applications and its Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate change and increasing droughts pose significant challenges to water resource management around the world. These problems lead to severe water shortages that threaten ecosystems, agriculture, and human communities. To advance the fight against these challenges, we present a new dataset, SEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL) framework for proactive drought-related analysis. The dataset, identified as a spatiotemporal datacube, integrates SAR polarization, elevation, slope, and multispectral optical bands. Our DL framework enables the analysis and estimation of water losses over time in reservoirs of interest, revealing significant insights into water dynamics for drought analysis by examining temporal changes in physical quantities such as water volume. Our methodology takes advantage of the multitemporal and multimodal characteristics of the proposed dataset, enabling robust generalization and advancing understanding of drought, contributing to climate change resilience and sustainable water resource management. The proposed framework involves, among the several components, speckle noise removal from SAR data, a water body segmentation through a U-Net architecture, the time series analysis, and the predictive capability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Results are validated through ground truth data acquired on-ground via dedicated sensors and (tailored) metrics, such as Precision, Recall, Intersection over Union, Mean Squared Error, Structural Similarity Index Measure and Peak Signal-to-Noise Ratio.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2407.08245.pdf' target='_blank'>https://arxiv.org/pdf/2407.08245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seunghan Yang, Seokeon Choi, Hyunsin Park, Sungha Choi, Simyung Chang, Sungrack Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08245">Feature Diversification and Adaptation for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning, a distributed learning paradigm, utilizes multiple clients to build a robust global model. In real-world applications, local clients often operate within their limited domains, leading to a `domain shift' across clients. Privacy concerns limit each client's learning to its own domain data, which increase the risk of overfitting. Moreover, the process of aggregating models trained on own limited domain can be potentially lead to a significant degradation in the global model performance. To deal with these challenges, we introduce the concept of federated feature diversification. Each client diversifies the own limited domain data by leveraging global feature statistics, i.e., the aggregated average statistics over all participating clients, shared through the global model's parameters. This data diversification helps local models to learn client-invariant representations while preserving privacy. Our resultant global model shows robust performance on unseen test domain data. To enhance performance further, we develop an instance-adaptive inference approach tailored for test domain data. Our proposed instance feature adapter dynamically adjusts feature statistics to align with the test input, thereby reducing the domain gap between the test and training domains. We show that our method achieves state-of-the-art performance on several domain generalization benchmarks within a federated learning setting.
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2407.05765.pdf' target='_blank'>https://arxiv.org/pdf/2407.05765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoyao Zhu, Xiuding Cai, Yingkai Wang, Dong Miao, Zhongliang Fu, Xu Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05765">Invariance Principle Meets Vicinal Risk Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models excel in computer vision tasks but often fail to generalize to out-of-distribution (OOD) domains. Invariant Risk Minimization (IRM) aims to address OOD generalization by learning domain-invariant features. However, IRM struggles with datasets exhibiting significant diversity shifts. While data augmentation methods like Mixup and Semantic Data Augmentation (SDA) enhance diversity, they risk over-augmentation and label instability. To address these challenges, we propose a domain-shared Semantic Data Augmentation (SDA) module, a novel implementation of Variance Risk Minimization (VRM) designed to enhance dataset diversity while maintaining label consistency. We further provide a Rademacher complexity analysis, establishing a tighter generalization error bound compared to baseline methods. Extensive evaluations on OOD benchmarks, including PACS, VLCS, OfficeHome, and TerraIncognita, demonstrate consistent performance improvements over state-of-the-art domain generalization methods.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2407.02403.pdf' target='_blank'>https://arxiv.org/pdf/2407.02403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoon Gyo Jung, Jaewoo Park, Xingbo Dong, Hojin Park, Andrew Beng Jin Teoh, Octavia Camps
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02403">Face Reconstruction Transfer Attack as Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the vulnerability of face recognition systems to malicious attacks is of critical importance. Previous works have focused on reconstructing face images that can penetrate a targeted verification system. Even in the white-box scenario, however, naively reconstructed images misrepresent the identity information, hence the attacks are easily neutralized once the face system is updated or changed. In this paper, we aim to reconstruct face images which are capable of transferring face attacks on unseen encoders. We term this problem as Face Reconstruction Transfer Attack (FRTA) and show that it can be formulated as an out-of-distribution (OOD) generalization problem. Inspired by its OOD nature, we propose to solve FRTA by Averaged Latent Search and Unsupervised Validation with pseudo target (ALSUV). To strengthen the reconstruction attack on OOD unseen encoders, ALSUV reconstructs the face by searching the latent of amortized generator StyleGAN2 through multiple latent optimization, latent optimization trajectory averaging, and unsupervised validation with a pseudo target. We demonstrate the efficacy and generalization of our method on widely used face datasets, accompanying it with extensive ablation studies and visually, qualitatively, and quantitatively analyses. The source code will be released.
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2406.09745.pdf' target='_blank'>https://arxiv.org/pdf/2406.09745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Dong, Tieliang Gong, Hong Chen, Shuangyong Song, Weizhan Zhang, Chen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09745">How Does Distribution Matching Help Domain Generalization: An Information-theoretic Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn invariance across multiple training domains, thereby enhancing generalization against out-of-distribution data. While gradient or representation matching algorithms have achieved remarkable success, these methods generally lack generalization guarantees or depend on strong assumptions, leaving a gap in understanding the underlying mechanism of distribution matching. In this work, we formulate domain generalization from a novel probabilistic perspective, ensuring robustness while avoiding overly conservative solutions. Through comprehensive information-theoretic analysis, we provide key insights into the roles of gradient and representation matching in promoting generalization. Our results reveal the complementary relationship between these two components, indicating that existing works focusing solely on either gradient or representation alignment are insufficient to solve the domain generalization problem. In light of these theoretical findings, we introduce IDM to simultaneously align the inter-domain gradients and representations. Integrated with the proposed PDM method for complex distribution matching, IDM achieves superior performance over various baseline methods.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2406.04981.pdf' target='_blank'>https://arxiv.org/pdf/2406.04981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Tsilivis, Natalie Frank, Nathan Srebro, Julia Kempe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04981">The Price of Implicit Bias in Adversarially Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and its connection with robust generalization. In classification settings under adversarial perturbations with linear models, we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization. We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture. We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2406.01895.pdf' target='_blank'>https://arxiv.org/pdf/2406.01895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Sabbaghi, George Pappas, Hamed Hassani, Surbhi Goel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01895">Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the success of Transformers on language understanding, code generation, and logical reasoning, they still fail to generalize over length on basic arithmetic tasks such as addition and multiplication. A major reason behind this failure is the vast difference in structure between numbers and text; For example, the numbers are typically parsed from right to left, and there is a correspondence between digits at the same position across different numbers. In contrast, for text, such symmetries are quite unnatural. In this work, we propose to encode these semantics explicitly into the model via modified number formatting and custom positional encodings. Empirically, our method allows a Transformer trained on numbers with at most 5-digits for addition and multiplication to generalize up to 50-digit numbers, without using additional data for longer sequences. We further demonstrate that traditional absolute positional encodings (APE) fail to generalize to longer sequences, even when trained with augmented data that captures task symmetries. To elucidate the importance of explicitly encoding structure, we prove that explicit incorporation of structure via positional encodings is necessary for out-of-distribution generalization. Finally, we pinpoint other challenges inherent to length generalization beyond capturing symmetries, in particular complexity of the underlying task, and propose changes in the training distribution to address them.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2406.00298.pdf' target='_blank'>https://arxiv.org/pdf/2406.00298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Spanos, Anastasios Arsenos, Paraskevi-Antonia Theofilou, Paraskevi Tzouveli, Athanasios Voulodimos, Stefanos Kollias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00298">Complex Style Image Transformations for Domain Generalization in Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The absence of well-structured large datasets in medical computer vision results in decreased performance of automated systems and, especially, of deep learning models. Domain generalization techniques aim to approach unknown domains from a single data source. In this paper we introduce a novel framework, named CompStyle, which leverages style transfer and adversarial training, along with high-level input complexity augmentation to effectively expand the domain space and address unknown distributions. State-of-the-art style transfer methods depend on the existence of subdomains within the source dataset. However, this can lead to an inherent dataset bias in the image creation. Input-level augmentation can provide a solution to this problem by widening the domain space in the source dataset and boost performance on out-of-domain distributions. We provide results from experiments on semantic segmentation on prostate data and corruption robustness on cardiac data which demonstrate the effectiveness of our approach. Our method increases performance in both tasks, without added cost to training time or resources.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2405.15173.pdf' target='_blank'>https://arxiv.org/pdf/2405.15173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinan He, Yue Zhou, Shu Hu, Bin Li, Jiwu Huang, Feng Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15173">Redundant Semantic Environment Filling via Misleading-Learning for Fair Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting falsified faces generated by Deepfake technology is essential for safeguarding trust in digital communication and protecting individuals. However, current detectors often suffer from a dual-overfitting: they become overly specialized in both specific forgery fingerprints and particular demographic attributes. Critically, most existing methods overlook the latter issue, which results in poor fairness: faces from certain demographic groups, such as different genders or ethnicities, are consequently more difficult to reliably detect. To address this challenge, we propose a novel strategy called misleading-learning, which populates the latent space with a multitude of redundant environments. By exposing the detector to a sufficiently rich and balanced variety of high-level information for demographic fairness, our approach mitigates demographic bias while maintaining a high detection performance level. We conduct extensive evaluations on fairness, intra-domain detection, cross-domain generalization, and robustness. Experimental results demonstrate that our framework achieves superior fairness and generalization compared to state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2405.15018.pdf' target='_blank'>https://arxiv.org/pdf/2405.15018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Yousuf Harun, Kyungbok Lee, Jhair Gallardo, Giri Krishnan, Christopher Kanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15018">What Variables Affect Out-of-Distribution Generalization in Pretrained Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embeddings produced by pre-trained deep neural networks (DNNs) are widely used; however, their efficacy for downstream tasks can vary widely. We study the factors influencing transferability and out-of-distribution (OOD) generalization of pre-trained DNN embeddings through the lens of the tunnel effect hypothesis, which is closely related to intermediate neural collapse. This hypothesis suggests that deeper DNN layers compress representations and hinder OOD generalization. Contrary to earlier work, our experiments show this is not a universal phenomenon. We comprehensively investigate the impact of DNN architecture, training data, image resolution, and augmentations on transferability. We identify that training with high-resolution datasets containing many classes greatly reduces representation compression and improves transferability. Our results emphasize the danger of generalizing findings from toy datasets to broader contexts.
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2405.11163.pdf' target='_blank'>https://arxiv.org/pdf/2405.11163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Liang, Zheng Zheng, Weihai Chen, Xinzhi Ma, Zhongcai Pei, Xiantao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11163">Domain Generalization for Zero-calibration BCIs with Knowledge Distillation-based Phase Invariant Feature Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The distribution shift of electroencephalography (EEG) data causes poor generalization of braincomputer interfaces (BCIs) in unseen domains. Some methods try to tackle this challenge by collecting a portion of user data for calibration. However, it is time-consuming, mentally fatiguing, and user-unfriendly. To achieve zerocalibration BCIs, most studies employ domain generalization (DG) techniques to learn invariant features across different domains in the training set. However, they fail to fully explore invariant features within the same domain, leading to limited performance. In this paper, we present an novel method to learn domain-invariant features from both interdomain and intra-domain perspectives. For intra-domain invariant features, we propose a knowledge distillation framework to extract EEG phase-invariant features within one domain. As for inter-domain invariant features, correlation alignment is used to bridge distribution gaps across multiple domains. Experimental results on three public datasets validate the effectiveness of our method, showcasing stateof-the-art performance. To the best of our knowledge, this is the first domain generalization study that exploit Fourier phase information as an intra-domain invariant feature to facilitate EEG generalization. More importantly, the zerocalibration BCI based on inter- and intra-domain invariant features has significant potential to advance the practical applications of BCIs in real world.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2404.04452.pdf' target='_blank'>https://arxiv.org/pdf/2404.04452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shadi Alijani, Jamil Fayyad, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04452">Vision transformers in domain adaptation and domain generalization: a study of robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models are often evaluated in scenarios where the data distribution is different from those used in the training and validation phases. The discrepancy presents a challenge for accurately predicting the performance of models once deployed on the target distribution. Domain adaptation and generalization are widely recognized as effective strategies for addressing such shifts, thereby ensuring reliable performance. The recent promising results in applying vision transformers in computer vision tasks, coupled with advancements in self-attention mechanisms, have demonstrated their significant potential for robustness and generalization in handling distribution shifts. Motivated by the increased interest from the research community, our paper investigates the deployment of vision transformers in domain adaptation and domain generalization scenarios. For domain adaptation methods, we categorize research into feature-level, instance-level, model-level adaptations, and hybrid approaches, along with other categorizations with respect to diverse strategies for enhancing domain adaptation. Similarly, for domain generalization, we categorize research into multi-domain learning, meta-learning, regularization techniques, and data augmentation strategies. We further classify diverse strategies in research, underscoring the various approaches researchers have taken to address distribution shifts by integrating vision transformers. The inclusion of comprehensive tables summarizing these categories is a distinct feature of our work, offering valuable insights for researchers. These findings highlight the versatility of vision transformers in managing distribution shifts, crucial for real-world applications, especially in critical safety and decision-making scenarios.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2404.00758.pdf' target='_blank'>https://arxiv.org/pdf/2404.00758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Josip JukiÄ, Jan Å najder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00758">From Robustness to Improved Generalization and Calibration in Pre-trained Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperforming unregularized fine-tuning and other similar regularization methods.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2403.19895.pdf' target='_blank'>https://arxiv.org/pdf/2403.19895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenliang Liu, Guanding Yu, Lele Wang, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19895">An Information-Theoretic Framework for Out-of-Distribution Generalization with Applications to Stochastic Gradient Langevin Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the Out-of-Distribution (OOD) generalization in machine learning and propose a general framework that establishes information-theoretic generalization bounds. Our framework interpolates freely between Integral Probability Metric (IPM) and $f$-divergence, which naturally recovers some known results (including Wasserstein- and KL-bounds), as well as yields new generalization bounds. Additionally, we show that our framework admits an optimal transport interpretation. When evaluated in two concrete examples, the proposed bounds either strictly improve upon existing bounds in some cases or match the best existing OOD generalization bounds. Moreover, by focusing on $f$-divergence and combining it with the Conditional Mutual Information (CMI) methods, we derive a family of CMI-based generalization bounds, which include the state-of-the-art ICIMI bound as a special instance. Finally, leveraging these findings, we analyze the generalization of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, showing that our derived generalization bounds outperform existing information-theoretic generalization bounds in certain scenarios.
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2403.15210.pdf' target='_blank'>https://arxiv.org/pdf/2403.15210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Cecilia Liu, Iryna Gurevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15210">Early Period of Training Impacts Adaptation for Out-of-Distribution Generalization: An Empirical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prior research shows that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) data of tasks. Yet, the implications of early learning dynamics on out-of-distribution (OOD) generalization remain poorly understood, primarily due to the complexities and limitations of existing analytical techniques. In this work, we investigate the relationship between learning dynamics, OOD generalization under covariate shift and the early period of neural network training. We utilize the trace of Fisher Information and sharpness, focusing on gradual unfreezing (i.e., progressively unfreezing parameters during training) as our methodology for investigation. Through a series of empirical experiments, we show that 1) changing the number of trainable parameters during the early period of training via gradual unfreezing can significantly improve OOD results; 2) the trace of Fisher Information and sharpness can be used as indicators for the removal of gradual unfreezing during the early period of training for better OOD generalization. Our experiments on both image and text data show that the early period of training is a general phenomenon that can provide Pareto improvements in ID and OOD performance with minimal complexity. Our work represents a first step towards understanding how early learning dynamics affect neural network OOD generalization under covariate shift and suggests a new avenue to improve and study this problem.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2403.05124.pdf' target='_blank'>https://arxiv.org/pdf/2403.05124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengwei Yin, Guanzhong Zeng, Jingjing Wang, Di Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05124">CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaze estimation methods often experience significant performance degradation when evaluated across different domains, due to the domain gap between the testing and training data. Existing methods try to address this issue using various domain generalization approaches, but with little success because of the limited diversity of gaze datasets, such as appearance, wearable, and image quality. To overcome these limitations, we propose a novel framework called CLIP-Gaze that utilizes a pre-trained vision-language model to leverage its transferable knowledge. Our framework is the first to leverage the vision-and-language cross-modality approach for gaze estimation task. Specifically, we extract gaze-relevant feature by pushing it away from gaze-irrelevant features which can be flexibly constructed via language descriptions. To learn more suitable prompts, we propose a personalized context optimization method for text prompt tuning. Furthermore, we utilize the relationship among gaze samples to refine the distribution of gaze-relevant features, thereby improving the generalization capability of the gaze estimation model. Extensive experiments demonstrate the excellent performance of CLIP-Gaze over existing methods on four cross-domain evaluations.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2402.15239.pdf' target='_blank'>https://arxiv.org/pdf/2402.15239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Nina Cheng, Nishant Ravikumar, Alejandro F. Frangi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15239">GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automated segmentation of cerebral aneurysms is pivotal for accurate diagnosis and treatment planning. Confronted with significant domain shifts and class imbalance in 3D Rotational Angiography (3DRA) data from various medical institutions, the task becomes challenging. These shifts include differences in image appearance, intensity distribution, resolution, and aneurysm size, all of which complicate the segmentation process. To tackle these issues, we propose a novel domain generalization strategy that employs gradient surgery exponential moving average (GS-EMA) optimization technique coupled with boundary-aware contrastive learning (BACL). Our approach is distinct in its ability to adapt to new, unseen domains by learning domain-invariant features, thereby improving the robustness and accuracy of aneurysm segmentation across diverse clinical datasets. The results demonstrate that our proposed approach can extract more domain-invariant features, minimizing over-segmentation and capturing more complete aneurysm structures.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2402.11095.pdf' target='_blank'>https://arxiv.org/pdf/2402.11095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuelun Shen, Zhipeng Cai, Wei Yin, Matthias MÃ¼ller, Zijun Li, Kaixuan Wang, Xiaozhi Chen, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11095">GIM: Learning Generalizable Image Matcher From Internet Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Applying GIM consistently improves the zero-shot performance of 3 state-of-the-art image matching architectures; with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4%-18.1%. GIM also enables generalization to extreme cross-domain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1(c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The video presentation is available at https://www.youtube.com/watch?v=FU_MJLD8LeY.
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2401.08703.pdf' target='_blank'>https://arxiv.org/pdf/2401.08703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guowei Wang, Changxing Ding, Wentao Tan, Mingkui Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08703">Decoupled Prototype Learning for Reliable Test-Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class's pseudo-feature from a memory in a momentum manner and insert an additional DPL loss. Finally, we introduce a consistency regularization-based approach to leverage samples with unconfident pseudo-labels. This approach transfers feature styles of samples with unconfident pseudo-labels to those with confident pseudo-labels. Thus, more reliable samples for TTA are created. The experimental results demonstrate that our methods achieve state-of-the-art performance on domain generalization benchmarks, and reliably improve the performance of self-training-based methods on image corruption benchmarks. The code will be released.
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2401.05363.pdf' target='_blank'>https://arxiv.org/pdf/2401.05363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiquan Wang, Sha Zhao, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05363">Generalizable Sleep Staging via Multi-Level Domain Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different domains, and a Sequence-level Feature Alignment to minimize the discrepancy of sequential features among different domains. SleepDG is validated on five public datasets, achieving the state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2401.01736.pdf' target='_blank'>https://arxiv.org/pdf/2401.01736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai, Xiaocong Zhou, Delong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01736">Few-shot Adaptation of Multi-modal Foundation Models: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models. These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: 1) prompt-based methods, 2) adapter-based methods, and 3) external knowledge-based methods. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: 1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive knowledge utilization.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2312.05387.pdf' target='_blank'>https://arxiv.org/pdf/2312.05387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sobhan Hemati, Mahdi Beitollahi, Amir Hossein Estiri, Bassel Al Omari, Xi Chen, Guojun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05387">Cross Domain Generative Augmentation: Domain Generalization with Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the huge effort in developing novel regularizers for Domain Generalization (DG), adding simple data augmentation to the vanilla ERM which is a practical implementation of the Vicinal Risk Minimization principle (VRM) \citep{chapelle2000vicinal} outperforms or stays competitive with many of the proposed regularizers. The VRM reduces the estimation error in ERM by replacing the point-wise kernel estimates with a more precise estimation of true data distribution that reduces the gap between data points \textbf{within each domain}. However, in the DG setting, the estimation error of true data distribution by ERM is mainly caused by the distribution shift \textbf{between domains} which cannot be fully addressed by simple data augmentation techniques within each domain. Inspired by this limitation of VRM, we propose a novel data augmentation named Cross Domain Generative Augmentation (CDGA) that replaces the pointwise kernel estimates in ERM with new density estimates in the \textbf{vicinity of domain pairs} so that the gap between domains is further reduced. To this end, CDGA, which is built upon latent diffusion models (LDM), generates synthetic images to fill the gap between all domains and as a result, reduces the non-iidness. We show that CDGA outperforms SOTA DG methods under the Domainbed benchmark. To explain the effectiveness of CDGA, we generate more than 5 Million synthetic images and perform extensive ablation studies including data scaling laws, distribution visualization, domain shift quantification, adversarial robustness, and loss landscape analysis.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2312.02253.pdf' target='_blank'>https://arxiv.org/pdf/2312.02253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoran Yu, Chenchen Zhu, Sean Culatana, Raghuraman Krishnamoorthi, Fanyi Xiao, Yong Jae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02253">Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative deep learning have enabled the creation of high-quality synthetic images in text-to-image generation. Prior work shows that fine-tuning a pretrained diffusion model on ImageNet and generating synthetic training images from the finetuned model can enhance an ImageNet classifier's performance. However, performance degrades as synthetic images outnumber real ones. In this paper, we explore whether generative fine-tuning is essential for this improvement and whether it is possible to further scale up training using more synthetic data. We present a new framework leveraging off-the-shelf generative models to generate synthetic training images, addressing multiple challenges: class name ambiguity, lack of diversity in naive prompts, and domain shifts. Specifically, we leverage large language models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we propose contextualized diversification (CD) and stylized diversification (SD) methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage domain adaptation techniques with auxiliary batch normalization for synthetic images. Our framework consistently enhances recognition model performance with more synthetic data, up to 6x of original ImageNet size showcasing the potential of synthetic data for improved recognition models and strong out-of-domain generalization.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2312.02021.pdf' target='_blank'>https://arxiv.org/pdf/2312.02021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph HÃ¼mmer, Manuel Schwonberg, Liangwei Zhou, Hu Cao, Alois Knoll, Hanno Gottschalk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02021">Strong but simple: A Baseline for Domain Generalized Dense Perception by CLIP-based Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) remains a significant challenge for perception based on deep neural networks (DNNs), where domain shifts occur due to synthetic data, lighting, weather, or location changes. Vision-language models (VLMs) marked a large step for the generalization capabilities and have been already applied to various tasks. Very recently, first approaches utilized VLMs for domain generalized segmentation and object detection and obtained strong generalization. However, all these approaches rely on complex modules, feature augmentation frameworks or additional models. Surprisingly and in contrast to that, we found that simple fine-tuning of vision-language pre-trained models yields competitive or even stronger generalization results while being extremely simple to apply. Moreover, we found that vision-language pre-training consistently provides better generalization than the previous standard of vision-only pre-training. This challenges the standard of using ImageNet-based transfer learning for domain generalization. Fully fine-tuning a vision-language pre-trained model is capable of reaching the domain generalization SOTA when training on the synthetic GTA5 dataset. Moreover, we confirm this observation for object detection on a novel synthetic-to-real benchmark. We further obtain superior generalization capabilities by reaching 77.9% mIoU on the popular Cityscapes-to-ACDC benchmark. We also found improved in-domain generalization, leading to an improved SOTA of 86.4% mIoU on the Cityscapes test set marking the first place on the leaderboard.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2311.18420.pdf' target='_blank'>https://arxiv.org/pdf/2311.18420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianrui Mu, Jianhong Bai, Xiaoxuan He, Jiangnan Ye, Xiaoyu Liang, Yuchen Yang, Jiedong Zhuang, Haoji Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18420">TeG-DG: Textually Guided Domain Generalization for Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing the domain generalization performance of Face Anti-Spoofing (FAS) techniques has emerged as a research focus. Existing methods are dedicated to extracting domain-invariant features from various training domains. Despite the promising performance, the extracted features inevitably contain residual style feature bias (e.g., illumination, capture device), resulting in inferior generalization performance. In this paper, we propose an alternative and effective solution, the Textually Guided Domain Generalization (TeG-DG) framework, which can effectively leverage text information for cross-domain alignment. Our core insight is that text, as a more abstract and universal form of expression, can capture the commonalities and essential characteristics across various attacks, bridging the gap between different image domains. Contrary to existing vision-language models, the proposed framework is elaborately designed to enhance the domain generalization ability of the FAS task. Concretely, we first design a Hierarchical Attention Fusion (HAF) module to enable adaptive aggregation of visual features at different levels; Then, a Textual-Enhanced Visual Discriminator (TEVD) is proposed for not only better alignment between the two modalities but also to regularize the classifier with unbiased text features. TeG-DG significantly outperforms previous approaches, especially in situations with extremely limited source domain data (~14% and ~12% improvements on HTER and AUC respectively), showcasing impressive few-shot performance.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2311.00966.pdf' target='_blank'>https://arxiv.org/pdf/2311.00966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxiang Wang, Gargi Balasubramaniam, Haozhe Si, Bo Li, Han Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00966">Invariant-Feature Subspace Recovery: A New Class of Provable Domain Generalization Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization asks for models trained over a set of training environments to generalize well in unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) have been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than $d_s+1$ training environments, where $d_s$ is the dimension of the spurious-feature subspace. In this work, we propose Invariant-feature Subspace Recovery (ISR): a new class of algorithms to achieve provable domain generalization across the settings of classification and regression problems. First, in the binary classification setup of Rosenfeld et al. (2021), we show that our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable domain generalization with $d_s+1$ training environments. Our second algorithm, ISR-Cov, further reduces the required number of training environments to $O(1)$ using the information of second-order moments. Notably, unlike IRM, our algorithms bypass non-convexity issues and enjoy global convergence guarantees. Next, we extend ISR-Mean to the more general setting of multi-class classification and propose ISR-Multiclass, which leverages class information and provably recovers the invariant-feature subspace with $\lceil d_s/k\rceil+1$ training environments for $k$-class classification. Finally, for regression problems, we propose ISR-Regression that can identify the invariant-feature subspace with $d_s+1$ training environments. Empirically, we demonstrate the superior performance of our ISRs on synthetic benchmarks. Further, ISR can be used as post-processing methods for feature extractors such as neural nets.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2310.19656.pdf' target='_blank'>https://arxiv.org/pdf/2310.19656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mostafa Jahanifar, Manahil Raza, Kesi Xu, Trinh Vuong, Rob Jewsbury, Adam Shephard, Neda Zamanitajeddin, Jin Tae Kwak, Shan E Ahmed Raza, Fayyaz Minhas, Nasir Rajpoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19656">Domain Generalization in Computational Pathology: Survey and Guidelines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have exhibited exceptional effectiveness in Computational Pathology (CPath) by tackling intricate tasks across an array of histology image analysis applications. Nevertheless, the presence of out-of-distribution data (stemming from a multitude of sources such as disparate imaging devices and diverse tissue preparation methods) can cause \emph{domain shift} (DS). DS decreases the generalization of trained models to unseen datasets with slightly different data distributions, prompting the need for innovative \emph{domain generalization} (DG) solutions. Recognizing the potential of DG methods to significantly influence diagnostic and prognostic models in cancer studies and clinical practice, we present this survey along with guidelines on achieving DG in CPath. We rigorously define various DS types, systematically review and categorize existing DG approaches and resources in CPath, and provide insights into their advantages, limitations, and applicability. We also conduct thorough benchmarking experiments with 28 cutting-edge DG algorithms to address a complex DG problem. Our findings suggest that careful experiment design and CPath-specific Stain Augmentation technique can be very effective. However, there is no one-size-fits-all solution for DG in CPath. Therefore, we establish clear guidelines for detecting and managing DS depending on different scenarios. While most of the concepts, guidelines, and recommendations are given for applications in CPath, we believe that they are applicable to most medical image analysis tasks as well.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2310.04414.pdf' target='_blank'>https://arxiv.org/pdf/2310.04414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Sun, Xingjian Leng, Zijian Wang, Yang Yang, Zi Huang, Liang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04414">CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyzing model performance in various unseen environments is a critical research problem in the machine learning community. To study this problem, it is important to construct a testbed with out-of-distribution test sets that have broad coverage of environmental discrepancies. However, existing testbeds typically either have a small number of domains or are synthesized by image corruptions, hindering algorithm design that demonstrates real-world effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of 180 datasets collected by prompting image search engines and diffusion models in various ways. Generally sized between 300 and 8,000 images, the datasets contain natural images, cartoons, certain colors, or objects that do not naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen the understanding of two generalization tasks: domain generalization and model accuracy prediction in various out-of-distribution environments. We conduct extensive benchmarking and comparison experiments and show that CIFAR-10-W offers new and interesting insights inherent to these tasks. We also discuss other fields that would benefit from CIFAR-10-W.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2307.04033.pdf' target='_blank'>https://arxiv.org/pdf/2307.04033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sameer Ambekar, Zehao Xiao, Jiayi Shen, Xiantong Zhen, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04033">Probabilistic Test-Time Generalization by Variational Neighbor-Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed on unseen target domains. We follow the strict separation of source training and target testing, but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem, by modeling pseudo labels as distributions, to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we introduce a meta-generalization stage during training to simulate the generalization procedure. Experiments on seven widely-used datasets demonstrate the benefits, abilities, and effectiveness of our proposal.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2305.17528.pdf' target='_blank'>https://arxiv.org/pdf/2305.17528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Palumbo, Yang Guo, Xi Wu, Jiefeng Chen, Yingyu Liang, Somesh Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17528">Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Both transduction and rejection have emerged as important techniques for defending against adversarial perturbations. A recent work by Goldwasser et al. showed that rejection combined with transduction can give provable guarantees (for certain problems) that cannot be achieved otherwise. Nevertheless, under recent strong adversarial attacks, their work was shown to have low performance in a practical deep-learning setting. In this paper, we take a step towards realizing the promise of transduction+rejection in more realistic scenarios. Our key observation is that a novel application of a reduction technique by TramÃ¨r, which was until now only used to demonstrate the vulnerability of certain defenses, can be used to actually construct effective defenses. Theoretically, we show that a careful application of this technique in the transductive setting can give significantly improved sample-complexity for robust generalization. Our theory guides us to design a new transductive algorithm for learning a selective model; extensive experiments using state of the art attacks show that our approach provides significantly better robust accuracy (81.6% on CIFAR-10 and 57.9% on CIFAR-100 under $l_\infty$ with budget 8/255) than existing techniques.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2305.16938.pdf' target='_blank'>https://arxiv.org/pdf/2305.16938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, Yanai Elazar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16938">Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2305.12800.pdf' target='_blank'>https://arxiv.org/pdf/2305.12800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yachun Li, Jingjing Wang, Yuhui Chen, Di Xie, Shiliang Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12800">Single Domain Dynamic Generalization for Iris Presentation Attack Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Iris presentation attack detection (PAD) has achieved great success under intra-domain settings but easily degrades on unseen domains. Conventional domain generalization methods mitigate the gap by learning domain-invariant features. However, they ignore the discriminative information in the domain-specific features. Moreover, we usually face a more realistic scenario with only one single domain available for training. To tackle the above issues, we propose a Single Domain Dynamic Generalization (SDDG) framework, which simultaneously exploits domain-invariant and domain-specific features on a per-sample basis and learns to generalize to various unseen domains with numerous natural images. Specifically, a dynamic block is designed to adaptively adjust the network with a dynamic adaptor. And an information maximization loss is further combined to increase diversity. The whole network is integrated into the meta-learning paradigm. We generate amplitude perturbed images and cover diverse domains with natural images. Therefore, the network can learn to generalize to the perturbed domains in the meta-test phase. Extensive experiments show the proposed method is effective and outperforms the state-of-the-art on LivDet-Iris 2017 dataset.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2304.13509.pdf' target='_blank'>https://arxiv.org/pdf/2304.13509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karina Kvanchiani, Elizaveta Petrova, Karen Efremyan, Alexander Sautin, Alexander Kapitanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13509">EasyPortrait -- Face Parsing and Portrait Segmentation Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, video conferencing apps have become functional by accomplishing such computer vision-based features as real-time background removal and face beautification. Limited variability in existing portrait segmentation and face parsing datasets, including head poses, ethnicity, scenes, and occlusions specific to video conferencing, motivated us to create a new dataset, EasyPortrait, for these tasks simultaneously. It contains 40,000 primarily indoor photos repeating video meeting scenarios with 13,705 unique users and fine-grained segmentation masks separated into 9 classes. Inappropriate annotation masks from other datasets caused a revision of annotator guidelines, resulting in EasyPortrait's ability to process cases, such as teeth whitening and skin smoothing. The pipeline for data mining and high-quality mask annotation via crowdsourcing is also proposed in this paper. In the ablation study experiments, we proved the importance of data quantity and diversity in head poses in our dataset for the effective learning of the model. The cross-dataset evaluation experiments confirmed the best domain generalization ability among portrait segmentation datasets. Moreover, we demonstrate the simplicity of training segmentation models on EasyPortrait without extra training tricks. The proposed dataset and trained models are publicly available.
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2304.00424.pdf' target='_blank'>https://arxiv.org/pdf/2304.00424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokeon Choi, Debasmit Das, Sungha Choi, Seunghan Yang, Hyunsin Park, Sungrack Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00424">Progressive Random Convolutions for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization aims to train a generalizable model with only one source domain to perform well on arbitrary unseen target domains. Image augmentation based on Random Convolutions (RandConv), consisting of one convolution layer randomly initialized for each mini-batch, enables the model to learn generalizable visual representations by distorting local textures despite its simple and lightweight structure. However, RandConv has structural limitations in that the generated image easily loses semantics as the kernel size increases, and lacks the inherent diversity of a single convolution operation. To solve the problem, we propose a Progressive Random Convolution (Pro-RandConv) method that recursively stacks random convolution layers with a small kernel size instead of increasing the kernel size. This progressive approach can not only mitigate semantic distortions by reducing the influence of pixels away from the center in the theoretical receptive field, but also create more effective virtual domains by gradually increasing the style diversity. In addition, we develop a basic random convolution layer into a random convolution block including deformable offsets and affine transformation to support texture and contrast diversification, both of which are also randomly initialized. Without complex generators or adversarial learning, we demonstrate that our simple yet effective augmentation strategy outperforms state-of-the-art methods on single domain generalization benchmarks.
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2303.13500.pdf' target='_blank'>https://arxiv.org/pdf/2303.13500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Puja Trivedi, Danai Koutra, Jayaraman J. Thiagarajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13500">A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to rely upon simple features, as SB has recently been shown to underlie several problems in robust generalization. Using a synthetic dataset, we demonstrate the susceptibility of existing protocols to SB. Given the strong effectiveness of LP+FT, we then propose modified linear probes that help mitigate SB, and lead to better initializations for subsequent FT. We verify the effectiveness of the proposed LP+FT variants for decreasing SB in a controlled setting, and their ability to improve OOD generalization and safety on three adaptation datasets.
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2301.13105.pdf' target='_blank'>https://arxiv.org/pdf/2301.13105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emmanuel Abbe, Samy Bengio, Aryo Lotfi, Kevin Rizk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.13105">Generalization on the Unseen, Logic Reasoning and Degree Curriculum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper considers the learning of logical (Boolean) functions with a focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. We study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for sparse functions and a class of network models including instances of Transformers, random features models, and linear networks, a min-degree-interpolator is learned on the unseen. More specifically, this means an interpolator of the training data that has minimal Fourier mass on the higher degree basis elements. These findings lead to two implications: (1) we provide an explanation to the length generalization problem for Boolean functions (e.g., Anil et al. 2022); (2) we introduce a curriculum learning algorithm called Degree-Curriculum that learns monomials more efficiently by incrementing supports. Finally, we discuss extensions to other models or non-sparse regimes where the min-degree bias may still occur or fade, as well as how it can be potentially corrected when undesirable.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2301.02145.pdf' target='_blank'>https://arxiv.org/pdf/2301.02145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Usman Muhammad, Jorma Laaksonen, Djamila Romaissa Beddiar, Mourad Oussalah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02145">Domain Generalization via Ensemble Stacking for Face Presentation Attack Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Presentation Attack Detection (PAD) plays a pivotal role in securing face recognition systems against spoofing attacks. Although great progress has been made in designing face PAD methods, developing a model that can generalize well to unseen test domains remains a significant challenge. Moreover, due to different types of spoofing attacks, creating a dataset with a sufficient number of samples for training deep neural networks is a laborious task. This work proposes a comprehensive solution that combines synthetic data generation and deep ensemble learning to enhance the generalization capabilities of face PAD. Specifically, synthetic data is generated by blending a static image with spatiotemporal encoded images using alpha composition and video distillation. This way, we simulate motion blur with varying alpha values, thereby generating diverse subsets of synthetic data that contribute to a more enriched training set. Furthermore, multiple base models are trained on each subset of synthetic data using stacked ensemble learning. This allows the models to learn complementary features and representations from different synthetic subsets. The meta-features generated by the base models are used as input to a new model called the meta-model. The latter combines the predictions from the base models, leveraging their complementary information to better handle unseen target domains and enhance the overall performance. Experimental results on four datasets demonstrate low half total error rates (HTERs) on three benchmark datasets: CASIA-MFSD (8.92%), MSU-MFSD (4.81%), and OULU-NPU (6.70%). The approach shows potential for advancing presentation attack detection by utilizing large-scale synthetic data and the meta-model.
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2212.14710.pdf' target='_blank'>https://arxiv.org/pdf/2212.14710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengwei Yin, Jiawu Dai, Jingjing Wang, Di Xie, Shiliang Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14710">NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gaze estimation is the fundamental basis for many visual tasks. Yet, the high cost of acquiring gaze datasets with 3D annotations hinders the optimization and application of gaze estimation models. In this work, we propose a novel Head-Eye redirection parametric model based on Neural Radiance Field, which allows dense gaze data generation with view consistency and accurate gaze direction. Moreover, our head-eye redirection parametric model can decouple the face and eyes for separate neural rendering, so it can achieve the purpose of separately controlling the attributes of the face, identity, illumination, and eye gaze direction. Thus diverse 3D-aware gaze datasets could be obtained by manipulating the latent code belonging to different face attributions in an unsupervised manner. Extensive experiments on several benchmarks demonstrate the effectiveness of our method in domain generalization and domain adaptation for gaze estimation tasks.
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2208.12587.pdf' target='_blank'>https://arxiv.org/pdf/2208.12587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mostafa Jahanifar, Adam Shephard, Neda Zamanitajeddin, Simon Graham, Shan E Ahmed Raza, Fayyaz Minhas, Nasir Rajpoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.12587">Mitosis Detection, Fast and Slow: Robust and Efficient Detection of Mitotic Figures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Counting of mitotic figures is a fundamental step in grading and prognostication of several cancers. However, manual mitosis counting is tedious and time-consuming. In addition, variation in the appearance of mitotic figures causes a high degree of discordance among pathologists. With advances in deep learning models, several automatic mitosis detection algorithms have been proposed but they are sensitive to {\em domain shift} often seen in histology images. We propose a robust and efficient two-stage mitosis detection framework, which comprises mitosis candidate segmentation ({\em Detecting Fast}) and candidate refinement ({\em Detecting Slow}) stages. The proposed candidate segmentation model, termed \textit{EUNet}, is fast and accurate due to its architectural design. EUNet can precisely segment candidates at a lower resolution to considerably speed up candidate detection. Candidates are then refined using a deeper classifier network, EfficientNet-B7, in the second stage. We make sure both stages are robust against domain shift by incorporating domain generalization methods. We demonstrate state-of-the-art performance and generalizability of the proposed model on the three largest publicly available mitosis datasets, winning the two mitosis domain generalization challenge contests (MIDOG21 and MIDOG22). Finally, we showcase the utility of the proposed algorithm by processing the TCGA breast cancer cohort (1,125 whole-slide images) to generate and release a repository of more than 620K mitotic figures.
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2207.04913.pdf' target='_blank'>https://arxiv.org/pdf/2207.04913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingge Wang, Liyan Xie, Yao Xie, Shao-Lun Huang, Yang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.04913">Generalizing to Unseen Domains with Wasserstein Distributional Robustness under Limited Source Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims at learning a universal model that performs well on unseen target domains, incorporating knowledge from multiple source domains. In this research, we consider the scenario where different domain shifts occur among conditional distributions of different classes across domains. When labeled samples in the source domains are limited, existing approaches are not sufficiently robust. To address this problem, we propose a novel domain generalization framework called {Wasserstein Distributionally Robust Domain Generalization} (WDRDG), inspired by the concept of distributionally robust optimization. We encourage robustness over conditional distributions within class-specific Wasserstein uncertainty sets and optimize the worst-case performance of a classifier over these uncertainty sets. We further develop a test-time adaptation module leveraging optimal transport to quantify the relationship between the unseen target domain and source domains to make adaptive inference for target data. Experiments on the Rotated MNIST, PACS and the VLCS datasets demonstrate that our method could effectively balance the robustness and discriminability in challenging generalization scenarios.
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2206.03198.pdf' target='_blank'>https://arxiv.org/pdf/2206.03198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Subel, Yifei Guan, Ashesh Chattopadhyay, Pedram Hassanzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.03198">Explaining the physics of transfer learning a data-driven subgrid-scale closure to a different turbulent flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning (TL) is becoming a powerful tool in scientific applications of neural networks (NNs), such as weather/climate prediction and turbulence modeling. TL enables out-of-distribution generalization (e.g., extrapolation in parameters) and effective blending of disparate training sets (e.g., simulations and observations). In TL, selected layers of a NN, already trained for a base system, are re-trained using a small dataset from a target system. For effective TL, we need to know 1) what are the best layers to re-train? and 2) what physics are learned during TL? Here, we present novel analyses and a new framework to address (1)-(2) for a broad range of multi-scale, nonlinear systems. Our approach combines spectral analyses of the systems' data with spectral analyses of convolutional NN's activations and kernels, explaining the inner-workings of TL in terms of the system's nonlinear physics. Using subgrid-scale modeling of several setups of 2D turbulence as test cases, we show that the learned kernels are combinations of low-, band-, and high-pass filters, and that TL learns new filters whose nature is consistent with the spectral differences of base and target systems. We also find the shallowest layers are the best to re-train in these cases, which is against the common wisdom guiding TL in machine learning literature. Our framework identifies the best layer(s) to re-train beforehand, based on physics and NN theory. Together, these analyses explain the physics learned in TL and provide a framework to guide TL for wide-ranging applications in science and engineering, such as climate change modeling.
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2510.03259.pdf' target='_blank'>https://arxiv.org/pdf/2510.03259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoonjeon Kim, Doohyuk Jang, Eunho Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03259">Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2509.26027.pdf' target='_blank'>https://arxiv.org/pdf/2509.26027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Pei, Yuguang Yang, Kexin Liu, Baochang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26027">Causally Guided Gaussian Perturbations for Out-Of-Distribution Generalization in Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization remains a central challenge in deploying deep learning models to real-world scenarios, particularly in domains such as biomedical images, where distribution shifts are both subtle and pervasive. While existing methods often pursue domain invariance through complex generative models or adversarial training, these approaches may overlook the underlying causal mechanisms of generalization.In this work, we propose Causally-Guided Gaussian Perturbations (CGP)-a lightweight framework that enhances OOD generalization by injecting spatially varying noise into input images, guided by soft causal masks derived from Vision Transformers. By applying stronger perturbations to background regions and weaker ones to foreground areas, CGP encourages the model to rely on causally relevant features rather than spurious correlations.Experimental results on the challenging WILDS benchmark Camelyon17 demonstrate consistent performance gains over state-of-the-art OOD baselines, highlighting the potential of causal perturbation as a tool for reliable and interpretable generalization.
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2509.22854.pdf' target='_blank'>https://arxiv.org/pdf/2509.22854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqian Li, Yanshu Li, Ligong Han, Ruixiang Tang, Wenya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22854">Towards Generalizable Implicit In-Context Learning with Attention Routing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Implicit in-context learning (ICL) has newly emerged as a promising paradigm that simulates ICL behaviors in the representation space of Large Language Models (LLMs), aiming to attain few-shot performance at zero-shot cost. However, existing approaches largely rely on injecting shift vectors into residual flows, which are typically constructed from labeled demonstrations or task-specific alignment. Such designs fall short of utilizing the structural mechanisms underlying ICL and suffer from limited generalizability. To address this, we propose In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level. It extracts reusable structural directions that emerge during ICL and employs a learnable input-conditioned router to modulate attention logits accordingly, enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world datasets spanning diverse domains and multiple LLMs. The results show that ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle. These findings position ICR to push the boundary of ICL's practical value.
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2509.22412.pdf' target='_blank'>https://arxiv.org/pdf/2509.22412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Kashiani, Niloufar Alipour Talemi, Fatemeh Afghah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22412">FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake detectors often struggle to generalize to novel forgery types due to biases learned from limited training data. In this paper, we identify a new type of model bias in the frequency domain, termed spectral bias, where detectors overly rely on specific frequency bands, restricting their ability to generalize across unseen forgeries. To address this, we propose FreqDebias, a frequency debiasing framework that mitigates spectral bias through two complementary strategies. First, we introduce a novel Forgery Mixup (Fo-Mixup) augmentation, which dynamically diversifies frequency characteristics of training samples. Second, we incorporate a dual consistency regularization (CR), which enforces both local consistency using class activation maps (CAMs) and global consistency through a von Mises-Fisher (vMF) distribution on a hyperspherical embedding space. This dual CR mitigates over-reliance on certain frequency components by promoting consistent representation learning under both local and global supervision. Extensive experiments show that FreqDebias significantly enhances cross-domain generalization and outperforms state-of-the-art methods in both cross-domain and in-domain settings.
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2508.21104.pdf' target='_blank'>https://arxiv.org/pdf/2508.21104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenfeng Feng, Penghong Zhao, Guochao Jiang, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21104">PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts during training. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Moreover, PVPO is orthogonal to other advanced critic-free RL algorithms, making it compatible with and complementary to these methods. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2507.21533.pdf' target='_blank'>https://arxiv.org/pdf/2507.21533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler Han, Yanda Bao, Bhaumik Mehta, Gabriel Guo, Anubhav Vishwakarma, Emily Kang, Sanghun Jung, Rosario Scalise, Jason Zhou, Bryan Xu, Byron Boots
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21533">Model Predictive Adversarial Imitation Learning for Planning from Observation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human demonstration data is often ambiguous and incomplete, motivating imitation learning approaches that also exhibit reliable planning behavior. A common paradigm to perform planning-from-demonstration involves learning a reward function via Inverse Reinforcement Learning (IRL) then deploying this reward via Model Predictive Control (MPC). Towards unifying these methods, we derive a replacement of the policy in IRL with a planning-based agent. With connections to Adversarial Imitation Learning, this formulation enables end-to-end interactive learning of planners from observation-only demonstrations. In addition to benefits in interpretability, complexity, and safety, we study and observe significant improvements on sample efficiency, out-of-distribution generalization, and robustness. The study includes evaluations in both simulated control benchmarks and real-world navigation experiments using few-to-single observation-only demonstrations.
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2507.13089.pdf' target='_blank'>https://arxiv.org/pdf/2507.13089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Peng, Pengfei Wang, Jianzhuang Liu, Shifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13089">GLAD: Generalizable Tuning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained vision-language models, such as CLIP, show impressive zero-shot recognition ability and can be easily transferred to specific downstream tasks via prompt tuning, even with limited training data. However, existing prompt tuning methods face two main challenges: (1) In few-shot scenarios, data scarcity often leads to overfitting, making the model sensitive to changes in the input domain. (2) To mitigate overfitting, these methods typically rely on complex task-specific model architectures and sensitive hyperparameter tuning, severely restricting their general applicability. To address these issues, we propose a simpler and more general framework called GLAD (Generalizable LoRA tuning with RegulArized GraDient). We show that merely applying LoRA achieves performance in downstream tasks comparable to current state-of-the-art prompt-based methods. While LoRA is effective and easy to use, it remains susceptible to overfitting in few-shot learning scenarios. To mitigate this risk, we introduce a gradient-based regularization technique. This technique effectively steers the optimization trajectory, encouraging the model to find a more stable parameter region that is robust to variations in data distribution. Through extensive experiments conducted on 15 benchmark datasets, we demonstrate that GLAD outperforms previous tuning approaches in terms of base-to-novel class generalization, image domain generalization, and cross-dataset generalization. The code will be publicly available.
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2507.07313.pdf' target='_blank'>https://arxiv.org/pdf/2507.07313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alan Malek, Jiawei Ge, Nevena Lazic, Chi Jin, AndrÃ¡s GyÃ¶rgy, Csaba SzepesvÃ¡ri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07313">Frontier LLMs Still Struggle with Simple Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While state-of-the-art large language models (LLMs) demonstrate advanced reasoning capabilities-achieving remarkable performance on challenging competitive math and coding benchmarks-they also frequently fail on tasks that are easy for humans. This work studies the performance of frontier LLMs on a broad set of such "easy" reasoning problems. By extending previous work in the literature, we create a suite of procedurally generated simple reasoning tasks, including counting, first-order logic, proof trees, and travel planning, with changeable parameters (such as document length. or the number of variables in a math problem) that can arbitrarily increase the amount of computation required to produce the answer while preserving the fundamental difficulty. While previous work showed that traditional, non-thinking models can be made to fail on such problems, we demonstrate that even state-of-the-art thinking models consistently fail on such problems and for similar reasons (e.g. statistical shortcuts, errors in intermediate steps, and difficulties in processing long contexts). To further understand the behavior of the models, we introduce the unpuzzles dataset, a different "easy" benchmark consisting of trivialized versions of well-known math and logic puzzles. Interestingly, while modern LLMs excel at solving the original puzzles, they tend to fail on the trivialized versions, exhibiting several systematic failure patterns related to memorizing the originals. We show that this happens even if the models are otherwise able to solve problems with different descriptions but requiring the same logic. Our results highlight that out-of-distribution generalization is still problematic for frontier language models and the new generation of thinking models, even for simple reasoning tasks, and making tasks easier does not necessarily imply improved performance.
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2506.21895.pdf' target='_blank'>https://arxiv.org/pdf/2506.21895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangling Jiang, Qi Li, Weining Wang, Gang Wang, Bing Liu, Zhenan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21895">Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently the emergence of novel presentation attacks has drawn increasing attention to face anti-spoofing. However, existing methods tend to memorize data patterns from the training set, resulting in poor generalization to unknown attack types across different scenarios and limited interpretability. To address these challenges, this paper presents a reinforcement fine-tuning-based face anti-spoofing method that stimulates the capabilities of multimodal large language models to think and learn how to solve the anti-spoofing task itself, rather than relying on the memorization of authenticity patterns. We design verifiable class consistent reward and reasoning consistent reward, and employ a GRPO-based optimization strategy to guide the model in exploring reasoning policies from multiple perspectives to maximize expected rewards. As a result, through iterative trial-and-error learning while retaining only high-reward trajectories, the model distills highly generalizable decision-making rules from the extensive solution space to effectively address cross-domain face anti-spoofing tasks. Extensive experimental results demonstrate that our method achieves state-of-the-art cross-domain generalization performance. It generalizes well to diverse unknown attack types in unseen target domains while providing interpretable reasoning for its authenticity decisions without requiring labor-intensive textual annotations for training.
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2506.17868.pdf' target='_blank'>https://arxiv.org/pdf/2506.17868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Testa, SÃ¸ren Hauberg, Tamim Asfour, Leonel Rozo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17868">Geometric Contact Flows: Contactomorphisms for Dynamics and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately modeling and predicting complex dynamical systems, particularly those involving force exchange and dissipation, is crucial for applications ranging from fluid dynamics to robotics, but presents significant challenges due to the intricate interplay of geometric constraints and energy transfer. This paper introduces Geometric Contact Flows (GFC), a novel framework leveraging Riemannian and Contact geometry as inductive biases to learn such systems. GCF constructs a latent contact Hamiltonian model encoding desirable properties like stability or energy conservation. An ensemble of contactomorphisms then adapts this model to the target dynamics while preserving these properties. This ensemble allows for uncertainty-aware geodesics that attract the system's behavior toward the data support, enabling robust generalization and adaptation to unseen scenarios. Experiments on learning dynamics for physical systems and for controlling robots on interaction tasks demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2506.13215.pdf' target='_blank'>https://arxiv.org/pdf/2506.13215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenlong Yuan, Dapeng Zhang, Zehao Li, Chengxuan Qian, Jianing Chen, Yinda Chen, Kehua Chen, Tianlu Mao, Zhaoxin Li, Hao Jiang, Zhaoqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13215">DVP-MVS++: Synergize Depth-Normal-Edge and Harmonized Visibility Prior for Multi-View Stereo</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, patch deformation-based methods have demonstrated significant effectiveness in multi-view stereo due to their incorporation of deformable and expandable perception for reconstructing textureless areas. However, these methods generally focus on identifying reliable pixel correlations to mitigate matching ambiguity of patch deformation, while neglecting the deformation instability caused by edge-skipping and visibility occlusions, which may cause potential estimation deviations. To address these issues, we propose DVP-MVS++, an innovative approach that synergizes both depth-normal-edge aligned and harmonized cross-view priors for robust and visibility-aware patch deformation. Specifically, to avoid edge-skipping, we first apply DepthPro, Metric3Dv2 and Roberts operator to generate coarse depth maps, normal maps and edge maps, respectively. These maps are then aligned via an erosion-dilation strategy to produce fine-grained homogeneous boundaries for facilitating robust patch deformation. Moreover, we reformulate view selection weights as visibility maps, and then implement both an enhanced cross-view depth reprojection and an area-maximization strategy to help reliably restore visible areas and effectively balance deformed patch, thus acquiring harmonized cross-view priors for visibility-aware patch deformation. Additionally, we obtain geometry consistency by adopting both aggregated normals via view selection and projection depth differences via epipolar lines, and then employ SHIQ for highlight correction to enable geometry consistency with highlight-aware perception, thus improving reconstruction quality during propagation and refinement stage. Evaluation results on ETH3D, Tanks & Temples and Strecha datasets exhibit the state-of-the-art performance and robust generalization capability of our proposed method.
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2506.09260.pdf' target='_blank'>https://arxiv.org/pdf/2506.09260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibin Lei, Tao Shen, Andrew Yates
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09260">ThinkQE: Query Expansion via an Evolving Thinking Process</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective query expansion for web search benefits from promoting both exploration and result diversity to capture multiple interpretations and facets of a query. While recent LLM-based methods have improved retrieval performance and demonstrate strong domain generalization without additional training, they often generate narrowly focused expansions that overlook these desiderata. We propose ThinkQE, a test-time query expansion framework addressing this limitation through two key components: a thinking-based expansion process that encourages deeper and comprehensive semantic exploration, and a corpus-interaction strategy that iteratively refines expansions using retrieval feedback from the corpus. Experiments on diverse web search benchmarks (DL19, DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches, including training-intensive dense retrievers and rerankers.
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2506.02935.pdf' target='_blank'>https://arxiv.org/pdf/2506.02935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuepeng Zheng, Fu Luo, Zhenkun Wang, Yaoxin Wu, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02935">MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a promising approach to train a unified model capable of solving multiple Vehicle Routing Problem (VRP) variants. However, existing Reinforcement Learning (RL)-based multi-task methods can only train light decoder models on small-scale problems, exhibiting limited generalization ability when solving large-scale problems. To overcome this limitation, this work introduces a novel multi-task learning method driven by knowledge distillation (MTL-KD), which enables the efficient training of heavy decoder models with strong generalization ability. The proposed MTL-KD method transfers policy knowledge from multiple distinct RL-based single-task models to a single heavy decoder model, facilitating label-free training and effectively improving the model's generalization ability across diverse tasks. In addition, we introduce a flexible inference strategy termed Random Reordering Re-Construction (R3C), which is specifically adapted for diverse VRP tasks and further boosts the performance of the multi-task model. Experimental results on 6 seen and 10 unseen VRP variants with up to 1000 nodes indicate that our proposed method consistently achieves superior performance on both uniform and real-world benchmarks, demonstrating robust generalization abilities.
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2505.19373.pdf' target='_blank'>https://arxiv.org/pdf/2505.19373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niloufar Alipour Talemi, Hossein Kashiani, Hossein R. Nowdeh, Fatemeh Afghah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19373">DiSa: Directional Saliency-Aware Prompt Learning for Generalizable Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has emerged as a powerful paradigm for adapting vision-language models such as CLIP to downstream tasks. However, existing methods often overfit to seen data, leading to significant performance degradation when generalizing to novel classes or unseen domains. To address this limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning framework that integrates two complementary regularization strategies to enhance generalization. First, our Cross-Interactive Regularization (CIR) fosters cross-modal alignment by enabling cooperative learning between prompted and frozen encoders. Within CIR, a saliency-aware masking strategy guides the image encoder to prioritize semantically critical image regions, reducing reliance on less informative patches. Second, we introduce a directional regularization strategy that aligns visual embeddings with class-wise prototype features in a directional manner to prioritize consistency in feature orientation over strict proximity. This approach ensures robust generalization by leveraging stable prototype directions derived from class-mean statistics. Extensive evaluations on 11 diverse image classification benchmarks demonstrate that DiSa consistently outperforms state-of-the-art prompt learning methods across various settings, including base-to-novel generalization, cross-dataset transfer, domain generalization, and few-shot learning.
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2505.15734.pdf' target='_blank'>https://arxiv.org/pdf/2505.15734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Srivastava, Zhenyu Bi, Meng Lu, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15734">DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on five reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities.
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2505.12391.pdf' target='_blank'>https://arxiv.org/pdf/2505.12391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyang Lu, Qian Xia, Weifan Wang, Feng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12391">CLIP-aware Domain-Adaptive Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a novel framework that addresses the critical challenge of domain generalization in single image super-resolution. By leveraging the semantic capabilities of CLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented performance across diverse domains and extreme scaling factors. The proposed method integrates CLIP-guided feature alignment mechanism with a meta-learning inspired few-shot adaptation strategy, enabling efficient knowledge transfer and rapid adaptation to target domains. A custom domain-adaptive module processes CLIP features alongside super-resolution features through a multi-stage transformation process, including CLIP feature processing, spatial feature generation, and feature fusion. This intricate process ensures effective incorporation of semantic information into the super-resolution pipeline. Additionally, CDASR employs a multi-component loss function that combines pixel-wise reconstruction, perceptual similarity, and semantic consistency. Extensive experiments on benchmark datasets demonstrate CDASR's superiority, particularly in challenging scenarios. On the Urban100 dataset at $\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over existing methods, with even larger improvements of up to 0.30dB observed at $\times$16 scaling.
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2504.14151.pdf' target='_blank'>https://arxiv.org/pdf/2504.14151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio Arnaud, Paul McVay, Ada Martin, Arjun Majumdar, Krishna Murthy Jatavallabhula, Phillip Thomas, Ruslan Partsey, Daniel Dugas, Abha Gejji, Alexander Sax, Vincent-Pierre Berges, Mikael Henaff, Ayush Jain, Ang Cao, Ishita Prasad, Mrinal Kalakrishnan, Michael Rabbat, Nicolas Ballas, Mido Assran, Oleksandr Maksymets, Aravind Rajeswaran, Franziska Meier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14151">Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like "the small coffee table between the sofa and the lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model.
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2504.13111.pdf' target='_blank'>https://arxiv.org/pdf/2504.13111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Manas, Christian Schlauch, Adrian Paschke, Christian Wirth, Nadja Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13111">Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their out-of-distribution generalization remains a significant challenge, particularly due to unbalanced data and a lack of enough data and diversity to ensure robustness and calibration. To address this, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework that uniquely combines well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and employs heteroscedastic spectral-normalized Gaussian processes to effectively disentangle epistemic and aleatoric uncertainties. We learn informative priors from training labels, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations over the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Project page: https://kumarmanas.github.io/SHIFT/.
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2504.13111.pdf' target='_blank'>https://arxiv.org/pdf/2504.13111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Manas, Christian Schlauch, Adrian Paschke, Christian Wirth, Nadja Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13111">Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their out-of-distribution generalization remains a significant challenge, particularly due to unbalanced data and a lack of enough data and diversity to ensure robustness and calibration. To address this, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework that uniquely combines well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and employs heteroscedastic spectral-normalized Gaussian processes to effectively disentangle epistemic and aleatoric uncertainties. We learn informative priors from training labels, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations over the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Project page: https://kumarmanas.github.io/SHIFT/.
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2503.13721.pdf' target='_blank'>https://arxiv.org/pdf/2503.13721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenlong Yuan, Zhidong Yang, Yujun Cai, Kuangxin Wu, Mufan Liu, Dapeng Zhang, Hao Jiang, Zhaoxin Li, Zhaoqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13721">SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, patch-deformation methods have exhibited significant effectiveness in multi-view stereo owing to the deformable and expandable patches in reconstructing textureless areas. However, such methods primarily emphasize broadening the receptive field in textureless areas, while neglecting deformation instability caused by easily overlooked edge-skipping, potentially leading to matching distortions. To address this, we propose SED-MVS, which adopts panoptic segmentation and multi-trajectory diffusion strategy for segmentation-driven and edge-aligned patch deformation. Specifically, to prevent unanticipated edge-skipping, we first employ SAM2 for panoptic segmentation as depth-edge guidance to guide patch deformation, followed by multi-trajectory diffusion strategy to ensure patches are comprehensively aligned with depth edges. Moreover, to avoid potential inaccuracy of random initialization, we combine both sparse points from LoFTR and monocular depth map from DepthAnything V2 to restore reliable and realistic depth map for initialization and supervised guidance. Finally, we integrate segmentation image with monocular depth map to exploit inter-instance occlusion relationship, then further regard them as occlusion map to implement two distinct edge constraint, thereby facilitating occlusion-aware patch deformation. Extensive results on ETH3D, Tanks & Temples, BlendedMVS and Strecha datasets validate the state-of-the-art performance and robust generalization capability of our proposed method.
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2503.10177.pdf' target='_blank'>https://arxiv.org/pdf/2503.10177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yirong Sun, Yanjun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10177">PRISM: Preference Refinement via Implicit Scene Modeling for 3D Vision-Language Preference-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose PRISM, a novel framework designed to overcome the limitations of 2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point cloud modeling and future-aware preference refinement. At its core, PRISM adopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and viewpoint biases, ensuring more stable and spatially consistent preference signals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to incorporate long-horizon considerations, thereby preventing the short-sighted feedback often seen in static preference comparisons. In contrast to conventional PBRL techniques, this integration of 3D perception and future-oriented reasoning leads to significant gains in preference agreement rates, faster policy convergence, and robust generalization across unseen robotic environments. Our empirical results, spanning tasks such as robotic manipulation and autonomous navigation, highlight PRISM's potential for real-world applications where precise spatial understanding and reliable long-term decision-making are critical. By bridging 3D geometric awareness with CoT-driven preference modeling, PRISM establishes a comprehensive foundation for scalable, human-aligned reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2503.06158.pdf' target='_blank'>https://arxiv.org/pdf/2503.06158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziruo Hao, Zhenhua Cui, Tao Yang, Bo Hu, Xiaofeng Wu, Hui Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06158">Invariant Federated Learning for Edge Intelligence: Mitigating Heterogeneity and Asynchrony via Exit Strategy and Invariant Penalty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides an invariant federated learning system for resource-constrained edge intelligence. This framework can mitigate the impact of heterogeneity and asynchrony via exit strategy and invariant penalty. We introduce parameter orthogonality into edge intelligence to measure the contribution or impact of heterogeneous and asynchronous clients. It is proved in this paper that the exit of abnormal edge clients can guarantee the effect of the model on most clients. Meanwhile, to ensure the models' performance on exited abnormal clients and those who lack training resources, we propose Federated Learning with Invariant Penalty for Generalization (FedIPG) by constructing the approximate orthogonality of the invariant parameters and the heterogeneous parameters. Theoretical proof shows that FedIPG reduces the Out-Of-Distribution prediction loss without increasing the communication burden. The performance of FedIPG combined with an exit strategy is tested empirically in multiple scales using four datasets. It shows our system can enhance In-Distribution performance and outperform the state-of-the-art algorithm in Out-Of-Distribution generalization while maintaining model convergence. Additionally, the results of the visual experiment prove that FedIPG contains preliminary causality in terms of ignoring confounding features.
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2502.20162.pdf' target='_blank'>https://arxiv.org/pdf/2502.20162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aristotelis Ballas, Christos Diou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20162">Gradient-Guided Annealing for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) research has gained considerable traction as of late, since the ability to generalize to unseen data distributions is a requirement that eludes even state-of-the-art training algorithms. In this paper we observe that the initial iterations of model training play a key role in domain generalization effectiveness, since the loss landscape may be significantly different across the training and test distributions, contrary to the case of i.i.d. data. Conflicts between gradients of the loss components of each domain lead the optimization procedure to undesirable local minima that do not capture the domain-invariant features of the target classes. We propose alleviating domain conflicts in model optimization, by iteratively annealing the parameters of a model in the early stages of training and searching for points where gradients align between domains. By discovering a set of parameter values where gradients are updated towards the same direction for each data distribution present in the training set, the proposed Gradient-Guided Annealing (GGA) algorithm encourages models to seek out minima that exhibit improved robustness against domain shifts. The efficacy of GGA is evaluated on five widely accepted and challenging image classification domain generalization benchmarks, where its use alone is able to establish highly competitive or even state-of-the-art performance. Moreover, when combined with previously proposed domain-generalization algorithms it is able to consistently improve their effectiveness by significant margins.
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2502.13310.pdf' target='_blank'>https://arxiv.org/pdf/2502.13310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adib Mosharrof, Moghis Fereidouni, A. B. Siddique
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13310">Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog Systems for Task Completion without Turn-level Dialog Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional task-oriented dialog (ToD) systems rely heavily on labor-intensive turn-level annotations, such as dialogue states and policy labels, for training. This work explores whether large language models (LLMs) can be fine-tuned solely on natural language dialogs to perform ToD tasks, without requiring such annotations. We evaluate their ability to generalize to unseen domains and compare their performance with models trained on fully annotated data. Through extensive experiments with three open-source LLMs of varying sizes and two diverse ToD datasets, we find that models fine-tuned without turn-level annotations generate coherent and contextually appropriate responses. However, their task completion performance - measured by accurate execution of API calls - remains suboptimal, with the best models achieving only around 53% success in unseen domains. To improve task completion, we propose ZeroToD, a framework that incorporates a schema augmentation mechanism to enhance API call accuracy and overall task completion rates, particularly in out-of-domain settings. We also compare ZeroToD with fine-tuning-free alternatives, such as prompting off-the-shelf LLMs, and find that our framework enables smaller, fine-tuned models that outperform large-scale proprietary LLMs in task completion. Additionally, a human study evaluating informativeness, fluency, and task completion confirms our empirical findings. These findings suggest the feasibility of developing cost-effective, scalable, and zero-shot generalizable ToD systems for real-world applications.
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2502.12195.pdf' target='_blank'>https://arxiv.org/pdf/2502.12195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sameer Ambekar, Zehao Xiao, Xiantong Zhen, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12195">GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of test-time domain generalization, where a model is trained on several source domains and adjusted on target domains never seen during training. Different from the common methods that fine-tune the model or adjust the classifier parameters online, we propose to generate multiple layer parameters on the fly during inference by a lightweight meta-learned transformer, which we call \textit{GeneralizeFormer}. The layer-wise parameters are generated per target batch without fine-tuning or online adjustment. By doing so, our method is more effective in dynamic scenarios with multiple target distributions and also avoids forgetting valuable source distribution characteristics. Moreover, by considering layer-wise gradients, the proposed method adapts itself to various distribution shifts. To reduce the computational and time cost, we fix the convolutional parameters while only generating parameters of the Batch Normalization layers and the linear classifier. Experiments on six widely used domain generalization datasets demonstrate the benefits and abilities of the proposed method to efficiently handle various distribution shifts, generalize in dynamic scenarios, and avoid forgetting.
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2502.09660.pdf' target='_blank'>https://arxiv.org/pdf/2502.09660.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Yao, Qiushi Yang, Miaomiao Cui, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09660">Towards Fine-grained Interactive Segmentation in Images and Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent Segment Anything Models (SAMs) have emerged as foundational visual models for general interactive segmentation. Despite demonstrating robust generalization abilities, they still suffer performance degradations in scenarios demanding accurate masks. Existing methods for high-precision interactive segmentation face a trade-off between the ability to perceive intricate local details and maintaining stable prompting capability, which hinders the applicability and effectiveness of foundational segmentation models. To this end, we present an SAM2Refiner framework built upon the SAM2 backbone. This architecture allows SAM2 to generate fine-grained segmentation masks for both images and videos while preserving its inherent strengths. Specifically, we design a localization augment module, which incorporates local contextual cues to enhance global features via a cross-attention mechanism, thereby exploiting potential detailed patterns and maintaining semantic information. Moreover, to strengthen the prompting ability toward the enhanced object embedding, we introduce a prompt retargeting module to renew the embedding with spatially aligned prompt features. In addition, to obtain accurate high resolution segmentation masks, a mask refinement module is devised by employing a multi-scale cascaded structure to fuse mask features with hierarchical representations from the encoder. Extensive experiments demonstrate the effectiveness of our approach, revealing that the proposed method can produce highly precise masks for both images and videos, surpassing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2502.00197.pdf' target='_blank'>https://arxiv.org/pdf/2502.00197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingshan Chang, Yonatan Bisk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00197">Learning Model Successors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The notion of generalization has moved away from the classical one defined in statistical learning theory towards an emphasis on out-of-domain generalization (OODG). There has been a growing focus on generalization from easy to hard, where a progression of difficulty implicitly governs the direction of domain shifts. This emerging regime has appeared in the literature under different names, such as length/logical/algorithmic extrapolation, but a formal definition is lacking. We argue that the unifying theme is induction -- based on finite samples observed in training, a learner should infer an inductive principle that applies in an unbounded manner. This work formalizes the notion of inductive generalization along a difficulty progression and argues that our path ahead lies in transforming the learning paradigm. We attempt to make inroads by proposing a novel learning paradigm, Inductive Learning, which involves a central concept called model successors. We outline practical steps to adapt well-established techniques towards learning model successors. This work calls for restructuring of the research discussion around induction and generalization from fragmented task-centric communities to a more unified effort, focused on universal properties of learning and computation.
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2501.04942.pdf' target='_blank'>https://arxiv.org/pdf/2501.04942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Falih Gozi Febrinanto, Kristen Moore, Chandra Thapa, Jiangang Ma, Vidya Saikrishna, Feng Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04942">Vision Graph Non-Contrastive Learning for Audio Deepfake Detection with Limited Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in audio deepfake detection have leveraged graph neural networks (GNNs) to model frequency and temporal interdependencies in audio data, effectively identifying deepfake artifacts. However, the reliance of GNN-based methods on substantial labeled data for graph construction and robust performance limits their applicability in scenarios with limited labeled data. Although vast amounts of audio data exist, the process of labeling samples as genuine or fake remains labor-intensive and costly. To address this challenge, we propose SIGNL (Spatio-temporal vIsion Graph Non-contrastive Learning), a novel framework that maintains high GNN performance in low-label settings. SIGNL constructs spatio-temporal graphs by representing patches from the audio's visual spectrogram as nodes. These graph structures are modeled using vision graph convolutional (GC) encoders pre-trained through graph non-contrastive learning, a label-free that maximizes the similarity between positive pairs. The pre-trained encoders are then fine-tuned for audio deepfake detection, reducing reliance on labeled data. Experiments demonstrate that SIGNL outperforms state-of-the-art baselines across multiple audio deepfake detection datasets, achieving the lowest Equal Error Rate (EER) with as little as 5% labeled data. Additionally, SIGNL exhibits strong cross-domain generalization, achieving the lowest EER in evaluations involving diverse attack types and languages in the In-The-Wild dataset.
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2412.14140.pdf' target='_blank'>https://arxiv.org/pdf/2412.14140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darshan Deshpande, Selvan Sunitha Ravi, Sky CH-Wang, Bartosz Mielczarek, Anand Kannappan, Rebecca Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14140">GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The LLM-as-judge paradigm is increasingly being adopted for automated evaluation of model outputs. While LLM judges have shown promise on constrained evaluation tasks, closed source LLMs display critical shortcomings when deployed in real world applications due to challenges of fine grained metrics and explainability, while task specific evaluation models lack cross-domain generalization. We introduce GLIDER, a powerful 3B evaluator LLM that can score any text input and associated context on arbitrary user defined criteria. GLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly outperforms prior evaluation models, achieving comparable performance to LLMs 17x its size. GLIDER supports fine-grained scoring, multilingual reasoning, span highlighting and was trained on 685 domains and 183 criteria. Extensive qualitative analysis shows that GLIDER scores are highly correlated with human judgments, with 91.3% human agreement. We have open-sourced GLIDER to facilitate future research.
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2412.04077.pdf' target='_blank'>https://arxiv.org/pdf/2412.04077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokju Yun, Seunghye Chae, Dongheon Lee, Youngmin Ro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04077">SoMA: Singular Value Decomposed Minor Components Adaptation for Domain Generalizable Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to adapt a model using one or multiple source domains to ensure robust performance in unseen target domains. Recently, Parameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising results in the context of DG problem. Nevertheless, existing PEFT methods still struggle to strike a balance between preserving generalizable components of the pre-trained model and learning task-specific features. To gain insights into the distribution of generalizable components, we begin by analyzing the pre-trained weights through the lens of singular value decomposition. Building on these insights, we introduce Singular Value Decomposed Minor Components Adaptation (SoMA), an approach that selectively tunes minor singular components while keeping the residual parts frozen. SoMA effectively retains the generalization ability of the pre-trained model while efficiently acquiring task-specific skills. Moreover, we freeze domain-generalizable blocks and employ an annealing weight decay strategy, thereby achieving an optimal balance in the delicate trade-off between generalizability and discriminability. SoMA attains state-of-the-art results on multiple benchmarks that span both domain generalized semantic segmentation to domain generalized object detection. In addition, our methods introduce no additional inference overhead or regularization loss, maintain compatibility with any backbone or head, and are designed to be versatile, allowing easy integration into a wide range of tasks.
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2412.03897.pdf' target='_blank'>https://arxiv.org/pdf/2412.03897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhu Han, Ce Zhang, Lianru Gao, Zhiqiang Zeng, Michael K. Ng, Bing Zhang, Jocelyn Chanussot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03897">Multisource Collaborative Domain Generalization for Cross-Scene Remote Sensing Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-scene image classification aims to transfer prior knowledge of ground materials to annotate regions with different distributions and reduce hand-crafted cost in the field of remote sensing. However, existing approaches focus on single-source domain generalization to unseen target domains, and are easily confused by large real-world domain shifts due to the limited training information and insufficient diversity modeling capacity. To address this gap, we propose a novel multi-source collaborative domain generalization framework (MS-CDG) based on homogeneity and heterogeneity characteristics of multi-source remote sensing data, which considers data-aware adversarial augmentation and model-aware multi-level diversification simultaneously to enhance cross-scene generalization performance. The data-aware adversarial augmentation adopts an adversary neural network with semantic guide to generate MS samples by adaptively learning realistic channel and distribution changes across domains. In views of cross-domain and intra-domain modeling, the model-aware diversification transforms the shared spatial-channel features of MS data into the class-wise prototype and kernel mixture module, to address domain discrepancies and cluster different classes effectively. Finally, the joint classification of original and augmented MS samples is employed by introducing a distribution consistency alignment to increase model diversity and ensure better domain-invariant representation learning. Extensive experiments on three public MS remote sensing datasets demonstrate the superior performance of the proposed method when benchmarked with the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2411.17763.pdf' target='_blank'>https://arxiv.org/pdf/2411.17763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Zixuan Huang, Anh Thai, James M. Rehg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17763">Symmetry Strikes Back: From Single-Image Symmetry Detection to 3D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Symmetry is a ubiquitous and fundamental property in the visual world, serving as a critical cue for perception and structure interpretation. This paper investigates the detection of 3D reflection symmetry from a single RGB image, and reveals its significant benefit on single-image 3D generation. We introduce Reflect3D, a scalable, zero-shot symmetry detector capable of robust generalization to diverse and real-world scenarios. Inspired by the success of foundation models, our method scales up symmetry detection with a transformer-based architecture. We also leverage generative priors from multi-view diffusion models to address the inherent ambiguity in single-view symmetry detection. Extensive evaluations on various data sources demonstrate that Reflect3D establishes a new state-of-the-art in single-image symmetry detection. Furthermore, we show the practical benefit of incorporating detected symmetry into single-image 3D generation pipelines through a symmetry-aware optimization process. The integration of symmetry significantly enhances the structural accuracy, cohesiveness, and visual fidelity of the reconstructed 3D geometry and textures, advancing the capabilities of 3D content creation.
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2411.17458.pdf' target='_blank'>https://arxiv.org/pdf/2411.17458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, Yueting Zhuang, Yiqi Huang, Luhui Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17458">Spatially Visual Perception for End-to-End Robotic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in imitation learning have shown significant promise for robotic control and embodied intelligence. However, achieving robust generalization across diverse mounted camera observations remains a critical challenge. In this paper, we introduce a video-based spatial perception framework that leverages 3D spatial representations to address environmental variability, with a focus on handling lighting changes. Our approach integrates a novel image augmentation technique, AugBlender, with a state-of-the-art monocular depth estimation model trained on internet-scale data. Together, these components form a cohesive system designed to enhance robustness and adaptability in dynamic scenarios. Our results demonstrate that our approach significantly boosts the success rate across diverse camera exposures, where previous models experience performance collapse. Our findings highlight the potential of video-based spatial perception models in advancing robustness for end-to-end robotic learning, paving the way for scalable, low-cost solutions in embodied intelligence.
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2411.16018.pdf' target='_blank'>https://arxiv.org/pdf/2411.16018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niloufar Alipour Talemi, Hossein Kashiani, Fatemeh Afghah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16018">Style-Pro: Style-Guided Prompt Learning for Generalizable Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained Vision-language (VL) models, such as CLIP, have shown significant generalization ability to downstream tasks, even with minimal fine-tuning. While prompt learning has emerged as an effective strategy to adapt pre-trained VL models for downstream tasks, current approaches frequently encounter severe overfitting to specific downstream data distributions. This overfitting constrains the original behavior of the VL models to generalize to new domains or unseen classes, posing a critical challenge in enhancing the adaptability and generalization of VL models. To address this limitation, we propose Style-Pro, a novel style-guided prompt learning framework that mitigates overfitting and preserves the zero-shot generalization capabilities of CLIP. Style-Pro employs learnable style bases to synthesize diverse distribution shifts, guided by two specialized loss functions that ensure style diversity and content integrity. Then, to minimize discrepancies between unseen domains and the source domain, Style-Pro maps the unseen styles into the known style representation space as a weighted combination of style bases. Moreover, to maintain consistency between the style-shifted prompted model and the original frozen CLIP, Style-Pro introduces consistency constraints to preserve alignment in the learned embeddings, minimizing deviation during adaptation to downstream tasks. Extensive experiments across 11 benchmark datasets demonstrate the effectiveness of Style-Pro, consistently surpassing state-of-the-art methods in various settings, including base-to-new generalization, cross-dataset transfer, and domain generalization.
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2411.03687.pdf' target='_blank'>https://arxiv.org/pdf/2411.03687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehao Xiao, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03687">Beyond Model Adaptation at Test Time: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning algorithms have achieved remarkable success across various disciplines, use cases and applications, under the prevailing assumption that training and test samples are drawn from the same distribution. Consequently, these algorithms struggle and become brittle even when samples in the test distribution start to deviate from the ones observed during training. Domain adaptation and domain generalization have been studied extensively as approaches to address distribution shifts across test and train domains, but each has its limitations. Test-time adaptation, a recently emerging learning paradigm, combines the benefits of domain adaptation and domain generalization by training models only on source data and adapting them to target data during test-time inference. In this survey, we provide a comprehensive and systematic review on test-time adaptation, covering more than 400 recent papers. We structure our review by categorizing existing methods into five distinct categories based on what component of the method is adjusted for test-time adaptation: the model, the inference, the normalization, the sample, or the prompt, providing detailed analysis of each. We further discuss the various preparation and adaptation settings for methods within these categories, offering deeper insights into the effective deployment for the evaluation of distribution shifts and their real-world application in understanding images, video and 3D, as well as modalities beyond vision. We close the survey with an outlook on emerging research opportunities for test-time adaptation.
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2410.06128.pdf' target='_blank'>https://arxiv.org/pdf/2410.06128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Divyat Mahajan, Jannes Gladrow, Agrin Hilmkil, Cheng Zhang, Meyer Scetbon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06128">Amortized Inference of Causal Models via Conditional Fixed-Point Iterations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structural Causal Models (SCMs) offer a principled framework to reason about interventions and support out-of-distribution generalization, which are key goals in scientific discovery. However, the task of learning SCMs from observed data poses formidable challenges, and often requires training a separate model for each dataset. In this work, we propose amortized inference of SCMs by training a single model on multiple datasets sampled from different SCMs. We first use a transformer-based architecture for amortized learning of dataset embeddings, and then extend the Fixed-Point Approach (FiP) (Scetbon et al.) to infer SCMs conditionally on their dataset embeddings. As a byproduct, our method can generate observational and interventional data from novel SCMs at inference time, without updating parameters. Empirical results show that our amortized procedure performs on par with baselines trained specifically for each dataset on both in and out-of-distribution problems, and also outperforms them in scare data regimes.
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2407.13431.pdf' target='_blank'>https://arxiv.org/pdf/2407.13431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Yao, Shengchao Yan, Daniel Goehring, Wolfram Burgard, Joerg Reichardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13431">Improving Out-of-Distribution Generalization of Trajectory Prediction for Autonomous Driving via Polynomial Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robustness against Out-of-Distribution (OoD) samples is a key performance indicator of a trajectory prediction model. However, the development and ranking of state-of-the-art (SotA) models are driven by their In-Distribution (ID) performance on individual competition datasets. We present an OoD testing protocol that homogenizes datasets and prediction tasks across two large-scale motion datasets. We introduce a novel prediction algorithm based on polynomial representations for agent trajectory and road geometry on both the input and output sides of the model. With a much smaller model size, training effort, and inference time, we reach near SotA performance for ID testing and significantly improve robustness in OoD testing. Within our OoD testing protocol, we further study two augmentation strategies of SotA models and their effects on model generalization. Highlighting the contrast between ID and OoD performance, we suggest adding OoD testing to the evaluation criteria of trajectory prediction models.
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2407.13421.pdf' target='_blank'>https://arxiv.org/pdf/2407.13421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aristotelis Ballas, Christos Diou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13421">CycleMix: Mixing Source Domains for Domain Generalization in Style-Dependent Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As deep learning-based systems have become an integral part of everyday life, limitations in their generalization ability have begun to emerge. Machine learning algorithms typically rely on the i.i.d. assumption, meaning that their training and validation data are expected to follow the same distribution, which does not necessarily hold in practice. In the case of image classification, one frequent reason that algorithms fail to generalize is that they rely on spurious correlations present in training data, such as associating image styles with target classes. These associations may not be present in the unseen test data, leading to significant degradation of their effectiveness. In this work, we attempt to mitigate this Domain Generalization (DG) problem by training a robust feature extractor which disregards features attributed to image-style but infers based on style-invariant image representations. To achieve this, we train CycleGAN models to learn the different styles present in the training data and randomly mix them together to create samples with novel style attributes to improve generalization. Experimental results on the PACS DG benchmark validate the proposed method.
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2407.03036.pdf' target='_blank'>https://arxiv.org/pdf/2407.03036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bac Nguyen, Stefan Uhlich, Fabien Cardinaux, Lukas Mauch, Marzieh Edraki, Aaron Courville
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03036">SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Handling distribution shifts from training data, known as out-of-distribution (OOD) generalization, poses a significant challenge in the field of machine learning. While a pre-trained vision-language model like CLIP has demonstrated remarkable zero-shot performance, further adaptation of the model to downstream tasks leads to undesirable degradation for OOD data. In this work, we introduce Sparse Adaptation for Fine-Tuning (SAFT), a method that prevents fine-tuning from forgetting the general knowledge in the pre-trained model. SAFT only updates a small subset of important parameters whose gradient magnitude is large, while keeping the other parameters frozen. SAFT is straightforward to implement and conceptually simple. Extensive experiments show that with only 0.1% of the model parameters, SAFT can significantly improve the performance of CLIP. It consistently outperforms baseline methods across several benchmarks. On the few-shot learning benchmark of ImageNet and its variants, SAFT gives a gain of 5.15% on average over the conventional fine-tuning method in OOD settings.
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2406.18569.pdf' target='_blank'>https://arxiv.org/pdf/2406.18569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Qiu, Tao Zhu, Furong Duan, Kevin I-Kai Wang, Liming Chen, Mingxing Nie, Mingxing Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18569">FLOW: Fusing and Shuffling Global and Local Views for Cross-User Human Activity Recognition with IMUs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inertial Measurement Unit (IMU) sensors are widely employed for Human Activity Recognition (HAR) due to their portability, energy efficiency, and growing research interest. However, a significant challenge for IMU-HAR models is achieving robust generalization performance across diverse users. This limitation stems from substantial variations in data distribution among individual users. One primary reason for this distribution disparity lies in the representation of IMU sensor data in the local coordinate system, which is susceptible to subtle user variations during IMU wearing. To address this issue, we propose a novel approach that extracts a global view representation based on the characteristics of IMU data, effectively alleviating the data distribution discrepancies induced by wearing styles. To validate the efficacy of the global view representation, we fed both global and local view data into model for experiments. The results demonstrate that global view data significantly outperforms local view data in cross-user experiments. Furthermore, we propose a Multi-view Supervised Network (MVFNet) based on Shuffling to effectively fuse local view and global view data. It supervises the feature extraction of each view through view division and view shuffling, so as to avoid the model ignoring important features as much as possible. Extensive experiments conducted on OPPORTUNITY and PAMAP2 datasets demonstrate that the proposed algorithm outperforms the current state-of-the-art methods in cross-user HAR.
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2406.16935.pdf' target='_blank'>https://arxiv.org/pdf/2406.16935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Spandan Madan, Will Xiao, Mingran Cao, Hanspeter Pfister, Margaret Livingstone, Gabriel Kreiman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16935">Benchmarking Out-of-Distribution Generalization Capabilities of DNN-based Encoding Models for the Ventral Visual Cortex</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We characterized the generalization capabilities of DNN-based encoding models when predicting neuronal responses from the visual cortex. We collected \textit{MacaqueITBench}, a large-scale dataset of neural population responses from the macaque inferior temporal (IT) cortex to over $300,000$ images, comprising $8,233$ unique natural images presented to seven monkeys over $109$ sessions. Using \textit{MacaqueITBench}, we investigated the impact of distribution shifts on models predicting neural activity by dividing the images into Out-Of-Distribution (OOD) train and test splits. The OOD splits included several different image-computable types including image contrast, hue, intensity, temperature, and saturation. Compared to the performance on in-distribution test images -- the conventional way these models have been evaluated -- models performed worse at predicting neuronal responses to out-of-distribution images, retaining as little as $20\%$ of the performance on in-distribution test images. The generalization performance under OOD shifts can be well accounted by a simple image similarity metric -- the cosine distance between image representations extracted from a pre-trained object recognition model is a strong predictor of neural predictivity under different distribution shifts. The dataset of images, neuronal firing rate recordings, and computational benchmarks are hosted publicly at: https://bit.ly/3zeutVd.
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2406.10584.pdf' target='_blank'>https://arxiv.org/pdf/2406.10584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhengxu Li, Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Chen Liu, Yu Lan, Chao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10584">Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in prompt optimization have notably enhanced the performance of pre-trained language models (PLMs) on downstream tasks. However, the potential of optimized prompts on domain generalization has been under-explored. To explore the nature of prompt generalization on unknown domains, we conduct pilot experiments and find that (i) Prompts gaining more attention weight from PLMs' deep layers are more generalizable and (ii) Prompts with more stable attention distributions in PLMs' deep layers are more generalizable. Thus, we offer a fresh objective towards domain-generalizable prompts optimization named "Concentration", which represents the "lookback" attention from the current decoding token to the prompt tokens, to increase the attention strength on prompts and reduce the fluctuation of attention distribution. We adapt this new objective to popular soft prompt and hard prompt optimization methods, respectively. Extensive experiments demonstrate that our idea improves comparison prompt optimization methods by 1.42% for soft prompt generalization and 2.16% for hard prompt generalization in accuracy on the multi-source domain generalization setting, while maintaining satisfying in-domain performance. The promising results validate the effectiveness of our proposed prompt optimization objective and provide key insights into domain-generalizable prompts.
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2406.04848.pdf' target='_blank'>https://arxiv.org/pdf/2406.04848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Mao, Stefan Balauca, Martin Vechev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04848">CTBENCH: A Library and Benchmark for Certified Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training certifiably robust neural networks is an important but challenging task. While many algorithms for (deterministic) certified training have been proposed, they are often evaluated on different training schedules, certification methods, and systematically under-tuned hyperparameters, making it difficult to compare their performance. To address this challenge, we introduce CTBench, a unified library and a high-quality benchmark for certified training that evaluates all algorithms under fair settings and systematically tuned hyperparameters. We show that (1) almost all algorithms in CTBench surpass the corresponding reported performance in literature in the magnitude of algorithmic improvements, thus establishing new state-of-the-art, and (2) the claimed advantage of recent algorithms drops significantly when we enhance the outdated baselines with a fair training schedule, a fair certification method and well-tuned hyperparameters. Based on CTBench, we provide new insights into the current state of certified training, including (1) certified models have less fragmented loss surface, (2) certified models share many mistakes, (3) certified models have more sparse activations, (4) reducing regularization cleverly is crucial for certified training especially for large radii and (5) certified training has the potential to improve out-of-distribution generalization. We are confident that CTBench will serve as a benchmark and testbed for future research in certified training.
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2405.18405.pdf' target='_blank'>https://arxiv.org/pdf/2405.18405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Ma, Yulei Niu, Shiyuan Huang, Guangxing Han, Shih-Fu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18405">WIDIn: Wording Image for Domain-Invariant Representation in Single-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language has been useful in extending the vision encoder to data from diverse distributions without empirical discovery in training domains. However, as the image description is mostly at coarse-grained level and ignores visual details, the resulted embeddings are still ineffective in overcoming complexity of domains at inference time. We present a self-supervision framework WIDIn, Wording Images for Domain-Invariant representation, to disentangle discriminative visual representation, by only leveraging data in a single domain and without any test prior. Specifically, for each image, we first estimate the language embedding with fine-grained alignment, which can be consequently used to adaptively identify and then remove domain-specific counterpart from the raw visual embedding. WIDIn can be applied to both pretrained vision-language models like CLIP, and separately trained uni-modal models like MoCo and BERT. Experimental studies on three domain generalization datasets demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2404.02353.pdf' target='_blank'>https://arxiv.org/pdf/2404.02353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahiti Yerramilli, Jayant Sravan Tamarapalli, Tanmay Girish Kulkarni, Jonathan Francis, Eric Nyberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02353">Semantic Augmentation in Images using Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2403.17853.pdf' target='_blank'>https://arxiv.org/pdf/2403.17853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17853">Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2402.09530.pdf' target='_blank'>https://arxiv.org/pdf/2402.09530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Heinert, Matthias Rottmann, Kira Maag, Karsten Kahl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09530">Reducing Texture Bias of Deep Neural Networks via Edge Enhancing Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Convolutional neural networks (CNNs) for image processing tend to focus on localized texture patterns, commonly referred to as texture bias. While most of the previous works in the literature focus on the task of image classification, we go beyond this and study the texture bias of CNNs in semantic segmentation. In this work, we propose to train CNNs on pre-processed images with less texture to reduce the texture bias. Therein, the challenge is to suppress image texture while preserving shape information. To this end, we utilize edge enhancing diffusion (EED), an anisotropic image diffusion method initially introduced for image compression, to create texture reduced duplicates of existing datasets. Extensive numerical studies are performed with both CNNs and vision transformer models trained on original data and EED-processed data from the Cityscapes dataset and the CARLA driving simulator. We observe strong texture-dependence of CNNs and moderate texture-dependence of transformers. Training CNNs on EED-processed images enables the models to become completely ignorant with respect to texture, demonstrating resilience with respect to texture re-introduction to any degree. Additionally we analyze the performance reduction in depth on a level of connected components in the semantic segmentation and study the influence of EED pre-processing on domain generalization as well as adversarial robustness.
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2402.07834.pdf' target='_blank'>https://arxiv.org/pdf/2402.07834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiuhao Zeng, Wei Wang, Fan Zhou, Gezheng Xu, Ruizhi Pu, Changjian Shui, Christian Gagne, Shichun Yang, Boyu Wang, Charles X. Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07834">Generalizing across Temporal Domains with Koopman Operators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations between evolving domains. Through empirical evaluations conducted on synthetic and real-world datasets, we validate the effectiveness of our proposed approach.
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2401.11311.pdf' target='_blank'>https://arxiv.org/pdf/2401.11311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reda Bensaid, Vincent Gripon, FranÃ§ois Leduc-Primeau, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11311">A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot semantic segmentation (FSS) is a crucial challenge in computer vision, driving extensive research into a diverse range of methods, from advanced meta-learning techniques to simple transfer learning baselines. With the emergence of vision foundation models (VFM) serving as generalist feature extractors, we seek to explore the adaptation of these models for FSS. While current FSS benchmarks focus on adapting pre-trained models to new tasks with few images, they emphasize in-domain generalization, making them less suitable for VFM trained on large-scale web datasets. To address this, we propose a novel realistic benchmark with a simple and straightforward adaptation process tailored for this task. Using this benchmark, we conduct a comprehensive comparative analysis of prominent VFM and semantic segmentation models. To evaluate their effectiveness, we leverage various adaption methods, ranging from linear probing to parameter efficient fine-tuning (PEFT) and full fine-tuning. Our findings show that models designed for segmentation can be outperformed by self-supervised (SSL) models. On the other hand, while PEFT methods yields competitive performance, they provide little discrepancy in the obtained results compared to other methods, highlighting the critical role of the feature extractor in determining results. To our knowledge, this is the first study on the adaptation of VFM for FSS.
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2401.03597.pdf' target='_blank'>https://arxiv.org/pdf/2401.03597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Ding, Yan Wang, Guanfeng Liu, Nan Wang, Xiaofang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03597">Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Heterogeneous graph few-shot learning (HGFL) has been developed to address the label sparsity issue in heterogeneous graphs (HGs), which consist of various types of nodes and edges. The core concept of HGFL is to extract knowledge from rich-labeled classes in a source HG, transfer this knowledge to a target HG to facilitate learning new classes with few-labeled training data, and finally make predictions on unlabeled testing data. Existing methods typically assume that the source HG, training data, and testing data all share the same distribution. However, in practice, distribution shifts among these three types of data are inevitable due to two reasons: (1) the limited availability of the source HG that matches the target HG distribution, and (2) the unpredictable data generation mechanism of the target HG. Such distribution shifts result in ineffective knowledge transfer and poor learning performance in existing methods, thereby leading to a novel problem of out-of-distribution (OOD) generalization in HGFL. To address this challenging problem, we propose a novel Causal OOD Heterogeneous graph Few-shot learning model, namely COHF. In COHF, we first characterize distribution shifts in HGs with a structural causal model, establishing an invariance principle for OOD generalization in HGFL. Then, following this invariance principle, we propose a new variational autoencoder-based heterogeneous graph neural network to mitigate the impact of distribution shifts. Finally, by integrating this network with a novel meta-learning framework, COHF effectively transfers knowledge to the target HG to predict new classes with few-labeled data. Extensive experiments on seven real-world datasets have demonstrated the superior performance of COHF over the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2401.03156.pdf' target='_blank'>https://arxiv.org/pdf/2401.03156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wang, Shuang Liu, Xiao-Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03156">Data-Dependent Stability Analysis of Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stability analysis is an essential aspect of studying the generalization ability of deep learning, as it involves deriving generalization bounds for stochastic gradient descent-based training algorithms. Adversarial training is the most widely used defense against adversarial example attacks. However, previous generalization bounds for adversarial training have not included information regarding the data distribution. In this paper, we fill this gap by providing generalization bounds for stochastic gradient descent-based adversarial training that incorporate data distribution information. We utilize the concepts of on-average stability and high-order approximate Lipschitz conditions to examine how changes in data distribution and adversarial budget can affect robust generalization gaps. Our derived generalization bounds for both convex and non-convex losses are at least as good as the uniform stability-based counterparts which do not include data distribution information. Furthermore, our findings demonstrate how distribution shifts from data poisoning attacks can impact robust generalization.
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2312.17263.pdf' target='_blank'>https://arxiv.org/pdf/2312.17263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Song, Fausto Giunchiglia, Yingji Li, Mingjie Tian, Hao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17263">TACIT: A Target-Agnostic Feature Disentanglement Framework for Cross-Domain Text Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain text classification aims to transfer models from label-rich source domains to label-poor target domains, giving it a wide range of practical applications. Many approaches promote cross-domain generalization by capturing domain-invariant features. However, these methods rely on unlabeled samples provided by the target domains, which renders the model ineffective when the target domain is agnostic. Furthermore, the models are easily disturbed by shortcut learning in the source domain, which also hinders the improvement of domain generalization ability. To solve the aforementioned issues, this paper proposes TACIT, a target domain agnostic feature disentanglement framework which adaptively decouples robust and unrobust features by Variational Auto-Encoders. Additionally, to encourage the separation of unrobust features from robust features, we design a feature distillation task that compels unrobust features to approximate the output of the teacher. The teacher model is trained with a few easy samples that are easy to carry potential unknown shortcuts. Experimental results verify that our framework achieves comparable results to state-of-the-art baselines while utilizing only source domain data.
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2312.10107.pdf' target='_blank'>https://arxiv.org/pdf/2312.10107.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jens MÃ¼ller, Lars KÃ¼hmichel, Martin Rohbeck, Stefan T. Radev, Ullrich KÃ¶the
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10107">Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we analyze the conditions under which information about the context of an input $X$ can improve the predictions of deep learning models in new domains. Following work in marginal transfer learning in Domain Generalization (DG), we formalize the notion of context as a permutation-invariant representation of a set of data points that originate from the same domain as the input itself. We offer a theoretical analysis of the conditions under which this approach can, in principle, yield benefits, and formulate two necessary criteria that can be easily verified in practice. Additionally, we contribute insights into the kind of distribution shifts for which the marginal transfer learning approach promises robustness. Empirical analysis shows that our criteria are effective in discerning both favorable and unfavorable scenarios. Finally, we demonstrate that we can reliably detect scenarios where a model is tasked with unwarranted extrapolation in out-of-distribution (OOD) domains, identifying potential failure cases. Consequently, we showcase a method to select between the most predictive and the most robust model, circumventing the well-known trade-off between predictive performance and robustness.
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2310.11011.pdf' target='_blank'>https://arxiv.org/pdf/2310.11011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aneesh Komanduri, Xintao Wu, Yongkai Wu, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11011">From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models have shown tremendous capability in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, tendency to induce spurious correlations, and poor out-of-distribution extrapolation. To remedy such challenges, recent work has proposed a shift toward causal generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interpretability. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual generation methods. We focus on fundamental theory, methodology, drawbacks, datasets, and metrics. Then, we cover applications of causal generative models in fairness, privacy, out-of-distribution generalization, precision medicine, and biological sciences. Lastly, we discuss open problems and fruitful research directions for future work in the field.
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2310.08215.pdf' target='_blank'>https://arxiv.org/pdf/2310.08215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BÃ¡lint MucsÃ¡nyi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08215">Trustworthy Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalize to small changes in the distribution, tend to be confident on novel data they have never seen, or cannot communicate the rationale behind their decisions effectively with the end users. Collectively, we face a trustworthiness issue with the current machine learning technology. This textbook on Trustworthy Machine Learning (TML) covers a theoretical and technical background of four key topics in TML: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. We discuss important classical and contemporary research papers of the aforementioned fields and uncover and connect their underlying intuitions. The book evolved from the homonymous course at the University of TÃ¼bingen, first offered in the Winter Semester of 2022/23. It is meant to be a stand-alone product accompanied by code snippets and various pointers to further sources on topics of TML. The dedicated website of the book is https://trustworthyml.io/.
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2308.14418.pdf' target='_blank'>https://arxiv.org/pdf/2308.14418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aristotelis Ballas, Christos Diou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14418">Multi-Scale and Multi-Layer Contrastive Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>During the past decade, deep neural networks have led to fast-paced progress and significant achievements in computer vision problems, for both academia and industry. Yet despite their success, state-of-the-art image classification approaches fail to generalize well in previously unseen visual contexts, as required by many real-world applications. In this paper, we focus on this domain generalization (DG) problem and argue that the generalization ability of deep convolutional neural networks can be improved by taking advantage of multi-layer and multi-scaled representations of the network. We introduce a framework that aims at improving domain generalization of image classifiers by combining both low-level and high-level features at multiple scales, enabling the network to implicitly disentangle representations in its latent space and learn domain-invariant attributes of the depicted objects. Additionally, to further facilitate robust representation learning, we propose a novel objective function, inspired by contrastive learning, which aims at constraining the extracted representations to remain invariant under distribution shifts. We demonstrate the effectiveness of our method by evaluating on the domain generalization datasets of PACS, VLCS, Office-Home and NICO. Through extensive experimentation, we show that our model is able to surpass the performance of previous DG methods and consistently produce competitive and state-of-the-art results in all datasets
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2308.06530.pdf' target='_blank'>https://arxiv.org/pdf/2308.06530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaoyu Li, Yachao Zhang, Xu MA, Yanyun Qu, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06530">BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal Unsupervised Domain Adaptation (UDA) aims to exploit the complementarity of 2D-3D data to overcome the lack of annotation in a new domain. However, UDA methods rely on access to the target domain during training, meaning the trained model only works in a specific target domain. In light of this, we propose cross-modal learning under bird's-eye view for Domain Generalization (DG) of 3D semantic segmentation, called BEV-DG. DG is more challenging because the model cannot access the target domain during training, meaning it needs to rely on cross-modal learning to alleviate the domain gap. Since 3D semantic segmentation requires the classification of each point, existing cross-modal learning is directly conducted point-to-point, which is sensitive to the misalignment in projections between pixels and points. To this end, our approach aims to optimize domain-irrelevant representation modeling with the aid of cross-modal learning under bird's-eye view. We propose BEV-based Area-to-area Fusion (BAF) to conduct cross-modal learning under bird's-eye view, which has a higher fault tolerance for point-level misalignment. Furthermore, to model domain-irrelevant representations, we propose BEV-driven Domain Contrastive Learning (BDCL) with the help of cross-modal learning under bird's-eye view. We design three domain generalization settings based on three 3D datasets, and BEV-DG significantly outperforms state-of-the-art competitors with tremendous margins in all settings.
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2307.15199.pdf' target='_blank'>https://arxiv.org/pdf/2307.15199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhyeong Cho, Gilhyun Nam, Sungyeon Kim, Hunmin Yang, Suha Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15199">PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. From these observations, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. The proposed method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training.
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2307.09723.pdf' target='_blank'>https://arxiv.org/pdf/2307.09723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honglin Mu, Wentian Xia, Wanxiang Che
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09723">Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sound classification models' performance suffers from generalizing on out-of-distribution (OOD) data. Numerous methods have been proposed to help the model generalize. However, most either introduce inference overheads or focus on long-lasting CNN-variants, while Transformers has been proven to outperform CNNs on numerous natural language processing and computer vision tasks. We propose FRITO, an effective regularization technique on Transformer's self-attention, to improve the model's generalization ability by limiting each sequence position's attention receptive field along the frequency dimension on the spectrogram. Experiments show that our method helps Transformer models achieve SOTA generalization performance on TAU 2020 and Nsynth datasets while saving 20% inference time.
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2305.16746.pdf' target='_blank'>https://arxiv.org/pdf/2305.16746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aristotelis Ballas, Christos Diou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16746">CNN Feature Map Augmentation for Single-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In search of robust and generalizable machine learning models, Domain Generalization (DG) has gained significant traction during the past few years. The goal in DG is to produce models which continue to perform well when presented with data distributions different from the ones available during training. While deep convolutional neural networks (CNN) have been able to achieve outstanding performance on downstream computer vision tasks, they still often fail to generalize on previously unseen data Domains. Therefore, in this work we focus on producing a model which is able to remain robust under data distribution shift and propose an alternative regularization technique for convolutional neural network architectures in the single-source DG image classification setting. To mitigate the problem caused by domain shift between source and target data, we propose augmenting intermediate feature maps of CNNs. Specifically, we pass them through a novel Augmentation Layer} to prevent models from overfitting on the training set and improve their cross-domain generalization. To the best of our knowledge, this is the first paper proposing such a setup for the DG image classification setting. Experiments on the DG benchmark datasets of PACS, VLCS, Office-Home and TerraIncognita validate the effectiveness of our method, in which our model surpasses state-of-the-art algorithms in most cases.
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2304.00502.pdf' target='_blank'>https://arxiv.org/pdf/2304.00502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aristotelis Ballas, Christos Diou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00502">CNNs with Multi-Level Attention for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the past decade, deep convolutional neural networks have achieved significant success in image classification and ranking and have therefore found numerous applications in multimedia content retrieval. Still, these models suffer from performance degradation when neural networks are tested on out-of-distribution scenarios or on data originating from previously unseen data Domains. In the present work, we focus on this problem of Domain Generalization and propose an alternative neural network architecture for robust, out-of-distribution image classification. We attempt to produce a model that focuses on the causal features of the depicted class for robust image classification in the Domain Generalization setting. To achieve this, we propose attending to multiple-levels of information throughout a Convolutional Neural Network and leveraging the most important attributes of an image by employing trainable attention mechanisms. To validate our method, we evaluate our model on four widely accepted Domain Generalization benchmarks, on which our model is able to surpass previously reported baselines in three out of four datasets and achieve the second best score in the fourth one.
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2303.16252.pdf' target='_blank'>https://arxiv.org/pdf/2303.16252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adib Mosharrof, M. H. Maqbool, A. B. Siddique
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.16252">Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-oriented dialog systems empower users to accomplish their goals by facilitating intuitive and expressive natural language interactions. State-of-the-art approaches in task-oriented dialog systems formulate the problem as a conditional sequence generation task and fine-tune pre-trained causal language models in the supervised setting. This requires labeled training data for each new domain or task, and acquiring such data is prohibitively laborious and expensive, thus making it a bottleneck for scaling systems to a wide range of domains. To overcome this challenge, we introduce a novel Zero-Shot generalizable end-to-end Task-oriented Dialog system, ZS-ToD, that leverages domain schemas to allow for robust generalization to unseen domains and exploits effective summarization of the dialog history. We employ GPT-2 as a backbone model and introduce a two-step training process where the goal of the first step is to learn the general structure of the dialog data and the second step optimizes the response generation as well as intermediate outputs, such as dialog state and system actions. As opposed to state-of-the-art systems that are trained to fulfill certain intents in the given domains and memorize task-specific conversational patterns, ZS-ToD learns generic task-completion skills by comprehending domain semantics via domain schemas and generalizing to unseen domains seamlessly. We conduct an extensive experimental evaluation on SGD and SGD-X datasets that span up to 20 unique domains and ZS-ToD outperforms state-of-the-art systems on key metrics, with an improvement of +17% on joint goal accuracy and +5 on inform. Additionally, we present a detailed ablation study to demonstrate the effectiveness of the proposed components and training mechanism
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2303.11338.pdf' target='_blank'>https://arxiv.org/pdf/2303.11338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aristotelis Ballas, Christos Diou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11338">Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their immense success in numerous fields, machine and deep learning systems have not yet been able to firmly establish themselves in mission-critical applications in healthcare. One of the main reasons lies in the fact that when models are presented with previously unseen, Out-of-Distribution samples, their performance deteriorates significantly. This is known as the Domain Generalization (DG) problem. Our objective in this work is to propose a benchmark for evaluating DG algorithms, in addition to introducing a novel architecture for tackling DG in biosignal classification. In this paper, we describe the Domain Generalization problem for biosignals, focusing on electrocardiograms (ECG) and electroencephalograms (EEG) and propose and implement an open-source biosignal DG evaluation benchmark. Furthermore, we adapt state-of-the-art DG algorithms from computer vision to the problem of 1D biosignal classification and evaluate their effectiveness. Finally, we also introduce a novel neural network architecture that leverages multi-layer representations for improved model generalizability. By implementing the above DG setup we are able to experimentally demonstrate the presence of the DG problem in ECG and EEG datasets. In addition, our proposed model demonstrates improved effectiveness compared to the baseline algorithms, exceeding the state-of-the-art in both datasets. Recognizing the significance of the distribution shift present in biosignal datasets, the presented benchmark aims at urging further research into the field of biomedical DG by simplifying the evaluation process of proposed algorithms. To our knowledge, this is the first attempt at developing an open-source framework for evaluating ECG and EEG DG algorithms.
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2303.09989.pdf' target='_blank'>https://arxiv.org/pdf/2303.09989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jens MÃ¼ller, Stefan T. Radev, Robert Schmier, Felix Draxler, Carsten Rother, Ullrich KÃ¶the
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09989">Finding Competence Regions in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data from a new domain whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of existing proxy scores as incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significant improvements of the average accuracy below a suitable incompetence threshold. However, the scores are not yet good enough to allow for a favorable accuracy/rejection trade-off in all tested domains. Surprisingly, our results also indicate that classifiers optimized for DG robustness do not outperform a naive Empirical Risk Minimization (ERM) baseline in the competence region, that is, where test samples elicit low incompetence scores.
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2303.02943.pdf' target='_blank'>https://arxiv.org/pdf/2303.02943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhui Li, Mingjia Li, Yaxing Wang, Chuan-Xian Ren, Xiaojie Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02943">Adaptive Texture Filtering for Single-Domain Generalized Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization in semantic segmentation aims to alleviate the performance degradation on unseen domains through learning domain-invariant features. Existing methods diversify images in the source domain by adding complex or even abnormal textures to reduce the sensitivity to domain specific features. However, these approaches depend heavily on the richness of the texture bank, and training them can be time-consuming. In contrast to importing textures arbitrarily or augmenting styles randomly, we focus on the single source domain itself to achieve generalization. In this paper, we present a novel adaptive texture filtering mechanism to suppress the influence of texture without using augmentation, thus eliminating the interference of domain-specific features. Further, we design a hierarchical guidance generalization network equipped with structure-guided enhancement modules, which purpose is to learn the domain-invariant generalized knowledge. Extensive experiments together with ablation studies on widely-used datasets are conducted to verify the effectiveness of the proposed model, and reveal its superiority over other state-of-the-art alternatives.
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2303.01710.pdf' target='_blank'>https://arxiv.org/pdf/2303.01710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangqi Gao, Hangqi Zhou, Yibo Gao, Xiahai Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01710">BayeSeg: Bayesian Modeling for Medical Image Segmentation with Interpretable Generalizability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the cross-domain distribution shift aroused from diverse medical imaging systems, many deep learning segmentation methods fail to perform well on unseen data, which limits their real-world applicability. Recent works have shown the benefits of extracting domain-invariant representations on domain generalization. However, the interpretability of domain-invariant features remains a great challenge. To address this problem, we propose an interpretable Bayesian framework (BayeSeg) through Bayesian modeling of image and label statistics to enhance model generalizability for medical image segmentation. Specifically, we first decompose an image into a spatial-correlated variable and a spatial-variant variable, assigning hierarchical Bayesian priors to explicitly force them to model the domain-stable shape and domain-specific appearance information respectively. Then, we model the segmentation as a locally smooth variable only related to the shape. Finally, we develop a variational Bayesian framework to infer the posterior distributions of these explainable variables. The framework is implemented with neural networks, and thus is referred to as deep Bayesian segmentation. Quantitative and qualitative experimental results on prostate segmentation and cardiac segmentation tasks have shown the effectiveness of our proposed method. Moreover, we investigated the interpretability of BayeSeg by explaining the posteriors and analyzed certain factors that affect the generalization ability through further ablation studies. Our code will be released via https://zmiclab.github.io/projects.html, once the manuscript is accepted for publication.
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2302.13046.pdf' target='_blank'>https://arxiv.org/pdf/2302.13046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sotiris Pelekis, Evangelos Karakolis, Francisco Silva, Vasileios Schoinas, Spiros Mouzakitis, Georgios Kormpakis, Nuno Amaro, John Psarras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13046">In Search of Deep Learning Architectures for Load Forecasting: A Comparative Analysis and the Impact of the Covid-19 Pandemic on Model Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In power grids, short-term load forecasting (STLF) is crucial as it contributes to the optimization of their reliability, emissions, and costs, while it enables the participation of energy companies in the energy market. STLF is a challenging task, due to the complex demand of active and reactive power from multiple types of electrical loads and their dependence on numerous exogenous variables. Amongst them, special circumstances, such as the COVID-19 pandemic, can often be the reason behind distribution shifts of load series. This work conducts a comparative study of Deep Learning (DL) architectures, namely Neural Basis Expansion Analysis Time Series Forecasting (N-BEATS), Long Short-Term Memory (LSTM), and Temporal Convolutional Networks (TCN), with respect to forecasting accuracy and training sustainability, meanwhile examining their out-of-distribution generalization capabilities during the COVID-19 pandemic era. A Pattern Sequence Forecasting (PSF) model is used as baseline. The case study focuses on day-ahead forecasts for the Portuguese national 15-minute resolution net load time series. The results can be leveraged by energy companies and network operators (i) to reinforce their forecasting toolkit with state-of-the-art DL models; (ii) to become aware of the serious consequences of crisis events on model performance; (iii) as a high-level model evaluation, deployment, and sustainability guide within a smart grid context.
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2302.11215.pdf' target='_blank'>https://arxiv.org/pdf/2302.11215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehao Xiao, Xiantong Zhen, Shengcai Liao, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11215">Energy-Based Test Sample Adaptation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose energy-based sample adaptation at test time for domain generalization. Where previous works adapt their models to target domains, we adapt the unseen target samples to source-trained models. To this end, we design a discriminative energy-based model, which is trained on source domains to jointly model the conditional distribution for classification and data distribution for sample adaptation. The model is optimized to simultaneously learn a classifier and an energy function. To adapt target samples to source distributions, we iteratively update the samples by energy minimization with stochastic gradient Langevin dynamics. Moreover, to preserve the categorical information in the sample during adaptation, we introduce a categorical latent variable into the energy-based model. The latent variable is learned from the original sample before adaptation by variational inference and fixed as a condition to guide the sample update. Experiments on six benchmarks for classification of images and microblog threads demonstrate the effectiveness of our proposal.
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2302.04419.pdf' target='_blank'>https://arxiv.org/pdf/2302.04419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaesik Yoon, Yi-Fu Wu, Heechul Bae, Sungjin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04419">An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised object-centric representation (OCR) learning has recently drawn attention as a new paradigm of visual representation. This is because of its potential of being an effective pre-training technique for various downstream tasks in terms of sample efficiency, systematic generalization, and reasoning. Although image-based reinforcement learning (RL) is one of the most important and thus frequently mentioned such downstream tasks, the benefit in RL has surprisingly not been investigated systematically thus far. Instead, most of the evaluations have focused on rather indirect metrics such as segmentation quality and object property prediction accuracy. In this paper, we investigate the effectiveness of OCR pre-training for image-based reinforcement learning via empirical experiments. For systematic evaluation, we introduce a simple object-centric visual RL benchmark and conduct experiments to answer questions such as ``Does OCR pre-training improve performance on object-centric tasks?'' and ``Can OCR pre-training help with out-of-distribution generalization?''. Our results provide empirical evidence for valuable insights into the effectiveness of OCR pre-training for RL and the potential limitations of its use in certain scenarios. Additionally, this study also examines the critical aspects of incorporating OCR pre-training in RL, including performance in a visually complex environment and the appropriate pooling layer to aggregate the object representations.
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2208.10930.pdf' target='_blank'>https://arxiv.org/pdf/2208.10930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunqing Zhao, Ngai-Man Cheung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.10930">FS-BAN: Born-Again Networks for Domain Generalization Few-Shot Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional Few-shot classification (FSC) aims to recognize samples from novel classes given limited labeled data. Recently, domain generalization FSC (DG-FSC) has been proposed with the goal to recognize novel class samples from unseen domains. DG-FSC poses considerable challenges to many models due to the domain shift between base classes (used in training) and novel classes (encountered in evaluation). In this work, we make two novel contributions to tackle DG-FSC. Our first contribution is to propose Born-Again Network (BAN) episodic training and comprehensively investigate its effectiveness for DG-FSC. As a specific form of knowledge distillation, BAN has been shown to achieve improved generalization in conventional supervised classification with a closed-set setup. This improved generalization motivates us to study BAN for DG-FSC, and we show that BAN is promising to address the domain shift encountered in DG-FSC. Building on the encouraging findings, our second (major) contribution is to propose Few-Shot BAN (FS-BAN), a novel BAN approach for DG-FSC. Our proposed FS-BAN includes novel multi-task learning objectives: Mutual Regularization, Mismatched Teacher, and Meta-Control Temperature, each of these is specifically designed to overcome central and unique challenges in DG-FSC, namely overfitting and domain discrepancy. We analyze different design choices of these techniques. We conduct comprehensive quantitative and qualitative analysis and evaluation over six datasets and three baseline models. The results suggest that our proposed FS-BAN consistently improves the generalization performance of baseline models and achieves state-of-the-art accuracy for DG-FSC. Project Page: https://yunqing-me.github.io/Born-Again-FS/.
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2205.14814.pdf' target='_blank'>https://arxiv.org/pdf/2205.14814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Hu, Zhili Liu, Fengwei Zhou, Wenjia Wang, Weiran Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.14814">Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to $t$-SNE can also be adopted in the SSCL setting, achieving significant improvement in both in-distribution and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2205.11110.pdf' target='_blank'>https://arxiv.org/pdf/2205.11110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Gao, Jingyu Zhang, Ruijie Chen, Ngo Anh Vien, Hanna Ziesche, Gerhard Neumann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.11110">Meta-Learning Regrasping Strategies for Physical-Agnostic Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grasping inhomogeneous objects in real-world applications remains a challenging task due to the unknown physical properties such as mass distribution and coefficient of friction. In this study, we propose a meta-learning algorithm called ConDex, which incorporates Conditional Neural Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical properties of objects using depth images. ConDex efficiently acquires physical embeddings from limited trials, enabling precise grasping point estimation. Furthermore, ConDex is capable of updating the predicted grasping quality iteratively from new trials in an online fashion. To the best of our knowledge, we are the first who generate two object datasets focusing on inhomogeneous physical properties with varying mass distributions and friction coefficients. Extensive evaluations in simulation demonstrate ConDex's superior performance over DexNet-2.0 and existing meta-learning-based grasping pipelines. Furthermore, ConDex shows robust generalization to previously unseen real-world objects despite training solely in the simulation. The synthetic and real-world datasets will be published as well.
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2202.13670.pdf' target='_blank'>https://arxiv.org/pdf/2202.13670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lidia Fantauzzo, Eros FanÃ¬, Debora Caldarola, Antonio Tavera, Fabio Cermelli, Marco Ciccone, Barbara Caputo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.13670">FedDrive: Generalizing Federated Learning to Semantic Segmentation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic Segmentation is essential to make self-driving vehicles autonomous, enabling them to understand their surroundings by assigning individual pixels to known categories. However, it operates on sensible data collected from the users' cars; thus, protecting the clients' privacy becomes a primary concern. For similar reasons, Federated Learning has been recently introduced as a new machine learning paradigm aiming to learn a global model while preserving privacy and leveraging data on millions of remote devices. Despite several efforts on this topic, no work has explicitly addressed the challenges of federated learning in semantic segmentation for driving so far. To fill this gap, we propose FedDrive, a new benchmark consisting of three settings and two datasets, incorporating the real-world challenges of statistical heterogeneity and domain generalization. We benchmark state-of-the-art algorithms from the federated learning literature through an in-depth analysis, combining them with style transfer methods to improve their generalization ability. We demonstrate that correctly handling normalization statistics is crucial to deal with the aforementioned challenges. Furthermore, style transfer improves performance when dealing with significant appearance shifts. Official website: https://feddrive.github.io.
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2104.02230.pdf' target='_blank'>https://arxiv.org/pdf/2104.02230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Chen, Pinhao Song, Hong Liu, Linhui Dai, Xiaochuan Zhang, Runwei Ding, Shengquan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2104.02230">Achieving Domain Generalization in Underwater Object Detection by Domain Mixup and Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of existing underwater object detection methods degrades seriously when facing domain shift caused by complicated underwater environments. Due to the limitation of the number of domains in the dataset, deep detectors easily memorize a few seen domains, which leads to low generalization ability. There are two common ideas to improve the domain generalization performance. First, it can be inferred that the detector trained on as many domains as possible is domain-invariant. Second, for the images with the same semantic content in different domains, their hidden features should be equivalent. This paper further excavates these two ideas and proposes a domain generalization framework (named DMC) that learns how to generalize across domains from Domain Mixup and Contrastive Learning. First, based on the formation of underwater images, an image in an underwater environment is the linear transformation of another underwater environment. Thus, a style transfer model, which outputs a linear transformation matrix instead of the whole image, is proposed to transform images from one source domain to another, enriching the domain diversity of the training data. Second, mixup operation interpolates different domains on the feature level, sampling new domains on the domain manifold. Third, contrastive loss is selectively applied to features from different domains to force the model to learn domain invariant features but retain the discriminative capacity. With our method, detectors will be robust to domain shift. Also, a domain generalization benchmark S-UODAC2020 for detection is set up to measure the performance of our method. Comprehensive experiments on S-UODAC2020 and two object recognition benchmarks (PACS and VLCS) demonstrate that the proposed method is able to learn domain-invariant representations, and outperforms other domain generalization methods.
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2510.00978.pdf' target='_blank'>https://arxiv.org/pdf/2510.00978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00978">A Scene is Worth a Thousand Features: Feed-Forward Camera Localization from a Collection of Image Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visually localizing an image, i.e., estimating its camera pose, requires building a scene representation that serves as a visual map. The representation we choose has direct consequences towards the practicability of our system. Even when starting from mapping images with known camera poses, state-of-the-art approaches still require hours of mapping time in the worst case, and several minutes in the best. This work raises the question whether we can achieve competitive accuracy much faster. We introduce FastForward, a method that creates a map representation and relocalizes a query image on-the-fly in a single feed-forward pass. At the core, we represent multiple mapping images as a collection of features anchored in 3D space. FastForward utilizes these mapping features to predict image-to-scene correspondences for the query image, enabling the estimation of its camera pose. We couple FastForward with image retrieval and achieve state-of-the-art accuracy when compared to other approaches with minimal map preparation time. Furthermore, FastForward demonstrates robust generalization to unseen domains, including challenging large-scale outdoor environments.
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2509.24704.pdf' target='_blank'>https://arxiv.org/pdf/2509.24704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guibin Zhang, Muxin Fu, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24704">MemGen: Weaving Generative Latent Memory for Self-Evolving Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions. Existing paradigms remain constrained: parametric memory forcibly adjusts model parameters, and retrieval-based memory externalizes experience into structured databases, yet neither captures the fluid interweaving of reasoning and memory that underlies human cognition. To address this gap, we propose MemGen, a dynamic generative memory framework that equips agents with a human-esque cognitive faculty. It consists of a \textit{memory trigger}, which monitors the agent's reasoning state to decide explicit memory invocation, and a \textit{memory weaver}, which takes the agent's current state as stimulus to construct a latent token sequence as machine-native memory to enrich its reasoning. In this way, MemGen enables agents to recall and augment latent memory throughout reasoning, producing a tightly interwoven cycle of memory and cognition. Extensive experiments across eight benchmarks show that MemGen surpasses leading external memory systems such as ExpeL and AWM by up to $38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain generalization ability. More importantly, we find that without explicit supervision, MemGen spontaneously evolves distinct human-like memory faculties, including planning memory, procedural memory, and working memory, suggesting an emergent trajectory toward more naturalistic forms of machine cognition.
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2509.23626.pdf' target='_blank'>https://arxiv.org/pdf/2509.23626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomseok Kang, Niluthpol Chowdhury Mithun, Mikhail Sizintsev, Han-Pang Chiu, Supun Samarasekera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23626">Efficient Domain-Adaptive Multi-Task Dense Prediction with Vision Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task dense prediction, which aims to jointly solve tasks like semantic segmentation and depth estimation, is crucial for robotics applications but suffers from domain shift when deploying models in new environments. While unsupervised domain adaptation (UDA) addresses this challenge for single tasks, existing multi-task UDA methods primarily rely on adversarial learning approaches that are less effective than recent self-training techniques. In this paper, we introduce FAMDA, a simple yet effective UDA framework that bridges this gap by leveraging Vision Foundation Models (VFMs) as powerful teachers. Our approach integrates Segmentation and Depth foundation models into a self-training paradigm to generate high-quality pseudo-labels for the target domain, effectively distilling their robust generalization capabilities into a single, efficient student network. Extensive experiments show that FAMDA achieves state-of-the-art (SOTA) performance on standard synthetic-to-real UDA multi-task learning (MTL) benchmarks and a challenging new day-to-night adaptation task. Our framework enables the training of highly efficient models; a lightweight variant achieves SOTA accuracy while being more than 10$\times$ smaller than foundation models, highlighting FAMDA's suitability for creating domain-adaptive and efficient models for resource-constrained robotics applications.
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2509.16063.pdf' target='_blank'>https://arxiv.org/pdf/2509.16063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Su, Chubin Zhang, Sijin Chen, Liufan Tan, Yansong Tang, Jianan Wang, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16063">DSPv2: Improved Dense Policy for Effective and Generalizable Whole-body Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning whole-body mobile manipulation via imitation is essential for generalizing robotic skills to diverse environments and complex tasks. However, this goal is hindered by significant challenges, particularly in effectively processing complex observation, achieving robust generalization, and generating coherent actions. To address these issues, we propose DSPv2, a novel policy architecture. DSPv2 introduces an effective encoding scheme that aligns 3D spatial features with multi-view 2D semantic features. This fusion enables the policy to achieve broad generalization while retaining the fine-grained perception necessary for precise control. Furthermore, we extend the Dense Policy paradigm to the whole-body mobile manipulation domain, demonstrating its effectiveness in generating coherent and precise actions for the whole-body robotic platform. Extensive experiments show that our method significantly outperforms existing approaches in both task performance and generalization ability. Project page is available at: https://selen-suyue.github.io/DSPv2Net/.
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2509.16063.pdf' target='_blank'>https://arxiv.org/pdf/2509.16063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Su, Chubin Zhang, Sijin Chen, Liufan Tan, Yansong Tang, Jianan Wang, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16063">DSPv2: Improved Dense Policy for Effective and Generalizable Whole-body Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning whole-body mobile manipulation via imitation is essential for generalizing robotic skills to diverse environments and complex tasks. However, this goal is hindered by significant challenges, particularly in effectively processing complex observation, achieving robust generalization, and generating coherent actions. To address these issues, we propose DSPv2, a novel policy architecture. DSPv2 introduces an effective encoding scheme that aligns 3D spatial features with multi-view 2D semantic features. This fusion enables the policy to achieve broad generalization while retaining the fine-grained perception necessary for precise control. Furthermore, we extend the Dense Policy paradigm to the whole-body mobile manipulation domain, demonstrating its effectiveness in generating coherent and precise actions for the whole-body robotic platform. Extensive experiments show that our method significantly outperforms existing approaches in both task performance and generalization ability. Project page is available at: https://selen-suyue.github.io/DSPv2Net/.
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2509.12602.pdf' target='_blank'>https://arxiv.org/pdf/2509.12602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyu Chen, Guoqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12602">DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of Conflict-Driven Clause Learning solvers hinges on internal heuristics, yet the heterogeneity of SAT problems makes a single, universally optimal configuration unattainable. While prior automated methods can find specialized configurations for specific problem families, this dataset-specific approach lacks generalizability and requires costly re-optimization for new problem types. We introduce DaSAThco, a framework that addresses this challenge by learning a generalizable mapping from instance features to tailored heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework uses a Large Language Model, guided by systematically defined Problem Archetypes, to generate a diverse portfolio of specialized heuristic ensembles and subsequently learns an adaptive selection mechanism to form the final mapping. Experiments show that DaSAThco achieves superior performance and, most notably, demonstrates robust out-of-domain generalization where non-adaptive methods show limitations. Our work establishes a more scalable and practical path toward automated algorithm design for complex, configurable systems.
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2509.12400.pdf' target='_blank'>https://arxiv.org/pdf/2509.12400.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongkun Zhu, Kangning Cui, Wei Tang, Rui-Feng Wang, Sarra Alqahtani, David Lutz, Fan Yang, Paul Fine, Jordan Karubian, Robert Plemmons, Jean-Michel Morel, Victor Pauca, Miles Silman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12400">From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate mapping of individual trees is essential for ecological monitoring and forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs) is widely used, but stitching artifacts and heavy preprocessing limit its suitability for field deployment. This study explores the use of raw UAV imagery for palm detection and crown-center localization in tropical forests. Two research questions are addressed: (1) how detection performance varies across orthomosaic and raw imagery, including within-domain and cross-domain transfer, and (2) to what extent crown-center annotations improve localization accuracy beyond bounding-box centroids. Using state-of-the-art detectors and keypoint models, we show that raw imagery yields superior performance in deployment-relevant scenarios, while orthomosaics retain value for robust cross-domain generalization. Incorporating crown-center annotations in training further improves localization and provides precise tree positions for downstream ecological analyses. These findings offer practical guidance for UAV-based biodiversity and conservation monitoring.
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2509.02600.pdf' target='_blank'>https://arxiv.org/pdf/2509.02600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengyou Xu, Haochen Yang, Xiang 'Anthony' Chen, Hongyan Gu, Mohammad Haeri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02600">Team Westwood Solution for MIDOG 2025 Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This abstract presents our solution (Team Westwood) for mitosis detection and atypical mitosis classification in the MItosis DOmain Generalization (MIDOG) 2025 challenge. For mitosis detection, we trained an nnUNetV2 for initial mitosis candidate screening with high sensitivity, followed by a random forest classifier ensembling predictions of three convolutional neural networks (CNNs): EfficientNet-b3, EfficientNet-b5, and EfficientNetV2-s. For the atypical mitosis classification, we trained another random forest classifier ensembling the predictions of three CNNs: EfficientNet-b3, EfficientNet-b5, and InceptionV3. On the preliminary test set, our solution achieved an F1 score of 0.7450 for track 1 mitosis detection, and a balanced accuracy of 0.8722 for track 2 atypical mitosis classification.
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2509.00956.pdf' target='_blank'>https://arxiv.org/pdf/2509.00956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Cescon, Andrea Martin, Giancarlo Ferrari-Trecate
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00956">On the Global Optimality of Linear Policies for Sinkhorn Distributionally Robust Linear Quadratic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Linear Quadratic Gaussian (LQG) regulator is a cornerstone of optimal control theory, yet its performance can degrade significantly when the noise distributions deviate from the assumed Gaussian model. To address this limitation, this work proposes a distributionally robust generalization of the finite-horizon LQG control problem. Specifically, we assume that the noise distributions are unknown and belong to ambiguity sets defined in terms of an entropy-regularized Wasserstein distance centered at a nominal Gaussian distribution. By deriving novel bounds on this Sinkhorn discrepancy and proving structural and topological properties of the resulting ambiguity sets, we establish global optimality of linear policies. Numerical experiments showcase improved distributional robustness of our control policy.
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2508.13768.pdf' target='_blank'>https://arxiv.org/pdf/2508.13768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengchao Liu, Xiaoming Liu, Chengzhengxu Li, Zhaohan Zhang, Guoxin Ma, Yu Lan, Shuai Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13768">MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models have shown growing ability to generate fluent and coherent texts that are highly similar to the writing style of humans. Current detectors for Machine-Generated Text (MGT) perform well when they are trained and tested in the same domain but generalize poorly to unseen domains, due to domain shift between data from different sources. In this work, we propose MGT-Prism, an MGT detection method from the perspective of the frequency domain for better domain generalization. Our key insight stems from analyzing text representations in the frequency domain, where we observe consistent spectral patterns across diverse domains, while significant discrepancies in magnitude emerge between MGT and human-written texts (HWTs). The observation initiates the design of a low frequency domain filtering module for filtering out the document-level features that are sensitive to domain shift, and a dynamic spectrum alignment strategy to extract the task-specific and domain-invariant features for improving the detector's performance in domain generalization. Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test datasets across three domain-generalization scenarios.
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2508.09418.pdf' target='_blank'>https://arxiv.org/pdf/2508.09418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Usman Anjum, Chris Stockman, Cat Luong, Justin Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09418">Domain-Generalization to Improve Learning in Meta-Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Domain Generalization Sharpness-Aware Minimization Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm designed to generalize across tasks with limited training data. DGS-MAML combines gradient matching with sharpness-aware minimization in a bi-level optimization framework to enhance model adaptability and robustness. We support our method with theoretical analysis using PAC-Bayes and convergence guarantees. Experimental results on benchmark datasets show that DGS-MAML outperforms existing approaches in terms of accuracy and generalization. The proposed method is particularly useful for scenarios requiring few-shot learning and quick adaptation, and the source code is publicly available at GitHub.
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2508.08570.pdf' target='_blank'>https://arxiv.org/pdf/2508.08570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenruo Liu, Hongjun Liu, Zeyu Lai, Yiqiu Shen, Chen Zhao, Qi Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08570">Superclass-Guided Representation Disentanglement for Spurious Correlation Mitigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enhance group robustness to spurious correlations, prior work often relies on auxiliary annotations for groups or spurious features and assumes identical sets of groups across source and target domains. These two requirements are both unnatural and impractical in real-world settings. To overcome these limitations, we propose a method that leverages the semantic structure inherent in class labels--specifically, superclass information--to naturally reduce reliance on spurious features. Our model employs gradient-based attention guided by a pre-trained vision-language model to disentangle superclass-relevant and irrelevant features. Then, by promoting the use of all superclass-relevant features for prediction, our approach achieves robustness to more complex spurious correlations without the need to annotate any source samples. Experiments across diverse datasets demonstrate that our method significantly outperforms baselines in domain generalization tasks, with clear improvements in both quantitative metrics and qualitative visualizations.
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2508.08030.pdf' target='_blank'>https://arxiv.org/pdf/2508.08030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Peng, Yuanyuan Zhang, Steve Jiang, Robert Timmerman, John Minna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08030">Exploring Strategies for Personalized Radiation Therapy: Part III Identifying genetic determinants for Radiation Response with Meta Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiation response in cancer is shaped by complex, patient specific biology, yet current treatment strategies often rely on uniform dose prescriptions without accounting for tumor heterogeneity. In this study, we introduce a meta learning framework for one-shot prediction of radiosensitivity measured by SF2 using cell line level gene expression data. Unlike the widely used Radiosensitivity Index RSI a rank-based linear model trained on a fixed 10-gene signature, our proposed meta-learned model allows the importance of each gene to vary by sample through fine tuning. This flexibility addresses key limitations of static models like RSI, which assume uniform gene contributions across tumor types and discard expression magnitude and gene gene interactions. Our results show that meta learning offers robust generalization to unseen samples and performs well in tumor subgroups with high radiosensitivity variability, such as adenocarcinoma and large cell carcinoma. By learning transferable structure across tasks while preserving sample specific adaptability, our approach enables rapid adaptation to individual samples, improving predictive accuracy across diverse tumor subtypes while uncovering context dependent patterns of gene influence that may inform personalized therapy.
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2508.07539.pdf' target='_blank'>https://arxiv.org/pdf/2508.07539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Shigeyasu, Shota Harada, Akihiko Yoshizawa, Kazuhiro Terada, Naoki Nakazima, Mariyo Kurata, Hiroyuki Abe, Tetsuo Ushiku, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07539">Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address domain shifts in pathological images by focusing on shifts within whole slide images~(WSIs), such as patient characteristics and tissue thickness, rather than shifts between hospitals. Traditional approaches rely on multi-hospital data, but data collection challenges often make this impractical. Therefore, the proposed domain generalization method captures and leverages intra-hospital domain shifts by clustering WSI-level features from non-tumor regions and treating these clusters as domains. To mitigate domain shift, we apply contrastive learning to reduce feature gaps between WSI pairs from different clusters. The proposed method introduces a two-stage contrastive learning approach WSI-level and patch-level contrastive learning to minimize these gaps effectively.
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2508.04552.pdf' target='_blank'>https://arxiv.org/pdf/2508.04552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franz Thaler, Darko Stern, Gernot Plank, Martin Urschler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04552">Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the leading cause of death worldwide, cardiovascular diseases motivate the development of more sophisticated methods to analyze the heart and its substructures from medical images like Computed Tomography (CT) and Magnetic Resonance (MR). Semantic segmentations of important cardiac structures that represent the whole heart are useful to assess patient-specific cardiac morphology and pathology. Furthermore, accurate semantic segmentations can be used to generate cardiac digital twin models which allows e.g. electrophysiological simulation and personalized therapy planning. Even though deep learning-based methods for medical image segmentation achieved great advancements over the last decade, retaining good performance under domain shift -- i.e. when training and test data are sampled from different data distributions -- remains challenging. In order to perform well on domains known at training-time, we employ a (1) balanced joint training approach that utilizes CT and MR data in equal amounts from different source domains. Further, aiming to alleviate domain shift towards domains only encountered at test-time, we rely on (2) strong intensity and spatial augmentation techniques to greatly diversify the available training data. Our proposed whole heart segmentation method, a 5-fold ensemble with our contributions, achieves the best performance for MR data overall and a performance similar to the best performance for CT data when compared to a model trained solely on CT. With 93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR data, our method demonstrates great potential to efficiently obtain accurate semantic segmentations from which patient-specific cardiac twin models can be generated.
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2508.01582.pdf' target='_blank'>https://arxiv.org/pdf/2508.01582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhui Li, Xinyu He, Qiming Hu, Xiaojie Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01582">Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce, for the first time, the concept of Set Pivot Learning, a paradigm shift that redefines domain generalization (DG) based on Vision Foundation Models (VFMs). Traditional DG assumes that the target domain is inaccessible during training, but the emergence of VFMs, trained on vast and diverse data, renders this assumption unclear and obsolete. Traditional DG assumes that the target domain is inaccessible during training, but the emergence of VFMs, which are trained on vast and diverse datasets, renders this assumption unclear and obsolete. To address this challenge, we propose Set Pivot Learning (SPL), a new definition of domain migration task based on VFMs, which is more suitable for current research and application requirements. Unlike conventional DG methods, SPL prioritizes adaptive refinement over rigid domain transfer, ensuring continuous alignment with evolving real-world conditions. Specifically, SPL features two key attributes: (i) Dynamic adaptation, transitioning from static domain alignment to flexible, task-driven feature optimization, enabling models to evolve with downstream scenarios; (ii) VFM-centric tuning, leveraging pretrained knowledge as a pivot to hone task-specific representations while preserving cross-domain robustness. Building on SPL, we propose a Dynamic Prompt Fine-Tuning method, which combines a Dynamic Class-aware Prompter with a Prompt-guided Feature Focuser, to elevate VFM performance in targeted scenarios. Extensive experiments on benchmark datasets show the effectiveness of our method, highlighting its superiority over state-of-the-art methods, particularly in generalized segmentation.
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2508.00304.pdf' target='_blank'>https://arxiv.org/pdf/2508.00304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyin Liao, Ziwei Zhang, Yufei Sun, Chunyu Hu, Jianxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00304">Invariant Graph Transformer for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Transformers (GTs) have demonstrated great effectiveness across various graph analytical tasks. However, the existing GTs focus on training and testing graph data originated from the same distribution, but fail to generalize under distribution shifts. Graph invariant learning, aiming to capture generalizable graph structural patterns with labels under distribution shifts, is potentially a promising solution, but how to design attention mechanisms and positional and structural encodings (PSEs) based on graph invariant learning principles remains challenging. To solve these challenges, we introduce Graph Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn generalized graph representations by capturing invariant relationships between predictive graph structures and labels through jointly optimizing three modules. Specifically, we first develop a GT-based entropy-guided invariant subgraph disentangler to separate invariant and variant subgraphs while preserving the sharpness of the attention function. Next, we design an evolving subgraph positional and structural encoder to effectively and efficiently capture the encoding information of dynamically changing subgraphs during training. Finally, we propose an invariant learning module utilizing subgraph node representations and encodings to derive generalizable graph representations that can to unseen graphs. We also provide theoretical justifications for our method. Extensive experiments on benchmark datasets demonstrate the superiority of our method over state-of-the-art baselines under distribution shifts.
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2507.22792.pdf' target='_blank'>https://arxiv.org/pdf/2507.22792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoping Xu, Jayaram K. Udupa, Yajun Yu, Hua-Chieh Shao, Songlin Zhao, Wei Liu, You Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22792">Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Object Segmentation and Tracking (VOST) presents a complex yet critical challenge in computer vision, requiring robust integration of segmentation and tracking across temporally dynamic frames. Traditional methods have struggled with domain generalization, temporal consistency, and computational efficiency. The emergence of foundation models like the Segment Anything Model (SAM) and its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven segmentation with strong generalization capabilities. Building upon these advances, this survey provides a comprehensive review of SAM/SAM2-based methods for VOST, structured along three temporal dimensions: past, present, and future. We examine strategies for retaining and updating historical information (past), approaches for extracting and optimizing discriminative features from the current frame (present), and motion prediction and trajectory estimation mechanisms for anticipating object dynamics in subsequent frames (future). In doing so, we highlight the evolution from early memory-based architectures to the streaming memory and real-time segmentation capabilities of SAM2. We also discuss recent innovations such as motion-aware memory selection and trajectory-guided prompting, which aim to enhance both accuracy and efficiency. Finally, we identify remaining challenges including memory redundancy, error accumulation, and prompt inefficiency, and suggest promising directions for future research. This survey offers a timely and structured overview of the field, aiming to guide researchers and practitioners in advancing the state of VOST through the lens of foundation models.
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2507.15349.pdf' target='_blank'>https://arxiv.org/pdf/2507.15349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15349">Scaling Decentralized Learning with FLock</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning the large language models (LLMs) are prevented by the deficiency of centralized control and the massive computing and communication overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative LLM fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B LLM in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data.
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2507.06190.pdf' target='_blank'>https://arxiv.org/pdf/2507.06190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kwanghyuk Park, Jiaxi Gu, Jae-Hun Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06190">Conservative approximation-based feedforward neural network for WENO schemes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present the feedforward neural network based on the conservative approximation to the derivative from point values, for the weighted essentially non-oscillatory (WENO) schemes in solving hyperbolic conservation laws. The feedforward neural network, whose inputs are point values from the three-point stencil and outputs are two nonlinear weights, takes the place of the classical WENO weighting procedure. For the training phase, we employ the supervised learning and create a new labeled dataset for one-dimensional conservative approximation, where we construct a numerical flux function from the given point values such that the flux difference approximates the derivative to high-order accuracy. The symmetric-balancing term is introduced for the loss function so that it propels the neural network to match the conservative approximation to the derivative and satisfy the symmetric property that WENO3-JS and WENO3-Z have in common. The consequent WENO schemes, WENO3-CADNNs, demonstrate robust generalization across various benchmark scenarios and resolutions, where they outperform WENO3-Z and achieve accuracy comparable to WENO5-JS.
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2506.23467.pdf' target='_blank'>https://arxiv.org/pdf/2506.23467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23467">AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis.
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2506.12009.pdf' target='_blank'>https://arxiv.org/pdf/2506.12009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junha Lee, Eunha Park, Chunghyun Park, Dahyun Kang, Minsu Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12009">Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance grounding-localizing object regions based on natural language descriptions of interactions-is a critical challenge for enabling intelligent agents to understand and interact with their environments. However, this task remains challenging due to the need for fine-grained part-level localization, the ambiguity arising from multiple valid interaction regions, and the scarcity of large-scale datasets. In this work, we introduce Affogato, a large-scale benchmark comprising 150K instances, annotated with open-vocabulary text descriptions and corresponding 3D affordance heatmaps across a diverse set of objects and interactions. Building on this benchmark, we develop simple yet effective vision-language models that leverage pretrained part-aware vision backbones and a text-conditional heatmap decoder. Our models trained with the Affogato dataset achieve promising performance on the existing 2D and 3D benchmarks, and notably, exhibit effectiveness in open-vocabulary cross-domain generalization. The Affogato dataset is shared in public: https://huggingface.co/datasets/project-affogato/affogato
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2506.10146.pdf' target='_blank'>https://arxiv.org/pdf/2506.10146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tejaswi Kasarla, Max van Spengler, Pascal Mettes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10146">Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution recognition forms an important and well-studied problem in deep learning, with the goal to filter out samples that do not belong to the distribution on which a network has been trained. The conclusion of this paper is simple: a good hierarchical hyperbolic embedding is preferred for discriminating in- and out-of-distribution samples. We introduce Balanced Hyperbolic Learning. We outline a hyperbolic class embedding algorithm that jointly optimizes for hierarchical distortion and balancing between shallow and wide subhierarchies. We then use the class embeddings as hyperbolic prototypes for classification on in-distribution data. We outline how to generalize existing out-of-distribution scoring functions to operate with hyperbolic prototypes. Empirical evaluations across 13 datasets and 13 scoring functions show that our hyperbolic embeddings outperform existing out-of-distribution approaches when trained on the same data with the same backbones. We also show that our hyperbolic embeddings outperform other hyperbolic approaches, beat state-of-the-art contrastive methods, and natively enable hierarchical out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2506.06977.pdf' target='_blank'>https://arxiv.org/pdf/2506.06977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Hu, Xiaoxue Han, Fei Wang, Yue Ning
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06977">UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization has become a critical challenge in clinical prediction, where patient cohorts often exhibit shifting data distributions that degrade model performance. Typical domain generalization approaches struggle in real-world healthcare settings for two main reasons: (1) patient-specific domain labels are typically unavailable, making domain discovery especially difficult; (2) purely data-driven approaches overlook key clinical insights, leading to a gap in medical knowledge integration. To address these problems, we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to group diseases into higher-level categories and discover more flexible latent domains. In this paper, we introduce UdonCare, a hierarchy-guided framework that iteratively prunes fine-grained domains, encodes these refined domains, and applies a Siamese-type inference mechanism to separate domain-related signals from patient-level features. Experimental results on clinical datasets (MIMIC-III and MIMIC-IV) show that the proposed model achieves higher performance compared to other domain generalization baselines when substantial domain gaps presents, highlighting the untapped potential of medical knowledge for enhancing domain generalization in practical healthcare applications.
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2506.02256.pdf' target='_blank'>https://arxiv.org/pdf/2506.02256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Xiao, Harshit Sharma, Sawinder Kaur, Dessa Bergen-Cico, Asif Salekin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02256">Human Heterogeneity Invariant Stress Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stress affects physical and mental health, and wearable devices have been widely used to detect daily stress through physiological signals. However, these signals vary due to factors such as individual differences and health conditions, making generalizing machine learning models difficult. To address these challenges, we present Human Heterogeneity Invariant Stress Sensing (HHISS), a domain generalization approach designed to find consistent patterns in stress signals by removing person-specific differences. This helps the model perform more accurately across new people, environments, and stress types not seen during training. Its novelty lies in proposing a novel technique called person-wise sub-network pruning intersection to focus on shared features across individuals, alongside preventing overfitting by leveraging continuous labels while training. The study focuses especially on people with opioid use disorder (OUD)-a group where stress responses can change dramatically depending on their time of daily medication taking. Since stress often triggers cravings, a model that can adapt well to these changes could support better OUD rehabilitation and recovery. We tested HHISS on seven different stress datasets-four of which we collected ourselves and three public ones. Four are from lab setups, one from a controlled real-world setting, driving, and two are from real-world in-the-wild field datasets without any constraints. This is the first study to evaluate how well a stress detection model works across such a wide range of data. Results show HHISS consistently outperformed state-of-the-art baseline methods, proving both effective and practical for real-world use. Ablation studies, empirical justifications, and runtime evaluations confirm HHISS's feasibility and scalability for mobile stress sensing in sensitive real-world applications.
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2506.01121.pdf' target='_blank'>https://arxiv.org/pdf/2506.01121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob K. Christopher, Michael Cardei, Jinhao Liang, Ferdinando Fioretto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01121">Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable generative capabilities of diffusion models, their integration into safety-critical or scientifically rigorous applications remains hindered by the need to ensure compliance with stringent physical, structural, and operational constraints. To address this challenge, this paper introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves diffusion steps with symbolic optimization, enabling the generation of certifiably consistent samples under user-defined functional and logic constraints. This key feature is provided for both standard and discrete diffusion models, enabling, for the first time, the generation of both continuous (e.g., images and trajectories) and discrete (e.g., molecular structures and natural language) outputs that comply with constraints. This ability is demonstrated on tasks spanning three key challenges: (1) Safety, in the context of non-toxic molecular generation and collision-free trajectory optimization; (2) Data scarcity, in domains such as drug discovery and materials engineering; and (3) Out-of-domain generalization, where enforcing symbolic constraints allows adaptation beyond the training distribution.
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2505.24149.pdf' target='_blank'>https://arxiv.org/pdf/2505.24149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Piaseczny, Md Kamran Chowdhury Shisher, Shiqiang Wang, Christopher G. Brinton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24149">RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring strict compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight policy based on a measurable accumulated loss threshold that provably limits update frequency and cost. Experimental results on three domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2505.23926.pdf' target='_blank'>https://arxiv.org/pdf/2505.23926.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuweiyi Chen, Wentao Zhou, Aruni RoyChowdhury, Zezhou Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23926">Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision.
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2505.19547.pdf' target='_blank'>https://arxiv.org/pdf/2505.19547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhang, Wentao Zhang, Hao Miao, Xinke Jiang, Yuchen Fang, Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19547">STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful tool for modeling dynamic graph-structured data across diverse domains. However, they often fail to generalize in Spatio-Temporal Out-of-Distribution (STOOD) scenarios, where both temporal dynamics and spatial structures evolve beyond the training distribution. To address this problem, we propose an innovative Spatio-Temporal Retrieval-Augmented Pattern Learning framework,STRAP, which enhances model generalization by integrating retrieval-augmented learning into the STGNN continue learning pipeline. The core of STRAP is a compact and expressive pattern library that stores representative spatio-temporal patterns enriched with historical, structural, and semantic information, which is obtained and optimized during the training phase. During inference, STRAP retrieves relevant patterns from this library based on similarity to the current input and injects them into the model via a plug-and-play prompting mechanism. This not only strengthens spatio-temporal representations but also mitigates catastrophic forgetting. Moreover, STRAP introduces a knowledge-balancing objective to harmonize new information with retrieved knowledge. Extensive experiments across multiple real-world streaming graph datasets show that STRAP consistently outperforms state-of-the-art STGNN baselines on STOOD tasks, demonstrating its robustness, adaptability, and strong generalization capability without task-specific fine-tuning.
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2505.18884.pdf' target='_blank'>https://arxiv.org/pdf/2505.18884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Borna Khodabandeh, Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, Sanjay Lall, Sajjad Amini, Seyed-Mohsen Moosavi-Dezfooli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18884">LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual encoders have become fundamental components in modern computer vision pipelines. However, ensuring robustness against adversarial perturbations remains a critical challenge. Recent efforts have explored both supervised and unsupervised adversarial fine-tuning strategies. We identify two key limitations in these approaches: (i) they often suffer from instability, especially during the early stages of fine-tuning, resulting in suboptimal convergence and degraded performance on clean data, and (ii) they exhibit a suboptimal trade-off between robustness and clean data accuracy, hindering the simultaneous optimization of both objectives. To overcome these challenges, we propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised adversarial fine-tuning framework. LORE utilizes constrained optimization, which offers a principled approach to balancing competing goals, such as improving robustness while preserving nominal performance. By enforcing embedding-space proximity constraints, LORE effectively maintains clean data performance throughout adversarial fine-tuning. Extensive experiments show that LORE significantly improves zero-shot adversarial robustness with minimal degradation in clean data accuracy. Furthermore, we demonstrate the effectiveness of the adversarially fine-tuned CLIP image encoder in out-of-distribution generalization and enhancing the interpretability of image embeddings.
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2505.12745.pdf' target='_blank'>https://arxiv.org/pdf/2505.12745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Kyu Cho, Inwoo Hwang, Sanghack Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12745">PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is a popular tool for single source domain generalization, which expands the source domain by generating simulated ones, improving generalization on unseen target domains. In this work, we show that the performance of such augmentation-based methods in the target domains universally fluctuates during training, posing challenges in model selection under realistic scenarios. We argue that the fluctuation stems from the inability of the model to accumulate the knowledge learned from diverse augmentations, exacerbating feature distortion during training. Based on this observation, we propose a novel generalization method, coined Parameter-Space Ensemble with Entropy Regularization (PEER), that uses a proxy model to learn the augmented data on behalf of the main model. The main model is updated by averaging its parameters with the proxy model, progressively accumulating knowledge over the training steps. Maximizing the mutual information between the output representations of the two models guides the learning process of the proxy model, mitigating feature distortion during training. Experimental results demonstrate the effectiveness of PEER in reducing the OOD performance fluctuation and enhancing generalization across various datasets, including PACS, Digits, Office-Home, and VLCS. Notably, our method with simple random augmentation achieves state-of-the-art performance, surpassing prior approaches on sDG that utilize complex data augmentation strategies.
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2505.12568.pdf' target='_blank'>https://arxiv.org/pdf/2505.12568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lekang Jiang, Chengzu Li, Stephan Goetz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12568">Enriching Patent Claim Generation with European Patent Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drafting patent claims is time-intensive, costly, and requires professional skill. Therefore, researchers have investigated large language models (LLMs) to assist inventors in writing claims. However, existing work has largely relied on datasets from the United States Patent and Trademark Office (USPTO). To enlarge research scope regarding various jurisdictions, drafting conventions, and legal standards, we introduce EPD, a European patent dataset. EPD presents rich textual data and structured metadata to support multiple patent-related tasks, including claim generation. This dataset enriches the field in three critical aspects: (1) Jurisdictional diversity: Patents from different offices vary in legal and drafting conventions. EPD fills a critical gap by providing a benchmark for European patents to enable more comprehensive evaluation. (2) Quality improvement: EPD offers high-quality granted patents with finalized and legally approved texts, whereas others consist of patent applications that are unexamined or provisional. Experiments show that LLMs fine-tuned on EPD significantly outperform those trained on previous datasets and even GPT-4o in claim quality and cross-domain generalization. (3) Real-world simulation: We propose a difficult subset of EPD to better reflect real-world challenges of claim generation. Results reveal that all tested LLMs perform substantially worse on these challenging samples, which highlights the need for future research.
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2505.06580.pdf' target='_blank'>https://arxiv.org/pdf/2505.06580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyoon Yang, Jihu Lee, Yongdai Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06580">TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust domain adaptation against adversarial attacks is a critical research area that aims to develop models capable of maintaining consistent performance across diverse and challenging domains. In this paper, we derive a new generalization bound for robust risk on the target domain using a novel divergence measure specifically designed for robust domain adaptation. Building upon this, we propose a new algorithm named TAROT, which is designed to enhance both domain adaptability and robustness. Through extensive experiments, TAROT not only surpasses state-of-the-art methods in accuracy and robustness but also significantly enhances domain generalization and scalability by effectively learning domain-invariant features. In particular, TAROT achieves superior performance on the challenging DomainNet dataset, demonstrating its ability to learn domain-invariant representations that generalize well across different domains, including unseen ones. These results highlight the broader applicability of our approach in real-world domain adaptation scenarios.
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2505.05291.pdf' target='_blank'>https://arxiv.org/pdf/2505.05291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05291">Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to learn robust representations from large-scale natural image datasets, enhancing their generalization across domains. In retinal imaging, foundation models pretrained on either natural or ophthalmic data have shown promise, but the benefits of in-domain pretraining remain uncertain. To investigate this, we benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets totaling 70,000 expert-annotated images for the task of moderate-to-late age-related macular degeneration (AMD) identification. Our results show that iBOT pretrained on natural images achieves the highest out-of-distribution generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models, which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining, which achieved AUROCs of 0.68-0.91. These findings highlight the value of foundation models in improving AMD identification and challenge the assumption that in-domain pretraining is necessary. Furthermore, we release BRAMD, an open-access dataset (n=587) of DFIs with AMD labels from Brazil.
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2504.13562.pdf' target='_blank'>https://arxiv.org/pdf/2504.13562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Li, Han Jiang, Zhihua Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13562">DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DETAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize the user's core intention, minimizing interference from attack tokens. Our experimental results demonstrate that DETAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, in evaluating the model's utility, we incorporated over-defense datasets, which further validate the superior performance of our approach. The code will be released immediately upon acceptance.
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2504.12456.pdf' target='_blank'>https://arxiv.org/pdf/2504.12456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huantao Ren, Minmin Yang, Senem Velipasalar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12456">DG-MVP: 3D Domain Generalization via Multiple Views of Point Clouds for Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks have achieved significant success in 3D point cloud classification while relying on large-scale, annotated point cloud datasets, which are labor-intensive to build. Compared to capturing data with LiDAR sensors and then performing annotation, it is relatively easier to sample point clouds from CAD models. Yet, data sampled from CAD models is regular, and does not suffer from occlusion and missing points, which are very common for LiDAR data, creating a large domain shift. Therefore, it is critical to develop methods that can generalize well across different point cloud domains. %In this paper, we focus on the 3D point cloud domain generalization problem. Existing 3D domain generalization methods employ point-based backbones to extract point cloud features. Yet, by analyzing point utilization of point-based methods and observing the geometry of point clouds from different domains, we have found that a large number of point features are discarded by point-based methods through the max-pooling operation. This is a significant waste especially considering the fact that domain generalization is more challenging than supervised learning, and point clouds are already affected by missing points and occlusion to begin with. To address these issues, we propose a novel method for 3D point cloud domain generalization, which can generalize to unseen domains of point clouds. Our proposed method employs multiple 2D projections of a 3D point cloud to alleviate the issue of missing points and involves a simple yet effective convolution-based model to extract features. The experiments, performed on the PointDA-10 and Sim-to-Real benchmarks, demonstrate the effectiveness of our proposed method, which outperforms different baselines, and can transfer well from synthetic domain to real-world domain.
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2504.08020.pdf' target='_blank'>https://arxiv.org/pdf/2504.08020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Bi, Jingjun Yi, Haolan Zhan, Wei Ji, Gui-Song Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08020">Learning Fine-grained Domain Generalization via Hyperbolic State Space Hallucination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained domain generalization (FGDG) aims to learn a fine-grained representation that can be well generalized to unseen target domains when only trained on the source domain data. Compared with generic domain generalization, FGDG is particularly challenging in that the fine-grained category can be only discerned by some subtle and tiny patterns. Such patterns are particularly fragile under the cross-domain style shifts caused by illumination, color and etc. To push this frontier, this paper presents a novel Hyperbolic State Space Hallucination (HSSH) method. It consists of two key components, namely, state space hallucination (SSH) and hyperbolic manifold consistency (HMC). SSH enriches the style diversity for the state embeddings by firstly extrapolating and then hallucinating the source images. Then, the pre- and post- style hallucinate state embeddings are projected into the hyperbolic manifold. The hyperbolic state space models the high-order statistics, and allows a better discernment of the fine-grained patterns. Finally, the hyperbolic distance is minimized, so that the impact of style variation on fine-grained patterns can be eliminated. Experiments on three FGDG benchmarks demonstrate its state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2504.00186.pdf' target='_blank'>https://arxiv.org/pdf/2504.00186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olawale Salaudeen, Nicole Chiou, Shiny Weng, Sanmi Koyejo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00186">Are Domain Generalization Benchmarks with Accuracy on the Line Misspecified?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spurious correlations, unstable statistical shortcuts a model can exploit, are expected to degrade performance out-of-distribution (OOD). However, across many popular OOD generalization benchmarks, vanilla empirical risk minimization (ERM) often achieves the highest OOD accuracy. Moreover, gains in in-distribution accuracy generally improve OOD accuracy, a phenomenon termed accuracy on the line, which contradicts the expected harm of spurious correlations. We show that these observations are an artifact of misspecified OOD datasets that do not include shifts in spurious correlations that harm OOD generalization, the setting they are meant to evaluate. Consequently, current practice evaluates "robustness" without truly stressing the spurious signals we seek to eliminate; our work pinpoints when that happens and how to fix it. Contributions. (i) We derive necessary and sufficient conditions for a distribution shift to reveal a model's reliance on spurious features; when these conditions hold, "accuracy on the line" disappears. (ii) We audit leading OOD datasets and find that most still display accuracy on the line, suggesting they are misspecified for evaluating robustness to spurious correlations. (iii) We catalog the few well-specified datasets and summarize generalizable design principles, such as identifying datasets of natural interventions (e.g., a pandemic), to guide future well-specified benchmarks.
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2503.18695.pdf' target='_blank'>https://arxiv.org/pdf/2503.18695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Zeyu Zhang, Yue Huang, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18695">OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although foundation models (FMs) claim to be powerful, their generalization ability significantly decreases when faced with distribution shifts, weak supervision, or malicious attacks in the open world. On the other hand, most domain generalization or adversarial fine-tuning methods are task-related or model-specific, ignoring the universality in practical applications and the transferability between FMs. This paper delves into the problem of generalizing FMs to the out-of-domain data. We propose a novel framework, the Object-Concept-Relation Triad (OCRT), that enables FMs to extract sparse, high-level concepts and intricate relational structures from raw visual inputs. The key idea is to bind objects in visual scenes and a set of object-centric representations through unsupervised decoupling and iterative refinement. To be specific, we project the object-centric representations onto a semantic concept space that the model can readily interpret and estimate their importance to filter out irrelevant elements. Then, a concept-based graph, which has a flexible degree, is constructed to incorporate the set of concepts and their corresponding importance, enabling the extraction of high-order factors from informative concepts and facilitating relational reasoning among these concepts. Extensive experiments demonstrate that OCRT can substantially boost the generalizability and robustness of SAM and CLIP across multiple downstream tasks.
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2503.15149.pdf' target='_blank'>https://arxiv.org/pdf/2503.15149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxiang Shen, RaÃºl I. Sosa, Jakub Lengiewicz, Alexandre Tkatchenko, StÃ©phane P. A. Bordas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15149">Machine learning surrogate models of many-body dispersion interactions in polymer melts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of many-body dispersion (MBD) interactions is essential for understanding the van der Waals forces that govern the behavior of many complex molecular systems. However, the high computational cost of MBD calculations limits their direct application in large-scale simulations. In this work, we introduce a machine learning surrogate model specifically designed to predict MBD forces in polymer melts, a system that demands accurate MBD description and offers structural advantages for machine learning approaches. Our model is based on a trimmed SchNet architecture that selectively retains the most relevant atomic connections and incorporates trainable radial basis functions for geometric encoding. We validate our surrogate model on datasets from polyethylene, polypropylene, and polyvinyl chloride melts, demonstrating high predictive accuracy and robust generalization across diverse polymer systems. In addition, the model captures key physical features, such as the characteristic decay behavior of MBD interactions, providing valuable insights for optimizing cutoff strategies. Characterized by high computational efficiency, our surrogate model enables practical incorporation of MBD effects into large-scale molecular simulations.
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2503.13939.pdf' target='_blank'>https://arxiv.org/pdf/2503.13939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13939">Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored. Medical vision-language tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional supervised fine-tuning (SFT) and Chain-of-Thought (CoT) strategies that work well in general domains. To address these challenges, we propose Med-R1, a reinforcement learning (RL)-enhanced vision-language model designed to improve generalization and reliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts Group Relative Policy Optimization (GRPO) to encourage reward-guided learning beyond static annotations. We comprehensively evaluate Med-R1 across eight distinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in average accuracy over its base model Qwen2-VL-2B, and even outperforms Qwen2-VL-72B-a model with 36x more parameters. To assess cross-task generalization, we further evaluate Med-R1 on five question types. Med-R1 outperforms Qwen2-VL-2B by 32.06% in question-type generalization, also surpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a crucial component for the success of Deepseek-R1. Our results show that omitting intermediate rationales (No-Thinking-Med-R1) not only improves in-domain and cross-domain generalization with less training, but also challenges the assumption that more reasoning always helps. These findings suggest that in medical VQA, it is not reasoning itself, but its quality and domain alignment, that determine effectiveness. Together, these results highlight that RL improves medical reasoning and generalization, enabling efficient and reliable VLMs for real-world deployment.
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2503.13579.pdf' target='_blank'>https://arxiv.org/pdf/2503.13579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokhyeon Hong, Soojin Choi, Chaelin Kim, Sihun Cha, Junyong Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13579">ASMR: Adaptive Skeleton-Mesh Rigging and Skinning via 2D Generative Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the growing accessibility of skeletal motion data, integrating it for animating character meshes remains challenging due to diverse configurations of both skeletons and meshes. Specifically, the body scale and bone lengths of the skeleton should be adjusted in accordance with the size and proportions of the mesh, ensuring that all joints are accurately positioned within the character mesh. Furthermore, defining skinning weights is complicated by variations in skeletal configurations, such as the number of joints and their hierarchy, as well as differences in mesh configurations, including their connectivity and shapes. While existing approaches have made efforts to automate this process, they hardly address the variations in both skeletal and mesh configurations. In this paper, we present a novel method for the automatic rigging and skinning of character meshes using skeletal motion data, accommodating arbitrary configurations of both meshes and skeletons. The proposed method predicts the optimal skeleton aligned with the size and proportion of the mesh as well as defines skinning weights for various mesh-skeleton configurations, without requiring explicit supervision tailored to each of them. By incorporating Diffusion 3D Features (Diff3F) as semantic descriptors of character meshes, our method achieves robust generalization across different configurations. To assess the performance of our method in comparison to existing approaches, we conducted comprehensive evaluations encompassing both quantitative and qualitative analyses, specifically examining the predicted skeletons, skinning weights, and deformation quality.
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2503.12406.pdf' target='_blank'>https://arxiv.org/pdf/2503.12406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binggwong Leung, Worasuchad Haomachai, Joachim Winther Pedersen, Sebastian Risi, Poramate Manoonpong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12406">Bio-Inspired Plastic Neural Networks for Zero-Shot Out-of-Distribution Generalization in Complex Animal-Inspired Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial neural networks can be used to solve a variety of robotic tasks. However, they risk failing catastrophically when faced with out-of-distribution (OOD) situations. Several approaches have employed a type of synaptic plasticity known as Hebbian learning that can dynamically adjust weights based on local neural activities. Research has shown that synaptic plasticity can make policies more robust and help them adapt to unforeseen changes in the environment. However, networks augmented with Hebbian learning can lead to weight divergence, resulting in network instability. Furthermore, such Hebbian networks have not yet been applied to solve legged locomotion in complex real robots with many degrees of freedom. In this work, we improve the Hebbian network with a weight normalization mechanism for preventing weight divergence, analyze the principal components of the Hebbian's weights, and perform a thorough evaluation of network performance in locomotion control for real 18-DOF dung beetle-like and 16-DOF gecko-like robots. We find that the Hebbian-based plastic network can execute zero-shot sim-to-real adaptation locomotion and generalize to unseen conditions, such as uneven terrain and morphological damage.
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2503.08639.pdf' target='_blank'>https://arxiv.org/pdf/2503.08639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>DuÅ¡an MaliÄ, Christian Fruhwirth-Reisinger, Samuel Schulter, Horst Possegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08639">GBlobs: Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain LiDAR-based 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D detectors need large datasets for training, yet they struggle to generalize to novel domains. Domain Generalization (DG) aims to mitigate this by training detectors that are invariant to such domain shifts. Current DG approaches exclusively rely on global geometric features (point cloud Cartesian coordinates) as input features. Over-reliance on these global geometric features can, however, cause 3D detectors to prioritize object location and absolute position, resulting in poor cross-domain performance. To mitigate this, we propose to exploit explicit local point cloud structure for DG, in particular by encoding point cloud neighborhoods with Gaussian blobs, GBlobs. Our proposed formulation is highly efficient and requires no additional parameters. Without any bells and whistles, simply by integrating GBlobs in existing detectors, we beat the current state-of-the-art in challenging single-source DG benchmarks by over 21 mAP (Waymo->KITTI), 13 mAP (KITTI->Waymo), and 12 mAP (nuScenes->KITTI), without sacrificing in-domain performance. Additionally, GBlobs demonstrate exceptional performance in multi-source DG, surpassing the current state-of-the-art by 17, 12, and 5 mAP on Waymo, KITTI, and ONCE, respectively.
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2503.01157.pdf' target='_blank'>https://arxiv.org/pdf/2503.01157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobin Hong, Jiawen Zhang, Wenzhong Li, Sanglu Lu, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01157">Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time Series Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of foundation models has revolutionized natural language processing and computer vision, yet their best practices to time series forecasting remains underexplored. Existing time series foundation models often adopt methodologies from these fields without addressing the unique characteristics of time series data. In this paper, we identify two key challenges in cross-domain time series forecasting: the complexity of temporal patterns and semantic misalignment. To tackle these issues, we propose the ``Unify and Anchor" transfer paradigm, which disentangles frequency components for a unified perspective and incorporates external context as domain anchors for guided adaptation. Based on this framework, we introduce ContexTST, a Transformer-based model that employs a time series coordinator for structured representation and the Transformer blocks with a context-informed mixture-of-experts mechanism for effective cross-domain generalization. Extensive experiments demonstrate that ContexTST advances state-of-the-art forecasting performance while achieving strong zero-shot transferability across diverse domains.
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2502.19712.pdf' target='_blank'>https://arxiv.org/pdf/2502.19712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manveer Singh Tamber, Suleman Kazi, Vivek Sourabh, Jimmy Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19712">Teaching Dense Retrieval Models to Specialize with Listwise Distillation and LLM Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the current state-of-the-art dense retrieval models exhibit strong out-of-domain generalization, they might fail to capture nuanced domain-specific knowledge. In principle, fine-tuning these models for specialized retrieval tasks should yield higher effectiveness than relying on a one-size-fits-all model, but in practice, results can disappoint. We show that standard fine-tuning methods using an InfoNCE loss can unexpectedly degrade effectiveness rather than improve it, even for domain-specific scenarios. This holds true even when applying widely adopted techniques such as hard-negative mining and negative de-noising. To address this, we explore a training strategy that uses listwise distillation from a teacher cross-encoder, leveraging rich relevance signals to fine-tune the retriever. We further explore synthetic query generation using large language models. Through listwise distillation and training with a diverse set of queries ranging from natural user searches and factual claims to keyword-based queries, we achieve consistent effectiveness gains across multiple datasets. Our results also reveal that synthetic queries can rival human-written queries in training utility. However, we also identify limitations, particularly in the effectiveness of cross-encoder teachers as a bottleneck. We release our code and scripts to encourage further research.
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2502.19671.pdf' target='_blank'>https://arxiv.org/pdf/2502.19671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ju-Hyeon Nam, Sang-Chul Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19671">Test-Time Modality Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable medical image segmentation is essential for ensuring consistent performance across diverse unseen clinical settings. However, existing methods often overlook the capability to generalize effectively across arbitrary unseen modalities. In this paper, we introduce a novel Test-Time Modality Generalization (TTMG) framework, which comprises two core components: Modality-Aware Style Projection (MASP) and Modality-Sensitive Instance Whitening (MSIW), designed to enhance generalization in arbitrary unseen modality datasets. The MASP estimates the likelihood of a test instance belonging to each seen modality and maps it onto a distribution using modality-specific style bases, guiding its projection effectively. Furthermore, as high feature covariance hinders generalization to unseen modalities, the MSIW is applied during training to selectively suppress modality-sensitive information while retaining modality-invariant features. By integrating MASP and MSIW, the TTMG framework demonstrates robust generalization capabilities for medical image segmentation in unseen modalities a challenge that current methods have largely neglected. We evaluated TTMG alongside other domain generalization techniques across eleven datasets spanning four modalities (colonoscopy, ultrasound, dermoscopy, and radiology), consistently achieving superior segmentation performance across various modality combinations.
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2502.14917.pdf' target='_blank'>https://arxiv.org/pdf/2502.14917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhao, Qirui Yuan, Jinyu Li, Haofeng Hu, Yun Li, Chengyuan Zheng, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14917">Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark.
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2502.09297.pdf' target='_blank'>https://arxiv.org/pdf/2502.09297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianren Zhang, Guanyu Chen, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09297">When Do Neural Networks Learn World Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions--even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2502.07847.pdf' target='_blank'>https://arxiv.org/pdf/2502.07847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Behraj Khan, Rizwan Qureshi, Nouman Muhammad Durrani, Tahir Syed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07847">Confidence-calibrated covariate shift correction for few-shot classification in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the establishment of vision-language foundation models as the new mainstay in low-shot vision classification tasks, the question of domain generalization arising from insufficient target data is assuming more importance. This scarcity challenge induces sampling bias and amplifies model sensitivity to variations and shifts in data distributions. While fine-tuning on multiple domains could mitigate such domain generalization issues, it is resource-intensive and demands diverse data sources.
  In this work, we systematically analyze two critical challenges: (1) covariate shift between the pre-training distribution and the underspecified target distribution, and (2) confidence misalignment, where predictions on novel data are overconfident.
  To address both challenges simultaneously, we introduce \textbf{Confidence-Calibrated Covariate Shift Correction (CalShift)} -- a unified approach that combines a Fisher information penalty to mitigate covariate shift and a Confidence Misalignment Penalty (CMP) to reduce overconfidence in misclassified examples.
  Experimental evaluations across various vision and covariate shift benchmarks demonstrate that CalShift significantly improves model calibration, achieving up to a 5.82\% reduction in Expected Calibration Error (ECE). Furthermore, CalShift enhances robustness, improving accuracy by 3.5\% on challenging datasets impacted by covariate shifts.
  Our results highlight CalShift as a promising strategy for building robust and reliable low-shot vision-language systems for real-world applications.
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2502.02017.pdf' target='_blank'>https://arxiv.org/pdf/2502.02017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Wang, Bokui Wang, Zhixiang Shen, Boyan Deng, Zhao Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02017">Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in CV and NLP have inspired researchers to develop general-purpose graph foundation models through pre-training across diverse domains. However, a fundamental challenge arises from the substantial differences in graph topologies across domains. Additionally, real-world graphs are often sparse and prone to noisy connections and adversarial attacks. To address these issues, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework that aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. MDGFM bridges different domains by adaptively balancing features and topology while refining original graphs to eliminate noise and align topological structures. To further enhance knowledge transfer, we introduce an efficient prompt-tuning approach. By aligning topologies, MDGFM not only improves multi-domain pre-training but also enables robust knowledge transfer to unseen domains. Theoretical analyses provide guarantees of MDGFM's effectiveness and domain generalization capabilities. Extensive experiments on both homophilic and heterophilic graph datasets validate the robustness and efficacy of our method.
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2501.17595.pdf' target='_blank'>https://arxiv.org/pdf/2501.17595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Behraj Khan, Tahir Syed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17595">Technical report on label-informed logit redistribution for better domain generalization in low-shot classification with foundation models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Confidence calibration is an emerging challenge in real-world decision systems based on foundations models when used for downstream vision classification tasks. Due to various reasons exposed, logit scores on the CLIP head remain large irrespective of whether the image-language pairs reconcile. It is difficult to address in data space, given the few-shot regime. We propose a penalty incorporated into loss objective that penalizes incorrect classifications whenever one is made during finetuning, by moving an amount of log-likelihood to the true class commensurate to the relative amplitudes of the two likelihoods. We refer to it as \textit{confidence misalignment penalty (CMP)}. Extensive experiments on $12$ vision datasets and $5$ domain generalization datasets supports the calibration performance of our method against stat-of-the-art. CMP outperforms the benchmarked prompt learning methods, demonstrating average improvement in Expected Calibration Error (ECE) by average $6.01$\%, $4.01$ \% at minimum and $9.72$\% at maximum.
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2501.14605.pdf' target='_blank'>https://arxiv.org/pdf/2501.14605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jules Sanchez, Jean-Emmanuel Deschaud, FranÃ§ois Goulette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14605">3DLabelProp: Geometric-Driven Domain Generalization for LiDAR Semantic Segmentation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to find ways for deep learning models to maintain their performance despite significant domain shifts between training and inference datasets. This is particularly important for models that need to be robust or are costly to train. LiDAR perception in autonomous driving is impacted by both of these concerns, leading to the emergence of various approaches. This work addresses the challenge by proposing a geometry-based approach, leveraging the sequential structure of LiDAR sensors, which sets it apart from the learning-based methods commonly found in the literature. The proposed method, called 3DLabelProp, is applied on the task of LiDAR Semantic Segmentation (LSS). Through extensive experimentation on seven datasets, it is demonstrated to be a state-of-the-art approach, outperforming both naive and other domain generalization methods.
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2412.09719.pdf' target='_blank'>https://arxiv.org/pdf/2412.09719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johann Schmidt, Frank Dreyer, Sayed Abid Hashimi, Sebastian Stober
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09719">TransferLight: Zero-Shot Traffic Signal Control on any Road-Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic signal control plays a crucial role in urban mobility. However, existing methods often struggle to generalize beyond their training environments to unseen scenarios with varying traffic dynamics. We present TransferLight, a novel framework designed for robust generalization across road-networks, diverse traffic conditions and intersection geometries. At its core, we propose a log-distance reward function, offering spatially-aware signal prioritization while remaining adaptable to varied lane configurations - overcoming the limitations of traditional pressure-based rewards. Our hierarchical, heterogeneous, and directed graph neural network architecture effectively captures granular traffic dynamics, enabling transferability to arbitrary intersection layouts. Using a decentralized multi-agent approach, global rewards, and novel state transition priors, we develop a single, weight-tied policy that scales zero-shot to any road network without re-training. Through domain randomization during training, we additionally enhance generalization capabilities. Experimental results validate TransferLight's superior performance in unseen scenarios, advancing practical, generalizable intelligent transportation systems to meet evolving urban traffic demands.
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2411.16833.pdf' target='_blank'>https://arxiv.org/pdf/2411.16833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Yao, Hao Gu, Xuweiyi Chen, Jiayun Wang, Zezhou Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16833">Open Vocabulary Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we pioneer the study of open-vocabulary monocular 3D object detection, a novel task that aims to detect and localize objects in 3D space from a single RGB image without limiting detection to a predefined set of categories. We formalize this problem, establish baseline methods, and introduce a class-agnostic approach that leverages open-vocabulary 2D detectors and lifts 2D bounding boxes into 3D space. Our approach decouples the recognition and localization of objects in 2D from the task of estimating 3D bounding boxes, enabling generalization across unseen categories. Additionally, we propose a target-aware evaluation protocol to address inconsistencies in existing datasets, improving the reliability of model performance assessment. Extensive experiments on the Omni3D dataset demonstrate the effectiveness of the proposed method in zero-shot 3D detection for novel object categories, validating its robust generalization capabilities. Our method and evaluation protocols contribute towards the development of open-vocabulary object detection models that can effectively operate in real-world, category-diverse environments.
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2410.09069.pdf' target='_blank'>https://arxiv.org/pdf/2410.09069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Hosseini Chagahi, Niloufar Delfan, Saeed Mohammadi Dashtaki, Behzad Moshiri, Md. Jalil Piran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09069">Explainable AI for Fraud Detection: An Attention-Based Ensemble of CNNs, GNNs, and A Confidence-Driven Gating Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid expansion of e-commerce and the widespread use of credit cards in online purchases and financial transactions have significantly heightened the importance of promptly and accurately detecting credit card fraud (CCF). Not only do fraudulent activities in financial transactions lead to substantial monetary losses for banks and financial institutions, but they also undermine user trust in digital services. This study presents a new stacking-based approach for CCF detection by adding two extra layers to the usual classification process: an attention layer and a confidence-based combination layer. In the attention layer, we combine soft outputs from a convolutional neural network (CNN) and a recurrent neural network (RNN) using the dependent ordered weighted averaging (DOWA) operator, and from a graph neural network (GNN) and a long short-term memory (LSTM) network using the induced ordered weighted averaging (IOWA) operator. These weighted outputs capture different predictive signals, increasing the model's accuracy. Next, in the confidence-based layer, we select whichever aggregate (DOWA or IOWA) shows lower uncertainty to feed into a meta-learner. To make the model more explainable, we use shapley additive explanations (SHAP) to identify the top ten most important features for distinguishing between fraud and normal transactions. These features are then used in our attention-based model. Experiments on three datasets show that our method achieves high accuracy and robust generalization, making it effective for CCF detection.
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2409.17063.pdf' target='_blank'>https://arxiv.org/pdf/2409.17063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neda Zamanitajeddin, Mostafa Jahanifar, Kesi Xu, Fouzia Siraj, Nasir Rajpoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17063">Benchmarking Domain Generalization Algorithms in Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have shown immense promise in computational pathology (CPath) tasks, but their performance often suffers when applied to unseen data due to domain shifts. Addressing this requires domain generalization (DG) algorithms. However, a systematic evaluation of DG algorithms in the CPath context is lacking. This study aims to benchmark the effectiveness of 30 DG algorithms on 3 CPath tasks of varying difficulty through 7,560 cross-validation runs. We evaluate these algorithms using a unified and robust platform, incorporating modality-specific techniques and recent advances like pretrained foundation models. Our extensive cross-validation experiments provide insights into the relative performance of various DG strategies. We observe that self-supervised learning and stain augmentation consistently outperform other methods, highlighting the potential of pretrained models and data augmentation. Furthermore, we introduce a new pan-cancer tumor detection dataset (HISTOPANTUM) as a benchmark for future research. This study offers valuable guidance to researchers in selecting appropriate DG approaches for CPath tasks.
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2409.08589.pdf' target='_blank'>https://arxiv.org/pdf/2409.08589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilyass Moummad, Romain Serizel, Emmanouil Benetos, Nicolas Farrugia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08589">Domain-Invariant Representation Learning of Bird Sounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Passive acoustic monitoring (PAM) is crucial for bioacoustic research, enabling non-invasive species tracking and biodiversity monitoring. Citizen science platforms provide large annotated datasets from focal recordings, where the target species is intentionally recorded. However, PAM requires monitoring in passive soundscapes, creating a domain shift between focal and passive recordings, challenging deep learning models trained on focal recordings. To address domain generalization, we leverage supervised contrastive learning by enforcing domain invariance across same-class examples from different domains. Additionally, we propose ProtoCLR, an alternative to SupCon loss which reduces the computational complexity by comparing examples to class prototypes instead of pairwise comparisons. We conduct few-shot classification based on BIRB, a large-scale bird sound benchmark to assess pre-trained bioacoustic models. Our findings suggest that ProtoCLR is a better alternative to SupCon.
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2409.05790.pdf' target='_blank'>https://arxiv.org/pdf/2409.05790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farah Alsafadi, Aidan Furlong, Xu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05790">Predicting Critical Heat Flux with Uncertainty Quantification and Domain Generalization Using Conditional Variational Autoencoders and Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models (DGMs) can generate synthetic data samples that closely resemble the original dataset, addressing data scarcity. In this work, we developed a conditional variational autoencoder (CVAE) to augment critical heat flux (CHF) data used for the 2006 Groeneveld lookup table. To compare with traditional methods, a fine-tuned deep neural network (DNN) regression model was evaluated on the same dataset. Both models achieved small mean absolute relative errors, with the CVAE showing more favorable results. Uncertainty quantification (UQ) was performed using repeated CVAE sampling and DNN ensembling. The DNN ensemble improved performance over the baseline, while the CVAE maintained consistent results with less variability and higher confidence. Both models achieved small errors inside and outside the training domain, with slightly larger errors outside. Overall, the CVAE performed better than the DNN in predicting CHF and exhibited better uncertainty behavior.
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2407.15155.pdf' target='_blank'>https://arxiv.org/pdf/2407.15155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyi Xuan, Weijie Chen, Shicai Yang, Di Xie, Luojun Lin, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15155">Distilling Vision-Language Foundation Models: A Data-Free Approach via Prompt Diversification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-Free Knowledge Distillation (DFKD) has shown great potential in creating a compact student model while alleviating the dependency on real training data by synthesizing surrogate data. However, prior arts are seldom discussed under distribution shifts, which may be vulnerable in real-world applications. Recent Vision-Language Foundation Models, e.g., CLIP, have demonstrated remarkable performance in zero-shot out-of-distribution generalization, yet consuming heavy computation resources. In this paper, we discuss the extension of DFKD to Vision-Language Foundation Models without access to the billion-level image-text datasets. The objective is to customize a student model for distribution-agnostic downstream tasks with given category concepts, inheriting the out-of-distribution generalization capability from the pre-trained foundation models. In order to avoid generalization degradation, the primary challenge of this task lies in synthesizing diverse surrogate images driven by text prompts. Since not only category concepts but also style information are encoded in text prompts, we propose three novel Prompt Diversification methods to encourage image synthesis with diverse styles, namely Mix-Prompt, Random-Prompt, and Contrastive-Prompt. Experiments on out-of-distribution generalization datasets demonstrate the effectiveness of the proposed methods, with Contrastive-Prompt performing the best.
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2407.13322.pdf' target='_blank'>https://arxiv.org/pdf/2407.13322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei-Kai Huang, Tzu-Hsien Chen, Ya-Ting Chan, Kuan-Wen Chen, Chiou-Ting Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13322">Fully Test-Time rPPG Estimation via Synthetic Signal-Guided Feature Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many remote photoplethysmography (rPPG) estimation models have achieved promising performance in the training domain but often fail to accurately estimate physiological signals or heart rates (HR) in the target domains. Domain generalization (DG) or domain adaptation (DA) techniques are therefore adopted during the offline training stage to adapt the model to either unobserved or observed target domains by utilizing all available source domain data. However, in rPPG estimation problems, the adapted model usually encounters challenges in estimating target data with significant domain variation. In contrast, Test-Time Adaptation (TTA) enables the model to adaptively estimate rPPG signals in various unseen domains by online adapting to unlabeled target data without referring to any source data. In this paper, we first establish a new TTA-rPPG benchmark that encompasses various domain information and HR distributions to simulate the challenges encountered in real-world rPPG estimation. Next, we propose a novel synthetic signal-guided rPPG estimation framework to address the forgetting issue during the TTA stage and to enhance the adaptation capability of the pre-trained rPPG model. To this end, we develop a synthetic signal-guided feature learning method by synthesizing pseudo rPPG signals as pseudo ground truths to guide a conditional generator in generating latent rPPG features. In addition, we design an effective spectral-based entropy minimization technique to encourage the rPPG model to learn new target domain information. Both the generated rPPG features and synthesized rPPG signals prevent the rPPG model from overfitting to target data and forgetting previously acquired knowledge, while also broadly covering various heart rate (HR) distributions. Our extensive experiments on the TTA-rPPG benchmark show that the proposed method achieves superior performance.
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2407.11942.pdf' target='_blank'>https://arxiv.org/pdf/2407.11942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Klarner, Tim G. J. Rudner, Garrett M. Morris, Charlotte M. Deane, Yee Whye Teh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11942">Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have the potential to accelerate key steps in the discovery of novel molecular therapeutics and materials. Diffusion models have recently emerged as a powerful approach, excelling at unconditional sample generation and, with data-driven guidance, conditional generation within their training domain. Reliably sampling from high-value regions beyond the training data, however, remains an open challenge -- with current methods predominantly focusing on modifying the diffusion process itself. In this paper, we develop context-guided diffusion (CGD), a simple plug-and-play method that leverages unlabeled data and smoothness constraints to improve the out-of-distribution generalization of guided diffusion models. We demonstrate that this approach leads to substantial performance gains across various settings, including continuous, discrete, and graph-structured diffusion processes with applications across drug discovery, materials science, and protein design.
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2407.02547.pdf' target='_blank'>https://arxiv.org/pdf/2407.02547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuquan Xie, Shengtao Peng, Wanqi Yang, Ming Yang, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02547">Domain Generalizable Knowledge Tracing via Concept Aggregation and Relation-Based Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge Tracing (KT) is a critical task in online education systems, aiming to monitor students' knowledge states throughout a learning period. Common KT approaches involve predicting the probability of a student correctly answering the next question based on their exercise history. However, these methods often suffer from performance degradation when faced with the scarcity of student interactions in new education systems. To address this, we leverage student interactions from existing education systems to mitigate performance degradation caused by limited training data. Nevertheless, these interactions exhibit significant differences since they are derived from different education systems. To address this issue, we propose a domain generalization approach for knowledge tracing, where existing education systems are considered source domains, and new education systems with limited data are considered target domains. Additionally, we design a domain-generalizable knowledge tracing framework (DGKT) that can be applied to any KT model. Specifically, we present a concept aggregation approach designed to reduce conceptual disparities within sequences of student interactions from diverse domains. To further mitigate domain discrepancies, we introduce a novel normalization module called Sequence Instance Normalization (SeqIN). Moreover, to fully leverage exercise information, we propose a new knowledge tracing model tailored for the domain generalization KT task, named Domain-Generalizable Relation-based Knowledge Tracing (DGRKT). Extensive experiments across five benchmark datasets demonstrate that the proposed method performs well despite limited training data.
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2407.00756.pdf' target='_blank'>https://arxiv.org/pdf/2407.00756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salah Zaiem, Titouan Parcollet, Slim Essid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00756">Less Forgetting for Better Generalization: Exploring Continual-learning Fine-tuning Methods for Speech Self-supervised Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite being trained on massive and diverse datasets, speech self-supervised encoders are generally used for downstream purposes as mere frozen feature extractors or model initializers before fine-tuning. The former severely limits the exploitation of large encoders, while the latter hurts the robustness acquired during pretraining, especially in low-resource scenarios. This work explores middle-ground solutions, conjecturing that reducing the forgetting of the self-supervised task during the downstream fine-tuning leads to better generalization. To prove this, focusing on speech recognition, we benchmark different continual-learning approaches during fine-tuning and show that they improve both in-domain and out-of-domain generalization abilities. Relative performance gains reach 15.7% and 22.5% with XLSR used as the encoder on two English and Danish speech recognition tasks. Further probing experiments show that these gains are indeed linked to less forgetting.
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2406.14308.pdf' target='_blank'>https://arxiv.org/pdf/2406.14308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kwanseok Oh, Eunjin Jeon, Da-Woon Heo, Yooseung Shin, Heung-Il Suk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14308">FIESTA: Fourier-Based Semantic Augmentation with Uncertainty Guidance for Enhanced Domain Generalizability in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization (SDG) in medical image segmentation (MIS) aims to generalize a model using data from only one source domain to segment data from an unseen target domain. Despite substantial advances in SDG with data augmentation, existing methods often fail to fully consider the details and uncertain areas prevalent in MIS, leading to mis-segmentation. This paper proposes a Fourier-based semantic augmentation method called FIESTA using uncertainty guidance to enhance the fundamental goals of MIS in an SDG context by manipulating the amplitude and phase components in the frequency domain. The proposed Fourier augmentative transformer addresses semantic amplitude modulation based on meaningful angular points to induce pertinent variations and harnesses the phase spectrum to ensure structural coherence. Moreover, FIESTA employs epistemic uncertainty to fine-tune the augmentation process, improving the ability of the model to adapt to diverse augmented data and concentrate on areas with higher ambiguity. Extensive experiments across three cross-domain scenarios demonstrate that FIESTA surpasses recent state-of-the-art SDG approaches in segmentation performance and significantly contributes to boosting the applicability of the model in medical imaging modalities.
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2406.09330.pdf' target='_blank'>https://arxiv.org/pdf/2406.09330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Somin Wadhwa, Adit Krishnan, Runhui Wang, Byron C. Wallace, Chris Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09330">Learning from Natural Language Explanations for Generalizable Entity Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Entity matching is the task of linking records from different sources that refer to the same real-world entity. Past work has primarily treated entity linking as a standard supervised learning problem. However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive. Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge. But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks.
  As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification. This enables us to "distill" LLM reasoning into smaller entity matching models via natural language explanations. This approach achieves strong performance, especially on out-of-domain generalization tests (10.85% F-1) where standalone generative methods struggle. We perform ablations that highlight the importance of explanations, both for performance and model robustness.
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2406.03345.pdf' target='_blank'>https://arxiv.org/pdf/2406.03345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianren Zhang, Chujie Zhao, Guanyu Chen, Yizhou Jiang, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03345">Feature contamination: Neural networks learn uncorrelated features and fail to generalize</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning representations that generalize under distribution shifts is critical for building robust machine learning models. However, despite significant efforts in recent years, algorithmic advances in this direction have been limited. In this work, we seek to understand the fundamental difficulty of out-of-distribution generalization with deep neural networks. We first empirically show that perhaps surprisingly, even allowing a neural network to explicitly fit the representations obtained from a teacher network that can generalize out-of-distribution is insufficient for the generalization of the student network. Then, by a theoretical study of two-layer ReLU networks optimized by stochastic gradient descent (SGD) under a structured feature model, we identify a fundamental yet unexplored feature learning proclivity of neural networks, feature contamination: neural networks can learn uncorrelated features together with predictive features, resulting in generalization failure under distribution shifts. Notably, this mechanism essentially differs from the prevailing narrative in the literature that attributes the generalization failure to spurious correlations. Overall, our results offer new insights into the non-linear feature learning dynamics of neural networks and highlight the necessity of considering inductive biases in out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2405.19703.pdf' target='_blank'>https://arxiv.org/pdf/2405.19703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duhun Hwang, Suhyun Kang, Moonjung Eo, Jimyeong Kim, Wonjong Rhee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19703">Towards a Better Evaluation of Out-of-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of Domain Generalization (DG) is to devise algorithms and models capable of achieving high performance on previously unseen test distributions. In the pursuit of this objective, average measure has been employed as the prevalent measure for evaluating models and comparing algorithms in the existing DG studies. Despite its significance, a comprehensive exploration of the average measure has been lacking and its suitability in approximating the true domain generalization performance has been questionable. In this study, we carefully investigate the limitations inherent in the average measure and propose worst+gap measure as a robust alternative. We establish theoretical grounds of the proposed measure by deriving two theorems starting from two different assumptions. We conduct extensive experimental investigations to compare the proposed worst+gap measure with the conventional average measure. Given the indispensable need to access the true DG performance for studying measures, we modify five existing datasets to come up with SR-CMNIST, C-Cats&Dogs, L-CIFAR10, PACS-corrupted, and VLCS-corrupted datasets. The experiment results unveil an inferior performance of the average measure in approximating the true DG performance and confirm the robustness of the theoretically supported worst+gap measure.
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2405.06995.pdf' target='_blank'>https://arxiv.org/pdf/2405.06995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobao Guo, Zitong Yu, Nithish Muthuchamy Selvaraj, Bingquan Shen, Adams Wai-Kin Kong, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06995">Benchmarking Cross-Domain Audio-Visual Deception Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated deception detection is crucial for assisting humans in accurately assessing truthfulness and identifying deceptive behavior. Conventional contact-based techniques, like polygraph devices, rely on physiological signals to determine the authenticity of an individual's statements. Nevertheless, recent developments in automated deception detection have demonstrated that multimodal features derived from both audio and video modalities may outperform human observers on publicly available datasets. Despite these positive findings, the generalizability of existing audio-visual deception detection approaches across different scenarios remains largely unexplored. To close this gap, we present the first cross-domain audio-visual deception detection benchmark, that enables us to assess how well these methods generalize for use in real-world scenarios. We used widely adopted audio and visual features and different architectures for benchmarking, comparing single-to-single and multi-to-single domain generalization performance. To further exploit the impacts using data from multiple source domains for training, we investigate three types of domain sampling strategies, including domain-simultaneous, domain-alternating, and domain-by-domain for multi-to-single domain generalization evaluation. We also propose an algorithm to enhance the generalization performance by maximizing the gradient inner products between modality encoders, named ``MM-IDGM". Furthermore, we proposed the Attention-Mixer fusion method to improve performance, and we believe that this new cross-domain benchmark will facilitate future research in audio-visual deception detection.
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2405.06816.pdf' target='_blank'>https://arxiv.org/pdf/2405.06816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thai-Hoang Pham, Xueru Zhang, Ping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06816">Non-stationary Domain Generalization: Theory and Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although recent advances in machine learning have shown its success to learn from independent and identically distributed (IID) data, it is vulnerable to out-of-distribution (OOD) data in an open world. Domain generalization (DG) deals with such an issue and it aims to learn a model from multiple source domains that can be generalized to unseen target domains. Existing studies on DG have largely focused on stationary settings with homogeneous source domains. However, in many applications, domains may evolve along a specific direction (e.g., time, space). Without accounting for such non-stationary patterns, models trained with existing methods may fail to generalize on OOD data. In this paper, we study domain generalization in non-stationary environment. We first examine the impact of environmental non-stationarity on model performance and establish the theoretical upper bounds for the model error at target domains. Then, we propose a novel algorithm based on adaptive invariant representation learning, which leverages the non-stationary pattern to train a model that attains good performance on target domains. Experiments on both synthetic and real data validate the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2404.11764.pdf' target='_blank'>https://arxiv.org/pdf/2404.11764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deepti Hegde, Suhas Lohit, Kuan-Chuan Peng, Michael J. Jones, Vishal M. Patel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11764">Multimodal 3D Object Detection on Unseen Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR datasets for autonomous driving exhibit biases in properties such as point cloud density, range, and object dimensions. As a result, object detection networks trained and evaluated in different environments often experience performance degradation. Domain adaptation approaches assume access to unannotated samples from the test distribution to address this problem. However, in the real world, the exact conditions of deployment and access to samples representative of the test dataset may be unavailable while training. We argue that the more realistic and challenging formulation is to require robustness in performance to unseen target domains. We propose to address this problem in a two-pronged manner. First, we leverage paired LiDAR-image data present in most autonomous driving datasets to perform multimodal object detection. We suggest that working with multimodal features by leveraging both images and LiDAR point clouds for scene understanding tasks results in object detectors more robust to unseen domain shifts. Second, we train a 3D object detector to learn multimodal object features across different distributions and promote feature invariance across these source domains to improve generalizability to unseen target domains. To this end, we propose CLIX$^\text{3D}$, a multimodal fusion and supervised contrastive learning framework for 3D object detection that performs alignment of object features from same-class samples of different domains while pushing the features from different classes apart. We show that CLIX$^\text{3D}$ yields state-of-the-art domain generalization performance under multiple dataset shifts.
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2404.10740.pdf' target='_blank'>https://arxiv.org/pdf/2404.10740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10740">N-Agent Ad Hoc Teamwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current approaches to learning cooperative multi-agent behaviors assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls $\textit{all}$ agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\textit{single}$ agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce $N$-agent ad hoc teamwork (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates. This paper formalizes the problem, and proposes the Policy Optimization with Agent Modelling (POAM) algorithm. POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors. Empirical evaluation on tasks from the multi-agent particle environment and StarCraft II shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates.
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2404.09277.pdf' target='_blank'>https://arxiv.org/pdf/2404.09277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasudha Venkatesan, Daniel Panangian, Mario Fuentes Reyes, Ksenia Bittner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09277">SyntStereo2Real: Edge-Aware GAN for Remote Sensing Image-to-Image Translation while Maintaining Stereo Constraint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of remote sensing, the scarcity of stereo-matched and particularly lack of accurate ground truth data often hinders the training of deep neural networks. The use of synthetically generated images as an alternative, alleviates this problem but suffers from the problem of domain generalization. Unifying the capabilities of image-to-image translation and stereo-matching presents an effective solution to address the issue of domain generalization. Current methods involve combining two networks, an unpaired image-to-image translation network and a stereo-matching network, while jointly optimizing them. We propose an edge-aware GAN-based network that effectively tackles both tasks simultaneously. We obtain edge maps of input images from the Sobel operator and use it as an additional input to the encoder in the generator to enforce geometric consistency during translation. We additionally include a warping loss calculated from the translated images to maintain the stereo consistency. We demonstrate that our model produces qualitatively and quantitatively superior results than existing models, and its applicability extends to diverse domains, including autonomous driving.
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2404.01217.pdf' target='_blank'>https://arxiv.org/pdf/2404.01217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S. Blum, Parv Venkitasubramaniam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01217">Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods.
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2403.13113.pdf' target='_blank'>https://arxiv.org/pdf/2403.13113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aneesh Rangnekar, Nishant Nadkarni, Jue Jiang, Harini Veeraraghavan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13113">Quantifying uncertainty in lung cancer segmentation with foundation models applied to mixed-domain datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image foundation models have shown the ability to segment organs and tumors with minimal fine-tuning. These models are typically evaluated on task-specific in-distribution (ID) datasets. However, reliable performance on ID datasets does not guarantee robust generalization on out-of-distribution (OOD) datasets. Importantly, once deployed for clinical use, it is impractical to have `ground truth' delineations to assess ongoing performance drifts, especially when images fall into the OOD category due to different imaging protocols. Hence, we introduced a comprehensive set of computationally fast metrics to evaluate the performance of multiple foundation models (Swin UNETR, SimMIM, iBOT, SMIT) trained with self-supervised learning (SSL). All models were fine-tuned on identical datasets for lung tumor segmentation from computed tomography (CT) scans. The evaluation was performed on two public lung cancer datasets (LRAD: n = 140, 5Rater: n = 21) with different image acquisitions and tumor stages compared to training data (n = 317 public resource with stage III-IV lung cancers) and a public non-cancer dataset containing volumetric CT scans of patients with pulmonary embolism (n = 120). All models produced similarly accurate tumor segmentation on the lung cancer testing datasets. SMIT produced the highest F1-score (LRAD: 0.60, 5Rater: 0.64) and lowest entropy (LRAD: 0.06, 5Rater: 0.12), indicating higher tumor detection rate and confident segmentations. In the OOD dataset, SMIT misdetected the least number of tumors, marked by a median volume occupancy of 5.67 cc compared to the best method SimMIM of 9.97 cc. Our analysis shows that additional metrics such as entropy and volume occupancy may help better understand model performance on mixed domain datasets.
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2402.18910.pdf' target='_blank'>https://arxiv.org/pdf/2402.18910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Chen, Yitao Liang, Zhouchen Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18910">DIGIC: Domain Generalizable Imitation Learning by Causal Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causality has been combined with machine learning to produce robust representations for domain generalization. Most existing methods of this type require massive data from multiple domains to identify causal features by cross-domain variations, which can be expensive or even infeasible and may lead to misidentification in some cases. In this work, we make a different attempt by leveraging the demonstration data distribution to discover the causal features for a domain generalizable policy. We design a novel framework, called DIGIC, to identify the causal features by finding the direct cause of the expert action from the demonstration data distribution via causal discovery. Our framework can achieve domain generalizable imitation learning with only single-domain data and serve as a complement for cross-domain variation-based methods under non-structural assumptions on the underlying causal models. Our empirical study in various control tasks shows that the proposed framework evidently improves the domain generalization performance and has comparable performance to the expert in the original domain simultaneously.
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2402.18377.pdf' target='_blank'>https://arxiv.org/pdf/2402.18377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niclas GÃ¶ring, Florian Hess, Manuel Brenner, Zahra Monfared, Daniel Durstewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18377">Out-of-Domain Generalization in Dynamical Systems Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model. We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model. We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space. Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice.
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2402.17407.pdf' target='_blank'>https://arxiv.org/pdf/2402.17407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17407">A Neural Rewriting System to Solve Algorithmic Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances. In this work, we focus on formula simplification problems, a class of synthetic benchmarks used to study the systematic generalization capabilities of neural architectures. We propose a modular architecture designed to learn a general procedure for solving nested mathematical formulas by only relying on a minimal set of training examples. Inspired by rewriting systems, a classic framework in symbolic artificial intelligence, we include in the architecture three specialized and interacting modules: the Selector, trained to identify solvable sub-expressions; the Solver, mapping sub-expressions to their values; and the Combiner, replacing sub-expressions in the original formula with the solution provided by the Solver. We benchmark our system against the Neural Data Router, a recent model specialized for systematic generalization, and a state-of-the-art large language model (GPT-4) probed with advanced prompting strategies. We demonstrate that our approach achieves a higher degree of out-of-distribution generalization compared to these alternative approaches on three different types of formula simplification problems, and we discuss its limitations by analyzing its failures.
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2401.05097.pdf' target='_blank'>https://arxiv.org/pdf/2401.05097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhoo Lee, Yearim Kim, Hyunho Lee, Nojun Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05097">Any-Way Meta Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although meta-learning seems promising performance in the realm of rapid adaptability, it is constrained by fixed cardinality. When faced with tasks of varying cardinalities that were unseen during training, the model lacks its ability. In this paper, we address and resolve this challenge by harnessing `label equivalence' emerged from stochastic numeric label assignments during episodic task sampling. Questioning what defines ``true" meta-learning, we introduce the ``any-way" learning paradigm, an innovative model training approach that liberates model from fixed cardinality constraints. Surprisingly, this model not only matches but often outperforms traditional fixed-way models in terms of performance, convergence speed, and stability. This disrupts established notions about domain generalization. Furthermore, we argue that the inherent label equivalence naturally lacks semantic information. To bridge this semantic information gap arising from label equivalence, we further propose a mechanism for infusing semantic class information into the model. This would enhance the model's comprehension and functionality. Experiments conducted on renowned architectures like MAML and ProtoNet affirm the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2401.03428.pdf' target='_blank'>https://arxiv.org/pdf/2401.03428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, Xiuqiang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03428">Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them. Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities. This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2401.03170.pdf' target='_blank'>https://arxiv.org/pdf/2401.03170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chujie Zhao, Tianren Zhang, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03170">Preserving Silent Features for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to improve the generalization ability of the model trained on several known training domains over unseen test domains. Previous work has shown that self-supervised contrastive pre-training improves the robustness of the model on downstream tasks. However, in this paper, we find that self-supervised models do not exhibit better generalization performance than supervised models pre-trained on the same dataset in the DG setting. We argue that this is owing to the fact that the richer intra-class discriminative features extracted by self-supervised contrastive learning, which we term silent features, are suppressed during supervised fine-tuning. These silent features are likely to contain features that are more generalizable on the test domain. In this work, we model and analyze this feature suppression phenomenon and theoretically prove that preserving silent features can achieve lower expected test domain risk under certain conditions. In light of this, we propose a simple yet effective method termed STEP (Silent Feature Preservation) to improve the generalization performance of the self-supervised contrastive learning pre-trained model by alleviating the suppression of silent features during the supervised fine-tuning process. Experimental results show that STEP exhibits state-of-the-art performance on standard DG benchmarks with significant distribution shifts.
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2311.09737.pdf' target='_blank'>https://arxiv.org/pdf/2311.09737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingnan Li, Zhitong Gao, Xuming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09737">Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality MRI Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal MRI segmentation is of great value for computer-aided medical diagnosis, enabling flexible data acquisition and model generalization. However, most existing methods have difficulty in handling local variations in domain shift and typically require a significant amount of data for training, which hinders their usage in practice. To address these problems, we propose a novel adaptive domain generalization framework, which integrates a learning-free cross-domain representation based on image gradient maps and a class prior-informed test-time adaptation strategy for mitigating local domain shift. We validate our approach on two multi-modal MRI datasets with six cross-modal segmentation tasks. Across all the task settings, our method consistently outperforms competing approaches and shows a stable performance even with limited training data.
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2311.07811.pdf' target='_blank'>https://arxiv.org/pdf/2311.07811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaron Mueller, Albert Webson, Jackson Petty, Tal Linzen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07811">In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-context learning (ICL) is now a common method for teaching large language models (LLMs) new tasks: given labeled examples in the input context, the LLM learns to perform the task without weight updates. Do models guided via ICL infer the underlying structure of the task defined by the context, or do they rely on superficial heuristics that only generalize to identically distributed examples? We address this question using transformations tasks and an NLI task that assess sensitivity to syntax - a requirement for robust language understanding. We further investigate whether out-of-distribution generalization can be improved via chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. In experiments with models from the GPT, PaLM, and Llama 2 families, we find large variance across LMs. The variance is explained more by the composition of the pre-training corpus and supervision methods than by model size; in particular, models pre-trained on code generalize better, and benefit more from chain-of-thought prompting.
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2311.03017.pdf' target='_blank'>https://arxiv.org/pdf/2311.03017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jules Sanchez, Jean-Emmanuel Deschaud, FranÃ§ois Goulette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03017">COLA: COarse-LAbel multi-source LiDAR semantic segmentation for autonomous driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR semantic segmentation for autonomous driving has been a growing field of interest in recent years. Datasets and methods have appeared and expanded very quickly, but methods have not been updated to exploit this new data availability and rely on the same classical datasets. Different ways of performing LIDAR semantic segmentation training and inference can be divided into several subfields, which include the following: domain generalization, source-to-source segmentation, and pre-training. In this work, we aim to improve results in all of these subfields with the novel approach of multi-source training. Multi-source training relies on the availability of various datasets at training time. To overcome the common obstacles in multi-source training, we introduce the coarse labels and call the newly created multi-source dataset COLA. We propose three applications of this new dataset that display systematic improvement over single-source strategies: COLA-DG for domain generalization (+10%), COLA-S2S for source-to-source segmentation (+5.3%), and COLA-PT for pre-training (+12%). We demonstrate that multi-source approaches bring systematic improvement over single-source approaches.
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2310.08598.pdf' target='_blank'>https://arxiv.org/pdf/2310.08598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jee Seok Yoon, Kwanseok Oh, Yooseung Shin, Maciej A. Mazurowski, Heung-Il Suk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08598">Domain Generalization for Medical Image Analysis: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, deploying DL models for MedIA in real-world situations remains challenging due to their failure to generalize across the distributional gap between training and testing samples - a problem known as domain shift. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution (OOD) data distributions. This article comprehensively reviews domain generalization (DG) studies specifically tailored for MedIA. We provide a holistic view of how DG techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize DG methods into data-level, feature-level, model-level, and analysis-level methods. We show how those methods can be used in various stages of the MedIA workflow with DL equipped from data acquisition to model prediction and analysis. Furthermore, we critically analyze the strengths and weaknesses of various methods, unveiling future research opportunities.
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2310.07176.pdf' target='_blank'>https://arxiv.org/pdf/2310.07176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiwen Ding, James Hall, Neil Tenenholtz, Kristen Severson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07176">Improving mitosis detection on histopathology images using large vision-language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In certain types of cancerous tissue, mitotic count has been shown to be associated with tumor proliferation, poor prognosis, and therapeutic resistance. Due to the high inter-rater variability of mitotic counting by pathologists, convolutional neural networks (CNNs) have been employed to reduce the subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained whole slide images. However, most existing models have performance that lags behind expert panel review and only incorporate visual information. In this work, we demonstrate that pre-trained large-scale vision-language models that leverage both visual features and natural language improve mitosis detection accuracy. We formulate the mitosis detection task as an image captioning task and a visual question answering (VQA) task by including metadata such as tumor and scanner types as context. The effectiveness of our pipeline is demonstrated via comparison with various baseline models using 9,501 mitotic figures and 11,051 hard negatives (non-mitotic figures that are difficult to characterize) from the publicly available Mitosis Domain Generalization Challenge (MIDOG22) dataset.
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2310.04407.pdf' target='_blank'>https://arxiv.org/pdf/2310.04407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ge Gao, Jonathan D. Chang, Claire Cardie, KiantÃ© Brantley, Thorsten Joachim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04407">Policy-Gradient Training of Language Models for Ranking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems via policy gradient, with little reliance on complex heuristics, and it effectively unifies the training objective with downstream decision-making quality. We conduct extensive experiments on various text retrieval benchmarks. The results demonstrate that when the training objective aligns with the evaluation setup, Neural PG-RANK yields remarkable in-domain performance improvement, with substantial out-of-domain generalization to some critical datasets employed in downstream question answering tasks.
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2310.01825.pdf' target='_blank'>https://arxiv.org/pdf/2310.01825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad Hasan Zahweh, Hasan Nasrallah, Mustafa Shukor, Ghaleb Faour, Ali J. Ghandour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01825">Empirical Study of PEFT techniques for Winter Wheat Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adapting the SOTA TSViT model to address winter wheat field segmentation, a critical task for crop monitoring and food security. This adaptation process involves integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and prompt tuning. Using PEFT techniques, we achieved notable results comparable to those achieved using full fine-tuning methods while training only a mere 0.7% parameters of the whole TSViT architecture. The in-house labeled data-set, referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over five consecutive years. Using Sentinel-2 images, our model achieved a 84% F1-score. We intend to publicly release the Lebanese winter wheat data set, code repository, and model weights.
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2309.10439.pdf' target='_blank'>https://arxiv.org/pdf/2309.10439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mostafa Sadeghi, Romain Serizel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10439">Posterior sampling algorithms for unsupervised speech enhancement with recurrent variational autoencoder</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the unsupervised speech enhancement problem based on recurrent variational autoencoder (RVAE). This approach offers promising generalization performance over the supervised counterpart. Nevertheless, the involved iterative variational expectation-maximization (VEM) process at test time, which relies on a variational inference method, results in high computational complexity. To tackle this issue, we present efficient sampling techniques based on Langevin dynamics and Metropolis-Hasting algorithms, adapted to the EM-based speech enhancement with RVAE. By directly sampling from the intractable posterior distribution within the EM process, we circumvent the intricacies of variational inference. We conduct a series of experiments, comparing the proposed methods with VEM and a state-of-the-art supervised speech enhancement approach based on diffusion models. The results reveal that our sampling-based algorithms significantly outperform VEM, not only in terms of computational efficiency but also in overall performance. Furthermore, when compared to the supervised baseline, our methods showcase robust generalization performance in mismatched test conditions.
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2308.14861.pdf' target='_blank'>https://arxiv.org/pdf/2308.14861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lynn Cherif, Mutahar Safdar, Guy Lamouche, Priti Wanjara, Padma Paul, Gentry Wood, Max Zimmermann, Florian Hannesen, Yaoyao Fiona Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14861">Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent applications of machine learning in metal additive manufacturing (MAM) have demonstrated significant potential in addressing critical barriers to the widespread adoption of MAM technology. Recent research in this field emphasizes the importance of utilizing melt pool signatures for real-time defect prediction. While high-quality melt pool image data holds the promise of enabling precise predictions, there has been limited exploration into the utilization of cutting-edge spatiotemporal models that can harness the inherent transient and sequential characteristics of the additive manufacturing process. This research introduces and puts into practice some of the leading deep spatiotemporal learning models that can be adapted for the classification of melt pool image streams originating from various materials, systems, and applications. Specifically, it investigates two-stream networks comprising spatial and temporal streams, a recurrent spatial network, and a factorized 3D convolutional neural network. The capacity of these models to generalize when exposed to perturbations in melt pool image data is examined using data perturbation techniques grounded in real-world process scenarios. The implemented architectures demonstrate the ability to capture the spatiotemporal features of melt pool image sequences. However, among these models, only the Kinetics400 pre-trained SlowFast network, categorized as a two-stream network, exhibits robust generalization capabilities in the presence of data perturbations.
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2308.03295.pdf' target='_blank'>https://arxiv.org/pdf/2308.03295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyao Wang, Luke Chen, Mohammad Abdullah Al Faruque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03295">DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning framework addressing the distribution shift problem in noisy multi-sensor time-series data. DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions. Our evaluation on a wide range of multi-sensor time series classification tasks shows that DOMINO achieves on average 2.04% higher accuracy than state-of-the-art (SOTA) DNN-based domain generalization techniques, and delivers 16.34x faster training and 2.89x faster inference. More importantly, DOMINO performs notably better when learning from partially labeled and highly imbalanced data, providing 10.93x higher robustness against hardware noises than SOTA DNNs.
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2307.06825.pdf' target='_blank'>https://arxiv.org/pdf/2307.06825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nevin L. Zhang, Kaican Li, Han Gao, Weiyan Xie, Zhi Lin, Zhenguo Li, Luning Wang, Yongxiang Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06825">A Causal Framework to Unify Common Domain Generalization Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s). It is a fundamental problem in machine learning and has attracted much attention in recent years. A large number of approaches have been proposed. Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area. In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework. Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method? (2) Why is it expected to improve generalization to new domains theoretically? (3) How are different DG methods related to each other and what are relative advantages and limitations? By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective approaches for this critical problem in machine learning.
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2306.09005.pdf' target='_blank'>https://arxiv.org/pdf/2306.09005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Mason, Anirban Sarkar, Tomotake Sasaki, Xavier Boix
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09005">Modularity Trumps Invariance for Compositional Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By default neural networks are not robust to changes in data distribution. This has been demonstrated with simple image corruptions, such as blurring or adding noise, degrading image classification performance. Many methods have been proposed to mitigate these issues but for the most part models are evaluated on single corruptions. In reality, visual space is compositional in nature, that is, that as well as robustness to elemental corruptions, robustness to compositions of corruptions is also needed. In this work we develop a compositional image classification task where, given a few elemental corruptions, models are asked to generalize to compositions of these corruptions. That is, to achieve compositional robustness. We experimentally compare empirical risk minimization with an invariance building pairwise contrastive loss and, counter to common intuitions in domain generalization, achieve only marginal improvements in compositional robustness by encouraging invariance. To move beyond invariance, following previously proposed inductive biases that model architectures should reflect data structure, we introduce a modular architecture whose structure replicates the compositional nature of the task. We then show that this modular approach consistently achieves better compositional robustness than non-modular approaches. We additionally find empirical evidence that the degree of invariance between representations of 'in-distribution' elemental corruptions fails to correlate with robustness to 'out-of-distribution' compositions of corruptions.
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2306.04527.pdf' target='_blank'>https://arxiv.org/pdf/2306.04527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan H. Nguyen, Dinkar Juyal, Jin Li, Aaditya Prakash, Shima Nofallah, Chintan Shah, Sai Chowdary Gullapally, Limin Yu, Michael Griffin, Anand Sampat, John Abel, Justin Lee, Amaro Taylor-Weiner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04527">ContriMix: Scalable stain color augmentation for domain generalization without domain labels in digital pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differences in staining and imaging procedures can cause significant color variations in histopathology images, leading to poor generalization when deploying deep-learning models trained from a different data source. Various color augmentation methods have been proposed to generate synthetic images during training to make models more robust, eliminating the need for stain normalization during test time. Many color augmentation methods leverage domain labels to generate synthetic images. This approach causes three significant challenges to scaling such a model. Firstly, incorporating data from a new domain into deep-learning models trained on existing domain labels is not straightforward. Secondly, dependency on domain labels prevents the use of pathology images without domain labels to improve model performance. Finally, implementation of these methods becomes complicated when multiple domain labels (e.g., patient identification, medical center, etc) are associated with a single image. We introduce ContriMix, a novel domain label free stain color augmentation method based on DRIT++, a style-transfer method. Contrimix leverages sample stain color variation within a training minibatch and random mixing to extract content and attribute information from pathology images. This information can be used by a trained ContriMix model to create synthetic images to improve the performance of existing classifiers. ContriMix outperforms competing methods on the Camelyon17-WILDS dataset. Its performance is consistent across different slides in the test set while being robust to the color variation from rare substances in pathology images. We make our code and trained ContriMix models available for research use. The code for ContriMix can be found at https://gitlab.com/huutan86/contrimix
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2305.19905.pdf' target='_blank'>https://arxiv.org/pdf/2305.19905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaron Mueller, Tal Linzen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19905">How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features - as opposed to incorrect linear features - when performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth, width, and number of parameters), as well as the genre and size of the pre-training corpus, diagnosing inductive biases using two syntactic transformation tasks: question formation and passivization, both in English. We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width. We also find that pre-training on simpler language, such as child-directed speech, induces a hierarchical bias using an order-of-magnitude less data than pre-training on more typical datasets based on web text or Wikipedia; this suggests that in cognitively plausible language acquisition settings, neural language models may be more data-efficient than previously thought.
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2304.07261.pdf' target='_blank'>https://arxiv.org/pdf/2304.07261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyue Yang, Hongjing Niu, Pengfei Xia, Wei Zhang, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07261">Frequency Decomposition to Tap the Potential of Single Domain for Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG), aiming at models able to work on multiple unseen domains, is a must-have characteristic of general artificial intelligence. DG based on single source domain training data is more challenging due to the lack of comparable information to help identify domain invariant features. In this paper, it is determined that the domain invariant features could be contained in the single source domain training samples, then the task is to find proper ways to extract such domain invariant features from the single source domain samples. An assumption is made that the domain invariant features are closely related to the frequency. Then, a new method that learns through multiple frequency domains is proposed. The key idea is, dividing the frequency domain of each original image into multiple subdomains, and learning features in the subdomain by a designed two branches network. In this way, the model is enforced to learn features from more samples of the specifically limited spectrum, which increases the possibility of obtaining the domain invariant features that might have previously been defiladed by easily learned features. Extensive experimental investigation reveals that 1) frequency decomposition can help the model learn features that are difficult to learn. 2) the proposed method outperforms the state-of-the-art methods of single-source domain generalization.
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2303.05153.pdf' target='_blank'>https://arxiv.org/pdf/2303.05153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasuto Hoshi, Daisuke Miyashita, Yasuhiro Morioka, Youyang Ng, Osamu Torii, Jun Deguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05153">Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural document retrievers, including dense passage retrieval (DPR), have outperformed classical lexical-matching retrievers, such as BM25, when fine-tuned and tested on specific question-answering datasets. However, it has been shown that the existing dense retrievers do not generalize well not only out of domain but even in domain such as Wikipedia, especially when a named entity in a question is a dominant clue for retrieval. In this paper, we propose an approach toward in-domain generalization using the embeddings generated by the frozen language model trained with the entities in the domain. By not fine-tuning, we explore the possibility that the rich knowledge contained in a pretrained language model can be used for retrieval tasks. The proposed method outperforms conventional DPRs on entity-centric questions in Wikipedia domain and achieves almost comparable performance to BM25 and state-of-the-art SPAR model. We also show that the contextualized keys lead to strong improvements compared to BM25 when the entity names consist of common words. Our results demonstrate the feasibility of the zero-shot retrieval method for entity-centric questions of Wikipedia domain, where DPR has struggled to perform.
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2302.12351.pdf' target='_blank'>https://arxiv.org/pdf/2302.12351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Deng, Nidham Gazagnadou, Junyuan Hong, Mehrdad Mahdavi, Lingjuan Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12351">On the Hardness of Robustness Transfer: A Perspective from Rademacher Complexity over Symmetric Difference Hypothesis Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies demonstrated that the adversarially robust learning under $\ell_\infty$ attack is harder to generalize to different domains than standard domain adaptation. How to transfer robustness across different domains has been a key question in domain adaptation field. To investigate the fundamental difficulty behind adversarially robust domain adaptation (or robustness transfer), we propose to analyze a key complexity measure that controls the cross-domain generalization: the adversarial Rademacher complexity over {\em symmetric difference hypothesis space} $\mathcal{H} Î\mathcal{H}$. For linear models, we show that adversarial version of this complexity is always greater than the non-adversarial one, which reveals the intrinsic hardness of adversarially robust domain adaptation. We also establish upper bounds on this complexity measure. Then we extend them to the ReLU neural network class by upper bounding the adversarial Rademacher complexity in the binary classification setting. Finally, even though the robust domain adaptation is provably harder, we do find positive relation between robust learning and standard domain adaptation. We explain \emph{how adversarial training helps domain adaptation in terms of standard risk}. We believe our results initiate the study of the generalization theory of adversarially robust domain adaptation, and could shed lights on distributed adversarially robust learning from heterogeneous sources, e.g., federated learning scenario.
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2209.08473.pdf' target='_blank'>https://arxiv.org/pdf/2209.08473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanran Chen, Shitong Shao, Ziyi Wang, Zirui Shang, Jin Chen, Xiaofeng Ji, Xinxiao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.08473">Bootstrap Generalization Ability from Loss Landscape Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn a model that can generalize well on the unseen test dataset, i.e., out-of-distribution data, which has different distribution from the training dataset. To address domain generalization in computer vision, we introduce the loss landscape theory into this field. Specifically, we bootstrap the generalization ability of the deep learning model from the loss landscape perspective in four aspects, including backbone, regularization, training paradigm, and learning rate. We verify the proposed theory on the NICO++, PACS, and VLCS datasets by doing extensive ablation studies as well as visualizations. In addition, we apply this theory in the ECCV 2022 NICO Challenge1 and achieve the 3rd place without using any domain invariant methods.
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2510.03540.pdf' target='_blank'>https://arxiv.org/pdf/2510.03540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Schwonberg, Hanno Gottschalk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03540">Domain Generalization for Semantic Segmentation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization of deep neural networks to unknown domains is a major challenge despite their tremendous progress in recent years. For this reason, the dynamic area of domain generalization (DG) has emerged. In contrast to unsupervised domain adaptation, there is no access to or knowledge about the target domains, and DG methods aim to generalize across multiple different unseen target domains. Domain generalization is particularly relevant for the task semantic segmentation which is used in several areas such as biomedicine or automated driving. This survey provides a comprehensive overview of the rapidly evolving topic of domain generalized semantic segmentation. We cluster and review existing approaches and identify the paradigm shift towards foundation-model-based domain generalization. Finally, we provide an extensive performance comparison of all approaches, which highlights the significant influence of foundation models on domain generalization. This survey seeks to advance domain generalization research and inspire scientists to explore new research directions.
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2509.26631.pdf' target='_blank'>https://arxiv.org/pdf/2509.26631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26631">Learning Generalizable Shape Completion with SIM(3) Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2509.23176.pdf' target='_blank'>https://arxiv.org/pdf/2509.23176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Behraj Khan, Tahir Qasim Syed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23176">Confidence-Calibrating Regularization for Robust Brain MRI Segmentation Under Domain Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Segment Anything Model (SAM) exhibits strong zero-shot performance on natural images but suffers from domain shift and overconfidence when applied to medical volumes. We propose \textbf{CalSAM}, a lightweight adaptation framework that (i) reduces encoder sensitivity to domain shift via a \emph{Feature Fisher Information Penalty} (FIP) computed on 3D feature maps and (ii) penalizes overconfident voxel-wise errors through a \emph{Confidence Misalignment Penalty} (CMP). The combined loss, \(\mathcal{L}_{\mathrm{CalSAM}}\) fine-tunes only the mask decoder while keeping SAM's encoders frozen. On cross-center and scanner-shift evaluations, CalSAM substantially improves accuracy and calibration: e.g., on the BraTS scanner split (Siemens$\to$GE) CalSAM shows a $+7.4\%$ relative improvement in $\mathrm{DSC}$ (80.1\% vs.\ 74.6\%), a $-26.9\%$ reduction in $\mathrm{HD95}$ (4.6 mm vs.\ 6.3 mm), and a $-39.5\%$ reduction in $\mathrm{ECE}$ (5.2\% vs.\ 8.6\%). On ATLAS-C (motion corruptions), CalSAM achieves a $+5.3\%$ relative improvement in $\mathrm{DSC}$ (75.9\%) and a $-32.6\%$ reduction in $\mathrm{ECE}$ (5.8\%). Ablations show FIP and CMP contribute complementary gains ($p<0.01$), and the Fisher penalty incurs a modest $\sim$15\% training-time overhead. CalSAM therefore delivers improved domain generalization and better-calibrated uncertainty estimates for brain MRI segmentation, while retaining the computational benefits of freezing SAM's encoder.
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2509.23097.pdf' target='_blank'>https://arxiv.org/pdf/2509.23097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Su, Abdul Rehman Akbar, Usama Sajjad, Anil V. Parwani, Muhammad Khalid Khan Niazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23097">Streamline pathology foundation model by cross-magnification distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models (FM) have transformed computational pathology but remain computationally prohibitive for clinical deployment due to their massive parameter counts and high-magnification processing requirements. Here, we introduce XMAG, a lightweight FM developed through corss-magnification distillation that transfers knowledge from state-of-the-art 20x magnification teacher to an efficient 5x magnification student architecture. XMAG employs a compact backbone and operates entirely at 5x, requiring 11.3 times fewer patches per whole slide image (WSI) compared to existing approaches. Our Novel distillation framework incorporates dual-level knowledge transfer, aligning both global image representations and local spatial token mapping. We trained XMAG on 3.49 million images curated from publicly available datasets and evaluated performance across six clinically relevant histopathology analysis tasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within 1% of substantially larger foundation models while delivering 30-fold processing acceleration, reaching 8.8 WSIs per minute processing speed. Our cross-institutional validation confirmed robust generalization. Further, we developed an end-to-end training strategy to further boost our model's performance to approach the larger FMs' performance. These results establish cross-magnification distillation as a viable approach for deploying FM capabilities in resource-constrained clinical environments, potentially enabling real-time pathology AI integration.
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2509.03394.pdf' target='_blank'>https://arxiv.org/pdf/2509.03394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03394">CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cloud platforms are increasingly relied upon to host diverse, resource-intensive workloads due to their scalability, flexibility, and cost-efficiency. In multi-tenant cloud environments, virtual machines are consolidated on shared physical servers to improve resource utilization. While virtualization guarantees resource partitioning for CPU, memory, and storage, it cannot ensure performance isolation. Competition for shared resources such as last-level cache, memory bandwidth, and network interfaces often leads to severe performance degradation. Existing management techniques, including VM scheduling and resource provisioning, require accurate performance prediction to mitigate interference. However, this remains challenging in public clouds due to the black-box nature of VMs and the highly dynamic nature of workloads. To address these limitations, we propose CloudFormer, a dual-branch Transformer-based model designed to predict VM performance degradation in black-box environments. CloudFormer jointly models temporal dynamics and system-level interactions, leveraging 206 system metrics at one-second resolution across both static and dynamic scenarios. This design enables the model to capture transient interference effects and adapt to varying workload conditions without scenario-specific tuning. Complementing the methodology, we provide a fine-grained dataset that significantly expands the temporal resolution and metric diversity compared to existing benchmarks. Experimental results demonstrate that CloudFormer consistently outperforms state-of-the-art baselines across multiple evaluation metrics, achieving robust generalization across diverse and previously unseen workloads. Notably, CloudFormer attains a mean absolute error (MAE) of just 7.8%, representing a substantial improvement in predictive accuracy and outperforming existing methods at least by 28%.
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2509.02918.pdf' target='_blank'>https://arxiv.org/pdf/2509.02918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Midhat Urooj, Ayan Banerjee, Farhat Shaikh, Kuntal Thakur, Sandeep Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02918">Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization remains a critical challenge in medical imaging, where models trained on single sources often fail under real-world distribution shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy (DR) classification that integrates vision transformers with expert-guided symbolic reasoning to enable robust generalization across unseen domains. Our approach leverages clinical lesion ontologies through structured, rule-based features and retinal vessel segmentation, fusing them with deep visual representations via a confidence-weighted integration strategy. The framework addresses both single-domain generalization (SDG) and multi-domain generalization (MDG) by minimizing the KL divergence between domain embeddings, thereby enforcing alignment of high-level clinical semantics. Extensive experiments across four public datasets (APTOS, EyePACS, Messidor-1, Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in cross-domain settings and a 6% improvement over baseline ViT models. Notably, our symbolic-only model achieves a 63.67% average accuracy in MDG, while the complete neuro-symbolic integration achieves the highest accuracy compared to existing published baselines and benchmarks in challenging SDG scenarios. Ablation studies reveal that lesion-based features (84.65% accuracy) substantially outperform purely neural approaches, confirming that symbolic components act as effective regularizers beyond merely enhancing interpretability. Our findings establish neuro-symbolic integration as a promising paradigm for building clinically robust, and domain-invariant medical AI systems.
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2509.02837.pdf' target='_blank'>https://arxiv.org/pdf/2509.02837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Payel Santra, Madhusudan Ghosh, Debasis Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02837">HF-RAG: Hierarchical Fusion-based RAG with Multiple Sources and Rankers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging both labeled (input-output associations) and unlabeled data (wider contextual grounding) may provide complementary benefits in retrieval augmented generation (RAG). However, effectively combining evidence from these heterogeneous sources is challenging as the respective similarity scores are not inter-comparable. Additionally, aggregating beliefs from the outputs of multiple rankers can improve the effectiveness of RAG. Our proposed method first aggregates the top-documents from a number of IR models using a standard rank fusion technique for each source (labeled and unlabeled). Next, we standardize the retrieval score distributions within each source by applying z-score transformation before merging the top-retrieved documents from the two sources. We evaluate our approach on the fact verification task, demonstrating that it consistently improves over the best-performing individual ranker or source and also shows better out-of-domain generalization.
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2509.02473.pdf' target='_blank'>https://arxiv.org/pdf/2509.02473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziting Wang, Shize Zhang, Haitao Yuan, Jinwei Zhu, Shifu Li, Wei Dong, Gao Cong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02473">FDABench: A Benchmark for Data Agents on Analytical Queries over Heterogeneous Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for data-driven decision-making has created an urgent need for data agents that can integrate structured and unstructured data for analysis. While data agents show promise for enabling users to perform complex analytics tasks, this field still suffers from three critical limitations: first, comprehensive data agent benchmarks remain absent due to the difficulty of designing test cases that evaluate agents' abilities across multi-source analytical tasks; second, constructing reliable test cases that combine structured and unstructured data remains costly and prohibitively complex; third, existing benchmarks exhibit limited adaptability and generalizability, resulting in narrow evaluation scope. To address these challenges, we present FDABench, the first data agent benchmark specifically designed for evaluating agents in multi-source data analytical scenarios. Our contributions include: (i) we construct a standardized benchmark with 2,007 diverse tasks across different data sources, domains, difficulty levels, and task types to comprehensively evaluate data agent performance; (ii) we design an agent-expert collaboration framework ensuring reliable and efficient benchmark construction over heterogeneous data; (iii) we equip FDABench with robust generalization capabilities across diverse target systems and frameworks. We use FDABench to evaluate various data agent systems, revealing that each system exhibits distinct advantages and limitations regarding response quality, accuracy, latency, and token cost.
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2509.01554.pdf' target='_blank'>https://arxiv.org/pdf/2509.01554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Chih Lee, Zelong Liu, Hamza Ahmed, Spencer Kim, Sean Huver, Vishwesh Nath, Zahi A. Fayad, Timothy Deyer, Xueyan Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01554">Unified Supervision For Vision-Language Modeling in 3D Computed Tomography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purpose vision-language models (VLMs) have emerged as promising tools in radiology, offering zero-shot capabilities that mitigate the need for large labeled datasets. However, in high-stakes domains like diagnostic radiology, these models often lack the discriminative precision required for reliable clinical use. This challenge is compounded by the scarcity and heterogeneity of publicly available volumetric CT datasets, which vary widely in annotation formats and granularity. To address these limitations, we introduce Uniferum, a volumetric VLM that unifies diverse supervision signals, encoded in classification labels and segmentation masks, into a single training framework. By harmonizing three public 3D CT datasets with distinct annotations, Uniferum achieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark by 7% compared to CLIP-based and conventional multi-label convolutional models. The model demonstrates robust out-of-distribution generalization, with observed evidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT datasets. Our results highlight the effectiveness of integrating heterogeneous annotations and body segmentation to enhance model performance, setting a new direction for clinically reliable, data-efficient VLMs in 3D medical imaging.
<div id='section'>Paperid: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2508.14079.pdf' target='_blank'>https://arxiv.org/pdf/2508.14079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Heuillet, Rishika Bhagwatkar, Jonas NgnawÃ©, Yann Pequignot, Alexandre Larouche, Christian GagnÃ©, Irina Rish, Ola Ahmad, Audrey Durand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14079">A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models operating in the image domain are vulnerable to small input perturbations. For years, robustness to such perturbations was pursued by training models from scratch (i.e., with random initializations) using specialized loss objectives. Recently, robust fine-tuning has emerged as a more efficient alternative: instead of training from scratch, pretrained models are adapted to maximize predictive performance and robustness. To conduct robust fine-tuning, practitioners design an optimization strategy that includes the model update protocol (e.g., full or partial) and the specialized loss objective. Additional design choices include the architecture type and size, and the pretrained representation. These design choices affect robust generalization, which is the model's ability to maintain performance when exposed to new and unseen perturbations at test time. Understanding how these design choices influence generalization remains an open question with significant practical implications. In response, we present an empirical study spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3 adaptation protocols, yielding 1,440 training configurations and 7,200 robustness measurements across five perturbation types. To our knowledge, this is the most diverse and comprehensive benchmark of robust fine-tuning to date. While attention-based architectures and robust pretrained representations are increasingly popular, we find that convolutional neural networks pretrained in a supervised manner on large datasets often perform best. Our analysis both confirms and challenges prior design assumptions, highlighting promising research directions and offering practical guidance.
<div id='section'>Paperid: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2508.04594.pdf' target='_blank'>https://arxiv.org/pdf/2508.04594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Sun, Qi Feng, Lehao Lin, Chris Ding, Jicong Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04594">GraphProp: Training the Graph Foundation Models using Graph Properties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on training graph foundation models (GFMs) that have strong generalization ability in graph-level tasks such as graph classification. Effective GFM training requires capturing information consistent across different domains. We discover that graph structures provide more consistent cross-domain information compared to node features and graph labels. However, traditional GFMs primarily focus on transferring node features from various domains into a unified representation space but often lack structural cross-domain generalization. To address this, we introduce GraphProp, which emphasizes structural generalization. The training process of GraphProp consists of two main phases. First, we train a structural GFM by predicting graph invariants. Since graph invariants are properties of graphs that depend only on the abstract structure, not on particular labellings or drawings of the graph, this structural GFM has a strong ability to capture the abstract structural information and provide discriminative graph representations comparable across diverse domains. In the second phase, we use the representations given by the structural GFM as positional encodings to train a comprehensive GFM. This phase utilizes domain-specific node attributes and graph labels to further improve cross-domain node feature generalization. Our experiments demonstrate that GraphProp significantly outperforms the competitors in supervised learning and few-shot learning, especially in handling graphs without node attributes.
<div id='section'>Paperid: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2507.13966.pdf' target='_blank'>https://arxiv.org/pdf/2507.13966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhishma Dedhia, Yuval Kansal, Niraj K. Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13966">Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. We present a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. We fine-tune language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, we validate our approach in medicine, where reliable KGs exist. Using a medical KG, we curate 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. We fine-tune the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Our experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, we envision a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents.
<div id='section'>Paperid: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2507.11955.pdf' target='_blank'>https://arxiv.org/pdf/2507.11955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Zhengyu Zhang, Muxin Liao, Shishun Tian, Wenbin Zou, Lu Zhang, Chen Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11955">Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable semantic segmentation aims to perform well on unseen target domains, a critical challenge due to real-world applications requiring high generalizability. Class-wise prototypes, representing class centroids, serve as domain-invariant cues that benefit generalization due to their stability and semantic consistency. However, this approach faces three challenges. First, existing methods often adopt coarse prototypical alignment strategies, which may hinder performance. Second, naive prototypes computed by averaging source batch features are prone to overfitting and may be negatively affected by unrelated source data. Third, most methods treat all source samples equally, ignoring the fact that different features have varying adaptation difficulties. To address these limitations, we propose a novel framework for generalizable semantic segmentation: Prototypical Progressive Alignment and Reweighting (PPAR), leveraging the strong generalization ability of the CLIP model. Specifically, we define two prototypes: the Original Text Prototype (OTP) and Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for alignment. We then introduce a progressive alignment strategy that aligns features in an easy-to-difficult manner, reducing domain gaps gradually. Furthermore, we propose a prototypical reweighting mechanism that estimates the reliability of source data and adjusts its contribution, mitigating the effect of irrelevant or harmful features (i.e., reducing negative transfer). We also provide a theoretical analysis showing the alignment between our method and domain generalization theory. Extensive experiments across multiple benchmarks demonstrate that PPAR achieves state-of-the-art performance, validating its effectiveness.
<div id='section'>Paperid: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2506.23088.pdf' target='_blank'>https://arxiv.org/pdf/2506.23088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Zhou, Jiayu Tang, Xiaoyan Xiao, Yueyao Lin, Linkai Liu, Zipeng Guo, Hao Fei, Xiaobo Xia, Chao Gou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23088">Where, What, Why: Towards Explainable Driver Attention Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling task-driven attention in driving is a fundamental challenge for both autonomous vehicles and cognitive science. Existing methods primarily predict where drivers look by generating spatial heatmaps, but fail to capture the cognitive motivations behind attention allocation in specific contexts, which limits deeper understanding of attention mechanisms. To bridge this gap, we introduce Explainable Driver Attention Prediction, a novel task paradigm that jointly predicts spatial attention regions (where), parses attended semantics (what), and provides cognitive reasoning for attention allocation (why). To support this, we present W3DA, the first large-scale explainable driver attention dataset. It enriches existing benchmarks with detailed semantic and causal annotations across diverse driving scenarios, including normal conditions, safety-critical situations, and traffic accidents. We further propose LLada, a Large Language model-driven framework for driver attention prediction, which unifies pixel modeling, semantic parsing, and cognitive reasoning within an end-to-end architecture. Extensive experiments demonstrate the effectiveness of LLada, exhibiting robust generalization across datasets and driving conditions. This work serves as a key step toward a deeper understanding of driver attention mechanisms, with significant implications for autonomous driving, intelligent driver training, and human-computer interaction.
<div id='section'>Paperid: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2506.08240.pdf' target='_blank'>https://arxiv.org/pdf/2506.08240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongkyu Cho, Rumi Chunara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08240">Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is a promising tool for enhancing out-of-distribution generalization, where the key is to produce diverse, challenging variations of the source domain via costly targeted augmentations that maximize its generalization effect. Conversely, random augmentation is inexpensive but is deemed suboptimal due to its limited effect. In this paper, we revisit random augmentation and explore methods to address its shortcomings. We show that the stochastic nature of random augmentation can produce a set of colliding augmentations that distorts the learned features, similar to catastrophic forgetting. We propose a simple solution that improves the generalization effect of random augmentation by addressing forgetting, which displays strong generalization performance across various single source domain generalization (sDG) benchmarks.
<div id='section'>Paperid: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2505.24346.pdf' target='_blank'>https://arxiv.org/pdf/2505.24346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wang, Zhi Gao, Boxuan Yu, Zirui Dai, Yuxiang Song, Qingyuan Lu, Jin Chen, Xinxiao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24346">VUDG: A Dataset for Video Understanding Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding has made remarkable progress in recent years, largely driven by advances in deep models and the availability of large-scale annotated datasets. However, existing works typically ignore the inherent domain shifts encountered in real-world video applications, leaving domain generalization (DG) in video understanding underexplored. Hence, we propose Video Understanding Domain Generalization (VUDG), a novel dataset designed specifically for evaluating the DG performance in video understanding. VUDG contains videos from 11 distinct domains that cover three types of domain shifts, and maintains semantic similarity across different domains to ensure fair and meaningful evaluation. We propose a multi-expert progressive annotation framework to annotate each video with both multiple-choice and open-ended question-answer pairs. Extensive experiments on 9 representative large video-language models (LVLMs) and several traditional video question answering methods show that most models (including state-of-the-art LVLMs) suffer performance degradation under domain shifts. These results highlight the challenges posed by VUDG and the difference in the robustness of current models to data distribution shifts. We believe VUDG provides a valuable resource for prompting future research in domain generalization video understanding.
<div id='section'>Paperid: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2505.21573.pdf' target='_blank'>https://arxiv.org/pdf/2505.21573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Wan, Rui Zhang, Hao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21573">Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial differential equations (PDEs) govern the spatiotemporal evolution of various physical systems. Classical numerical solvers, while accurate, require fine discretization and full knowledge of the governing PDEs, limiting their applicability when the physics is unknown or fast inference is required. Data-driven neural PDE solvers alleviate these constraints by learning from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes, restricting their ability to handle unknown or globally coupled systems. In this work, we propose the Spectral-inspired Neural Operator (SINO), a novel framework that learns PDE operators from limited trajectories (as few as 2-5), without any known PDE terms. SINO operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. To model nonlinear physical interactions, we design a nonlinear operator block that includes a $Î $-Block with low-pass filtering to prevent aliasing. Finally, we introduce an operator distillation technique to distill the trained model for efficient inference. SINO achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. To our knowledge, SINO is the first physics-aware method capable of accurately simulating globally coupled systems (e.g., the Navier-Stokes equations) from limited data without any explicit PDE terms.
<div id='section'>Paperid: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2505.18035.pdf' target='_blank'>https://arxiv.org/pdf/2505.18035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naseem Khan, Tuan Nguyen, Amine Bermak, Issa Khalil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18035">CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of sophisticated AI-generated deepfakes poses critical challenges for digital media authentication and societal security. While existing detection methods perform well within specific generative domains, they exhibit significant performance degradation when applied to manipulations produced by unseen architectures--a fundamental limitation as generative technologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal Embeddings), a framework that dynamically integrates visual, textual, and frequency-domain features through a multi-head cross-attention mechanism to establish robust cross-domain generalization. Extensive experiments demonstrate CAMME's superiority over state-of-the-art methods, yielding improvements of 12.56% on natural scenes and 13.25% on facial deepfakes. The framework demonstrates exceptional resilience, maintaining (over 91%) accuracy under natural image perturbations and achieving 89.01% and 96.14% accuracy against PGD and FGSM adversarial attacks, respectively. Our findings validate that integrating complementary modalities through cross-attention enables more effective decision boundary realignment for reliable deepfake detection across heterogeneous generative architectures.
<div id='section'>Paperid: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2505.16253.pdf' target='_blank'>https://arxiv.org/pdf/2505.16253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Preeti Mehta, Aman Sagar, Suchi Kumari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16253">Swin Transformer for Robust CGI Images Detection: Intra- and Inter-Dataset Analysis across Multiple Color Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study aims to address the growing challenge of distinguishing computer-generated imagery (CGI) from authentic digital images across three different color spaces; RGB, YCbCr, and HSV. Given the limitations of existing classification methods in handling the complexity and variability of CGI, this research proposes a Swin Transformer based model for accurate differentiation between natural and synthetic images. The proposed model leverages the Swin Transformer's hierarchical architecture to capture local and global features for distinguishing CGI from natural images. Its performance was assessed through intra- and inter-dataset testing across three datasets: CiFAKE, JSSSTU, and Columbia. The model was evaluated individually on each dataset (D1, D2, D3) and on the combined datasets (D1+D2+D3) to test its robustness and domain generalization. To address dataset imbalance, data augmentation techniques were applied. Additionally, t-SNE visualization was used to demonstrate the feature separability achieved by the Swin Transformer across the selected color spaces. The model's performance was tested across all color schemes, with the RGB color scheme yielding the highest accuracy for each dataset. As a result, RGB was selected for domain generalization analysis and compared with other CNN-based models, VGG-19 and ResNet-50. The comparative results demonstrate the proposed model's effectiveness in detecting CGI, highlighting its robustness and reliability in both intra-dataset and inter-dataset evaluations. The findings of this study highlight the Swin Transformer model's potential as an advanced tool for digital image forensics, particularly in distinguishing CGI from natural images. The model's strong performance indicates its capability for domain generalization, making it a valuable asset in scenarios requiring precise and reliable image classification.
<div id='section'>Paperid: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2505.15422.pdf' target='_blank'>https://arxiv.org/pdf/2505.15422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nudrat Habib, Tosin Adewumi, Marcus Liwicki, Elisa Barney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15422">Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Authorship analysis plays an important role in diverse domains, including forensic linguistics, academia, cybersecurity, and digital content authentication. This paper presents a systematic literature review on two key sub-tasks of authorship analysis; Author Attribution and Author Verification. The review explores SOTA methodologies, ranging from traditional ML approaches to DL models and LLMs, highlighting their evolution, strengths, and limitations, based on studies conducted from 2015 to 2024. Key contributions include a comprehensive analysis of methods, techniques, their corresponding feature extraction techniques, datasets used, and emerging challenges in authorship analysis. The study highlights critical research gaps, particularly in low-resource language processing, multilingual adaptation, cross-domain generalization, and AI-generated text detection. This review aims to help researchers by giving an overview of the latest trends and challenges in authorship analysis. It also points out possible areas for future study. The goal is to support the development of better, more reliable, and accurate authorship analysis system in diverse textual domain.
<div id='section'>Paperid: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2505.07050.pdf' target='_blank'>https://arxiv.org/pdf/2505.07050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binbin Wei, Yuhang Zhang, Shishun Tian, Muxin Liao, Wei Li, Wenbin Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07050">Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised Domain Adaptation (UDA) aims to align source and target domain distributions to close the domain gap, but still struggles with obtaining the target data. Fortunately, Domain Generalization (DG) excels without the need for any target data. Recent works expose that depth maps contribute to improved generalized performance in the UDA tasks, but they ignore the noise and holes in depth maps due to device and environmental factors, failing to sufficiently and effectively learn domain-invariant representation. Although high-sensitivity region suppression has shown promising results in learning domain-invariant features, existing methods cannot be directly applicable to depth maps due to their unique characteristics. Hence, we propose a novel framework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal stylization flow (DSSS), focusing on learning domain-invariant features from depth maps for the DG semantic segmentation. Specifically, we propose the RGB-D inter-modal stylization flow to generate stylized depth maps for sensitivity detection, cleverly utilizing RGB information as the stylization source. Then, a class-wise soft spatial sensitivity suppression is designed to identify and emphasize non-sensitive depth features that contain more domain-invariant information. Furthermore, an RGB-D soft alignment loss is proposed to ensure that the stylized depth maps only align part of the RGB features while still retaining the unique depth information. To our best knowledge, our DSSS framework is the first work to integrate RGB and Depth information in the multi-class DG semantic segmentation task. Extensive experiments over multiple backbone networks show that our framework achieves remarkable performance improvement.
<div id='section'>Paperid: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2504.14810.pdf' target='_blank'>https://arxiv.org/pdf/2504.14810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jucheng Hu, Surong Yang, Lijun Wu, Dongzhan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14810">DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ad-hoc instruction fine-tuning of large language models (LLMs) is widely adopted for domain-specific adaptation. While domain-specific supervised fine-tuning (SFT) is effective and efficient, it often weakens cross-domain generalization and struggles with noisy training data. To address these challenges, we propose DONOD, a lightweight model-intrinsic data pruning method. Our approach evaluates data using two model-parameter-based metrics: Delta of Norm (DON), which captures the cumulative influence on model weights, and Norm of Delta (NOD), which quantifies weight instability. Moreover, by employing the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) algorithm, we effectively filter noisy, unlearnable, and generalization-harming samples without relying on auxiliary models during the SFT process. Experiments on mathematical tasks demonstrate that data selected by DONOD achieves superior fine-tuning efficiency and improved robustness against noisy data. By filtering out 70% of the whole dataset, we improve target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile, our selected data present superior cross-architecture generalization. Data pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD demonstrates comparable or superior performance while remaining dataset-agnostic, enabling broader applicability. Code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2504.06637.pdf' target='_blank'>https://arxiv.org/pdf/2504.06637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Ma, Haihong E., Junpeng Ding, Jun Zhang, Ziyan Ma, Huang Qing, Bofei Gao, Liang Chen, Meina Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06637">SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex Multimodal Reasoning in Academic Areas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate impressive problem-solving skills in many tasks and domains. However, their ability to reason with complex images in academic domains has not been systematically investigated. To bridge this gap, we present SCI-Reason, a dataset for complex multimodel reasoning in academic areas. SCI-Reason aims to test and improve the reasoning ability of large multimodal models using real complex images in academic domains. The dataset contains 12,066 images and 12,626 question-answer pairs extracted from PubMed, divided into training, validation and test splits. Each question-answer pair also contains an accurate and efficient inference chain as a guide to improving the inference properties of the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8 well-known models. The best performing model, Claude-3.7-Sonnet, only achieved an accuracy of 55.19%. Error analysis shows that more than half of the model failures are due to breakdowns in multi-step inference chains rather than errors in primary visual feature extraction. This finding underscores the inherent limitations in reasoning capabilities exhibited by current multimodal models when processing complex image analysis tasks within authentic academic contexts. Experiments on open-source models show that SCI-Reason not only enhances reasoning ability but also demonstrates cross-domain generalization in VQA tasks. We also explore future applications of model inference capabilities in this domain, highlighting its potential for future research.
<div id='section'>Paperid: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2504.04981.pdf' target='_blank'>https://arxiv.org/pdf/2504.04981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sohyun Lee, Nayeong Kim, Juwon Kang, Seong Joon Oh, Suha Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04981">TestDG: Test-time Domain Generalization for Continual Test-time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies continual test-time adaptation (CTTA), the task of adapting a model to constantly changing unseen domains in testing while preserving previously learned knowledge. Existing CTTA methods mostly focus on adaptation to the current test domain only, overlooking generalization to arbitrary test domains a model may face in the future. To tackle this limitation, we present a novel online test-time domain generalization framework for CTTA, dubbed TestDG. TestDG aims to learn features invariant to both current and previous test domains on the fly during testing, improving the potential for effective generalization to future domains. To this end, we propose a new model architecture and a test-time adaptation strategy dedicated to learning domain-invariant features, along with a new data structure and optimization algorithm for effectively managing information from previous test domains. TestDG achieved state of the art on four public CTTA benchmarks. Moreover, it showed superior generalization to unseen test domains.
<div id='section'>Paperid: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2504.03064.pdf' target='_blank'>https://arxiv.org/pdf/2504.03064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yan, Yuhong Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03064">Context-Aware Self-Adaptation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims at developing suitable learning algorithms in source training domains such that the model learned can generalize well on a different unseen testing domain. We present a novel two-stage approach called Context-Aware Self-Adaptation (CASA) for domain generalization. CASA simulates an approximate meta-generalization scenario and incorporates a self-adaptation module to adjust pre-trained meta source models to the meta-target domains while maintaining their predictive capability on the meta-source domains. The core concept of self-adaptation involves leveraging contextual information, such as the mean of mini-batch features, as domain knowledge to automatically adapt a model trained in the first stage to new contexts in the second stage. Lastly, we utilize an ensemble of multiple meta-source models to perform inference on the testing domain. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on standard benchmarks.
<div id='section'>Paperid: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2504.00839.pdf' target='_blank'>https://arxiv.org/pdf/2504.00839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00839">Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame.
<div id='section'>Paperid: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2503.14526.pdf' target='_blank'>https://arxiv.org/pdf/2503.14526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Fang, Yue Yang, Xinghao Zhu, Kaiyuan Zheng, Gedas Bertasius, Daniel Szafir, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14526">ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models present a promising paradigm by training policies directly on real robot datasets like Open X-Embodiment. However, the high cost of real-world data collection hinders further data scaling, thereby restricting the generalizability of VLAs. In this paper, we introduce ReBot, a novel real-to-sim-to-real approach for scaling real robot datasets and adapting VLA models to target domains, which is the last-mile deployment challenge in robot manipulation. Specifically, ReBot replays real-world robot trajectories in simulation to diversify manipulated objects (real-to-sim), and integrates the simulated movements with inpainted real-world background to synthesize physically realistic and temporally consistent robot videos (sim-to-real). Our approach has several advantages: 1) it enjoys the benefit of real data to minimize the sim-to-real gap; 2) it leverages the scalability of simulation; and 3) it can generalize a pretrained VLA to a target domain with fully automated data pipelines. Extensive experiments in both simulation and real-world environments show that ReBot significantly enhances the performance and robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot improved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and out-of-domain generalization by 19.9% and 9.4%, respectively. For real-world evaluation with a Franka robot, ReBot increased the success rates of Octo by 17% and OpenVLA by 20%. More information can be found at: https://yuffish.github.io/rebot/
<div id='section'>Paperid: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2503.11774.pdf' target='_blank'>https://arxiv.org/pdf/2503.11774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Lian, Shangyu Li, Qixuan Huang, Zijian Huang, Haifei Liu, Jianan Qiu, Puyu Yang, Laifa Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11774">UBMF: Uncertainty-Aware Bayesian Meta-Learning Framework for Fault Diagnosis with Imbalanced Industrial Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fault diagnosis of mechanical equipment involves data collection, feature extraction, and pattern recognition but is often hindered by the imbalanced nature of industrial data, introducing significant uncertainty and reducing diagnostic reliability. To address these challenges, this study proposes the Uncertainty-Aware Bayesian Meta-Learning Framework (UBMF), which integrates four key modules: data perturbation injection for enhancing feature robustness, cross-task self-supervised feature extraction for improving transferability, uncertainty-based sample filtering for robust out-of-domain generalization, and Bayesian meta-knowledge integration for fine-grained classification. Experimental results on ten open-source datasets under various imbalanced conditions, including cross-task, small-sample, and unseen-sample scenarios, demonstrate the superiority of UBMF, achieving an average improvement of 42.22% across ten Any-way 1-5-shot diagnostic tasks. This integrated framework effectively enhances diagnostic accuracy, generalization, and adaptability, providing a reliable solution for complex industrial fault diagnosis.
<div id='section'>Paperid: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2503.00470.pdf' target='_blank'>https://arxiv.org/pdf/2503.00470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junqi He, Yujie Zhang, Jialu Wang, Tao Wang, Pan Zhang, Chengjie Cai, Jinxing Yang, Xiao Lin, Xiaohui Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00470">Rapid morphology characterization of two-dimensional TMDs and lateral heterostructures based on deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two-dimensional (2D) materials and heterostructures exhibit unique physical properties, necessitating efficient and accurate characterization methods. Leveraging advancements in artificial intelligence, we introduce a deep learning-based method for efficiently characterizing heterostructures and 2D materials, specifically MoS2-MoSe2 lateral heterostructures and MoS2 flakes with varying shapes and thicknesses. By utilizing YOLO models, we achieve an accuracy rate of over 94.67% in identifying these materials. Additionally, we explore the application of transfer learning across different materials, which further enhances model performance. This model exhibits robust generalization and anti-interference ability, ensuring reliable results in diverse scenarios. To facilitate practical use, we have developed an application that enables real-time analysis directly from optical microscope images, making the process significantly faster and more cost-effective than traditional methods. This deep learning-driven approach represents a promising tool for the rapid and accurate characterization of 2D materials, opening new avenues for research and development in material science.
<div id='section'>Paperid: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2502.19173.pdf' target='_blank'>https://arxiv.org/pdf/2502.19173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gregory W. Kyro, Tianyin Qiu, Victor S. Batista
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19173">A Model-Centric Review of Deep Learning for Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has transformed protein design, enabling accurate structure prediction, sequence optimization, and de novo protein generation. Advances in single-chain protein structure prediction via AlphaFold2, RoseTTAFold, ESMFold, and others have achieved near-experimental accuracy, inspiring successive work extended to biomolecular complexes via AlphaFold Multimer, RoseTTAFold All-Atom, AlphaFold 3, Chai-1, Boltz-1 and others. Generative models such as ProtGPT2, ProteinMPNN, and RFdiffusion have enabled sequence and backbone design beyond natural evolution-based limitations. More recently, joint sequence-structure co-design models, including ESM3, have integrated both modalities into a unified framework, resulting in improved designability. Despite these advances, challenges still exist pertaining to modeling sequence-structure-function relationships and ensuring robust generalization beyond the regions of protein space spanned by the training data. Future advances will likely focus on joint sequence-structure-function co-design frameworks that are able to model the fitness landscape more effectively than models that treat these modalities independently. Current capabilities, coupled with the dizzying rate of progress, suggest that the field will soon enable rapid, rational design of proteins with tailored structures and functions that transcend the limitations imposed by natural evolution. In this review, we discuss the current capabilities of deep learning methods for protein design, focusing on some of the most revolutionary and capable models with respect to their functionality and the applications that they enable, leading up to the current challenges of the field and the optimal path forward.
<div id='section'>Paperid: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2502.12859.pdf' target='_blank'>https://arxiv.org/pdf/2502.12859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxing Wei, Yao Shu, Mingwen Ou, Ying Tiffany He, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12859">PAFT: Prompt-Agnostic Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning large language models (LLMs) often causes overfitting to specific prompt wording, where minor phrasing variations drastically reduce performance. To address this, we propose Prompt-Agnostic Fine-Tuning (PAFT), a method that enhances robustness through dynamic prompt variation during training. PAFT first generates diverse synthetic prompts, then continuously samples from this set to construct training instances, forcing models to learn fundamental task principles rather than surface-level patterns. Across systematic evaluations using both supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT), PAFT demonstrates substantially improved prompt robustness, achieving 7% higher generalization accuracy on unseen prompts than standard methods. In addition to enhanced robustness, PAFT consistently yields superior overall performance on established benchmarks for question answering, mathematical reasoning, and tool use. Notably, models trained with PAFT attain 3.2 faster inference speeds due to reduced prompt sensitivity. Ablation studies further validate effectiveness of PAFT, while theoretical analysis reveals that PAFT can effectively enhance the cross-domain generalization ability of LLM.
<div id='section'>Paperid: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2502.07951.pdf' target='_blank'>https://arxiv.org/pdf/2502.07951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Tan, Jiacheng Wang, Liansheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07951">Federated Self-supervised Domain Generalization for Label-efficient Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Employing self-supervised learning (SSL) methodologies assumes par-amount significance in handling unlabeled polyp datasets when building deep learning-based automatic polyp segmentation models. However, the intricate privacy dynamics surrounding medical data often preclude seamless data sharing among disparate medical centers. Federated learning (FL) emerges as a formidable solution to this privacy conundrum, yet within the realm of FL, optimizing model generalization stands as a pressing imperative. Robust generalization capabilities are imperative to ensure the model's efficacy across diverse geographical domains post-training on localized client datasets. In this paper, a Federated self-supervised Domain Generalization method is proposed to enhance the generalization capacity of federated and Label-efficient intestinal polyp segmentation, named LFDG. Based on a classical SSL method, DropPos, LFDG proposes an adversarial learning-based data augmentation method (SSADA) to enhance the data diversity. LFDG further proposes a relaxation module based on Source-reconstruction and Augmentation-masking (SRAM) to maintain stability in feature learning. We have validated LFDG on polyp images from six medical centers. The performance of our method achieves 3.80% and 3.92% better than the baseline and other recent FL methods and SSL methods, respectively.
<div id='section'>Paperid: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2502.07281.pdf' target='_blank'>https://arxiv.org/pdf/2502.07281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taro Makino, Ji Won Park, Natasa Tagasovska, Takamasa Kudo, Paula Coelho, Jan-Christian Huetter, Heming Yao, Burkhard Hoeckendorf, Ana Carolina Leote, Stephen Ra, David Richmond, Kyunghyun Cho, Aviv Regev, Romain Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07281">Supervised Contrastive Block Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world datasets often combine data collected under different experimental conditions. This yields larger datasets, but also introduces spurious correlations that make it difficult to model the phenomena of interest. We address this by learning two embeddings to independently represent the phenomena of interest and the spurious correlations. The embedding representing the phenomena of interest is correlated with the target variable $y$, and is invariant to the environment variable $e$. In contrast, the embedding representing the spurious correlations is correlated with $e$. The invariance to $e$ is difficult to achieve on real-world datasets. Our primary contribution is an algorithm called Supervised Contrastive Block Disentanglement (SCBD) that effectively enforces this invariance. It is based purely on Supervised Contrastive Learning, and applies to real-world data better than existing approaches. We empirically validate SCBD on two challenging problems. The first problem is domain generalization, where we achieve strong performance on a synthetic dataset, as well as on Camelyon17-WILDS. We introduce a single hyperparameter $Î±$ to control the degree of invariance to $e$. When we increase $Î±$ to strengthen the degree of invariance, out-of-distribution performance improves at the expense of in-distribution performance. The second problem is batch correction, in which we apply SCBD to preserve biological signal and remove inter-well batch effects when modeling single-cell perturbations from 26 million Optical Pooled Screening images.
<div id='section'>Paperid: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2502.02717.pdf' target='_blank'>https://arxiv.org/pdf/2502.02717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristobal Donoso-Oliva, Ignacio Becker, Pavlos Protopapas, Guillermo Cabrera-Vives, Martina CÃ¡diz-Leyton, Daniel Moreno-Cartagena
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02717">Astromer 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundational models have emerged as a powerful paradigm in deep learning field, leveraging their capacity to learn robust representations from large-scale datasets and effectively to diverse downstream applications such as classification. In this paper, we present Astromer 2 a foundational model specifically designed for extracting light curve embeddings. We introduce Astromer 2 as an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model's representations. Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings. Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer's attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.
<div id='section'>Paperid: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2501.17888.pdf' target='_blank'>https://arxiv.org/pdf/2501.17888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Chen, Yong Zu, Zhixi Feng, Shuyuan Yang, Mengchang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17888">RadioLLM: Introducing Large Language Model into Cognitive Radio via Hybrid Prompt and Token Reprogrammings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing scarcity of spectrum resources and rapid proliferation of wireless devices make efficient radio network management critical. While deep learning-enhanced Cognitive Radio Technology (CRT) provides promising solutions for tasks such as radio signal classification (RSC), denoising, and spectrum allocation, existing DL-based CRT frameworks are typically task-specific and lack scalability in diverse real-world applications. This limitation naturally leads to the exploration of Large Language Models (LLMs), whose exceptional cross-domain generalization capabilities offer new potential for advancing CRT. To bridge this gap, we propose RadioLLM, a novel framework that integrates Hybrid Prompt and Token Reprogramming (HPTR) for combining radio signal features with expert knowledge, and a Frequency-Attuned Fusion (FAF) module for enhanced high-frequency feature modeling. Extensive evaluations on multiple benchmark datasets demonstrate that RadioLLM achieves superior performance compared to existing baselines in the majority of testing scenarios.
<div id='section'>Paperid: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2501.07114.pdf' target='_blank'>https://arxiv.org/pdf/2501.07114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhong Peng, Yishi Xu, Gerong Wang, Wenchao Chen, Bo Chen, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07114">Duplex: Dual Prototype Learning for Compositional Zero-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compositional Zero-Shot Learning (CZSL) aims to enable models to recognize novel compositions of visual states and objects that were absent during training. Existing methods predominantly focus on learning semantic representations of seen compositions but often fail to disentangle the independent features of states and objects in images, thereby limiting their ability to generalize to unseen compositions. To address this challenge, we propose Duplex, a novel dual-prototype learning method that integrates semantic and visual prototypes through a carefully designed dual-branch architecture, enabling effective representation learning for compositional tasks. Duplex utilizes a Graph Neural Network (GNN) to adaptively update visual prototypes, capturing complex interactions between states and objects. Additionally, it leverages the strong visual-semantic alignment of pre-trained Vision-Language Models (VLMs) and employs a multi-path architecture combined with prompt engineering to align image and text representations, ensuring robust generalization. Extensive experiments on three benchmark datasets demonstrate that Duplex outperforms state-of-the-art methods in both closed-world and open-world settings.
<div id='section'>Paperid: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2412.08946.pdf' target='_blank'>https://arxiv.org/pdf/2412.08946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08946">MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, LoRA has emerged as a crucial technique for fine-tuning large pre-trained models, yet its performance in multi-task learning scenarios often falls short. In contrast, the MoE architecture presents a natural solution to this issue. However, it introduces challenges such as mutual interference of data across multiple domains and knowledge forgetting of various tasks. Additionally, MoE significantly increases the number of parameters, posing a computational cost challenge. Therefore, in this paper, we propose MoSLD, a mixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these challenges by sharing the upper projection matrix in LoRA among different experts, encouraging the model to learn general knowledge across tasks, while still allowing the lower projection matrix to focus on the unique features of each task. The application of dropout alleviates the imbalanced update of parameter matrix and mitigates parameter overfitting in LoRA. Extensive experiments demonstrate that our model exhibits excellent performance in both single-task and multi-task scenarios, with robust out-of-domain generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2412.02980.pdf' target='_blank'>https://arxiv.org/pdf/2412.02980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Havrilla, Andrew Dai, Laura O'Mahony, Koen Oostermeijer, Vera Zisler, Alon Albalak, Fabrizio Milo, Sharath Chandra Raparthy, Kanishk Gandhi, Baber Abbasi, Duy Phung, Maia Iyer, Dakota Mahan, Chase Blagden, Srishti Gureja, Mohammed Hamdy, Wen-Ding Li, Giovanni Paolini, Pawan Sasanka Ammanamanchi, Elliot Meyerson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02980">Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.
<div id='section'>Paperid: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2412.00767.pdf' target='_blank'>https://arxiv.org/pdf/2412.00767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhai Zhuo, Zheng Wang, Yuqian Fu, Tianwen Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00767">Prompt as Free Lunch: Enhancing Diversity in Source-Free Cross-domain Few-shot Learning through Semantic-Guided Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The source-free cross-domain few-shot learning (CD-FSL) task aims to transfer pretrained models to target domains utilizing minimal samples, eliminating the need for source domain data. Addressing this issue requires models to have robust generalization abilities and strong feature representation, aligning with the characteristics of large-scale pretrained models. However, large-scale models tend to lose representational ability in cross-domain scenarios due to limited sample diversity. \zlh{Given the abundant diversity provided by semantic modality, this paper leverages textual modality to enhance training sample diversity with CLP model}, meanwhile improving model transfer efficiency. Specifically, we propose the SeGD-VPT framework, which is divided into two phases. The first step aims to increase feature diversity by adding diversity prompts to each support sample, thereby generating varying input and enhancing sample diversity. Furthermore, we use diversity descriptions of classes to guide semantically meaningful learning of diversity prompts, proposing random combinations and selections of texts to increase textual diversity. Additionally, deep prompt tuning is introduced to enhance the model's transfer capability. After training of the first step, support samples with different diversity prompts are input into the CLIP backbone to generate enhanced features. After generation, the second phase trains classifiers using the generated features. Extensive experimental results across several benchmarks verify our method is comparable to SOTA source-utilized models and attain the best performance under the source-free CD-FSL setting.
<div id='section'>Paperid: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2411.10745.pdf' target='_blank'>https://arxiv.org/pdf/2411.10745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeonghyeok Do, Munchurl Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10745">Bridging the Skeleton-Text Modality Gap: Diffusion-Powered Modality Alignment for Zero-shot Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In zero-shot skeleton-based action recognition (ZSAR), aligning skeleton features with the text features of action labels is essential for accurately predicting unseen actions. ZSAR faces a fundamental challenge in bridging the modality gap between the two-kind features, which severely limits generalization to unseen actions. Previous methods focus on direct alignment between skeleton and text latent spaces, but the modality gaps between these spaces hinder robust generalization learning. Motivated by the success of diffusion models in multi-modal alignment (e.g., text-to-image, text-to-video), we firstly present a diffusion-based skeleton-text alignment framework for ZSAR. Our approach, Triplet Diffusion for Skeleton-Text Matching (TDSM), focuses on cross-alignment power of diffusion models rather than their generative capability. Specifically, TDSM aligns skeleton features with text prompts by incorporating text features into the reverse diffusion process, where skeleton features are denoised under text guidance, forming a unified skeleton-text latent space for robust matching. To enhance discriminative power, we introduce a triplet diffusion (TD) loss that encourages our TDSM to correct skeleton-text matches while pushing them apart for different action classes. Our TDSM significantly outperforms very recent state-of-the-art methods with significantly large margins of 2.36%-point to 13.05%-point, demonstrating superior accuracy and scalability in zero-shot settings through effective skeleton-text matching.
<div id='section'>Paperid: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2410.15766.pdf' target='_blank'>https://arxiv.org/pdf/2410.15766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Ulmer, Leonard KlÃ¼pfel, Maximilian Durner, Rudolph Triebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15766">How Important are Data Augmentations to Close the Domain Gap for Object Detection in Orbit?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the efficacy of data augmentations to close the domain gap in spaceborne computer vision, crucial for autonomous operations like on-orbit servicing. As the use of computer vision in space increases, challenges such as hostile illumination and low signal-to-noise ratios significantly hinder performance. While learning-based algorithms show promising results, their adoption is limited by the need for extensive annotated training data and the domain gap that arises from differences between synthesized and real-world imagery. This study explores domain generalization in terms of data augmentations -- classical color and geometric transformations, corruptions, and noise -- to enhance model performance across the domain gap. To this end, we conduct an large scale experiment using a hyperparameter optimization pipeline that samples hundreds of different configurations and searches for the best set to bridge the domain gap. As a reference task, we use 2D object detection and evaluate on the SPEED+ dataset that contains real hardware-in-the-loop satellite images in its test set. Moreover, we evaluate four popular object detectors, including Mask R-CNN, Faster R-CNN, YOLO-v7, and the open set detector GroundingDINO, and highlight their trade-offs between performance, inference speed, and training time. Our results underscore the vital role of data augmentations in bridging the domain gap, improving model performance, robustness, and reliability for critical space applications. As a result, we propose two novel data augmentations specifically developed to emulate the visual effects observed in orbital imagery. We conclude by recommending the most effective augmentations for advancing computer vision in challenging orbital environments. Code for training detectors and hyperparameter search will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2410.07717.pdf' target='_blank'>https://arxiv.org/pdf/2410.07717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Jarry, Ramon Dalmau, Philippe Very, Junzi Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07717">On the Generalization Properties of Deep Learning for Aircraft Fuel Flow Estimation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating aircraft fuel flow is essential for evaluating new procedures, designing next-generation aircraft, and monitoring the environmental impact of current aviation practices. This paper investigates the generalization capabilities of deep learning models in predicting fuel consumption, focusing particularly on their performance for aircraft types absent from the training data. We propose a novel methodology that integrates neural network architectures with domain generalization techniques to enhance robustness and reliability across a wide range of aircraft. A comprehensive dataset containing 101 different aircraft types, separated into training and generalization sets, with each aircraft type set containing 1,000 flights. We employed the base of aircraft data (BADA) model for fuel flow estimates, introduced a pseudo-distance metric to assess aircraft type similarity, and explored various sampling strategies to optimize model performance in data-sparse regions. Our results reveal that for previously unseen aircraft types, the introduction of noise into aircraft and engine parameters improved model generalization. The model is able to generalize with acceptable mean absolute percentage error between 2\% and 10\% for aircraft close to existing aircraft, while performance is below 1\% error for known aircraft in the training set. This study highlights the potential of combining domain-specific insights with advanced machine learning techniques to develop scalable, accurate, and generalizable fuel flow estimation models.
<div id='section'>Paperid: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2410.06020.pdf' target='_blank'>https://arxiv.org/pdf/2410.06020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saqib Javed, Hieu Le, Mathieu Salzmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06020">QT-DoG: Quantization-aware Training for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key challenge in Domain Generalization (DG) is preventing overfitting to source domains, which can be mitigated by finding flatter minima in the loss landscape. In this work, we propose Quantization-aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization effectively leads to flatter minima in the loss landscape, thereby enhancing domain generalization. Unlike traditional quantization methods focused on model compression, QT-DoG exploits quantization as an implicit regularizer by inducing noise in model weights, guiding the optimization process toward flatter minima that are less sensitive to perturbations and overfitting. We provide both an analytical perspective and empirical evidence demonstrating that quantization inherently encourages flatter minima, leading to better generalization across domains. Moreover, with the benefit of reducing the model size through quantization, we demonstrate that an ensemble of multiple quantized models further yields superior accuracy than the state-of-the-art DG approaches with no computational or memory overheads. Code is released at: https://saqibjaved1.github.io/QT_DoG/.
<div id='section'>Paperid: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2410.05980.pdf' target='_blank'>https://arxiv.org/pdf/2410.05980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Loukas, Karolis Martinkus, Ed Wagstaff, Kyunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05980">Generalizing to any diverse distribution: uniformity, gentle finetuning and rebalancing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As training datasets grow larger, we aspire to develop models that generalize well to any diverse test distribution, even if the latter deviates significantly from the training data. Various approaches like domain adaptation, domain generalization, and robust optimization attempt to address the out-of-distribution challenge by posing assumptions about the relation between training and test distribution. Differently, we adopt a more conservative perspective by accounting for the worst-case error across all sufficiently diverse test distributions within a known domain. Our first finding is that training on a uniform distribution over this domain is optimal. We also interrogate practical remedies when uniform samples are unavailable by considering methods for mitigating non-uniformity through finetuning and rebalancing. Our theory provides a mathematical grounding for previous observations on the role of entropy and rebalancing for o.o.d. generalization and foundation model training. We also provide new empirical evidence across tasks involving o.o.d. shifts which illustrate the broad applicability of our perspective.
<div id='section'>Paperid: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2409.18592.pdf' target='_blank'>https://arxiv.org/pdf/2409.18592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Uecker, J. Marius ZÃ¶llner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18592">From One to the Power of Many: Invariance to Multi-LiDAR Perception from Single-Sensor Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, LiDAR segmentation methods for autonomous vehicles, powered by deep neural networks, have experienced steep growth in performance on classic benchmarks, such as nuScenes and SemanticKITTI. However, there are still large gaps in performance when deploying models trained on such single-sensor setups to modern vehicles with multiple high-resolution LiDAR sensors. In this work, we introduce a new metric for feature-level invariance which can serve as a proxy to measure cross-domain generalization without requiring labeled data. Additionally, we propose two application-specific data augmentations, which facilitate better transfer to multi-sensor LiDAR setups, when trained on single-sensor datasets. We provide experimental evidence on both simulated and real data, that our proposed augmentations improve invariance across LiDAR setups, leading to improved generalization.
<div id='section'>Paperid: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2409.05817.pdf' target='_blank'>https://arxiv.org/pdf/2409.05817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad-Javad Darvishi-Bayazi, Md Rifat Arefin, Jocelyn Faubert, Irina Rish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05817">VFA: Vision Frequency Analysis of Foundation Models and Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models often struggle with distribution shifts in real-world scenarios, whereas humans exhibit robust adaptation. Models that better align with human perception may achieve higher out-of-distribution generalization. In this study, we investigate how various characteristics of large-scale computer vision models influence their alignment with human capabilities and robustness. Our findings indicate that increasing model and data size and incorporating rich semantic information and multiple modalities enhance models' alignment with human perception and their overall robustness. Our empirical analysis demonstrates a strong correlation between out-of-distribution accuracy and human alignment.
<div id='section'>Paperid: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2409.04734.pdf' target='_blank'>https://arxiv.org/pdf/2409.04734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Preetu Mehta, Aman Sagar, Suchi Kumari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04734">Swin Transformer for Robust Differentiation of Real and Synthetic Images: Intra- and Inter-Dataset Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>\textbf{Purpose} This study aims to address the growing challenge of distinguishing computer-generated imagery (CGI) from authentic digital images in the RGB color space. Given the limitations of existing classification methods in handling the complexity and variability of CGI, this research proposes a Swin Transformer-based model for accurate differentiation between natural and synthetic images.
  \textbf{Methods} The proposed model leverages the Swin Transformer's hierarchical architecture to capture local and global features crucial for distinguishing CGI from natural images. The model's performance was evaluated through intra-dataset and inter-dataset testing across three distinct datasets: CiFAKE, JSSSTU, and Columbia. The datasets were tested individually (D1, D2, D3) and in combination (D1+D2+D3) to assess the model's robustness and domain generalization capabilities.
  \textbf{Results} The Swin Transformer-based model demonstrated high accuracy, consistently achieving a range of 97-99\% across all datasets and testing scenarios. These results confirm the model's effectiveness in detecting CGI, showcasing its robustness and reliability in both intra-dataset and inter-dataset evaluations.
  \textbf{Conclusion} The findings of this study highlight the Swin Transformer model's potential as an advanced tool for digital image forensics, particularly in distinguishing CGI from natural images. The model's strong performance across multiple datasets indicates its capability for domain generalization, making it a valuable asset in scenarios requiring precise and reliable image classification.
<div id='section'>Paperid: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2408.13575.pdf' target='_blank'>https://arxiv.org/pdf/2408.13575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>GÃ¶rkay Aydemir, Weidi Xie, Fatma GÃ¼ney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13575">Can Visual Foundation Models Achieve Long-term Point Tracking?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale vision foundation models have demonstrated remarkable success across various tasks, underscoring their robust generalization capabilities. While their proficiency in two-view correspondence has been explored, their effectiveness in long-term correspondence within complex environments remains unexplored. To address this, we evaluate the geometric awareness of visual foundation models in the context of point tracking: (i) in zero-shot settings, without any training; (ii) by probing with low-capacity layers; (iii) by fine-tuning with Low Rank Adaptation (LoRA). Our findings indicate that features from Stable Diffusion and DINOv2 exhibit superior geometric correspondence abilities in zero-shot settings. Furthermore, DINOv2 achieves performance comparable to supervised models in adaptation settings, demonstrating its potential as a strong initialization for correspondence learning.
<div id='section'>Paperid: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2408.05437.pdf' target='_blank'>https://arxiv.org/pdf/2408.05437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Gao, Michael Cooper, Maryam Naghibzadeh, Amirhossein Azhie, Mamatha Bhat, Rahul G. Krishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05437">Predicting Long-Term Allograft Survival in Liver Transplant Recipients</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Liver allograft failure occurs in approximately 20% of liver transplant recipients within five years post-transplant, leading to mortality or the need for retransplantation. Providing an accurate and interpretable model for individualized risk estimation of graft failure is essential for improving post-transplant care. To this end, we introduce the Model for Allograft Survival (MAS), a simple linear risk score that outperforms other advanced survival models. Using longitudinal patient follow-up data from the United States (U.S.), we develop our models on 82,959 liver transplant recipients and conduct multi-site evaluations on 11 regions. Additionally, by testing on a separate non-U.S. cohort, we explore the out-of-distribution generalization performance of various models without additional fine-tuning, a crucial property for clinical deployment. We find that the most complex models are also the ones most vulnerable to distribution shifts despite achieving the best in-distribution performance. Our findings not only provide a strong risk score for predicting long-term graft failure but also suggest that the routine machine learning pipeline with only in-distribution held-out validation could create harmful consequences for patients at deployment.
<div id='section'>Paperid: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2408.01977.pdf' target='_blank'>https://arxiv.org/pdf/2408.01977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Amerehi, Patrick Healy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01977">Label Augmentation for Neural Networks Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution generalization can be categorized into two types: common perturbations arising from natural variations in the real world and adversarial perturbations that are intentionally crafted to deceive neural networks. While deep neural networks excel in accuracy under the assumption of identical distributions between training and test data, they often encounter out-of-distribution scenarios resulting in a significant decline in accuracy. Data augmentation methods can effectively enhance robustness against common corruptions, but they typically fall short in improving robustness against adversarial perturbations. In this study, we develop Label Augmentation (LA), which enhances robustness against both common and intentional perturbations and improves uncertainty estimation. Our findings indicate a Clean error rate improvement of up to 23.29% when employing LA in comparisons to the baseline. Additionally, it enhances robustness under common corruptions benchmark by up to 24.23%. When tested against FGSM and PGD attacks, improvements in adversarial robustness are noticeable, with enhancements of up to 53.18% for FGSM and 24.46% for PGD attacks.
<div id='section'>Paperid: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2407.21028.pdf' target='_blank'>https://arxiv.org/pdf/2407.21028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NataÅ¡a Tagasovska, Ji Won Park, Matthieu Kirchmeyer, Nathan C. Frey, Andrew Martin Watkins, Aya Abdelsalam Ismail, Arian Rokkum Jamasb, Edith Lee, Tyler Bryson, Stephen Ra, Kyunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21028">Antibody DomainBed: Out-of-Distribution Generalization in Therapeutic Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) has demonstrated significant promise in accelerating drug design. Active ML-guided optimization of therapeutic molecules typically relies on a surrogate model predicting the target property of interest. The model predictions are used to determine which designs to evaluate in the lab, and the model is updated on the new measurements to inform the next cycle of decisions. A key challenge is that the experimental feedback from each cycle inspires changes in the candidate proposal or experimental protocol for the next cycle, which lead to distribution shifts. To promote robustness to these shifts, we must account for them explicitly in the model training. We apply domain generalization (DG) methods to classify the stability of interactions between an antibody and antigen across five domains defined by design cycles. Our results suggest that foundational models and ensembling improve predictive performance on out-of-distribution domains. We publicly release our codebase extending the DG benchmark ``DomainBed,'' and the associated dataset of antibody sequences and structures emulating distribution shifts across design cycles.
<div id='section'>Paperid: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2407.17963.pdf' target='_blank'>https://arxiv.org/pdf/2407.17963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingcheng Xu, Zibo Zhao, Haipeng Zhang, Yanqing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17963">Principled Understanding of Generalization for Generative Transformer Models in Arithmetic Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based models excel in various tasks but their generalization capabilities, especially in arithmetic reasoning, remain incompletely understood. Arithmetic tasks provide a controlled framework to explore these capabilities, yet performance anomalies persist, such as inconsistent effectiveness in multiplication and erratic generalization in modular addition (e.g., modulo 100 vs. 101). This paper develops a unified theoretical framework for understanding the generalization behaviors of transformers in arithmetic tasks, focusing on length generalization. Through detailed analysis of addition, multiplication, and modular operations, we reveal that translation invariance in addition aligns with relative positional encoding for robust generalization, while base mismatch in modular operations disrupts this alignment. Experiments across GPT-family models validate our framework, confirming its ability to predict generalization behaviors. Our work highlights the importance of task structure and training data distribution for achieving data-efficient and structure-aware training, providing a systematic approach to understanding of length generalization in transformers.
<div id='section'>Paperid: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2407.17170.pdf' target='_blank'>https://arxiv.org/pdf/2407.17170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Preeti Mehta, Aman Sagar, Suchi Kumari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17170">Domain Generalized Recaptured Screen Image Identification Using SWIN Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An increasing number of classification approaches have been developed to address the issue of image rebroadcast and recapturing, a standard attack strategy in insurance frauds, face spoofing, and video piracy. However, most of them neglected scale variations and domain generalization scenarios, performing poorly in instances involving domain shifts, typically made worse by inter-domain and cross-domain scale variances. To overcome these issues, we propose a cascaded data augmentation and SWIN transformer domain generalization framework (DAST-DG) in the current research work Initially, we examine the disparity in dataset representation. A feature generator is trained to make authentic images from various domains indistinguishable. This process is then applied to recaptured images, creating a dual adversarial learning setup. Extensive experiments demonstrate that our approach is practical and surpasses state-of-the-art methods across different databases. Our model achieves an accuracy of approximately 82\% with a precision of 95\% on high-variance datasets.
<div id='section'>Paperid: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2407.10762.pdf' target='_blank'>https://arxiv.org/pdf/2407.10762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10762">Domain Generalization for 6D Pose Estimation Through NeRF-based Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces a novel augmentation method that increases the diversity of a train set to improve the generalization abilities of a 6D pose estimation network. For this purpose, a Neural Radiance Field is trained from synthetic images and exploited to generate an augmented set. Our method enriches the initial set by enabling the synthesis of images with (i) unseen viewpoints, (ii) rich illumination conditions through appearance extrapolation, and (iii) randomized textures. We validate our augmentation method on the challenging use-case of spacecraft pose estimation and show that it significantly improves the pose estimation generalization capabilities. On the SPEED+ dataset, our method reduces the error on the pose by 50% on both target domains.
<div id='section'>Paperid: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2406.19803.pdf' target='_blank'>https://arxiv.org/pdf/2406.19803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Javad Hosseini, Yang Gao, Tim BaumgÃ¤rtner, Alex Fabrikant, Reinald Kim Amplayo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19803">Scalable and Domain-General Abstractive Proposition Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmenting text into fine-grained units of meaning is important to a wide range of NLP applications. The default approach of segmenting text into sentences is often insufficient, especially since sentences are usually complex enough to include multiple units of meaning that merit separate treatment in the downstream task. We focus on the task of abstractive proposition segmentation (APS): transforming text into simple, self-contained, well-formed sentences. Several recent works have demonstrated the utility of proposition segmentation with few-shot prompted LLMs for downstream tasks such as retrieval-augmented grounding and fact verification. However, this approach does not scale to large amounts of text and may not always extract all the facts from the input text. In this paper, we first introduce evaluation metrics for the task to measure several dimensions of quality. We then propose a scalable, yet accurate, proposition segmentation model. We model proposition segmentation as a supervised task by training LLMs on existing annotated datasets and show that training yields significantly improved results. We further show that by using the fine-tuned LLMs (Gemini Pro and Gemini Ultra) as teachers for annotating large amounts of multi-domain synthetic distillation data, we can train smaller student models (Gemma 1 2B and 7B) with results similar to the teacher LLMs. We then demonstrate that our technique leads to effective domain generalization, by annotating data in two domains outside the original training data and evaluating on them. Finally, as a key contribution of the paper, we share an easy-to-use API for NLP practitioners to use.
<div id='section'>Paperid: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2406.11743.pdf' target='_blank'>https://arxiv.org/pdf/2406.11743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11743">Domain Generalization for In-Orbit 6D Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of estimating the relative 6D pose, i.e., position and orientation, of a target spacecraft, from a monocular image, a key capability for future autonomous Rendezvous and Proximity Operations. Due to the difficulty of acquiring large sets of real images, spacecraft pose estimation networks are exclusively trained on synthetic ones. However, because those images do not capture the illumination conditions encountered in orbit, pose estimation networks face a domain gap problem, i.e., they do not generalize to real images. Our work introduces a method that bridges this domain gap. It relies on a novel, end-to-end, neural-based architecture as well as a novel learning strategy. This strategy improves the domain generalization abilities of the network through multi-task learning and aggressive data augmentation policies, thereby enforcing the network to learn domain-invariant features. We demonstrate that our method effectively closes the domain gap, achieving state-of-the-art accuracy on the widespread SPEED+ dataset. Finally, ablation studies assess the impact of key components of our method on its generalization abilities.
<div id='section'>Paperid: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2406.08316.pdf' target='_blank'>https://arxiv.org/pdf/2406.08316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Ding Li, Kevin Ellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08316">Is Programming by Example solved by LLMs?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have "solved" PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.
<div id='section'>Paperid: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2406.02550.pdf' target='_blank'>https://arxiv.org/pdf/2406.02550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02550">Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a \, x + b \, y \;\mathrm{mod}\; p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is \emph{transient}, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.
<div id='section'>Paperid: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2405.17299.pdf' target='_blank'>https://arxiv.org/pdf/2405.17299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Tsoy, Nikola Konstantinov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17299">Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simplicity bias, the propensity of deep models to over-rely on simple features, has been identified as a potential reason for limited out-of-distribution generalization of neural networks (Shah et al., 2020). Despite the important implications, this phenomenon has been theoretically confirmed and characterized only under strong dataset assumptions, such as linear separability (Lyu et al., 2021). In this work, we characterize simplicity bias for general datasets in the context of two-layer neural networks initialized with small weights and trained with gradient flow. Specifically, we prove that in the early training phases, network features cluster around a few directions that do not depend on the size of the hidden layer. Furthermore, for datasets with an XOR-like pattern, we precisely identify the learned features and demonstrate that simplicity bias intensifies during later training stages. These results indicate that features learned in the middle stages of training may be more useful for OOD transfer. We support this hypothesis with experiments on image data.
<div id='section'>Paperid: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2403.11963.pdf' target='_blank'>https://arxiv.org/pdf/2403.11963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alkis Kalavasis, Ilias Zadik, Manolis Zampetakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11963">Transfer Learning Beyond Bounded Density Ratios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the fundamental problem of transfer learning where a learning algorithm collects data from some source distribution $P$ but needs to perform well with respect to a different target distribution $Q$. A standard change of measure argument implies that transfer learning happens when the density ratio $dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio $dQ/dP$ is unbounded, but transfer learning is possible.
  In this work, we focus on transfer learning over the class of low-degree polynomial estimators. Our main result is a general transfer inequality over the domain $\mathbb{R}^n$, proving that non-trivial transfer learning for low-degree polynomials is possible under very mild assumptions, going well beyond the classical assumption that $dQ/dP$ is bounded. For instance, it always applies if $Q$ is a log-concave measure and the inverse ratio $dP/dQ$ is bounded. To demonstrate the applicability of our inequality, we obtain new results in the settings of: (1) the classical truncated regression setting, where $dQ/dP$ equals infinity, and (2) the more recent out-of-distribution generalization setting for in-context learning linear functions with transformers. We also provide a discrete analogue of our transfer inequality on the Boolean Hypercube $\{-1,1\}^n$, and study its connections with the recent problem of Generalization on the Unseen of Abbe, Bengio, Lotfi and Rizk (ICML, 2023). Our main conceptual contribution is that the maximum influence of the error of the estimator $\widehat{f}-f^*$ under $Q$, $\mathrm{I}_{\max}(\widehat{f}-f^*)$, acts as a sufficient condition for transferability; when $\mathrm{I}_{\max}(\widehat{f}-f^*)$ is appropriately bounded, transfer is possible over the Boolean domain.
<div id='section'>Paperid: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2403.11674.pdf' target='_blank'>https://arxiv.org/pdf/2403.11674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chamuditha Jayanga Galappaththige, Sanoojan Baliah, Malitha Gunawardhana, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11674">Towards Generalizing to Unseen Domains with Few Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We approach the challenge of addressing semi-supervised domain generalization (SSDG). Specifically, our aim is to obtain a model that learns domain-generalizable features by leveraging a limited subset of labelled data alongside a substantially larger pool of unlabeled data. Existing domain generalization (DG) methods which are unable to exploit unlabeled data perform poorly compared to semi-supervised learning (SSL) methods under SSDG setting. Nevertheless, SSL methods have considerable room for performance improvement when compared to fully-supervised DG training. To tackle this underexplored, yet highly practical problem of SSDG, we make the following core contributions. First, we propose a feature-based conformity technique that matches the posterior distributions from the feature space with the pseudo-label from the model's output space. Second, we develop a semantics alignment loss to learn semantically-compatible representations by regularizing the semantic structure in the feature space. Our method is plug-and-play and can be readily integrated with different SSL-based SSDG baselines without introducing any additional parameters. Extensive experimental results across five challenging DG benchmarks with four strong SSL baselines suggest that our method provides consistent and notable gains in two different SSDG settings.
<div id='section'>Paperid: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2403.05209.pdf' target='_blank'>https://arxiv.org/pdf/2403.05209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinha Park, Wonguk Cho, Taesup Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05209">Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While there have been considerable advancements in machine learning driven by extensive datasets, a significant disparity still persists in the availability of data across various sources and populations. This inequality across domains poses challenges in modeling for those with limited data, which can lead to profound practical and ethical concerns. In this paper, we address a representative case of data inequality problem across domains termed Semi-Supervised Domain Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled. We propose a novel algorithm, ProUD, which can effectively learn domain-invariant features via domain-aware prototypes along with progressive generalization via uncertainty-adaptive mixing of labeled and unlabeled domains. Our experiments on three different benchmark datasets demonstrate the effectiveness of ProUD, outperforming all baseline models including single domain generalization and semi-supervised learning. Source code will be released upon acceptance of the paper.
<div id='section'>Paperid: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2403.03863.pdf' target='_blank'>https://arxiv.org/pdf/2403.03863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzi Xu, Muhao Chen, Lifu Huang, Slobodan Vucetic, Wenpeng Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03863">X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some of them might appear thousands of times, while others might only appear sporadically or not at all. For practical deployment, it is crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits. Here, X can span from 0 to positive infinity. The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios. To solve X-shot, we propose BinBin (Binary INference Based on INstruction following) that leverages the Indirect Supervision from a large collection of NLP tasks via instruction following, bolstered by Weak Supervision provided by large language models. BinBin surpasses previous state-of-the-art techniques on three benchmark datasets across multiple domains. To our knowledge, this is the first work addressing X-shot learning, where X remains variable.
<div id='section'>Paperid: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2403.01773.pdf' target='_blank'>https://arxiv.org/pdf/2403.01773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinhua Piao, Sangseon Lee, Yijingxiu Lu, Sun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01773">Improving out-of-distribution generalization in graphs via hierarchical semantic environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attention mechanisms are employed to re-extract the subgraphs for regenerating global environments in a hierarchical manner. In addition, we introduce a new learning objective that guides our model to learn the diversity of environments within the same hierarchy while maintaining consistency across different hierarchies. This approach enables our model to consider the relationships between environments and facilitates robust graph invariant learning. Extensive experiments on real-world graph data have demonstrated the effectiveness of our framework. Particularly, in the challenging dataset DrugOOD, our method achieves up to 1.29% and 2.83% improvement over the best baselines on IC50 and EC50 prediction tasks, respectively.
<div id='section'>Paperid: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2402.12368.pdf' target='_blank'>https://arxiv.org/pdf/2402.12368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Javad Hosseini, Andrey Petrov, Alex Fabrikant, Annie Louis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12368">A synthetic data approach for domain generalization of NLI models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We explore the opportunity for synthetic high-quality datasets to adapt NLI models for zero-shot use in downstream applications across new and unseen text domains. We demonstrate a new approach for generating NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the best generalization to completely new downstream test settings. On the TRUE benchmark, a T5-small model trained with our data improves around $7\%$ on average compared to training on the best alternative dataset. The improvements are more pronounced for smaller models, while still meaningful on a T5 XXL model. We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data.
<div id='section'>Paperid: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2402.00353.pdf' target='_blank'>https://arxiv.org/pdf/2402.00353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Huu Cap, Atsushi Fukuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00353">High-Quality Medical Image Generation from Free-hand Sketch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating medical images from human-drawn free-hand sketches holds promise for various important medical imaging applications. Due to the extreme difficulty in collecting free-hand sketch data in the medical domain, most deep learning-based methods have been proposed to generate medical images from the synthesized sketches (e.g., edge maps or contours of segmentation masks from real images). However, these models often fail to generalize on the free-hand sketches, leading to unsatisfactory results. In this paper, we propose a practical free-hand sketch-to-image generation model called Sketch2MedI that learns to represent sketches in StyleGAN's latent space and generate medical images from it. Thanks to the ability to encode sketches into this meaningful representation space, Sketch2MedI only requires synthesized sketches for training, enabling a cost-effective learning process. Our Sketch2MedI demonstrates a robust generalization to free-hand sketches, resulting in high-quality and realistic medical image generations. Comparative evaluations of Sketch2MedI against the pix2pix, CycleGAN, UNIT, and U-GAT-IT models show superior performance in generating pharyngeal images, both quantitative and qualitative across various metrics.
<div id='section'>Paperid: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2311.12589.pdf' target='_blank'>https://arxiv.org/pdf/2311.12589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gauransh Sawhney, Daksh Dave, Adeel Ahmed, Jiechao Gao, Khalid Saleem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12589">Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer from a labeled source domain to an unlabeled target domain, navigating the obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for domain generalization. This paper presents an innovative method to bolster ViT performance in source-free target adaptation, beginning with an evaluation of how key, query, and value elements affect ViT outcomes. Experiments indicate that altering the key component has negligible effects on Transformer performance. Leveraging this discovery, we introduce Domain Representation Images (DRIs), feeding embeddings through the key element. DRIs act as domain-specific markers, effortlessly merging with the training regimen. To assess our method, we perform target adaptation tests on the Cross Instance DRI source-only (SO) control. We measure the efficacy of target adaptation with and without DRIs, against existing benchmarks like SHOT-B* and adaptations via CDTrans. Findings demonstrate that excluding DRIs offers limited gains over SHOT-B*, while their inclusion in the key segment boosts average precision promoting superior domain generalization. This research underscores the vital role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent for further domain adaptation explorations.
<div id='section'>Paperid: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2311.02777.pdf' target='_blank'>https://arxiv.org/pdf/2311.02777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Ginn, Alexis Palmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02777">Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization is of particular importance in resource-constrained settings, where the available training data may represent only a small fraction of the distribution of possible texts. We investigate the ability of morpheme labeling models to generalize by evaluating their performance on unseen genres of text, and we experiment with strategies for closing the gap between performance on in-distribution and out-of-distribution data. Specifically, we use weight decay optimization, output denoising, and iterative pseudo-labeling, and achieve a 2% improvement on a test set containing texts from unseen genres. All experiments are performed using texts written in the Mayan language Uspanteko.
<div id='section'>Paperid: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2310.16542.pdf' target='_blank'>https://arxiv.org/pdf/2310.16542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jules Sanchez, Louis Soum-Fontez, Jean-Emmanuel Deschaud, Francois Goulette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16542">ParisLuco3D: A high-quality target dataset for domain generalization of LiDAR perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR is an essential sensor for autonomous driving by collecting precise geometric information regarding a scene. %Exploiting this information for perception is interesting as the amount of available data increases. As the performance of various LiDAR perception tasks has improved, generalizations to new environments and sensors has emerged to test these optimized models in real-world conditions.
  This paper provides a novel dataset, ParisLuco3D, specifically designed for cross-domain evaluation to make it easier to evaluate the performance utilizing various source datasets. Alongside the dataset, online benchmarks for LiDAR semantic segmentation, LiDAR object detection, and LiDAR tracking are provided to ensure a fair comparison across methods.
  The ParisLuco3D dataset, evaluation scripts, and links to benchmarks can be found at the following website:https://npm3d.fr/parisluco3d
<div id='section'>Paperid: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2310.15402.pdf' target='_blank'>https://arxiv.org/pdf/2310.15402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandrine BÃ©dard, Enamundram Naga Karthik, Charidimos Tsagkas, Emanuele PravatÃ, Cristina Granziera, Andrew Smith, Kenneth Arnold Weber, Julien Cohen-Adad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15402">Towards contrast-agnostic soft segmentation of the spinal cord</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spinal cord segmentation is clinically relevant and is notably used to compute spinal cord cross-sectional area (CSA) for the diagnosis and monitoring of cord compression or neurodegenerative diseases such as multiple sclerosis. While several semi and automatic methods exist, one key limitation remains: the segmentation depends on the MRI contrast, resulting in different CSA across contrasts. This is partly due to the varying appearance of the boundary between the spinal cord and the cerebrospinal fluid that depends on the sequence and acquisition parameters. This contrast-sensitive CSA adds variability in multi-center studies where protocols can vary, reducing the sensitivity to detect subtle atrophies. Moreover, existing methods enhance the CSA variability by training one model per contrast, while also producing binary masks that do not account for partial volume effects. In this work, we present a deep learning-based method that produces soft segmentations of the spinal cord. Using the Spine Generic Public Database of healthy participants ($\text{n}=267$; $\text{contrasts}=6$), we first generated participant-wise soft ground truth (GT) by averaging the binary segmentations across all 6 contrasts. These soft GT, along with aggressive data augmentation and a regression-based loss function, were used to train a U-Net model for spinal cord segmentation. We evaluated our model against state-of-the-art methods and performed ablation studies involving different loss functions and domain generalization methods. Our results show that using the soft segmentations along with a regression loss function reduces CSA variability ($p < 0.05$, Wilcoxon signed-rank test). The proposed spinal cord segmentation model generalizes better than the state-of-the-art methods amongst unseen datasets, vendors, contrasts, and pathologies (compression, lesions), while accounting for partial volume effects.
<div id='section'>Paperid: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2310.13061.pdf' target='_blank'>https://arxiv.org/pdf/2310.13061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darshil Doshi, Aritra Das, Tianyu He, Andrey Gromov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13061">To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider multi-layer perceptron (MLP) and Transformer architectures trained on modular arithmetic tasks, where ($Î¾\cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels \emph{and} achieve $100\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as weight decay, dropout and BatchNorm force the network to ignore the corrupted data during optimization, and achieve $100\%$ accuracy on the uncorrupted dataset; and (iv) the effect of these regularization methods is (``mechanistically'') interpretable: weight decay and dropout force all the neurons to learn generalizing representations, while BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of the generalizing ones. Finally, we show that in the presence of regularization, the training dynamics involves two consecutive stages: first, the network undergoes \emph{grokking} dynamics reaching high train \emph{and} test accuracy; second, it unlearns the memorizing representations, where the train accuracy suddenly jumps from $100\%$ to $100 (1-Î¾)\%$.
<div id='section'>Paperid: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2310.08255.pdf' target='_blank'>https://arxiv.org/pdf/2310.08255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, R. Venkatesh Babu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08255">Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) such as CLIP are trained on large amounts of image-text pairs, resulting in remarkable generalization across several data distributions. However, in several cases, their expensive training and data collection/curation costs do not justify the end application. This motivates a vendor-client paradigm, where a vendor trains a large-scale VLM and grants only input-output access to clients on a pay-per-query basis in a black-box setting. The client aims to minimize inference cost by distilling the VLM to a student model using the limited available task-specific data, and further deploying this student model in the downstream application. While naive distillation largely improves the In-Domain (ID) accuracy of the student, it fails to transfer the superior out-of-distribution (OOD) generalization of the VLM teacher using the limited available labeled images. To mitigate this, we propose Vision-Language to Vision - Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and language modalities of the teacher model with the vision modality of a pre-trained student model, and further distills the aligned VLM representations to the student. This maximally retains the pre-trained features of the student, while also incorporating the rich representations of the VLM image encoder and the superior generalization of the text embeddings. The proposed approach achieves state-of-the-art results on the standard Domain Generalization benchmarks in a black-box teacher setting as well as a white-box setting where the weights of the VLM are accessible.
<div id='section'>Paperid: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2310.03320.pdf' target='_blank'>https://arxiv.org/pdf/2310.03320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Wang, Zichen Wang, Balasubramaniam Srinivasan, Vassilis N. Ioannidis, Huzefa Rangwala, Rishita Anubhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03320">BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relations. Additionally, we also show that BioBridge presents itself as a general purpose retriever that can aid biomedical multimodal question answering as well as enhance the guided generation of novel drugs.
<div id='section'>Paperid: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2310.01029.pdf' target='_blank'>https://arxiv.org/pdf/2310.01029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shohei Enomoto, Monikka Roslianna Busto, Takeharu Eda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01029">Incorporating Supervised Domain Generalization into Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing utilization of deep learning in outdoor settings, its robustness needs to be enhanced to preserve accuracy in the face of distribution shifts, such as compression artifacts. Data augmentation is a widely used technique to improve robustness, thanks to its ease of use and numerous benefits. However, it requires more training epochs, making it difficult to train large models with limited computational resources. To address this problem, we treat data augmentation as supervised domain generalization~(SDG) and benefit from the SDG method, contrastive semantic alignment~(CSA) loss, to improve the robustness and training efficiency of data augmentation. The proposed method only adds loss during model training and can be used as a plug-in for existing data augmentation methods. Experiments on the CIFAR-100 and CUB datasets show that the proposed method improves the robustness and training efficiency of typical data augmentations.
<div id='section'>Paperid: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2309.14282.pdf' target='_blank'>https://arxiv.org/pdf/2309.14282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muxin Liao, Shishun Tian, Yuhang Zhang, Guoguang Hua, Wenbin Zou, Xia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14282">Calibration-based Dual Prototypical Contrastive Learning Approach for Domain Generalization Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prototypical contrastive learning (PCL) has been widely used to learn class-wise domain-invariant features recently. These methods are based on the assumption that the prototypes, which are represented as the central value of the same class in a certain domain, are domain-invariant. Since the prototypes of different domains have discrepancies as well, the class-wise domain-invariant features learned from the source domain by PCL need to be aligned with the prototypes of other domains simultaneously. However, the prototypes of the same class in different domains may be different while the prototypes of different classes may be similar, which may affect the learning of class-wise domain-invariant features. Based on these observations, a calibration-based dual prototypical contrastive learning (CDPCL) approach is proposed to reduce the domain discrepancy between the learned class-wise features and the prototypes of different domains for domain generalization semantic segmentation. It contains an uncertainty-guided PCL (UPCL) and a hard-weighted PCL (HPCL). Since the domain discrepancies of the prototypes of different classes may be different, we propose an uncertainty probability matrix to represent the domain discrepancies of the prototypes of all the classes. The UPCL estimates the uncertainty probability matrix to calibrate the weights of the prototypes during the PCL. Moreover, considering that the prototypes of different classes may be similar in some circumstances, which means these prototypes are hard-aligned, the HPCL is proposed to generate a hard-weighted matrix to calibrate the weights of the hard-aligned prototypes during the PCL. Extensive experiments demonstrate that our approach achieves superior performance over current approaches on domain generalization semantic segmentation tasks.
<div id='section'>Paperid: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2307.08187.pdf' target='_blank'>https://arxiv.org/pdf/2307.08187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiroki Naganuma, Ryuichiro Hataya, Kotaro Yoshida, Ioannis Mitliagkas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08187">An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of computer vision, fine-tuning pre-trained models has become a prevalent strategy for out-of-distribution (OOD) generalization tasks. Different from most prior work that has focused on advancing learning algorithms, we systematically examined how pre-trained model size, pre-training dataset size, and training strategies impact generalization and confidence calibration on downstream tasks. We evaluated 100 models across diverse pre-trained model sizes, five pre-training datasets, and five data augmentations through extensive experiments on four distribution shift datasets totaling over 120,000 GPU hours. Our results demonstrate the significant impact of pre-trained model selection, with optimal choices substantially improving OOD accuracy over algorithm improvement alone. Additionally, we find that larger models and bigger pre-training datasets not only enhance OOD performance but also improve calibration, helping to mitigate overconfidence, contrary to some prior studies that found modern deep networks to calibrate worse than classical shallow models. Our work underscores the overlooked importance of pre-trained model selection for out-of-distribution generalization and calibration.
<div id='section'>Paperid: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2307.05901.pdf' target='_blank'>https://arxiv.org/pdf/2307.05901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad, David Suter, Alireza Bab-Hadiashar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05901">Single Domain Generalization via Normalised Cross-correlation Based Convolutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning techniques often perform poorly in the presence of domain shift, where the test data follows a different distribution than the training data. The most practically desirable approach to address this issue is Single Domain Generalization (S-DG), which aims to train robust models using data from a single source. Prior work on S-DG has primarily focused on using data augmentation techniques to generate diverse training data. In this paper, we explore an alternative approach by investigating the robustness of linear operators, such as convolution and dense layers commonly used in deep learning. We propose a novel operator called XCNorm that computes the normalized cross-correlation between weights and an input feature patch. This approach is invariant to both affine shifts and changes in energy within a local feature patch and eliminates the need for commonly used non-linear activation functions. We show that deep neural networks composed of this operator are robust to common semantic distribution shifts. Furthermore, our empirical results on single-domain generalization benchmarks demonstrate that our proposed technique performs comparably to the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2306.15261.pdf' target='_blank'>https://arxiv.org/pdf/2306.15261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzhe Li, Ming Liu, Shang Gao, Wray Buntine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15261">A Survey on Out-of-Distribution Evaluation of Neural NLP Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial robustness, domain generalization and dataset biases are three active lines of research contributing to out-of-distribution (OOD) evaluation on neural NLP models. However, a comprehensive, integrated discussion of the three research lines is still lacking in the literature. In this survey, we 1) compare the three lines of research under a unifying definition; 2) summarize the data-generating processes and evaluation protocols for each line of research; and 3) emphasize the challenges and opportunities for future work.
<div id='section'>Paperid: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2306.13515.pdf' target='_blank'>https://arxiv.org/pdf/2306.13515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Schiavone, Francesco Galati, Maria A. Zuluaga
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13515">Binary domain generalization for sparsifying binary neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Binary neural networks (BNNs) are an attractive solution for developing and deploying deep neural network (DNN)-based applications in resource constrained devices. Despite their success, BNNs still suffer from a fixed and limited compression factor that may be explained by the fact that existing pruning methods for full-precision DNNs cannot be directly applied to BNNs. In fact, weight pruning of BNNs leads to performance degradation, which suggests that the standard binarization domain of BNNs is not well adapted for the task. This work proposes a novel more general binary domain that extends the standard binary one that is more robust to pruning techniques, thus guaranteeing improved compression and avoiding severe performance losses. We demonstrate a closed-form solution for quantizing the weights of a full-precision network into the proposed binary domain. Finally, we show the flexibility of our method, which can be combined with other pruning strategies. Experiments over CIFAR-10 and CIFAR-100 demonstrate that the novel approach is able to generate efficient sparse networks with reduced memory usage and run-time latency, while maintaining performance.
<div id='section'>Paperid: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2306.04911.pdf' target='_blank'>https://arxiv.org/pdf/2306.04911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungwuk Park, Dong-Jun Han, Soyeong Kim, Jaekyun Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04911">Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In domain generalization (DG), the target domain is unknown when the model is being trained, and the trained model should successfully work on an arbitrary (and possibly unseen) target domain during inference. This is a difficult problem, and despite active studies in recent years, it remains a great challenge. In this paper, we take a simple yet effective approach to tackle this issue. We propose test-time style shifting, which shifts the style of the test sample (that has a large style gap with the source domains) to the nearest source domain that the model is already familiar with, before making the prediction. This strategy enables the model to handle any target domains with arbitrary style statistics, without additional model update at test-time. Additionally, we propose style balancing, which provides a great platform for maximizing the advantage of test-time style shifting by handling the DG-specific imbalance issues. The proposed ideas are easy to implement and successfully work in conjunction with various other DG schemes. Experimental results on different datasets show the effectiveness of our methods.
<div id='section'>Paperid: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2306.02827.pdf' target='_blank'>https://arxiv.org/pdf/2306.02827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aswathy Velutharambath, Roman Klinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02827">UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Verbal deception has been studied in psychology, forensics, and computational linguistics for a variety of reasons, like understanding behaviour patterns, identifying false testimonies, and detecting deception in online communication. Varying motivations across research fields lead to differences in the domain choices to study and in the conceptualization of deception, making it hard to compare models and build robust deception detection systems for a given language. With this paper, we improve this situation by surveying available English deception datasets which include domains like social media reviews, court testimonials, opinion statements on specific topics, and deceptive dialogues from online strategy games. We consolidate these datasets into a single unified corpus. Based on this resource, we conduct a correlation analysis of linguistic cues of deception across datasets to understand the differences and perform cross-corpus modeling experiments which show that a cross-domain generalization is challenging to achieve. The unified deception corpus (UNIDECOR) can be obtained from https://www.ims.uni-stuttgart.de/data/unidecor.
<div id='section'>Paperid: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2305.19753.pdf' target='_blank'>https://arxiv.org/pdf/2305.19753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wojciech Masarczyk, Mateusz Ostaszewski, Ehsan Imani, Razvan Pascanu, Piotr MiÅoÅ, Tomasz TrzciÅski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19753">The Tunnel Effect: Building Data Representations in Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks are widely known for their remarkable effectiveness across various tasks, with the consensus that deeper networks implicitly learn more complex data representations. This paper shows that sufficiently deep networks trained for supervised image classification split into two distinct parts that contribute to the resulting data representations differently. The initial layers create linearly-separable representations, while the subsequent layers, which we refer to as \textit{the tunnel}, compress these representations and have a minimal impact on the overall performance. We explore the tunnel's behavior through comprehensive empirical studies, highlighting that it emerges early in the training process. Its depth depends on the relation between the network's capacity and task complexity. Furthermore, we show that the tunnel degrades out-of-distribution generalization and discuss its implications for continual learning.
<div id='section'>Paperid: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2304.12122.pdf' target='_blank'>https://arxiv.org/pdf/2304.12122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Schwonberg, Fadoua El Bouazati, Nico M. Schmidt, Hanno Gottschalk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12122">Augmentation-based Domain Generalization for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised Domain Adaptation (UDA) and domain generalization (DG) are two research areas that aim to tackle the lack of generalization of Deep Neural Networks (DNNs) towards unseen domains. While UDA methods have access to unlabeled target images, domain generalization does not involve any target data and only learns generalized features from a source domain. Image-style randomization or augmentation is a popular approach to improve network generalization without access to the target domain. Complex methods are often proposed that disregard the potential of simple image augmentations for out-of-domain generalization. For this reason, we systematically study the in- and out-of-domain generalization capabilities of simple, rule-based image augmentations like blur, noise, color jitter and many more. Based on a full factorial design of experiment design we provide a systematic statistical evaluation of augmentations and their interactions. Our analysis provides both, expected and unexpected, outcomes. Expected, because our experiments confirm the common scientific standard that combination of multiple different augmentations out-performs single augmentations. Unexpected, because combined augmentations perform competitive to state-of-the-art domain generalization approaches, while being significantly simpler and without training overhead. On the challenging synthetic-to-real domain shift between Synthia and Cityscapes we reach 39.5% mIoU compared to 40.9% mIoU of the best previous work. When additionally employing the recent vision transformer architecture DAFormer we outperform these benchmarks with a performance of 44.2% mIoU
<div id='section'>Paperid: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2304.07919.pdf' target='_blank'>https://arxiv.org/pdf/2304.07919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Ge, Hongyin Luo, Siyuan Qian, Yulu Gan, Jie Fu, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07919">Chain of Thought Prompt Tuning in Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and has stronger domain generalization performance, but also performs much better in imagetext retrieval and visual question answering, which require more reasoning capabilities. We are the first to successfully adapt chain-of-thought prompting that combines visual and textual embeddings. We will release our codes
<div id='section'>Paperid: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2304.04415.pdf' target='_blank'>https://arxiv.org/pdf/2304.04415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Xu, Mark He Huang, Xindi Shang, Zehuan Yuan, Ying Sun, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04415">Meta Compositional Referring Expression Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring expression segmentation aims to segment an object described by a language expression from an image. Despite the recent progress on this task, existing models tackling this task may not be able to fully capture semantics and visual representations of individual concepts, which limits their generalization capability, especially when handling novel compositions of learned concepts. In this work, through the lens of meta learning, we propose a Meta Compositional Referring Expression Segmentation (MCRES) framework to enhance model compositional generalization performance. Specifically, to handle various levels of novel compositions, our framework first uses training data to construct a virtual training set and multiple virtual testing sets, where data samples in each virtual testing set contain a level of novel compositions w.r.t. the virtual training set. Then, following a novel meta optimization scheme to optimize the model to obtain good testing performance on the virtual testing sets after training on the virtual training set, our framework can effectively drive the model to better capture semantics and visual representations of individual concepts, and thus obtain robust generalization performance even when handling novel compositions. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our framework.
<div id='section'>Paperid: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2304.03709.pdf' target='_blank'>https://arxiv.org/pdf/2304.03709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Chen, Zhi Gao, Xinxiao Wu, Jiebo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03709">Meta-causal Learning for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization aims to learn a model from a single training domain (source domain) and apply it to multiple unseen test domains (target domains). Existing methods focus on expanding the distribution of the training domain to cover the target domains, but without estimating the domain shift between the source and target domains. In this paper, we propose a new learning paradigm, namely simulate-analyze-reduce, which first simulates the domain shift by building an auxiliary domain as the target domain, then learns to analyze the causes of domain shift, and finally learns to reduce the domain shift for model adaptation. Under this paradigm, we propose a meta-causal learning method to learn meta-knowledge, that is, how to infer the causes of domain shift between the auxiliary and source domains during training. We use the meta-knowledge to analyze the shift between the target and source domains during testing. Specifically, we perform multiple transformations on source data to generate the auxiliary domain, perform counterfactual inference to learn to discover the causal factors of the shift between the auxiliary and source domains, and incorporate the inferred causality into factor-aware domain alignments. Extensive experiments on several benchmarks of image classification show the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2304.00629.pdf' target='_blank'>https://arxiv.org/pdf/2304.00629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyang Lyu, Thuan Nguyen, Matthias Scheutz, Prakash Ishwar, Shuchin Aeron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00629">A principled approach to model validation in domain generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn a model with good generalization ability, that is, the learned model should not only perform well on several seen domains but also on unseen domains with different data distributions. State-of-the-art domain generalization methods typically train a representation function followed by a classifier jointly to minimize both the classification risk and the domain discrepancy. However, when it comes to model selection, most of these methods rely on traditional validation routines that select models solely based on the lowest classification risk on the validation set. In this paper, we theoretically demonstrate a trade-off between minimizing classification risk and mitigating domain discrepancy, i.e., it is impossible to achieve the minimum of these two objectives simultaneously. Motivated by this theoretical result, we propose a novel model selection method suggesting that the validation process should account for both the classification risk and the domain discrepancy. We validate the effectiveness of the proposed method by numerical results on several domain generalization datasets.
<div id='section'>Paperid: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2303.13297.pdf' target='_blank'>https://arxiv.org/pdf/2303.13297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangrui Lv, Jian Liang, Shuang Li, Jinming Zhang, Di Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13297">Improving Generalization with Domain Convex Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) tends to alleviate the poor generalization capability of deep neural networks by learning model with multiple source domains. A classical solution to DG is domain augmentation, the common belief of which is that diversifying source domains will be conducive to the out-of-distribution generalization. However, these claims are understood intuitively, rather than mathematically. Our explorations empirically reveal that the correlation between model generalization and the diversity of domains may be not strictly positive, which limits the effectiveness of domain augmentation. This work therefore aim to guarantee and further enhance the validity of this strand. To this end, we propose a new perspective on DG that recasts it as a convex game between domains. We first encourage each diversified domain to enhance model generalization by elaborately designing a regularization term based on supermodularity. Meanwhile, a sample filter is constructed to eliminate low-quality samples, thereby avoiding the impact of potentially harmful information. Our framework presents a new avenue for the formal analysis of DG, heuristic analysis and extensive experiments demonstrate the rationality and effectiveness.
<div id='section'>Paperid: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2303.02328.pdf' target='_blank'>https://arxiv.org/pdf/2303.02328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangrok Lee, Jongseong Bae, Ha Young Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02328">Decompose, Adjust, Compose: Effective Normalization by Playing with Frequency for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) is a principal task to evaluate the robustness of computer vision models. Many previous studies have used normalization for DG. In normalization, statistics and normalized features are regarded as style and content, respectively. However, it has a content variation problem when removing style because the boundary between content and style is unclear. This study addresses this problem from the frequency domain perspective, where amplitude and phase are considered as style and content, respectively. First, we verify the quantitative phase variation of normalization through the mathematical derivation of the Fourier transform formula. Then, based on this, we propose a novel normalization method, PCNorm, which eliminates style only as the preserving content through spectral decomposition. Furthermore, we propose advanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees of variations in content and style, respectively. Thus, they can learn domain-agnostic representations for DG. With the normalization methods, we propose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domain gap. The proposed models outperform other recent DG methods. The DAC-SC achieves an average state-of-the-art performance of 65.6% on five datasets: PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.
<div id='section'>Paperid: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2302.14685.pdf' target='_blank'>https://arxiv.org/pdf/2302.14685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samyak Jain, Sravanti Addepalli, Pawan Sahu, Priyam Dey, R. Venkatesh Babu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14685">DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization of neural networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have a sufficiently low loss barrier to obtain improved generalization on combining them. We shed light on our approach by casting it in the framework proposed by Shen et al. and theoretically show that it indeed generalizes better. In addition to improvements in In- Domain generalization, we demonstrate SOTA performance on the Domain Generalization benchmarks in the popular DomainBed framework as well. Our method is generic and can easily be integrated with several base training algorithms to achieve performance gains.
<div id='section'>Paperid: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2302.13139.pdf' target='_blank'>https://arxiv.org/pdf/2302.13139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruce W. Lee, Jason Hyung-Jong Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13139">Prompt-based Learning for Text Readability Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose the novel adaptation of a pre-trained seq2seq model for readability assessment. We prove that a seq2seq model - T5 or BART - can be adapted to discern which text is more difficult from two given texts (pairwise). As an exploratory study to prompt-learn a neural network for text readability in a text-to-text manner, we report useful tips for future work in seq2seq training and ranking-based approach to readability assessment. Specifically, we test nine input-output formats/prefixes and show that they can significantly influence the final model performance.
  Also, we argue that the combination of text-to-text training and pairwise ranking setup 1) enables leveraging multiple parallel text simplification data for teaching readability and 2) trains a neural model for the general concept of readability (therefore, better cross-domain generalization). At last, we report a 99.6% pairwise classification accuracy on Newsela and a 98.7% for OneStopEnglish, through a joint training approach.
<div id='section'>Paperid: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2302.10503.pdf' target='_blank'>https://arxiv.org/pdf/2302.10503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Trang Nguyen, Amin Mansouri, Kanika Madan, Khuong Nguyen, Kartik Ahuja, Dianbo Liu, Yoshua Bengio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10503">Reusable Slotwise Mechanisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agents with the ability to comprehend and reason about the dynamics of objects would be expected to exhibit improved robustness and generalization in novel scenarios. However, achieving this capability necessitates not only an effective scene representation but also an understanding of the mechanisms governing interactions among object subsets. Recent studies have made significant progress in representing scenes using object slots. In this work, we introduce Reusable Slotwise Mechanisms, or RSM, a framework that models object dynamics by leveraging communication among slots along with a modular architecture capable of dynamically selecting reusable mechanisms for predicting the future states of each object slot. Crucially, RSM leverages the Central Contextual Information (CCI), enabling selected mechanisms to access the remaining slots through a bottleneck, effectively allowing for modeling of higher order and complex interactions that might require a sparse subset of objects. Experimental results demonstrate the superior performance of RSM compared to state-of-the-art methods across various future prediction and related downstream tasks, including Visual Question Answering and action planning. Furthermore, we showcase RSM's Out-of-Distribution generalization ability to handle scenes in intricate scenarios.
<div id='section'>Paperid: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2302.06495.pdf' target='_blank'>https://arxiv.org/pdf/2302.06495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Manh Bui, Anqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06495">Density-Softmax: Efficient Test-time Model for Uncertainty Estimation and Robustness under Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sampling-based methods, e.g., Deep Ensembles and Bayesian Neural Nets have become promising approaches to improve the quality of uncertainty estimation and robust generalization. However, they suffer from a large model size and high latency at test-time, which limits the scalability needed for low-resource devices and real-time applications. To resolve these computational issues, we propose Density-Softmax, a sampling-free deterministic framework via combining a density function built on a Lipschitz-constrained feature extractor with the softmax layer. Theoretically, we show that our model is the solution of minimax uncertainty risk and is distance-aware on feature space, thus reducing the over-confidence of the standard softmax under distribution shifts. Empirically, our method enjoys competitive results with state-of-the-art techniques in terms of uncertainty and robustness, while having a lower number of model parameters and a lower latency at test-time.
<div id='section'>Paperid: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2302.06378.pdf' target='_blank'>https://arxiv.org/pdf/2302.06378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriela Csurka, Riccardo Volpi, Boris Chidlovskii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06378">Semantic Image Segmentation: Two Decades of Research</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic image segmentation (SiS) plays a fundamental role in a broad variety of computer vision applications, providing key information for the global understanding of an image. This survey is an effort to summarize two decades of research in the field of SiS, where we propose a literature review of solutions starting from early historical methods followed by an overview of more recent deep learning methods including the latest trend of using transformers. We complement the review by discussing particular cases of the weak supervision and side machine learning techniques that can be used to improve the semantic segmentation such as curriculum, incremental or self-supervised learning.
  State-of-the-art SiS models rely on a large amount of annotated samples, which are more expensive to obtain than labels for tasks such as image classification. Since unlabeled data is instead significantly cheaper to obtain, it is not surprising that Unsupervised Domain Adaptation (UDA) reached a broad success within the semantic segmentation community. Therefore, a second core contribution of this book is to summarize five years of a rapidly growing field, Domain Adaptation for Semantic Image Segmentation (DASiS) which embraces the importance of semantic segmentation itself and a critical need of adapting segmentation models to new environments. In addition to providing a comprehensive survey on DASiS techniques, we unveil also newer trends such as multi-domain learning, domain generalization, domain incremental learning, test-time adaptation and source-free domain adaptation. Finally, we conclude this survey by describing datasets and benchmarks most widely used in SiS and DASiS and briefly discuss related tasks such as instance and panoptic image segmentation, as well as applications such as medical image segmentation.
<div id='section'>Paperid: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2302.04724.pdf' target='_blank'>https://arxiv.org/pdf/2302.04724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus Holzleitner, Sergei V. Pereverzyev, Werner Zellinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04724">Domain Generalization by Functional Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of domain generalization is to learn, given data from different source distributions, a model that can be expected to generalize well on new target distributions which are only seen through unlabeled samples. In this paper, we study domain generalization as a problem of functional regression. Our concept leads to a new algorithm for learning a linear operator from marginal distributions of inputs to the corresponding conditional distributions of outputs given inputs. Our algorithm allows a source distribution-dependent construction of reproducing kernel Hilbert spaces for prediction, and, satisfies finite sample error bounds for the idealized risk. Numerical implementations and source code are available.
<div id='section'>Paperid: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2302.04132.pdf' target='_blank'>https://arxiv.org/pdf/2302.04132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lily H. Zhang, Rajesh Ranganath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04132">Robustness to Spurious Correlations Improves Semantic Out-of-Distribution Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Methods which utilize the outputs or feature representations of predictive models have emerged as promising approaches for out-of-distribution (OOD) detection of image inputs. However, these methods struggle to detect OOD inputs that share nuisance values (e.g. background) with in-distribution inputs. The detection of shared-nuisance out-of-distribution (SN-OOD) inputs is particularly relevant in real-world applications, as anomalies and in-distribution inputs tend to be captured in the same settings during deployment. In this work, we provide a possible explanation for SN-OOD detection failures and propose nuisance-aware OOD detection to address them. Nuisance-aware OOD detection substitutes a classifier trained via empirical risk minimization and cross-entropy loss with one that 1. is trained under a distribution where the nuisance-label relationship is broken and 2. yields representations that are independent of the nuisance under this distribution, both marginally and conditioned on the label. We can train a classifier to achieve these objectives using Nuisance-Randomized Distillation (NuRD), an algorithm developed for OOD generalization under spurious correlations. Output- and feature-based nuisance-aware OOD detection perform substantially better than their original counterparts, succeeding even when detection based on domain generalization algorithms fails to improve performance.
<div id='section'>Paperid: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2301.01079.pdf' target='_blank'>https://arxiv.org/pdf/2301.01079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime W. Lafarge, Viktor H. Koelzer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01079">Fine-Grained Hard Negative Mining: Generalizing Mitosis Detection with a Fifth of the MIDOG 2022 Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Making histopathology image classifiers robust to a wide range of real-world variability is a challenging task. Here, we describe a candidate deep learning solution for the Mitosis Domain Generalization Challenge 2022 (MIDOG) to address the problem of generalization for mitosis detection in images of hematoxylin-eosin-stained histology slides under high variability (scanner, tissue type and species variability). Our approach consists in training a rotation-invariant deep learning model using aggressive data augmentation with a training set enriched with hard negative examples and automatically selected negative examples from the unlabeled part of the challenge dataset. To optimize the performance of our models, we investigated a hard negative mining regime search procedure that lead us to train our best model using a subset of image patches representing 19.6% of our training partition of the challenge dataset. Our candidate model ensemble achieved a F1-score of .697 on the final test set after automated evaluation on the challenge platform, achieving the third best overall score in the MIDOG 2022 Challenge.
<div id='section'>Paperid: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2212.14154.pdf' target='_blank'>https://arxiv.org/pdf/2212.14154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Shishun Tian, Muxin Liao, Zhengyu Zhang, Wenbin Zou, Chen Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14154">A Class-wise Non-salient Region Generalized Framework for Video Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video semantic segmentation (VSS) is beneficial for dealing with dynamic scenes due to the continuous property of the real-world environment. On the one hand, some methods alleviate the predicted inconsistent problem between continuous frames. On the other hand, other methods employ the previous frame as the prior information to assist in segmenting the current frame. Although the previous methods achieve superior performances on the independent and identically distributed (i.i.d) data, they can not generalize well on other unseen domains. Thus, we explore a new task, the video generalizable semantic segmentation (VGSS) task that considers both continuous frames and domain generalization. In this paper, we propose a class-wise non-salient region generalized (CNSG) framework for the VGSS task. Concretely, we first define the class-wise non-salient feature, which describes features of the class-wise non-salient region that carry more generalizable information. Then, we propose a class-wise non-salient feature reasoning strategy to select and enhance the most generalized channels adaptively. Finally, we propose an inter-frame non-salient centroid alignment loss to alleviate the predicted inconsistent problem in the VGSS task. We also extend our video-based framework to the image-based generalizable semantic segmentation (IGSS) task. Experiments demonstrate that our CNSG framework yields significant improvement in the VGSS and IGSS tasks.
<div id='section'>Paperid: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2211.14594.pdf' target='_blank'>https://arxiv.org/pdf/2211.14594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhui Li, Zejia Wu, Chao Zhang, Hongyang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14594">Direct-Effect Risk Minimization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of out-of-distribution (o.o.d.) generalization where spurious correlations of attributes vary across training and test domains. This is known as the problem of correlation shift and has posed concerns on the reliability of machine learning. In this work, we introduce the concepts of direct and indirect effects from causal inference to the domain generalization problem. We argue that models that learn direct effects minimize the worst-case risk across correlation-shifted domains. To eliminate the indirect effects, our algorithm consists of two stages: in the first stage, we learn an indirect-effect representation by minimizing the prediction error of domain labels using the representation and the class labels; in the second stage, we remove the indirect effects learned in the first stage by matching each data with another data of similar indirect-effect representation but of different class labels in the training and validation phase. Our approach is shown to be compatible with existing methods and improve the generalization performance of them on correlation-shifted datasets. Experiments on 5 correlation-shifted datasets and the DomainBed benchmark verify the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2211.14282.pdf' target='_blank'>https://arxiv.org/pdf/2211.14282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Priscille de Dumast, Meritxell Bach Cuadra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14282">Domain generalization in fetal brain MRI segmentation \\with multi-reconstruction augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative analysis of in utero human brain development is crucial for abnormal characterization. Magnetic resonance image (MRI) segmentation is therefore an asset for quantitative analysis. However, the development of automated segmentation methods is hampered by the scarce availability of fetal brain MRI annotated datasets and the limited variability within these cohorts. In this context, we propose to leverage the power of fetal brain MRI super-resolution (SR) reconstruction methods to generate multiple reconstructions of a single subject with different parameters, thus as an efficient tuning-free data augmentation strategy. Overall, the latter significantly improves the generalization of segmentation methods over SR pipelines.
<div id='section'>Paperid: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2209.02700.pdf' target='_blank'>https://arxiv.org/pdf/2209.02700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Zhang, Mengmeng Zhang, Wei Li, Shuai Wang, Ran Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.02700">Language-aware Domain Generalization Network for Cross-Scene Hyperspectral Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text information including extensive prior knowledge about land cover classes has been ignored in hyperspectral image classification (HSI) tasks. It is necessary to explore the effectiveness of linguistic mode in assisting HSI classification. In addition, the large-scale pre-training image-text foundation models have demonstrated great performance in a variety of downstream applications, including zero-shot transfer. However, most domain generalization methods have never addressed mining linguistic modal knowledge to improve the generalization performance of model. To compensate for the inadequacies listed above, a Language-aware Domain Generalization Network (LDGnet) is proposed to learn cross-domain invariant representation from cross-domain shared prior knowledge. The proposed method only trains on the source domain (SD) and then transfers the model to the target domain (TD). The dual-stream architecture including image encoder and text encoder is used to extract visual and linguistic features, in which coarse-grained and fine-grained text representations are designed to extract two levels of linguistic features. Furthermore, linguistic features are used as cross-domain shared semantic space, and visual-linguistic alignment is completed by supervised contrastive learning in semantic space. Extensive experiments on three datasets demonstrate the superiority of the proposed method when compared with state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2209.01634.pdf' target='_blank'>https://arxiv.org/pdf/2209.01634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Zhang, Wei Li, Weidong Sun, Ran Tao, Qian Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01634">Single-source Domain Expansion Network for Cross-Scene Hyperspectral Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, cross-scene hyperspectral image (HSI) classification has drawn increasing attention. It is necessary to train a model only on source domain (SD) and directly transferring the model to target domain (TD), when TD needs to be processed in real time and cannot be reused for training. Based on the idea of domain generalization, a Single-source Domain Expansion Network (SDEnet) is developed to ensure the reliability and effectiveness of domain extension. The method uses generative adversarial learning to train in SD and test in TD. A generator including semantic encoder and morph encoder is designed to generate the extended domain (ED) based on encoder-randomization-decoder architecture, where spatial and spectral randomization are specifically used to generate variable spatial and spectral information, and the morphological knowledge is implicitly applied as domain invariant information during domain expansion. Furthermore, the supervised contrastive learning is employed in the discriminator to learn class-wise domain invariant representation, which drives intra-class samples of SD and ED. Meanwhile, adversarial training is designed to optimize the generator to drive intra-class samples of SD and ED to be separated. Extensive experiments on two public HSI datasets and one additional multispectral image (MSI) dataset demonstrate the superiority of the proposed method when compared with state-of-the-art techniques.
<div id='section'>Paperid: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2206.01251.pdf' target='_blank'>https://arxiv.org/pdf/2206.01251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Lu, Zhen Liu, Aristide Baratin, Romain Laroche, Aaron Courville, Alessandro Sordoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.01251">Using Representation Expressiveness and Learnability to Evaluate Self-Supervised Learning Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of evaluating the quality of self-supervised learning (SSL) models without access to supervised labels, while being agnostic to the architecture, learning algorithm or data manipulation used during training. We argue that representations can be evaluated through the lens of expressiveness and learnability. We propose to use the Intrinsic Dimension (ID) to assess expressiveness and introduce Cluster Learnability (CL) to assess learnability. CL is measured in terms of the performance of a KNN classifier trained to predict labels obtained by clustering the representations with K-means. We thus combine CL and ID into a single predictor -- CLID. Through a large-scale empirical study with a diverse family of SSL algorithms, we find that CLID better correlates with in-distribution model performance than other competing recent evaluation schemes. We also benchmark CLID on out-of-domain generalization, where CLID serves as a predictor of the transfer performance of SSL models on several visual classification tasks, yielding improvements with respect to the competing baselines.
<div id='section'>Paperid: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2205.12191.pdf' target='_blank'>https://arxiv.org/pdf/2205.12191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aishwarya Agrawal, Ivana KajiÄ, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, Aida Nematzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.12191">Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-language (V&L) models pretrained on large-scale multimodal data have demonstrated strong performance on various tasks such as image captioning and visual question answering (VQA). The quality of such models is commonly assessed by measuring their performance on unseen data that typically comes from the same distribution as the training data. However, when evaluated under out-of-distribution (out-of-dataset) settings for VQA, we observe that these models exhibit poor generalization. We comprehensively evaluate two pretrained V&L models under different settings (i.e. classification and open-ended text generation) by conducting cross-dataset evaluations. We find that these models tend to learn to solve the benchmark, rather than learning the high-level skills required by the VQA task. We also find that in most cases generative models are less susceptible to shifts in data distribution compared to discriminative ones, and that multimodal pretraining is generally helpful for OOD generalization. Finally, we revisit assumptions underlying the use of automatic VQA evaluation metrics, and empirically show that their stringent nature repeatedly penalizes models for correct responses.
<div id='section'>Paperid: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2203.09978.pdf' target='_blank'>https://arxiv.org/pdf/2203.09978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad-Javad Darvishi-Bayazi, Pooneh Mousavi, Guillaume Dumas, Irina Rish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.09978">WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models often fail to generalize well under distributional shifts. Understanding and overcoming these failures have led to a research field of Out-of-Distribution (OOD) generalization. Despite being extensively studied for static computer vision tasks, OOD generalization has been underexplored for time series tasks. To shine light on this gap, we present WOODS: eight challenging open-source time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals. We revise the existing OOD generalization algorithms for time series tasks and evaluate them using our systematic framework. Our experiments show a large room for improvement for empirical risk minimization and OOD generalization algorithms on our datasets, thus underscoring the new challenges posed by time series tasks. Code and documentation are available at https://woods-benchmarks.github.io .
<div id='section'>Paperid: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2201.12293.pdf' target='_blank'>https://arxiv.org/pdf/2201.12293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runtian Zhai, Chen Dan, Zico Kolter, Pradeep Ravikumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.12293">Understanding Why Generalized Reweighting Does Not Improve Over ERM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empirical risk minimization (ERM) is known in practice to be non-robust to distributional shift where the training and the test distributions are different. A suite of approaches, such as importance weighting, and variants of distributionally robust optimization (DRO), have been proposed to solve this problem. But a line of recent work has empirically shown that these approaches do not significantly improve over ERM in real applications with distribution shift. The goal of this work is to obtain a comprehensive theoretical understanding of this intriguing phenomenon. We first posit the class of Generalized Reweighting (GRW) algorithms, as a broad category of approaches that iteratively update model parameters based on iterative reweighting of the training samples. We show that when overparameterized models are trained under GRW, the resulting models are close to that obtained by ERM. We also show that adding small regularization which does not greatly affect the empirical training accuracy does not help. Together, our results show that a broad category of what we term GRW approaches are not able to achieve distributionally robust generalization. Our work thus has the following sobering takeaway: to make progress towards distributionally robust generalization, we either have to develop non-GRW approaches, or perhaps devise novel classification/regression loss functions that are adapted to the class of GRW approaches.
<div id='section'>Paperid: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2111.10221.pdf' target='_blank'>https://arxiv.org/pdf/2111.10221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luojun Lin, Han Xie, Zhishu Sun, Weijie Chen, Wenxi Liu, Yuanlong Yu, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.10221">Semi-Supervised Domain Generalization with Evolving Intermediate Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to generalize a model trained on multiple source domains to an unseen target domain. The source domains always require precise annotations, which can be cumbersome or even infeasible to obtain in practice due to the vast amount of data involved. Web data, however, offers an opportunity to access large amounts of unlabeled data with rich style information, which can be leveraged to improve DG. From this perspective, we introduce a novel paradigm of DG, termed as Semi-Supervised Domain Generalization (SSDG), to explore how the labeled and unlabeled source domains can interact, and establish two settings, including the close-set and open-set SSDG. The close-set SSDG is based on existing public DG datasets, while the open-set SSDG, built on the newly-collected web-crawled datasets, presents a novel yet realistic challenge that pushes the limits of current technologies. A natural approach of SSDG is to transfer knowledge from labeled data to unlabeled data via pseudo labeling, and train the model on both labeled and pseudo-labeled data for generalization. Since there are conflicting goals between domain-oriented pseudo labeling and out-of-domain generalization, we develop a pseudo labeling phase and a generalization phase independently for SSDG. Unfortunately, due to the large domain gap, the pseudo labels provided in the pseudo labeling phase inevitably contain noise, which has negative affect on the subsequent generalization phase. Therefore, to improve the quality of pseudo labels and further enhance generalizability, we propose a cyclic learning framework to encourage a positive feedback between these two phases, utilizing an evolving intermediate domain that bridges the labeled and unlabeled domains in a curriculum learning manner...
<div id='section'>Paperid: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2109.01902.pdf' target='_blank'>https://arxiv.org/pdf/2109.01902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyang Lyu, Thuan Nguyen, Prakash Ishwar, Matthias Scheutz, Shuchin Aeron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.01902">Barycentric-alignment and reconstruction loss minimization for domain generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper advances the theory and practice of Domain Generalization (DG) in machine learning. We consider the typical DG setting where the hypothesis is composed of a representation mapping followed by a labeling function. Within this setting, the majority of popular DG methods aim to jointly learn the representation and the labeling functions by minimizing a well-known upper bound for the classification risk in the unseen domain. In practice, however, methods based on this theoretical upper bound ignore a term that cannot be directly optimized due to its dual dependence on both the representation mapping and the unknown optimal labeling function in the unseen domain. To bridge this gap between theory and practice, we introduce a new upper bound that is free of terms having such dual dependence, resulting in a fully optimizable risk upper bound for the unseen domain. Our derivation leverages classical and recent transport inequalities that link optimal transport metrics with information-theoretic measures. Compared to previous bounds, our bound introduces two new terms: (i) the Wasserstein-2 barycenter term that aligns distributions between domains, and (ii) the reconstruction loss term that assesses the quality of representation in reconstructing the original data. Based on this new upper bound, we propose a novel DG algorithm named Wasserstein Barycenter Auto-Encoder (WBAE) that simultaneously minimizes the classification loss, the barycenter loss, and the reconstruction loss. Numerical results demonstrate that the proposed method outperforms current state-of-the-art DG algorithms on several datasets.
<div id='section'>Paperid: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2105.14111.pdf' target='_blank'>https://arxiv.org/pdf/2105.14111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauro Langosco, Jack Koch, Lee Sharkey, Jacob Pfau, Laurent Orseau, David Krueger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.14111">Goal Misgeneralization in Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes.
<div id='section'>Paperid: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2010.15775.pdf' target='_blank'>https://arxiv.org/pdf/2010.15775.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaishnavh Nagarajan, Anders Andreassen, Behnam Neyshabur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2010.15775">Understanding the Failure Modes of Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {\em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.
<div id='section'>Paperid: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2509.23688.pdf' target='_blank'>https://arxiv.org/pdf/2509.23688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soroosh Safari Loaliyan, Jose-Luis Ambite, Paul M. Thompson, Neda Jahanshad, Greg Ver Steeg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23688">FedDAPL: Toward Client-Private Generalization in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) trains models locally at each research center or clinic and aggregates only model updates, making it a natural fit for medical imaging, where strict privacy laws forbid raw data sharing. A major obstacle is scanner-induced domain shift: non-biological variations in hardware or acquisition protocols can cause models to fail on external sites. Most harmonization methods correct this shift by directly comparing data across sites, conflicting with FL's privacy constraints. Domain Generalization (DG) offers a privacy-friendly alternative - learning site-invariant representations without sharing raw data - but standard DG pipelines still assume centralized access to multi-site data, again violating FL's guarantees. This paper meets these difficulties with a straightforward integration of a Domain-Adversarial Neural Network (DANN) within the FL process. After demonstrating that a naive federated DANN fails to converge, we propose a proximal regularization method that stabilizes adversarial training among clients. Experiments on T1-weighted 3-D brain MRIs from the OpenBHB dataset, performing brain-age prediction on participants aged 6-64 y (mean 22+/-6 y; 45 percent male) in training and 6-79 y (mean 19+/-13 y; 55 percent male) in validation, show that training on 15 sites and testing on 19 unseen sites yields superior cross-site generalization over FedAvg and ERM while preserving data privacy.
<div id='section'>Paperid: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2509.22913.pdf' target='_blank'>https://arxiv.org/pdf/2509.22913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jake S. Rhodes, Adam G. Rustad, Marshall S. Nielsen, Morgan Chase McClellan, Dallan Gardner, Dawson Hedges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22913">Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manifold alignment (MA) involves a set of techniques for learning shared representations across domains, yet many traditional MA methods are incapable of performing out-of-sample extension, limiting their real-world applicability. We propose a guided representation learning framework leveraging a geometry-regularized twin autoencoder (AE) architecture to enhance MA while enabling generalization to unseen data. Our method enforces structured cross-modal mappings to maintain geometric fidelity in learned embeddings. By incorporating a pre-trained alignment model and a multitask learning formulation, we improve cross-domain generalization and representation robustness while maintaining alignment fidelity. We evaluate our approach using several MA methods, showing improvements in embedding consistency, information preservation, and cross-domain transfer. Additionally, we apply our framework to Alzheimer's disease diagnosis, demonstrating its ability to integrate multi-modal patient data and enhance predictive accuracy in cases limited to a single domain by leveraging insights from the multi-modal problem.
<div id='section'>Paperid: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2509.08570.pdf' target='_blank'>https://arxiv.org/pdf/2509.08570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjun Yu, Yinchen Zhou, Jia-Xuan Jiang, Shubin Zeng, Yuee Li, Zhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08570">Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal models have achieved remarkable success in natural image segmentation, yet they often underperform when applied to the medical domain. Through extensive study, we attribute this performance gap to the challenges of multimodal fusion, primarily the significant semantic gap between abstract textual prompts and fine-grained medical visual features, as well as the resulting feature dispersion. To address these issues, we revisit the problem from the perspective of semantic aggregation. Specifically, we propose an Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel Decoder. The former mitigates feature dispersion by dynamically clustering features into compact semantic centers to enhance cross-modal correspondence. The latter is designed to bridge the semantic gap by leveraging domain-invariant textual knowledge to effectively guide deep visual representations. The synergy between these two mechanisms significantly improves the model's generalization ability. Extensive experiments on public cardiac and fundus datasets demonstrate that our method consistently outperforms existing SOTA approaches across multiple domain generalization benchmarks.
<div id='section'>Paperid: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2509.05975.pdf' target='_blank'>https://arxiv.org/pdf/2509.05975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nam Duong Tran, Nam Nguyen Phuong, Hieu H. Pham, Phi Le Nguyen, My T. Thai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05975">ConstStyle: Robust Domain Generalization with Unified Style Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks often suffer performance drops when test data distribution differs from training data. Domain Generalization (DG) aims to address this by focusing on domain-invariant features or augmenting data for greater diversity. However, these methods often struggle with limited training domains or significant gaps between seen (training) and unseen (test) domains. To enhance DG robustness, we hypothesize that it is essential for the model to be trained on data from domains that closely resemble unseen test domains-an inherently difficult task due to the absence of prior knowledge about the unseen domains. Accordingly, we propose ConstStyle, a novel approach that leverages a unified domain to capture domain-invariant features and bridge the domain gap with theoretical analysis. During training, all samples are mapped onto this unified domain, optimized for seen domains. During testing, unseen domain samples are projected similarly before predictions. By aligning both training and testing data within this unified domain, ConstStyle effectively reduces the impact of domain shifts, even with large domain gaps or few seen domains. Extensive experiments demonstrate that ConstStyle consistently outperforms existing methods across diverse scenarios. Notably, when only a limited number of seen domains are available, ConstStyle can boost accuracy up to 19.82\% compared to the next best approach.
<div id='section'>Paperid: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2509.02593.pdf' target='_blank'>https://arxiv.org/pdf/2509.02593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raphaël Bourgade, Guillaume Balezo, Thomas Walter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02593">Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data.
<div id='section'>Paperid: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2509.02593.pdf' target='_blank'>https://arxiv.org/pdf/2509.02593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raphaël Bourgade, Guillaume Balezo, Thomas Walter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02593">Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data.
<div id='section'>Paperid: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2508.19595.pdf' target='_blank'>https://arxiv.org/pdf/2508.19595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Kazemi Eskeri, Thomas Wiedemann, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19595">A Lightweight Crowd Model for Robot Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots operating in human-populated environments must navigate safely and efficiently while minimizing social disruption. Achieving this requires estimating crowd movement to avoid congested areas in real-time. Traditional microscopic models struggle to scale in dense crowds due to high computational cost, while existing macroscopic crowd prediction models tend to be either overly simplistic or computationally intensive. In this work, we propose a lightweight, real-time macroscopic crowd prediction model tailored for human motion, which balances prediction accuracy and computational efficiency. Our approach simplifies both spatial and temporal processing based on the inherent characteristics of pedestrian flow, enabling robust generalization without the overhead of complex architectures. We demonstrate a 3.6 times reduction in inference time, while improving prediction accuracy by 3.1 %. Integrated into a socially aware planning framework, the model enables efficient and socially compliant robot navigation in dynamic environments. This work highlights that efficient human crowd modeling enables robots to navigate dense environments without costly computations.
<div id='section'>Paperid: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2508.18859.pdf' target='_blank'>https://arxiv.org/pdf/2508.18859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Kashif Ali, Eun Woo Im, Dongjin Kim, Tae Hyun Kim, Vivek Gupta, Haonan Luo, Tianrui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18859">Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video stabilization remains a fundamental problem in computer vision, particularly pixel-level synthesis solutions for video stabilization, which synthesize full-frame outputs, add to the complexity of this task. These methods aim to enhance stability while synthesizing full-frame videos, but the inherent diversity in motion profiles and visual content present in each video sequence makes robust generalization with fixed parameters difficult. To address this, we present a novel method that improves pixel-level synthesis video stabilization methods by rapidly adapting models to each input video at test time. The proposed approach takes advantage of low-level visual cues available during inference to improve both the stability and visual quality of the output. Notably, the proposed rapid adaptation achieves significant performance gains even with a single adaptation pass. We further propose a jerk localization module and a targeted adaptation strategy, which focuses the adaptation on high-jerk segments for maximizing stability with fewer adaptation steps. The proposed methodology enables modern stabilizers to overcome the longstanding SOTA approaches while maintaining the full frame nature of the modern methods, while offering users with control mechanisms akin to classical approaches. Extensive experiments on diverse real-world datasets demonstrate the versatility of the proposed method. Our approach consistently improves the performance of various full-frame synthesis models in both qualitative and quantitative terms, including results on downstream applications.
<div id='section'>Paperid: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2508.15716.pdf' target='_blank'>https://arxiv.org/pdf/2508.15716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongqi Li, Yitong Chen, Yujuan Wang, Weihang Ni, Haodong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15716">Foundation Models for Cross-Domain EEG Analysis Application: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography (EEG) analysis stands at the forefront of neuroscience and artificial intelligence research, where foundation models are reshaping the traditional EEG analysis paradigm by leveraging their powerful representational capacity and cross-modal generalization. However, the rapid proliferation of these techniques has led to a fragmented research landscape, characterized by diverse model roles, inconsistent architectures, and a lack of systematic categorization. To bridge this gap, this study presents the first comprehensive modality-oriented taxonomy for foundation models in EEG analysis, systematically organizing research advances based on output modalities of the native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal frameworks. We rigorously analyze each category's research ideas, theoretical foundations, and architectural innovations, while highlighting open challenges such as model interpretability, cross-domain generalization, and real-world applicability in EEG-based systems. By unifying this dispersed field, our work not only provides a reference framework for future methodology development but accelerates the translation of EEG foundation models into scalable, interpretable, and online actionable solutions.
<div id='section'>Paperid: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2508.09886.pdf' target='_blank'>https://arxiv.org/pdf/2508.09886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyu Chen, Yawen Zeng, Yue Wang, Peng Wan, Guo-chen Ning, Hongen Liao, Daoqiang Zhang, Fang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09886">COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional single-dataset training often fails with new data distributions, especially in ultrasound (US) image analysis due to limited data, acoustic shadows, and speckle noise. Therefore, constructing a universal framework for multi-heterogeneous US datasets is imperative. However, a key challenge arises: how to effectively mitigate inter-dataset interference while preserving dataset-specific discriminative features for robust downstream task? Previous approaches utilize either a single source-specific decoder or a domain adaptation strategy, but these methods experienced a decline in performance when applied to other domains. Considering this, we propose a Universal Collaborative Mixture of Heterogeneous Source-Specific Experts (COME). Specifically, COME establishes dual structure-semantic shared experts that create a universal representation space and then collaborate with source-specific experts to extract discriminative features through providing complementary features. This design enables robust generalization by leveraging cross-datasets experience distributions and providing universal US priors for small-batch or unseen data scenarios. Extensive experiments under three evaluation modes (single-dataset, intra-organ, and inter-organ integration datasets) demonstrate COME's superiority, achieving significant mean AP improvements over state-of-the-art methods. Our project is available at: https://universalcome.github.io/UniversalCOME/.
<div id='section'>Paperid: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2508.07950.pdf' target='_blank'>https://arxiv.org/pdf/2508.07950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Shen, Wanqing Zhang, Kehan Li, Erwen Huang, Haitao Bi, Aiying Fan, Yiwen Shen, Hongmei Dong, Ji Zhang, Yuming Shao, Zengjia Liu, Xinshe Liu, Tao Li, Chunxia Yan, Shuanliang Fan, Di Wu, Jianhua Ma, Bin Cong, Zhenyuan Wang, Chunfeng Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07950">FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.
<div id='section'>Paperid: <span id='pid'>1498, <a href='https://arxiv.org/pdf/2508.03007.pdf' target='_blank'>https://arxiv.org/pdf/2508.03007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhui Li, Xiaojie Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03007">Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalized Semantic Segmentation (DGSS) aims to improve the generalization ability of models across unseen domains without access to target data during training. Recent advances in DGSS have increasingly exploited vision foundation models (VFMs) via parameter-efficient fine-tuning strategies. However, most existing approaches concentrate on global feature fine-tuning, while overlooking hierarchical adaptation across feature levels, which is crucial for precise dense prediction. In this paper, we propose Multi-Granularity Feature Calibration (MGFC), a novel framework that performs coarse-to-fine alignment of VFM features to enhance robustness under domain shifts. Specifically, MGFC first calibrates coarse-grained features to capture global contextual semantics and scene-level structure. Then, it refines medium-grained features by promoting category-level feature discriminability. Finally, fine-grained features are calibrated through high-frequency spatial detail enhancement. By performing hierarchical and granularity-aware calibration, MGFC effectively transfers the generalization strengths of VFMs to the domain-specific task of DGSS. Extensive experiments on benchmark datasets demonstrate that our method outperforms state-of-the-art DGSS approaches, highlighting the effectiveness of multi-granularity adaptation for the semantic segmentation task of domain generalization.
<div id='section'>Paperid: <span id='pid'>1499, <a href='https://arxiv.org/pdf/2507.20907.pdf' target='_blank'>https://arxiv.org/pdf/2507.20907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongun Ryu, Heon Song, Seungeun Lee, Soo Ick Cho, Jiwon Shin, Kyunghyun Paeng, Sérgio Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20907">SCORPION: Addressing Scanner-Induced Variability in Histopathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring reliable model performance across diverse domains is a critical challenge in computational pathology. A particular source of variability in Whole-Slide Images is introduced by differences in digital scanners, thus calling for better scanner generalization. This is critical for the real-world adoption of computational pathology, where the scanning devices may differ per institution or hospital, and the model should not be dependent on scanner-induced details, which can ultimately affect the patient's diagnosis and treatment planning. However, past efforts have primarily focused on standard domain generalization settings, evaluating on unseen scanners during training, without directly evaluating consistency across scanners for the same tissue. To overcome this limitation, we introduce SCORPION, a new dataset explicitly designed to evaluate model reliability under scanner variability. SCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding 2,400 spatially aligned patches. This scanner-paired design allows for the isolation of scanner-induced variability, enabling a rigorous evaluation of model consistency while controlling for differences in tissue composition. Furthermore, we propose SimCons, a flexible framework that combines augmentation-based domain generalization techniques with a consistency loss to explicitly address scanner generalization. We empirically show that SimCons improves model consistency on varying scanners without compromising task-specific performance. By releasing the SCORPION dataset and proposing SimCons, we provide the research community with a crucial resource for evaluating and improving model consistency across diverse scanners, setting a new standard for reliability testing.
<div id='section'>Paperid: <span id='pid'>1500, <a href='https://arxiv.org/pdf/2507.19881.pdf' target='_blank'>https://arxiv.org/pdf/2507.19881.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Lian, Jose L. GÃ³mez, Antonio M. LÃ³pez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19881">FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization has shown promising progress in image classification by enabling collaborative training across multiple clients without sharing raw data. However, its potential in the semantic segmentation of autonomous driving remains underexplored. In this paper, we propose FedS2R, the first one-shot federated domain generalization framework for synthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises two components: an inconsistency-driven data augmentation strategy that generates images for unstable classes, and a multi-client knowledge distillation scheme with feature fusion that distills a global model from multiple client models. Experiments on five real-world datasets, Cityscapes, BDD100K, Mapillary, IDD, and ACDC, show that the global model significantly outperforms individual client models and is only 2 mIoU points behind the model trained with simultaneous access to all client data. These results demonstrate the effectiveness of FedS2R in synthetic-to-real semantic segmentation for autonomous driving under federated learning
<div id='section'>Paperid: <span id='pid'>1501, <a href='https://arxiv.org/pdf/2507.16238.pdf' target='_blank'>https://arxiv.org/pdf/2507.16238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Xu, Chaoyue Ren, Wei Liu, Wenke Huang, Bin Yang, Zhixi Yu, Kui Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16238">Positive Style Accumulation: A Style Screening and Continuous Utilization Framework for Federated DG-ReID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Federated Domain Generalization for Person re-identification (FedDG-ReID) aims to learn a global server model that can be effectively generalized to source and target domains through distributed source domain data. Existing methods mainly improve the diversity of samples through style transformation, which to some extent enhances the generalization performance of the model. However, we discover that not all styles contribute to the generalization performance. Therefore, we define styles that are beneficial or harmful to the model's generalization performance as positive or negative styles. Based on this, new issues arise: How to effectively screen and continuously utilize the positive styles. To solve these problems, we propose a Style Screening and Continuous Utilization (SSCU) framework. Firstly, we design a Generalization Gain-guided Dynamic Style Memory (GGDSM) for each client model to screen and accumulate generated positive styles. Meanwhile, we propose a style memory recognition loss to fully leverage the positive styles memorized by Memory. Furthermore, we propose a Collaborative Style Training (CST) strategy to make full use of positive styles. Unlike traditional learning strategies, our approach leverages both newly generated styles and the accumulated positive styles stored in memory to train client models on two distinct branches. This training strategy is designed to effectively promote the rapid acquisition of new styles by the client models, and guarantees the continuous and thorough utilization of positive styles, which is highly beneficial for the model's generalization performance. Extensive experimental results demonstrate that our method outperforms existing methods in both the source domain and the target domain.
<div id='section'>Paperid: <span id='pid'>1502, <a href='https://arxiv.org/pdf/2507.12630.pdf' target='_blank'>https://arxiv.org/pdf/2507.12630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianxin Luan, John Thompson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12630">Achieving Robust Channel Estimation Neural Networks by Designed Training Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Channel estimation is crucial in wireless communications. However, in many papers neural networks are frequently tested by training and testing on one example channel or similar channels. This is because data-driven methods often degrade on new data which they are not trained on, as they cannot extrapolate their training knowledge. This is despite the fact physical channels are often assumed to be time-variant. However, due to the low latency requirements and limited computing resources, neural networks may not have enough time and computing resources to execute online training to fine-tune the parameters. This motivates us to design offline-trained neural networks that can perform robustly over wireless channels, but without any actual channel information being known at design time. In this paper, we propose design criteria to generate synthetic training datasets for neural networks, which guarantee that after training the resulting networks achieve a certain mean squared error (MSE) on new and previously unseen channels. Therefore, trained neural networks require no prior channel information or parameters update for real-world implementations. Based on the proposed design criteria, we further propose a benchmark design which ensures intelligent operation for different channel profiles. To demonstrate general applicability, we use neural networks with different levels of complexity to show that the generalization achieved appears to be independent of neural network architecture. From simulations, neural networks achieve robust generalization to wireless channels with both fixed channel profiles and variable delay spreads.
<div id='section'>Paperid: <span id='pid'>1503, <a href='https://arxiv.org/pdf/2507.05302.pdf' target='_blank'>https://arxiv.org/pdf/2507.05302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binjia Zhou, Hengrui Lou, Lizhe Chen, Haoyuan Li, Dawei Luo, Shuai Chen, Jie Lei, Zunlei Feng, Yijun Bei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05302">CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake detection.Existing techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of hallucinations.To address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias reduction.Experimental results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1504, <a href='https://arxiv.org/pdf/2507.04048.pdf' target='_blank'>https://arxiv.org/pdf/2507.04048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Shi, Yanfu Zhang, Ye Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04048">CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech Emotion Recognition (SER) is fundamental to affective computing and human-computer interaction, yet existing models struggle to generalize across diverse acoustic conditions. While Contrastive Language-Audio Pretraining (CLAP) provides strong multimodal alignment, it lacks dedicated mechanisms for capturing emotional cues, making it suboptimal for SER. To address this, we propose CLEP-DG, a framework that enhances CLAP's robustness in emotion recognition. First, we fine-tune CLAP to obtain CLEP, adapting it on large-scale emotional speech datasets to better encode emotion-relevant features. Then, we introduce Acoustic Context Prompt Tuning (ACPT), a text-driven augmentation strategy that optimizes learnable prompt vectors to model diverse acoustic environments without additional labeled audio. Finally, leveraging cross-modal transferability, we train a classifier on text-derived embeddings and apply it to the audio encoder during inference, mitigating domain shifts between textual supervision and audio-based emotion recognition. Experiments across five benchmark datasets show that CLEP-DG outperforms prior CLAP-based approaches, achieving state-of-the-art performance in both supervised and domain generalization settings.
<div id='section'>Paperid: <span id='pid'>1505, <a href='https://arxiv.org/pdf/2507.03898.pdf' target='_blank'>https://arxiv.org/pdf/2507.03898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Xiong, Lei Zhang, Shuoyuan Wang, Dongzhou Cheng, Wenbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03898">Deconfounding Causal Inference through Two-Branch Framework with Early-Forking for Sensor-Based Cross-Domain Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, domain generalization (DG) has emerged as a promising solution to mitigate distribution-shift issue in sensor-based human activity recognition (HAR) scenario. However, most existing DG-based works have merely focused on modeling statistical dependence between sensor data and activity labels, neglecting the importance of intrinsic casual mechanism. Intuitively, every sensor input can be viewed as a mixture of causal (category-aware) and non-causal factors (domain-specific), where only the former affects activity classification judgment. In this paper, by casting such DG-based HAR as a casual inference problem, we propose a causality-inspired representation learning algorithm for cross-domain activity recognition. To this end, an early-forking two-branch framework is designed, where two separate branches are respectively responsible for learning casual and non-causal features, while an independence-based Hilbert-Schmidt Information Criterion is employed to implicitly disentangling them. Additionally, an inhomogeneous domain sampling strategy is designed to enhance disentanglement, while a category-aware domain perturbation layer is performed to prevent representation collapse. Extensive experiments on several public HAR benchmarks demonstrate that our causality-inspired approach significantly outperforms eleven related state-of-the-art baselines under cross-person, cross-dataset, and cross-position settings. Detailed ablation and visualizations analyses reveal underlying casual mechanism, indicating its effectiveness, efficiency, and universality in cross-domain activity recognition scenario.
<div id='section'>Paperid: <span id='pid'>1506, <a href='https://arxiv.org/pdf/2506.07378.pdf' target='_blank'>https://arxiv.org/pdf/2506.07378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuen Chen, Haozhe Si, Guojun Zhang, Han Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07378">Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) seeks to develop models that generalize well to unseen target domains, addressing the prevalent issue of distribution shifts in real-world applications. One line of research in DG focuses on aligning domain-level gradients and Hessians to enhance generalization. However, existing methods are computationally inefficient and the underlying principles of these approaches are not well understood. In this paper, we develop the theory of moment alignment for DG. Grounded in \textit{transfer measure}, a principled framework for quantifying generalizability between two domains, we first extend the definition of transfer measure to domain generalization that includes multiple source domains and establish a target error bound. Then, we prove that aligning derivatives across domains improves transfer measure both when the feature extractor induces an invariant optimal predictor across domains and when it does not. Notably, moment alignment provides a unifying understanding of Invariant Risk Minimization, gradient matching, and Hessian matching, three previously disconnected approaches to DG. We further connect feature moments and derivatives of the classifier head, and establish the duality between feature learning and classifier fitting. Building upon our theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment (CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in closed-form. Our method overcomes the computational inefficiencies of existing gradient and Hessian-based techniques by eliminating the need for repeated backpropagation or sampling-based Hessian estimation. We validate the efficacy of our approach through two sets of experiments: linear probing and full fine-tuning. CMA demonstrates superior performance in both settings compared to Empirical Risk Minimization and state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>1507, <a href='https://arxiv.org/pdf/2505.22434.pdf' target='_blank'>https://arxiv.org/pdf/2505.22434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zobia Batool, Huseyin Ozkan, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22434">Distance Transform Guided Mixup for Alzheimer's Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Alzheimer's detection efforts aim to develop accurate models for early disease diagnosis. Significant advances have been achieved with convolutional neural networks and vision transformer based approaches. However, medical datasets suffer heavily from class imbalance, variations in imaging protocols, and limited dataset diversity, which hinder model generalization. To overcome these challenges, this study focuses on single-domain generalization by extending the well-known mixup method. The key idea is to compute the distance transform of MRI scans, separate them spatially into multiple layers and then combine layers stemming from distinct samples to produce augmented images. The proposed approach generates diverse data while preserving the brain's structure. Experimental results show generalization performance improvement across both ADNI and AIBL datasets.
<div id='section'>Paperid: <span id='pid'>1508, <a href='https://arxiv.org/pdf/2505.21780.pdf' target='_blank'>https://arxiv.org/pdf/2505.21780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Justin Dauwels, Yilun Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21780">Compositional Scene Understanding through Inverse Generative Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have demonstrated remarkable abilities in generating high-fidelity visual content. In this work, we explore how generative models can further be used not only to synthesize visual content but also to understand the properties of a scene given a natural image. We formulate scene understanding as an inverse generative modeling problem, where we seek to find conditional parameters of a visual generative model to best fit a given natural image. To enable this procedure to infer scene structure from images substantially different than those seen during training, we further propose to build this visual generative model compositionally from smaller models over pieces of a scene. We illustrate how this procedure enables us to infer the set of objects in a scene, enabling robust generalization to new test scenes with an increased number of objects of new shapes. We further illustrate how this enables us to infer global scene factors, likewise enabling robust generalization to new scenes. Finally, we illustrate how this approach can be directly applied to existing pretrained text-to-image generative models for zero-shot multi-object perception. Code and visualizations are at https://energy-based-model.github.io/compositional-inference.
<div id='section'>Paperid: <span id='pid'>1509, <a href='https://arxiv.org/pdf/2505.21581.pdf' target='_blank'>https://arxiv.org/pdf/2505.21581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhennan Wang, Jianing Teng, Canqun Xiang, Kangliang Chen, Xing Pan, Lu Deng, Weihao Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21581">CogAD: Cognitive-Hierarchy Guided End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While end-to-end autonomous driving has advanced significantly, prevailing methods remain fundamentally misaligned with human cognitive principles in both perception and planning. In this paper, we propose CogAD, a novel end-to-end autonomous driving model that emulates the hierarchical cognition mechanisms of human drivers. CogAD implements dual hierarchical mechanisms: global-to-local context processing for human-like perception and intent-conditioned multi-mode trajectory generation for cognitively-inspired planning. The proposed method demonstrates three principal advantages: comprehensive environmental understanding through hierarchical perception, robust planning exploration enabled by multi-level planning, and diverse yet reasonable multi-modal trajectory generation facilitated by dual-level uncertainty modeling. Extensive experiments on nuScenes and Bench2Drive demonstrate that CogAD achieves state-of-the-art performance in end-to-end planning, exhibiting particular superiority in long-tail scenarios and robust generalization to complex real-world driving conditions.
<div id='section'>Paperid: <span id='pid'>1510, <a href='https://arxiv.org/pdf/2505.21156.pdf' target='_blank'>https://arxiv.org/pdf/2505.21156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saisamarth Rajesh Phaye, Milos Cernak, Andrew Harper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21156">Model as Loss: A Self-Consistent Training Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional methods for speech enhancement rely on handcrafted loss functions (e.g., time or frequency domain losses) or deep feature losses (e.g., using WavLM or wav2vec), which often fail to capture subtle signal properties essential for optimal performance. To address this, we propose Model as Loss, a novel training paradigm that utilizes the encoder from the same model as a loss function to guide the training.
  The Model as Loss paradigm leverages the encoder's task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal. By using the encoder's learned features as a loss function, this framework enforces self-consistency between the clean reference speech and the enhanced model output. Our approach outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets.
<div id='section'>Paperid: <span id='pid'>1511, <a href='https://arxiv.org/pdf/2505.20235.pdf' target='_blank'>https://arxiv.org/pdf/2505.20235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Wenger, Beau Coker, Juraj Marusic, John P. Cunningham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20235">Variational Deep Learning via Implicit Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern deep learning models generalize remarkably well in-distribution, despite being overparametrized and trained with little to no explicit regularization. Instead, current theory credits implicit regularization imposed by the choice of architecture, hyperparameters and optimization procedure. However, deep neural networks can be surprisingly non-robust, resulting in overconfident predictions and poor out-of-distribution generalization. Bayesian deep learning addresses this via model averaging, but typically requires significant computational resources as well as carefully elicited priors to avoid overriding the benefits of implicit regularization. Instead, in this work, we propose to regularize variational neural networks solely by relying on the implicit bias of (stochastic) gradient descent. We theoretically characterize this inductive bias in overparametrized linear models as generalized variational inference and demonstrate the importance of the choice of parametrization. Empirically, our approach demonstrates strong in- and out-of-distribution performance without additional hyperparameter tuning and with minimal computational overhead.
<div id='section'>Paperid: <span id='pid'>1512, <a href='https://arxiv.org/pdf/2505.16778.pdf' target='_blank'>https://arxiv.org/pdf/2505.16778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianing Chen, Si Huo, Borui Jiang, Hailin Hu, Xinghao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16778">Single Domain Generalization for Few-Shot Counting via Universal Representation Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot counting estimates the number of target objects in an image using only a few annotated exemplars. However, domain shift severely hinders existing methods to generalize to unseen scenarios. This falls into the realm of single domain generalization that remains unexplored in few-shot counting. To solve this problem, we begin by analyzing the main limitations of current methods, which typically follow a standard pipeline that extract the object prototypes from exemplars and then match them with image feature to construct the correlation map. We argue that existing methods overlook the significance of learning highly generalized prototypes. Building on this insight, we propose the first single domain generalization few-shot counting model, Universal Representation Matching, termed URM. Our primary contribution is the discovery that incorporating universal vision-language representations distilled from a large scale pretrained vision-language model into the correlation construction process substantially improves robustness to domain shifts without compromising in domain performance. As a result, URM achieves state-of-the-art performance on both in domain and the newly introduced domain generalization setting.
<div id='section'>Paperid: <span id='pid'>1513, <a href='https://arxiv.org/pdf/2505.15191.pdf' target='_blank'>https://arxiv.org/pdf/2505.15191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hana Satou, Alan Mitkiy, F Monkey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15191">Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning under domain shift remains a fundamental challenge due to the divergence between source and target data manifolds. In this paper, we propose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework that decomposes adversarial perturbations into on-manifold and off-manifold components to simultaneously capture semantic variation and model brittleness. We theoretically demonstrate that enforcing on-manifold consistency reduces hypothesis complexity and improves generalization, while off-manifold regularization smooths decision boundaries in low-density regions. Moreover, we introduce a geometry-aware alignment loss that minimizes geodesic discrepancy between source and target manifolds. Experiments on DomainNet, VisDA, and Office-Home show that MAADA consistently outperforms existing adversarial and adaptation methods in both unsupervised and few-shot settings, demonstrating superior structural robustness and cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>1514, <a href='https://arxiv.org/pdf/2505.12681.pdf' target='_blank'>https://arxiv.org/pdf/2505.12681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hana Satou, Alan Mitkiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12681">On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning across domains with distribution shift remains a fundamental challenge in building robust and adaptable machine learning systems. While adversarial perturbations are traditionally viewed as threats that expose model vulnerabilities, recent studies suggest that they can also serve as constructive tools for data augmentation. In this work, we systematically investigate the role of adversarial data augmentation (ADA) in enhancing both robustness and adaptivity in transfer learning settings. We analyze how adversarial examples, when used strategically during training, improve domain generalization by enriching decision boundaries and reducing overfitting to source-domain-specific features. We further propose a unified framework that integrates ADA with consistency regularization and domain-invariant representation learning. Extensive experiments across multiple benchmark datasets -- including VisDA, DomainNet, and Office-Home -- demonstrate that our method consistently improves target-domain performance under both unsupervised and few-shot domain adaptation settings. Our results highlight a constructive perspective of adversarial learning, transforming perturbation from a destructive attack into a regularizing force for cross-domain transferability.
<div id='section'>Paperid: <span id='pid'>1515, <a href='https://arxiv.org/pdf/2505.08426.pdf' target='_blank'>https://arxiv.org/pdf/2505.08426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franko Å ikiÄ, Donik VrÅ¡nak, Sven LonÄariÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08426">DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unconstrained gaze estimation is the process of determining where a subject is directing their visual attention in uncontrolled environments. Gaze estimation systems are important for a myriad of tasks such as driver distraction monitoring, exam proctoring, accessibility features in modern software, etc. However, these systems face challenges in real-world scenarios, partially due to the low resolution of in-the-wild images and partially due to insufficient modeling of head-eye interactions in current state-of-the-art (SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based method that advances gaze prediction through super-resolution (SR) and a dual head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone processes eye and multiscale SR head images, while the proposed DHECA module enables bidirectional feature refinement between the extracted visual features through cross-attention mechanisms. Furthermore, we identified critical annotation errors in one of the most diverse and widely used gaze estimation datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on Gaze360 and GFIE datasets demonstrates superior within-dataset performance of the proposed method, reducing angular error (AE) by 0.48Â° (Gaze360) and 2.95Â° (GFIE) in static configurations, and 0.59Â° (Gaze360) and 3.00Â° (GFIE) in temporal settings compared to prior SOTA methods. Cross-dataset testing shows improvements in AE of more than 1.53Â° (Gaze360) and 3.99Â° (GFIE) in both static and temporal settings, validating the robust generalization properties of our approach.
<div id='section'>Paperid: <span id='pid'>1516, <a href='https://arxiv.org/pdf/2505.06894.pdf' target='_blank'>https://arxiv.org/pdf/2505.06894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Qazi, Abdul Basit, Asim Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06894">NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts the domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures and cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows improved performance on benchmarks on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.
<div id='section'>Paperid: <span id='pid'>1517, <a href='https://arxiv.org/pdf/2505.06886.pdf' target='_blank'>https://arxiv.org/pdf/2505.06886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Qazi, Hamd Jalil, Asim Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06886">Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The mouse is one of the most studied animal models in the field of systems neuroscience. Understanding the generalized patterns and decoding the neural representations that are evoked by the diverse range of natural scene stimuli in the mouse visual cortex is one of the key quests in computational vision. In recent years, significant parallels have been drawn between the primate visual cortex and hierarchical deep neural networks. However, their generalized efficacy in understanding mouse vision has been limited. In this study, we investigate the functional alignment between the mouse visual cortex and deep learning models for object classification tasks. We first introduce a generalized representational learning strategy that uncovers a striking resemblance between the functional mapping of the mouse visual cortex and high-performing deep learning models on both top-down (population-level) and bottom-up (single cell-level) scenarios. Next, this representational similarity across the two systems is further enhanced by the addition of Neural Response Normalization (NeuRN) layer, inspired by the activation profile of excitatory and inhibitory neurons in the visual cortex. To test the performance effect of NeuRN on real-world tasks, we integrate it into deep learning models and observe significant improvements in their robustness against data shifts in domain generalization tasks. Our work proposes a novel framework for comparing the functional architecture of the mouse visual cortex with deep learning models. Our findings carry broad implications for the development of advanced AI models that draw inspiration from the mouse visual cortex, suggesting that these models serve as valuable tools for studying the neural representations of the mouse visual cortex and, as a result, enhancing their performance on real-world tasks.
<div id='section'>Paperid: <span id='pid'>1518, <a href='https://arxiv.org/pdf/2505.06881.pdf' target='_blank'>https://arxiv.org/pdf/2505.06881.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamd Jalil, Ahmed Qazi, Asim Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06881">NeuRN: Neuro-inspired Domain Generalization for Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization in image classification is a crucial challenge, with models often failing to generalize well across unseen datasets. We address this issue by introducing a neuro-inspired Neural Response Normalization (NeuRN) layer which draws inspiration from neurons in the mammalian visual cortex, which aims to enhance the performance of deep learning architectures on unseen target domains by training deep learning models on a source domain. The performance of these models is considered as a baseline and then compared against models integrated with NeuRN on image classification tasks. We perform experiments across a range of deep learning architectures, including ones derived from Neural Architecture Search and Vision Transformer. Additionally, in order to shortlist models for our experiment from amongst the vast range of deep neural networks available which have shown promising results, we also propose a novel method that uses the Needleman-Wunsch algorithm to compute similarity between deep learning architectures. Our results demonstrate the effectiveness of NeuRN by showing improvement against baseline in cross-domain image classification tasks. Our framework attempts to establish a foundation for future neuro-inspired deep learning models.
<div id='section'>Paperid: <span id='pid'>1519, <a href='https://arxiv.org/pdf/2505.01168.pdf' target='_blank'>https://arxiv.org/pdf/2505.01168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyang Ma, Zhihao Wu, Wang Lu, Xin Gao, Jinghang Yue, Taolin Zhang, Lipo Wang, Youfang Lin, Jing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01168">Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of model ensemble attacks has significantly improved the transferability of adversarial examples, but this progress also poses severe threats to the security of deep neural networks. Existing methods, however, face two critical challenges: insufficient capture of shared gradient directions across models and a lack of adaptive weight allocation mechanisms. To address these issues, we propose a novel method Harmonized Ensemble for Adversarial Transferability (HEAT), which introduces domain generalization into adversarial example generation for the first time. HEAT consists of two key modules: Consensus Gradient Direction Synthesizer, which uses Singular Value Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight Orchestrator which dynamically balances intra-domain coherence, stabilizing gradients within individual models, and inter-domain diversity, enhancing transferability across models. Experimental results demonstrate that HEAT significantly outperforms existing methods across various datasets and settings, offering a new perspective and direction for adversarial attack research.
<div id='section'>Paperid: <span id='pid'>1520, <a href='https://arxiv.org/pdf/2504.21385.pdf' target='_blank'>https://arxiv.org/pdf/2504.21385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijun Zhou, Baojie Fan, Jiandong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21385">Physics-Guided Image Dehazing Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the domain gap between real-world and synthetic hazy images, current data-driven dehazing algorithms trained on synthetic datasets perform well on synthetic data but struggle to generalize to real-world scenarios. To address this challenge, we propose \textbf{I}mage \textbf{D}ehazing \textbf{D}iffusion \textbf{M}odels (IDDM), a novel diffusion process that incorporates the atmospheric scattering model into noise diffusion. IDDM aims to use the gradual haze formation process to help the denoising Unet robustly learn the distribution of clear images from the conditional input hazy images. We design a specialized training strategy centered around IDDM. Diffusion models are leveraged to bridge the domain gap from synthetic to real-world, while the atmospheric scattering model provides physical guidance for haze formation. During the forward process, IDDM simultaneously introduces haze and noise into clear images, and then robustly separates them during the sampling process. By training with physics-guided information, IDDM shows the ability of domain generalization, and effectively restores the real-world hazy images despite being trained on synthetic datasets. Extensive experiments demonstrate the effectiveness of our method through both quantitative and qualitative comparisons with state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>1521, <a href='https://arxiv.org/pdf/2504.15900.pdf' target='_blank'>https://arxiv.org/pdf/2504.15900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15900">SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to "think before answering." Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding.
<div id='section'>Paperid: <span id='pid'>1522, <a href='https://arxiv.org/pdf/2504.14373.pdf' target='_blank'>https://arxiv.org/pdf/2504.14373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Zhuo Su, Jian Wang, Shuang Li, Xu Chang, Zhaohu Li, Yang Zhao, Guidong Wang, Ruqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14373">SEGA: Drivable 3D Gaussian Head Avatar from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.
<div id='section'>Paperid: <span id='pid'>1523, <a href='https://arxiv.org/pdf/2504.10001.pdf' target='_blank'>https://arxiv.org/pdf/2504.10001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlin Hao, Peiheng Wang, Haoyang Wang, Xinggong Zhang, Zongming Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10001">GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes.
<div id='section'>Paperid: <span id='pid'>1524, <a href='https://arxiv.org/pdf/2504.02996.pdf' target='_blank'>https://arxiv.org/pdf/2504.02996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Wang, Aoming Liu, Bryan A. Plummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02996">Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-source Domain Generalization (DG) aims to improve model robustness to new distributions. However, DG methods often overlook the effect of label noise, which can confuse a model during training, reducing performance. Limited prior work has analyzed DG method's noise-robustness, typically focused on an analysis of existing methods rather than new solutions. In this paper, we investigate this underexplored space, where models are evaluated under both distribution shifts and label noise, which we refer to as Noise-Aware Generalization (NAG). A natural solution to address label noise would be to combine a Learning with Noisy Labels (LNL) method with those from DG. Many LNL methods aim to detect distribution shifts in a class's samples, i.e., they assume that distribution shifts often correspond to label noise. However, in NAG distribution shifts can be due to label noise or domain shifts, breaking the assumptions used by LNL methods. A naive solution is to make a similar assumption made by many DG methods, where we presume to have domain labels during training, enabling us to isolate the two types of shifts. However, this ignores valuable cross-domain information. Specifically, our proposed DL4ND approach improves noise detection by taking advantage of the observation that noisy samples that may appear indistinguishable within a single domain often show greater variation when compared across domains. Experiments show that DL4ND significantly improves performance across four diverse datasets, offering a promising direction for tackling NAG.
<div id='section'>Paperid: <span id='pid'>1525, <a href='https://arxiv.org/pdf/2503.22936.pdf' target='_blank'>https://arxiv.org/pdf/2503.22936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei-Kai Huanga, Jun-Xiong Chong, Ming-Tsung Hsu, Fang-Yu Hsu, Chiou-Ting Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22936">Enhancing Learnable Descriptive Convolutional Vision Transformer for Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face anti-spoofing (FAS) heavily relies on identifying live/spoof discriminative features to counter face presentation attacks. Recently, we proposed LDCformer to successfully incorporate the Learnable Descriptive Convolution (LDC) into ViT, to model long-range dependency of locally descriptive features for FAS. In this paper, we propose three novel training strategies to effectively enhance the training of LDCformer to largely boost its feature characterization capability. The first strategy, dual-attention supervision, is developed to learn fine-grained liveness features guided by regional live/spoof attentions. The second strategy, self-challenging supervision, is designed to enhance the discriminability of the features by generating challenging training data. In addition, we propose a third training strategy, transitional triplet mining strategy, through narrowing the cross-domain gap while maintaining the transitional relationship between live and spoof features, to enlarge the domain-generalization capability of LDCformer. Extensive experiments show that LDCformer under joint supervision of the three novel training strategies outperforms previous methods.
<div id='section'>Paperid: <span id='pid'>1526, <a href='https://arxiv.org/pdf/2503.22862.pdf' target='_blank'>https://arxiv.org/pdf/2503.22862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumitri Chattopadhyay, Basar Demir, Marc Niethammer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22862">Zero-shot Domain Generalization of Foundational Models for 3D Medical Image Segmentation: An Experimental Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift, caused by variations in imaging modalities and acquisition protocols, limits model generalization in medical image segmentation. While foundation models (FMs) trained on diverse large-scale data hold promise for zero-shot generalization, their application to volumetric medical data remains underexplored. In this study, we examine their ability towards domain generalization (DG), by conducting a comprehensive experimental study encompassing 6 medical segmentation FMs and 12 public datasets spanning multiple modalities and anatomies. Our findings reveal the potential of promptable FMs in bridging the domain gap via smart prompting techniques. Additionally, by probing into multiple facets of zero-shot DG, we offer valuable insights into the viability of FMs for DG and identify promising avenues for future research.
<div id='section'>Paperid: <span id='pid'>1527, <a href='https://arxiv.org/pdf/2503.18114.pdf' target='_blank'>https://arxiv.org/pdf/2503.18114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-Ning Chou, Hang Le, Yichen Wang, SueYeon Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18114">Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating task-relevant information into neural representations is a fundamental ability of both biological and artificial intelligence systems. Recent theories have categorized learning into two regimes: the rich regime, where neural networks actively learn task-relevant features, and the lazy regime, where networks behave like random feature models. Yet this simple lazy-rich dichotomy overlooks a diverse underlying taxonomy of feature learning, shaped by differences in learning algorithms, network architectures, and data properties. To address this gap, we introduce an analysis framework to study feature learning via the geometry of neural representations. Rather than inspecting individual learned features, we characterize how task-relevant representational manifolds evolve throughout the learning process. We show, in both theoretical and empirical settings, that as networks learn features, task-relevant manifolds untangle, with changes in manifold geometry revealing distinct learning stages and strategies beyond the lazy-rich dichotomy. This framework provides novel insights into feature learning across neuroscience and machine learning, shedding light on structural inductive biases in neural circuits and the mechanisms underlying out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1528, <a href='https://arxiv.org/pdf/2503.15910.pdf' target='_blank'>https://arxiv.org/pdf/2503.15910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junsung Park, Hwijeong Lee, Inha Kang, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15910">No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing domain generalization methods for LiDAR semantic segmentation under adverse weather struggle to accurately predict "things" categories compared to "stuff" categories. In typical driving scenes, "things" categories can be dynamic and associated with higher collision risks, making them crucial for safe navigation and planning. Recognizing the importance of "things" categories, we identify their performance drop as a serious bottleneck in existing approaches. We observed that adverse weather induces degradation of semantic-level features and both corruption of local features, leading to a misprediction of "things" as "stuff". To mitigate these corruptions, we suggest our method, NTN - segmeNt Things for No-accident. To address semantic-level feature corruption, we bind each point feature to its superclass, preventing the misprediction of things classes into visually dissimilar categories. Additionally, to enhance robustness against local corruption caused by adverse weather, we define each LiDAR beam as a local region and propose a regularization term that aligns the clean data with its corrupted counterpart in feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU gain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the SemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9 mIoU improvement on "things" classes, respectively, highlighting its effectiveness.
<div id='section'>Paperid: <span id='pid'>1529, <a href='https://arxiv.org/pdf/2503.14897.pdf' target='_blank'>https://arxiv.org/pdf/2503.14897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaibhav Rathore, Shubhranil B, Saikat Dutta, Sarthak Mehrotra, Zsolt Kira, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14897">When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalized Class Discovery (GCD) clusters base and novel classes in a target domain using supervision from a source domain with only base classes. Current methods often falter with distribution shifts and typically require access to target data during training, which can sometimes be impractical. To address this issue, we introduce the novel paradigm of Domain Generalization in GCD (DG-GCD), where only source data is available for training, while the target domain, with a distinct data distribution, remains unseen until inference. To this end, our solution, DG2CD-Net, aims to construct a domain-independent, discriminative embedding space for GCD. The core innovation is an episodic training strategy that enhances cross-domain generalization by adapting a base model on tasks derived from source and synthetic domains generated by a foundation model. Each episode focuses on a cross-domain GCD task, diversifying task setups over episodes and combining open-set domain adaptation with a novel margin loss and representation learning for optimizing the feature space progressively. To capture the effects of fine-tuning on the base model, we extend task arithmetic by adaptively weighting the local task vectors concerning the fine-tuned models based on their GCD performance on a validation distribution. This episodic update mechanism boosts the adaptability of the base model to unseen targets. Experiments across three datasets confirm that DG2CD-Net outperforms existing GCD methods customized for DG-GCD.
<div id='section'>Paperid: <span id='pid'>1530, <a href='https://arxiv.org/pdf/2503.14321.pdf' target='_blank'>https://arxiv.org/pdf/2503.14321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AdriÃ¡n Javaloy, Antonio Vergari, Isabel Valera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14321">COPA: Comparing the incomparable in multi-objective model evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning (ML) practitioners, we often have hundreds of (trained) ML models at hand from which we need to choose one, based on various objectives such as accuracy, robustness, fairness, scalability, etc. However, how to compare, aggregate and, ultimately, trade-off these objectives is usually a time-consuming task that requires of expert knowledge, as they may be measured in different units or scales. In this work, we investigate how objectives can be automatically normalized and aggregated to systematically navigate their Pareto front. To do so, we make incomparable objectives comparable using their CDFs, approximated by their relative rankings. As a result, we can aggregate them while matching user-specific preferences, allowing practitioners to meaningfully navigate and search for models in the Pareto front. We demonstrate the potential impact of our approach, COPA, in both model selection and benchmarking tasks across diverse ML areas such as fair ML, domain generalization, AutoML and foundation models, where classical ways to normalize and aggregate objectives fall short.
<div id='section'>Paperid: <span id='pid'>1531, <a href='https://arxiv.org/pdf/2503.12678.pdf' target='_blank'>https://arxiv.org/pdf/2503.12678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Partho Ghosh, Raisa Bentay Hossain, Mohammad Zunaed, Taufiq Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12678">Domain Generalization for Improved Human Activity Recognition in Office Space Videos Using Adaptive Pre-processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic video activity recognition is crucial across numerous domains like surveillance, healthcare, and robotics. However, recognizing human activities from video data becomes challenging when training and test data stem from diverse domains. Domain generalization, adapting to unforeseen domains, is thus essential. This paper focuses on office activity recognition amidst environmental variability. We propose three pre-processing techniques applicable to any video encoder, enhancing robustness against environmental variations. Our study showcases the efficacy of MViT, a leading state-of-the-art video classification model, and other video encoders combined with our techniques, outperforming state-of-the-art domain adaptation methods. Our approach significantly boosts accuracy, precision, recall and F1 score on unseen domains, emphasizing its adaptability in real-world scenarios with diverse video data sources. This method lays a foundation for more reliable video activity recognition systems across heterogeneous data domains.
<div id='section'>Paperid: <span id='pid'>1532, <a href='https://arxiv.org/pdf/2503.10435.pdf' target='_blank'>https://arxiv.org/pdf/2503.10435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Wilkinghoff, Takuya Fujimura, Keisuke Imoto, Jonathan Le Roux, Zheng-Hua Tan, Tomoki Toda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10435">Handling Domain Shifts for Anomalous Sound Detection: A Review of DCASE-Related Work</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When detecting anomalous sounds in complex environments, one of the main difficulties is that trained models must be sensitive to subtle differences in monitored target signals, while many practical applications also require them to be insensitive to changes in acoustic domains. Examples of such domain shifts include changing the type of microphone or the location of acoustic sensors, which can have a much stronger impact on the acoustic signal than subtle anomalies themselves. Moreover, users typically aim to train a model only on source domain data, which they may have a relatively large collection of, and they hope that such a trained model will be able to generalize well to an unseen target domain by providing only a minimal number of samples to characterize the acoustic signals in that domain. In this work, we review and discuss recent publications focusing on this domain generalization problem for anomalous sound detection in the context of the DCASE challenges on acoustic machine condition monitoring.
<div id='section'>Paperid: <span id='pid'>1533, <a href='https://arxiv.org/pdf/2503.07906.pdf' target='_blank'>https://arxiv.org/pdf/2503.07906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinghao Ye, Xianhan Zeng, Fu Li, Chunyuan Li, Haoqi Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07906">Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated evaluation metrics and coarse annotations. In this paper, we introduce DeCapBench along with a novel metric, DCScore, specifically designed for detailed captioning tasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by deconstructing responses into the smallest self-sufficient units, termed primitive information units, and assessing them individually. Our evaluation shows that DCScore aligns more closely with human judgment than other rule-based or model-based metrics. Concurrently, DeCapBench exhibits a high correlation with VLM arena results on descriptive tasks, surpassing existing benchmarks for vision-language models. Additionally, we present an automatic fine-grained feedback collection method, FeedQuill, for preference optimization based on our advanced metric, showing robust generalization capabilities across auto-generated preference data. Extensive experiments on multiple VLMs demonstrate that our method not only significantly reduces hallucinations but also enhances performance across various benchmarks, achieving superior detail captioning performance while surpassing GPT-4o.
<div id='section'>Paperid: <span id='pid'>1534, <a href='https://arxiv.org/pdf/2503.06860.pdf' target='_blank'>https://arxiv.org/pdf/2503.06860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Gungor, Derek Eppinger, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06860">Towards Generalization of Tactile Image Generation: Reference-Free Evaluation in a Leakage-Free Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing, which relies on direct physical contact, is critical for human perception and underpins applications in computer vision, robotics, and multimodal learning. Because tactile data is often scarce and costly to acquire, generating synthetic tactile images provides a scalable solution to augment real-world measurements. However, ensuring robust generalization in synthesizing tactile images-capturing subtle, material-specific contact features-remains challenging. We demonstrate that overlapping training and test samples in commonly used datasets inflate performance metrics, obscuring the true generalizability of tactile models. To address this, we propose a leakage-free evaluation protocol coupled with novel, reference-free metrics-TMMD, I-TMMD, CI-TMMD, and D-TMMD-tailored for tactile generation. Moreover, we propose a vision-to-touch generation method that leverages text as an intermediate modality by incorporating concise, material-specific descriptions during training to better capture essential tactile features. Experiments on two popular visuo-tactile datasets, Touch and Go and HCT, show that our approach achieves superior performance and enhanced generalization in a leakage-free setting.
<div id='section'>Paperid: <span id='pid'>1535, <a href='https://arxiv.org/pdf/2503.06026.pdf' target='_blank'>https://arxiv.org/pdf/2503.06026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masaru Yajima, Kei Ota, Asako Kanezaki, Rei Kawakami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06026">Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving zero-shot peg insertion, where inserting an arbitrary peg into an unseen hole without task-specific training, remains a fundamental challenge in robotics. This task demands a highly generalizable perception system capable of detecting potential holes, selecting the correct mating hole from multiple candidates, estimating its precise pose, and executing insertion despite uncertainties. While learning-based methods have been applied to peg insertion, they often fail to generalize beyond the specific peg-hole pairs encountered during training. Recent advancements in Vision-Language Models (VLMs) offer a promising alternative, leveraging large-scale datasets to enable robust generalization across diverse tasks. Inspired by their success, we introduce a novel zero-shot peg insertion framework that utilizes a VLM to identify mating holes and estimate their poses without prior knowledge of their geometry. Extensive experiments demonstrate that our method achieves 90.2% accuracy, significantly outperforming baselines in identifying the correct mating hole across a wide range of previously unseen peg-hole pairs, including 3D-printed objects, toy puzzles, and industrial connectors. Furthermore, we validate the effectiveness of our approach in a real-world connector insertion task on a backpanel of a PC, where our system successfully detects holes, identifies the correct mating hole, estimates its pose, and completes the insertion with a success rate of 88.3%. These results highlight the potential of VLM-driven zero-shot reasoning for enabling robust and generalizable robotic assembly.
<div id='section'>Paperid: <span id='pid'>1536, <a href='https://arxiv.org/pdf/2503.04363.pdf' target='_blank'>https://arxiv.org/pdf/2503.04363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni De Felice, Arianna Casanova Flores, Francesco De Santis, Silvia Santini, Johannes Schneider, Pietro Barbiero, Alberto Termine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04363">Causally Reliable Concept Bottleneck Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Concept-based models are an emerging paradigm in deep learning that constrains the inference process to operate through human-interpretable variables, facilitating explainability and human interaction. However, these architectures, on par with popular opaque neural models, fail to account for the true causal mechanisms underlying the target phenomena represented in the data. This hampers their ability to support causal reasoning tasks, limits out-of-distribution generalization, and hinders the implementation of fairness constraints. To overcome these issues, we propose Causally reliable Concept Bottleneck Models (C$^2$BMs), a class of concept-based architectures that enforce reasoning through a bottleneck of concepts structured according to a model of the real-world causal mechanisms. We also introduce a pipeline to automatically learn this structure from observational data and unstructured background knowledge (e.g., scientific literature). Experimental evidence suggests that C$^2$BMs are more interpretable, causally reliable, and improve responsiveness to interventions w.r.t. standard opaque and concept-based models, while maintaining their accuracy.
<div id='section'>Paperid: <span id='pid'>1537, <a href='https://arxiv.org/pdf/2502.20499.pdf' target='_blank'>https://arxiv.org/pdf/2502.20499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felipe del Rio, Alain Raymond-Saez, Daniel Florea, Rodrigo Toro Icarte, Julio Hurtado, Cristian B. Calderon, Alvaro Soto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20499">Data Distributional Properties As Inductive Bias for Systematic Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) struggle at systematic generalization (SG). Several studies have evaluated the possibility to promote SG through the proposal of novel architectures, loss functions or training methodologies. Few studies, however, have focused on the role of training data properties in promoting SG. In this work, we investigate the impact of certain data distributional properties, as inductive biases for the SG ability of a multi-modal language model. To this end, we study three different properties. First, data diversity, instantiated as an increase in the possible values a latent property in the training distribution may take. Second, burstiness, where we probabilistically restrict the number of possible values of latent factors on particular inputs during training. Third, latent intervention, where a particular latent factor is altered randomly during training. We find that all three factors significantly enhance SG, with diversity contributing an 89% absolute increase in accuracy in the most affected property. Through a series of experiments, we test various hypotheses to understand why these properties promote SG. Finally, we find that Normalized Mutual Information (NMI) between latent attributes in the training distribution is strongly predictive of out-of-distribution generalization. We find that a mechanism by which lower NMI induces SG is in the geometry of representations. In particular, we find that NMI induces more parallelism in neural representations (i.e., input features coded in parallel neural vectors) of the model, a property related to the capacity of reasoning by analogy.
<div id='section'>Paperid: <span id='pid'>1538, <a href='https://arxiv.org/pdf/2502.13522.pdf' target='_blank'>https://arxiv.org/pdf/2502.13522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastien RÃ¶cken, Julija Zavadlav
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13522">Enhancing Machine Learning Potentials through Transfer Learning across Chemical Elements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine Learning Potentials (MLPs) can enable simulations of ab initio accuracy at orders of magnitude lower computational cost. However, their effectiveness hinges on the availability of considerable datasets to ensure robust generalization across chemical space and thermodynamic conditions. The generation of such datasets can be labor-intensive, highlighting the need for innovative methods to train MLPs in data-scarce scenarios. Here, we introduce transfer learning of potential energy surfaces between chemically similar elements. Specifically, we leverage the trained MLP for silicon to initialize and expedite the training of an MLP for germanium. Utilizing classical force field and ab initio datasets, we demonstrate that transfer learning surpasses traditional training from scratch in force prediction, leading to more stable simulations and improved temperature transferability. These advantages become even more pronounced as the training dataset size decreases. The out-of-target property analysis shows that transfer learning leads to beneficial but sometimes adversarial effects. Our findings demonstrate that transfer learning across chemical elements is a promising technique for developing accurate and numerically stable MLPs, particularly in a data-scarce regime.
<div id='section'>Paperid: <span id='pid'>1539, <a href='https://arxiv.org/pdf/2502.05726.pdf' target='_blank'>https://arxiv.org/pdf/2502.05726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jayden Teoh, Wenjun Li, Pradeep Varakantham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05726">Improving Environment Novelty Quantification for Effective Unsupervised Environment Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised Environment Design (UED) formalizes the problem of autocurricula through interactive training between a teacher agent and a student agent. The teacher generates new training environments with high learning potential, curating an adaptive curriculum that strengthens the student's ability to handle unseen scenarios. Existing UED methods mainly rely on regret, a metric that measures the difference between the agent's optimal and actual performance, to guide curriculum design. Regret-driven methods generate curricula that progressively increase environment complexity for the student but overlook environment novelty -- a critical element for enhancing an agent's generalizability. Measuring environment novelty is especially challenging due to the underspecified nature of environment parameters in UED, and existing approaches face significant limitations. To address this, this paper introduces the Coverage-based Evaluation of Novelty In Environment (CENIE) framework. CENIE proposes a scalable, domain-agnostic, and curriculum-aware approach to quantifying environment novelty by leveraging the student's state-action space coverage from previous curriculum experiences. We then propose an implementation of CENIE that models this coverage and measures environment novelty using Gaussian Mixture Models. By integrating both regret and novelty as complementary objectives for curriculum design, CENIE facilitates effective exploration across the state-action space while progressively increasing curriculum complexity. Empirical evaluations demonstrate that augmenting existing regret-based UED algorithms with CENIE achieves state-of-the-art performance across multiple benchmarks, underscoring the effectiveness of novelty-driven autocurricula for robust generalization.
<div id='section'>Paperid: <span id='pid'>1540, <a href='https://arxiv.org/pdf/2501.18864.pdf' target='_blank'>https://arxiv.org/pdf/2501.18864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aodi Li, Liansheng Zhuang, Xiao Long, Minghong Yao, Shafei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18864">Test-time Loss Landscape Adaptation for Zero-Shot Generalization in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time adaptation of pre-trained vision-language models has emerged as a technique for tackling distribution shifts during the test time. Although existing methods, especially those based on Test-time Prompt Tuning (TPT), have shown promising results, their high computational cost associated with parameter optimization presents challenges for scalability and practical application. This paper unveils the unnecessary nature of backpropagation in existing methods from a loss landscape perspective. Building on this insight, this paper proposes a simple yet effective framework called Test-time Loss Landscape Adaptation (TLLA). TLLA leverages the relative position between the training minimum and test loss landscapes to guide the adaptation process, avoiding the update of model parameters at test time. Specifically, it mainly consists of two main stages: In the prompt tuning stage, a Sharpness-Aware Prompt Tuning (SAPT) method is introduced to identify the training flat minimum, setting the foundation for the subsequent test-time adaptation; In the test stage, a Sharpness-based Test Sample Selection (STSS) approach is utilized to ensure the alignment of flat minima within the training loss landscape and each augmented test sample's loss landscape. Extensive experiments on both domain generalization and cross-dataset benchmarks demonstrate that TLLA achieves state-of-the-art performances while significantly reducing computational overhead. Notably, TLLA surpasses TPT by an average of 5.32\% and 6.98\% on four ImageNet variant datasets when employing ResNet50 and ViT-B/16 image encoders, respectively. The code will be available soon.
<div id='section'>Paperid: <span id='pid'>1541, <a href='https://arxiv.org/pdf/2501.18801.pdf' target='_blank'>https://arxiv.org/pdf/2501.18801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhikang Dong, Weituo Hao, Ju-Chiang Wang, Peng Zhang, Pawel Polak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18801">Every Image Listens, Every Image Dances: Music-Driven Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image animation has become a promising area in multimodal research, with a focus on generating videos from reference images. While prior work has largely emphasized generic video generation guided by text, music-driven dance video generation remains underexplored. In this paper, we introduce MuseDance, an innovative end-to-end model that animates reference images using both music and text inputs. This dual input enables MuseDance to generate personalized videos that follow text descriptions and synchronize character movements with the music. Unlike existing approaches, MuseDance eliminates the need for complex motion guidance inputs, such as pose or depth sequences, making flexible and creative video generation accessible to users of all expertise levels. To advance research in this field, we present a new multimodal dataset comprising 2,904 dance videos with corresponding background music and text descriptions. Our approach leverages diffusion-based methods to achieve robust generalization, precise control, and temporal consistency, setting a new baseline for the music-driven image animation task.
<div id='section'>Paperid: <span id='pid'>1542, <a href='https://arxiv.org/pdf/2501.17871.pdf' target='_blank'>https://arxiv.org/pdf/2501.17871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Mishra, David Joffe, Sankara Surendra Telidevara, David S Oakley, Anqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17871">On the challenges of detecting MCI using EEG in the wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have shown promising results in the detection of Mild Cognitive Impairment (MCI) using easily accessible Electroencephalogram (EEG) data which would help administer early and effective treatment for dementia patients. However, the reliability and practicality of such systems remains unclear. In this work, we investigate the potential limitations and challenges in developing a robust MCI detection method using two contrasting datasets: 1) CAUEEG, collected and annotated by expert neurologists in controlled settings and 2) GENEEG, a new dataset collected and annotated in general practice clinics, a setting where routine MCI diagnoses are typically made. We find that training on small datasets, as is done by most previous works, tends to produce high variance models that make overconfident predictions, and are unreliable in practice. Additionally, distribution shifts between datasets make cross-domain generalization challenging. Finally, we show that MCI detection using EEG may suffer from fundamental limitations because of the overlapping nature of feature distributions with control groups. We call for more effort in high-quality data collection in actionable settings (like general practice clinics) to make progress towards this salient goal of non-invasive MCI detection.
<div id='section'>Paperid: <span id='pid'>1543, <a href='https://arxiv.org/pdf/2501.03782.pdf' target='_blank'>https://arxiv.org/pdf/2501.03782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sy-Tuyen Ho, Tuan Van Vo, Somayeh Ebrahimkhani, Ngai-Man Cheung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03782">Vision Transformer Neural Architecture Search for Out-of-Distribution Generalization: Benchmark and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While ViTs have achieved across machine learning tasks, deploying them in real-world scenarios faces a critical challenge: generalizing under OoD shifts. A crucial research gap exists in understanding how to design ViT architectures, both manually and automatically, for better OoD generalization. To this end, we introduce OoD-ViT-NAS, the first systematic benchmark for ViTs NAS focused on OoD generalization. This benchmark includes 3000 ViT architectures of varying computational budgets evaluated on 8 common OoD datasets. Using this benchmark, we analyze factors contributing to OoD generalization. Our findings reveal key insights. First, ViT architecture designs significantly affect OoD generalization. Second, ID accuracy is often a poor indicator of OoD accuracy, highlighting the risk of optimizing ViT architectures solely for ID performance. Third, we perform the first study of NAS for ViTs OoD robustness, analyzing 9 Training-free NAS methods. We find that existing Training-free NAS methods are largely ineffective in predicting OoD accuracy despite excelling at ID accuracy. Simple proxies like Param or Flop surprisingly outperform complex Training-free NAS methods in predicting OoD accuracy. Finally, we study how ViT architectural attributes impact OoD generalization and discover that increasing embedding dimensions generally enhances performance. Our benchmark shows that ViT architectures exhibit a wide range of OoD accuracy, with up to 11.85% improvement for some OoD shifts. This underscores the importance of studying ViT architecture design for OoD. We believe OoD-ViT-NAS can catalyze further research into how ViT designs influence OoD generalization.
<div id='section'>Paperid: <span id='pid'>1544, <a href='https://arxiv.org/pdf/2412.13573.pdf' target='_blank'>https://arxiv.org/pdf/2412.13573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aodi Li, Liansheng Zhuang, Xiao Long, Minghong Yao, Shafei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13573">Seeking Consistent Flat Minima for Better Domain Generalization via Refining Loss Landscapes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn a model from multiple training domains and generalize it to unseen test domains. Recent theory has shown that seeking the deep models, whose parameters lie in the flat minima of the loss landscape, can significantly reduce the out-of-domain generalization error. However, existing methods often neglect the consistency of loss landscapes in different domains, resulting in models that are not simultaneously in the optimal flat minima in all domains, which limits their generalization ability. To address this issue, this paper proposes an iterative Self-Feedback Training (SFT) framework to seek consistent flat minima that are shared across different domains by progressively refining loss landscapes during training. It alternatively generates a feedback signal by measuring the inconsistency of loss landscapes in different domains and refines these loss landscapes for greater consistency using this feedback signal. Benefiting from the consistency of the flat minima within these refined loss landscapes, our SFT helps achieve better out-of-domain generalization. Extensive experiments on DomainBed demonstrate superior performances of SFT when compared to state-of-the-art sharpness-aware methods and other prevalent DG baselines. On average across five DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with ResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available soon.
<div id='section'>Paperid: <span id='pid'>1545, <a href='https://arxiv.org/pdf/2412.07010.pdf' target='_blank'>https://arxiv.org/pdf/2412.07010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai V. Nguyen, Tan Bui-Thanh, Clint Dawson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07010">TAEN: A Model-Constrained Tikhonov Autoencoder Network for Forward and Inverse Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient real-time solvers for forward and inverse problems are essential in engineering and science applications. Machine learning surrogate models have emerged as promising alternatives to traditional methods, offering substantially reduced computational time. Nevertheless, these models typically demand extensive training datasets to achieve robust generalization across diverse scenarios. While physics-based approaches can partially mitigate this data dependency and ensure physics-interpretable solutions, addressing scarce data regimes remains a challenge. Both purely data-driven and physics-based machine learning approaches demonstrate severe overfitting issues when trained with insufficient data. We propose a novel Tikhonov autoencoder model-constrained framework, called TAE, capable of learning both forward and inverse surrogate models using a single arbitrary observation sample. We develop comprehensive theoretical foundations including forward and inverse inference error bounds for the proposed approach for linear cases. For comparative analysis, we derive equivalent formulations for pure data-driven and model-constrained approach counterparts. At the heart of our approach is a data randomization strategy, which functions as a generative mechanism for exploring the training data space, enabling effective training of both forward and inverse surrogate models from a single observation, while regularizing the learning process. We validate our approach through extensive numerical experiments on two challenging inverse problems: 2D heat conductivity inversion and initial condition reconstruction for time-dependent 2D Navier-Stokes equations. Results demonstrate that TAE achieves accuracy comparable to traditional Tikhonov solvers and numerical forward solvers for both inverse and forward problems, respectively, while delivering orders of magnitude computational speedups.
<div id='section'>Paperid: <span id='pid'>1546, <a href='https://arxiv.org/pdf/2412.03927.pdf' target='_blank'>https://arxiv.org/pdf/2412.03927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming-Chang Chiu, Shicheng Wen, Pin-Yu Chen, Xuezhe Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03927">MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In vision-language models (VLMs), the ability to perceive and interpret color and physical environment is crucial for achieving contextually accurate understanding and interaction. However, despite advances in multimodal modeling, there remains a significant lack of specialized datasets that rigorously evaluate a model's capacity to discern subtle color variations and spatial context -- critical elements for situational comprehension and reliable deployment across real-world applications. Toward that goal, we curate MegaCOIN, a high-quality, human-labeled dataset based on \emph{real} images with various contextual attributes. MegaCOIN consists of two parts: MegaCOIN-Instruct, which serves as a supervised fine-tuning (SFT) dataset for VLMs; and MegaCOIN-Bench, an annotated test set that can be used as a stand-alone QA dataset. MegaCOIN~provides three annotated features for 220,000 real images: foreground color, background color, and description of an object's physical environment, constituting 660k human annotations. In addition, MegaCOIN can be applied to benchmark domain generalization (DG) algorithms. We explore benchmarking DG methods in the linear probing setup for VLM and show some new insights. Last but not least, we show that VLMs, including GPT-4o, have subpar color recognition capabilities, and fine-tuning with MegaCOIN can result in improved performance on visual evaluation tasks. In certain cases, MegaCOIN fine-tuned small-scale opensource models such as LLaVA and Bunny can outperform closed-source GPT-4o. We hope the utilities of MegaCOIN can shed light on the directions VLMs can improve and provide a more complex platform for domain generalization algorithms.
<div id='section'>Paperid: <span id='pid'>1547, <a href='https://arxiv.org/pdf/2411.10063.pdf' target='_blank'>https://arxiv.org/pdf/2411.10063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Gong, Chaoran Cui, Chunyun Zhang, Wenna Wang, Xiushan Nie, Lei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10063">Federated Domain Generalization via Prompt Learning and Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization (FedDG) aims to improve the global model generalization in unseen domains by addressing data heterogeneity under privacy-preserving constraints. A common strategy in existing FedDG studies involves sharing domain-specific knowledge among clients, such as spectrum information, class prototypes, and data styles. However, this knowledge is extracted directly from local client samples, and sharing such sensitive information poses a potential risk of data leakage, which might not fully meet the requirements of FedDG. In this paper, we introduce prompt learning to adapt pre-trained vision-language models (VLMs) in the FedDG scenario, and leverage locally learned prompts as a more secure bridge to facilitate knowledge transfer among clients. Specifically, we propose a novel FedDG framework through Prompt Learning and AggregatioN (PLAN), which comprises two training stages to collaboratively generate local prompts and global prompts at each federated round. First, each client performs both text and visual prompt learning using their own data, with local prompts indirectly synchronized by regarding the global prompts as a common reference. Second, all domain-specific local prompts are exchanged among clients and selectively aggregated into the global prompts using lightweight attention-based aggregators. The global prompts are finally applied to adapt VLMs to unseen target domains. As our PLAN framework requires training only a limited number of prompts and lightweight aggregators, it offers notable advantages in computational and communication efficiency for FedDG. Extensive experiments demonstrate the superior generalization ability of PLAN across four benchmark datasets.
<div id='section'>Paperid: <span id='pid'>1548, <a href='https://arxiv.org/pdf/2411.03799.pdf' target='_blank'>https://arxiv.org/pdf/2411.03799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edvin Listo Zec, Adam Breitholtz, Fredrik D. Johansson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03799">Overcoming label shift with target-aware federated learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning enables multiple actors to collaboratively train models without sharing private data. Existing algorithms are successful and well-justified in this task when the intended target domain, where the trained model will be used, shares data distribution with the aggregate of clients, but this is often violated in practice. A common reason is label shift -- that the label distributions differ between clients and the target domain. We demonstrate empirically that this can significantly degrade performance. To address this problem, we propose FedPALS, a principled and practical model aggregation scheme that adapts to label shifts to improve performance in the target domain by leveraging knowledge of label distributions at the central server. Our approach ensures unbiased updates under federated stochastic gradient descent which yields robust generalization across clients with diverse, label-shifted data. Extensive experiments on image classification tasks demonstrate that FedPALS consistently outperforms baselines by aligning model aggregation with the target domain. Our findings reveal that conventional federated learning methods suffer severely in cases of extreme label sparsity on clients, highlighting the critical need for target-aware aggregation as offered by FedPALS.
<div id='section'>Paperid: <span id='pid'>1549, <a href='https://arxiv.org/pdf/2410.23641.pdf' target='_blank'>https://arxiv.org/pdf/2410.23641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanchao Liu, Yujiang Li, Tai-Jiang Mu, Shi-Min Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23641">Recovering Complete Actions for Cross-dataset Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite huge progress in skeleton-based action recognition, its generalizability to different domains remains a challenging issue. In this paper, to solve the skeleton action generalization problem, we present a recover-and-resample augmentation framework based on a novel complete action prior. We observe that human daily actions are confronted with temporal mismatch across different datasets, as they are usually partial observations of their complete action sequences. By recovering complete actions and resampling from these full sequences, we can generate strong augmentations for unseen domains. At the same time, we discover the nature of general action completeness within large datasets, indicated by the per-frame diversity over time. This allows us to exploit two assets of transferable knowledge that can be shared across action samples and be helpful for action completion: boundary poses for determining the action start, and linear temporal transforms for capturing global action patterns. Therefore, we formulate the recovering stage as a two-step stochastic action completion with boundary pose-conditioned extrapolation followed by smooth linear transforms. Both the boundary poses and linear transforms can be efficiently learned from the whole dataset via clustering. We validate our approach on a cross-dataset setting with three skeleton action datasets, outperforming other domain generalization approaches by a considerable margin.
<div id='section'>Paperid: <span id='pid'>1550, <a href='https://arxiv.org/pdf/2410.23625.pdf' target='_blank'>https://arxiv.org/pdf/2410.23625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jehan Yang, Maxwell Soh, Vivianna Lieu, Douglas J Weber, Zackory Erickson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23625">EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the first generalization and adaptation benchmark using machine learning for evaluating out-of-distribution performance of electromyography (EMG) classification algorithms. The ability of an EMG classifier to handle inputs drawn from a different distribution than the training distribution is critical for real-world deployment as a control interface. By predicting the user's intended gesture using EMG signals, we can create a wearable solution to control assistive technologies, such as computers, prosthetics, and mobile manipulator robots. This new out-of-distribution benchmark consists of two major tasks that have utility for building robust and adaptable control interfaces: 1) intersubject classification and 2) adaptation using train-test splits for time-series. This benchmark spans nine datasets--the largest collection of EMG datasets in a benchmark. Among these, a new dataset is introduced, featuring a novel, easy-to-wear high-density EMG wearable for data collection. The lack of open-source benchmarks has made comparing accuracy results between papers challenging for the EMG research community. This new benchmark provides researchers with a valuable resource for analyzing practical measures of out-of-distribution performance for EMG datasets. Our code and data from our new dataset can be found at emgbench.github.io.
<div id='section'>Paperid: <span id='pid'>1551, <a href='https://arxiv.org/pdf/2410.15687.pdf' target='_blank'>https://arxiv.org/pdf/2410.15687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haohan Yuan, Haopeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15687">DomainSum: A Hierarchical Benchmark for Fine-Grained Domain Shift in Abstractive Text Summarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most research on abstractive summarization focuses on single-domain applications, often neglecting how domain shifts between documents affect performance and the generalization ability of summarization models. To address this issue, we introduce DomainSum, a hierarchical benchmark designed to capture fine-grained domain shifts in abstractive summarization. We categorize these shifts into three levels: genre, style, and topic, and demonstrate through comprehensive benchmark analysis that they follow a hierarchical structure. Furthermore, we evaluate the domain generalization capabilities of commonly used pre-trained language models (PLMs) and large language models (LLMs) in in-domain and cross-domain settings.
<div id='section'>Paperid: <span id='pid'>1552, <a href='https://arxiv.org/pdf/2410.11646.pdf' target='_blank'>https://arxiv.org/pdf/2410.11646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zahra Kadkhodaie, StÃ©phane Mallat, Eero P. Simoncelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11646">Feature-guided score diffusion for sampling conditional densities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Score diffusion methods can learn probability densities from samples. The score of the noise-corrupted density is estimated using a deep neural network, which is then used to iteratively transport a Gaussian white noise density to a target density. Variants for conditional densities have been developed, but correct estimation of the corresponding scores is difficult. We avoid these difficulties by introducing an algorithm that guides the diffusion with a projected score. The projection pushes the image feature vector towards the feature vector centroid of the target class. The projected score and the feature vectors are learned by the same network. Specifically, the image feature vector is defined as the spatial averages of the channels activations in select layers of the network. Optimizing the projected score for denoising loss encourages image feature vectors of each class to cluster around their centroids. It also leads to the separations of the centroids. We show that these centroids provide a low-dimensional Euclidean embedding of the class conditional densities. We demonstrate that the algorithm can generate high quality and diverse samples from the conditioning class. Conditional generation can be performed using feature vectors interpolated between those of the training set, demonstrating out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1553, <a href='https://arxiv.org/pdf/2410.08466.pdf' target='_blank'>https://arxiv.org/pdf/2410.08466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eugene P. W. Ang, Shan Lin, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08466">Aligned Divergent Pathways for Omni-Domain Generalized Person Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person Re-identification (Person ReID) has advanced significantly in fully supervised and domain generalized Person R e ID. However, methods developed for one task domain transfer poorly to the other. An ideal Person ReID method should be effective regardless of the number of domains involved in training or testing. Furthermore, given training data from the target domain, it should perform at least as well as state-of-the-art (SOTA) fully supervised Person ReID methods. We call this paradigm Omni-Domain Generalization Person ReID, referred to as ODG-ReID, and propose a way to achieve this by expanding compatible backbone architectures into multiple diverse pathways. Our method, Aligned Divergent Pathways (ADP), first converts a base architecture into a multi-branch structure by copying the tail of the original backbone. We design our module Dynamic Max-Deviance Adaptive Instance Normalization (DyMAIN) that encourages learning of generalized features that are robust to omni-domain directions and apply DyMAIN to the branches of ADP. Our proposed Phased Mixture-of-Cosines (PMoC) coordinates a mix of stable and turbulent learning rate schedules among branches for further diversified learning. Finally, we realign the feature space between branches with our proposed Dimensional Consistency Metric Loss (DCML). ADP outperforms the state-of-the-art (SOTA) results for multi-source domain generalization and supervised ReID within the same domain. Furthermore, our method demonstrates improvement on a wide range of single-source domain generalization benchmarks, achieving Omni-Domain Generalization over Person ReID tasks.
<div id='section'>Paperid: <span id='pid'>1554, <a href='https://arxiv.org/pdf/2410.08460.pdf' target='_blank'>https://arxiv.org/pdf/2410.08460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eugene P. W. Ang, Shan Lin, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08460">Diverse Deep Feature Ensemble Learning for Omni-Domain Generalized Person Re-identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person Re-identification (Person ReID) has progressed to a level where single-domain supervised Person ReID performance has saturated. However, such methods experience a significant drop in performance when trained and tested across different datasets, motivating the development of domain generalization techniques. However, our research reveals that domain generalization methods significantly underperform single-domain supervised methods on single dataset benchmarks. An ideal Person ReID method should be effective regardless of the number of domains involved, and when test domain data is available for training it should perform as well as state-of-the-art (SOTA) fully supervised methods. This is a paradigm that we call Omni-Domain Generalization Person ReID (ODG-ReID). We propose a way to achieve ODG-ReID by creating deep feature diversity with self-ensembles. Our method, Diverse Deep Feature Ensemble Learning (D2FEL), deploys unique instance normalization patterns that generate multiple diverse views and recombines these views into a compact encoding. To the best of our knowledge, our work is one of few to consider omni-domain generalization in Person ReID, and we advance the study of applying feature ensembles in Person ReID. D2FEL significantly improves and matches the SOTA performance for major domain generalization and single-domain supervised benchmarks.
<div id='section'>Paperid: <span id='pid'>1555, <a href='https://arxiv.org/pdf/2410.08388.pdf' target='_blank'>https://arxiv.org/pdf/2410.08388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximus Powers, Shaina Raza, Alex Chang, Rehana Riaz, Umang Mavani, Harshitha Reddy Jonala, Ansh Tiwari, Hua Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08388">Responsible AI in NLP: GUS-Net Span-Level Bias Detection Dataset and Benchmark for Generalizations, Unfairness, and Stereotypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Representational harms in language technologies often occur in short spans within otherwise neutral text, where phrases may simultaneously convey generalizations, unfairness, or stereotypes. Framing bias detection as sentence-level classification obscures which words carry bias and what type is present, limiting both auditability and targeted mitigation. We introduce the GUS-Net Framework, comprising the GUS dataset and a multi-label token-level detector for span-level analysis of social bias. The GUS dataset contains 3,739 unique snippets across multiple domains, with over 69,000 token-level annotations. Each token is labeled using BIO tags (Begin, Inside, Outside) for three pathways of representational harm: Generalizations, Unfairness, and Stereotypes. To ensure reliable data annotation, we employ an automated multi-agent pipeline that proposes candidate spans which are subsequently verified and corrected by human experts. We formulate bias detection as multi-label token-level classification and benchmark both encoder-based models (e.g., BERT family variants) and decoder-based large language models (LLMs). Our evaluations cover token-level identification and span-level entity recognition on our test set, and out-of-distribution generalization. Empirical results show that encoder-based models consistently outperform decoder-based baselines on nuanced and overlapping spans while being more computationally efficient. The framework delivers interpretable, fine-grained diagnostics that enable systematic auditing and mitigation of representational harms in real-world NLP systems.
<div id='section'>Paperid: <span id='pid'>1556, <a href='https://arxiv.org/pdf/2410.04717.pdf' target='_blank'>https://arxiv.org/pdf/2410.04717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dylan Zhang, Justin Wang, Francois Charton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04717">$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization $\textbf{only emerges}$ when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of $\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.
<div id='section'>Paperid: <span id='pid'>1557, <a href='https://arxiv.org/pdf/2409.18529.pdf' target='_blank'>https://arxiv.org/pdf/2409.18529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Rackow, Nikolay Koldunov, Christian Lessig, Irina Sandu, Mihai Alexe, Matthew Chantry, Mariana Clare, Jesper Dramsch, Florian Pappenberger, Xabier Pedruzo-Bagazgoitia, Steffen Tietsche, Thomas Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18529">Robustness of AI-based weather forecasts in a changing climate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven machine learning models for weather forecasting have made transformational progress in the last 1-2 years, with state-of-the-art ones now outperforming the best physics-based models for a wide range of skill scores. Given the strong links between weather and climate modelling, this raises the question whether machine learning models could also revolutionize climate science, for example by informing mitigation and adaptation to climate change or to generate larger ensembles for more robust uncertainty estimates. Here, we show that current state-of-the-art machine learning models trained for weather forecasting in present-day climate produce skillful forecasts across different climate states corresponding to pre-industrial, present-day, and future 2.9K warmer climates. This indicates that the dynamics shaping the weather on short timescales may not differ fundamentally in a changing climate. It also demonstrates out-of-distribution generalization capabilities of the machine learning models that are a critical prerequisite for climate applications. Nonetheless, two of the models show a global-mean cold bias in the forecasts for the future warmer climate state, i.e. they drift towards the colder present-day climate they have been trained for. A similar result is obtained for the pre-industrial case where two out of three models show a warming. We discuss possible remedies for these biases and analyze their spatial distribution, revealing complex warming and cooling patterns that are partly related to missing ocean-sea ice and land surface information in the training data. Despite these current limitations, our results suggest that data-driven machine learning models will provide powerful tools for climate science and transform established approaches by complementing conventional physics-based models.
<div id='section'>Paperid: <span id='pid'>1558, <a href='https://arxiv.org/pdf/2409.18446.pdf' target='_blank'>https://arxiv.org/pdf/2409.18446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saptarshi Sengupta, Wenpeng Yin, Preslav Nakov, Shreya Ghosh, Suhang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18446">Exploring Language Model Generalization in Low-Resource Extractive QA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate Extractive Question Answering (EQA) with Large Language Models (LLMs) under domain drift, i.e., can LLMs generalize to domains that require specific knowledge such as medicine and law in a zero-shot fashion without additional in-domain training? To this end, we devise a series of experiments to explain the performance gap empirically. Our findings suggest that: (a) LLMs struggle with dataset demands of closed domains such as retrieving long answer spans; (b) Certain LLMs, despite showing strong overall performance, display weaknesses in meeting basic requirements as discriminating between domain-specific senses of words which we link to pre-processing decisions; (c) Scaling model parameters is not always effective for cross domain generalization; and (d) Closed-domain datasets are quantitatively much different than open-domain EQA datasets and current LLMs struggle to deal with them. Our findings point out important directions for improving existing LLMs.
<div id='section'>Paperid: <span id='pid'>1559, <a href='https://arxiv.org/pdf/2409.18371.pdf' target='_blank'>https://arxiv.org/pdf/2409.18371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai V. Nguyen, Jau-Uei Chen, Tan Bui-Thanh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18371">A Model-Constrained Discontinuous Galerkin Network (DGNet) for Compressible Euler Equations with Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time accurate solutions of large-scale complex dynamical systems are critically needed for control, optimization, uncertainty quantification, and decision-making in practical engineering and science applications, particularly in digital twin contexts. In this work, we develop a model-constrained discontinuous Galerkin Network (DGNet) approach, a significant extension to our previous work [Model-constrained Tagent Slope Learning Approach for Dynamical Systems], for compressible Euler equations with out-of-distribution generalization. The core of DGNet is the synergy of several key strategies: (i) leveraging time integration schemes to capture temporal correlation and taking advantage of neural network speed for computation time reduction; (ii) employing a model-constrained approach to ensure the learned tangent slope satisfies governing equations; (iii) utilizing a GNN-inspired architecture where edges represent Riemann solver surrogate models and nodes represent volume integration correction surrogate models, enabling capturing discontinuity capability, aliasing error reduction, and mesh discretization generalizability; (iv) implementing the input normalization technique that allows surrogate models to generalize across different initial conditions, geometries, meshes, boundary conditions, and solution orders; and (v) incorporating a data randomization technique that not only implicitly promotes agreement between surrogate models and true numerical models up to second-order derivatives, ensuring long-term stability and prediction capacity, but also serves as a data generation engine during training, leading to enhanced generalization on unseen data. To validate the effectiveness, stability, and generalizability of our novel DGNet approach, we present comprehensive numerical results for 1D and 2D compressible Euler equation problems.
<div id='section'>Paperid: <span id='pid'>1560, <a href='https://arxiv.org/pdf/2409.14671.pdf' target='_blank'>https://arxiv.org/pdf/2409.14671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Liu, Shu Wang, Zhe Qu, Xingyu Li, Shichao Kan, Jianxin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14671">FedGCA: Global Consistent Augmentation Based Single-Source Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Domain Generalization (FedDG) aims to train the global model for generalization ability to unseen domains with multi-domain training samples. However, clients in federated learning networks are often confined to a single, non-IID domain due to inherent sampling and temporal limitations. The lack of cross-domain interaction and the in-domain divergence impede the learning of domain-common features and limit the effectiveness of existing FedDG, referred to as the single-source FedDG (sFedDG) problem. To address this, we introduce the Federated Global Consistent Augmentation (FedGCA) method, which incorporates a style-complement module to augment data samples with diverse domain styles. To ensure the effective integration of augmented samples, FedGCA employs both global guided semantic consistency and class consistency, mitigating inconsistencies from local semantics within individual clients and classes across multiple clients. The conducted extensive experiments demonstrate the superiority of FedGCA.
<div id='section'>Paperid: <span id='pid'>1561, <a href='https://arxiv.org/pdf/2409.09611.pdf' target='_blank'>https://arxiv.org/pdf/2409.09611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Gungor, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09611">Integrating Audio Narrations to Strengthen Domain Generalization in Multimodal First-Person Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>First-person activity recognition is rapidly growing due to the widespread use of wearable cameras but faces challenges from domain shifts across different environments, such as varying objects or background scenes. We propose a multimodal framework that improves domain generalization by integrating motion, audio, and appearance features. Key contributions include analyzing the resilience of audio and motion features to domain shifts, using audio narrations for enhanced audio-text alignment, and applying consistency ratings between audio and visual narrations to optimize the impact of audio in recognition during training. Our approach achieves state-of-the-art performance on the ARGO1M dataset, effectively generalizing across unseen scenarios and locations.
<div id='section'>Paperid: <span id='pid'>1562, <a href='https://arxiv.org/pdf/2409.07308.pdf' target='_blank'>https://arxiv.org/pdf/2409.07308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Sun, Panagiotis Kosmas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07308">Non-Invasive Glucose Prediction System Enhanced by Mixed Linear Models and Meta-Forests for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we present a non-invasive glucose prediction system that integrates Near-Infrared (NIR) spectroscopy and millimeter-wave (mm-wave) sensing. We employ a Mixed Linear Model (MixedLM) to analyze the association between mm-wave frequency S_21 parameters and blood glucose levels within a heterogeneous dataset. The MixedLM method considers inter-subject variability and integrates multiple predictors, offering a more comprehensive analysis than traditional correlation analysis. Additionally, we incorporate a Domain Generalization (DG) model, Meta-forests, to effectively handle domain variance in the dataset, enhancing the model's adaptability to individual differences. Our results demonstrate promising accuracy in glucose prediction for unseen subjects, with a mean absolute error (MAE) of 17.47 mg/dL, a root mean square error (RMSE) of 31.83 mg/dL, and a mean absolute percentage error (MAPE) of 10.88%, highlighting its potential for clinical application. This study marks a significant step towards developing accurate, personalized, and non-invasive glucose monitoring systems, contributing to improved diabetes management.
<div id='section'>Paperid: <span id='pid'>1563, <a href='https://arxiv.org/pdf/2409.01374.pdf' target='_blank'>https://arxiv.org/pdf/2409.01374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Solim LeGris, Wai Keen Vong, Brenden M. Lake, Todd M. Gureckis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01374">H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Abstraction and Reasoning Corpus (ARC) is a visual program synthesis benchmark designed to test challenging out-of-distribution generalization in humans and machines. Since 2019, limited progress has been observed on the challenge using existing artificial intelligence methods. Comparing human and machine performance is important for the validity of the benchmark. While previous work explored how well humans can solve tasks from the ARC benchmark, they either did so using only a subset of tasks from the original dataset, or from variants of ARC, and therefore only provided a tentative estimate of human performance. In this work, we obtain a more robust estimate of human performance by evaluating 1729 humans on the full set of 400 training and 400 evaluation tasks from the original ARC problem set. We estimate that average human performance lies between 73.3% and 77.2% correct with a reported empirical average of 76.2% on the training set, and between 55.9% and 68.9% correct with a reported empirical average of 64.2% on the public evaluation set. However, we also find that 790 out of the 800 tasks were solvable by at least one person in three attempts, suggesting that the vast majority of the publicly available ARC tasks are in principle solvable by typical crowd-workers recruited over the internet. Notably, while these numbers are slightly lower than earlier estimates, human performance still greatly exceeds current state-of-the-art approaches for solving ARC. To facilitate research on ARC, we publicly release our dataset, called H-ARC (human-ARC), which includes all of the submissions and action traces from human participants.
<div id='section'>Paperid: <span id='pid'>1564, <a href='https://arxiv.org/pdf/2408.05831.pdf' target='_blank'>https://arxiv.org/pdf/2408.05831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Qiao, Keqin Li, Junhong Lin, Rong Wei, Chufeng Jiang, Yang Luo, Haoyu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05831">Robust Domain Generalization for Multi-modal Object Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-label classification, machine learning encounters the challenge of domain generalization when handling tasks with distributions differing from the training data. Existing approaches primarily focus on vision object recognition and neglect the integration of natural language. Recent advancements in vision-language pre-training leverage supervision from extensive visual-language pairs, enabling learning across diverse domains and enhancing recognition in multi-modal scenarios. However, these approaches face limitations in loss function utilization, generality across backbones, and class-aware visual fusion. This paper proposes solutions to these limitations by inferring the actual loss, broadening evaluations to larger vision-language backbones, and introducing Mixup-CLIPood, which incorporates a novel mix-up loss for enhanced class-aware visual fusion. Our method demonstrates superior performance in domain generalization across multiple datasets.
<div id='section'>Paperid: <span id='pid'>1565, <a href='https://arxiv.org/pdf/2408.02382.pdf' target='_blank'>https://arxiv.org/pdf/2408.02382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yash Dixit, Naman Srivastava, Joel D Joy, Rohan Olikara, Swarup E, Rakshit Ramesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02382">Cross Pseudo Supervision Framework for Sparsely Labelled Geospatial Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Land Use Land Cover (LULC) mapping is a vital tool for urban and resource planning, playing a key role in the development of innovative and sustainable cities. This study introduces a semi-supervised segmentation model for LULC prediction using high-resolution satellite images with a vast diversity of data distributions in different areas of India. Our approach ensures a robust generalization across different types of buildings, roads, trees, and water bodies within these distinct areas. We propose a modified Cross Pseudo Supervision framework to train image segmentation models on sparsely labelled data. The proposed framework addresses the limitations of the famous 'Cross Pseudo Supervision' technique for semi-supervised learning, specifically tackling the challenges of training segmentation models on noisy satellite image data with sparse and inaccurate labels. This comprehensive approach significantly enhances the accuracy and utility of LULC mapping, providing valuable insights for urban and resource planning applications.
<div id='section'>Paperid: <span id='pid'>1566, <a href='https://arxiv.org/pdf/2408.01774.pdf' target='_blank'>https://arxiv.org/pdf/2408.01774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyang Xu, Yiran Luo, Tianle Lu, Qingfan Wang, Qing Zhou, Bingbing Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01774">STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver Attention to Predict Driver Behaviors Under Safety-Critical Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate behavior prediction for vehicles is essential but challenging for autonomous driving. Most existing studies show satisfying performance under regular scenarios, but most neglected safety-critical scenarios. In this study, a spatio-temporal dual-encoder network named STDA for safety-critical scenarios was developed. Considering the exceptional capabilities of human drivers in terms of situational awareness and comprehending risks, driver attention was incorporated into STDA to facilitate swift identification of the critical regions, which is expected to improve both performance and interpretability. STDA contains four parts: the driver attention prediction module, which predicts driver attention; the fusion module designed to fuse the features between driver attention and raw images; the temporary encoder module used to enhance the capability to interpret dynamic scenes; and the behavior prediction module to predict the behavior. The experiment data are used to train and validate the model. The results show that STDA improves the G-mean from 0.659 to 0.719 when incorporating driver attention and adopting a temporal encoder module. In addition, extensive experimentation has been conducted to validate that the proposed module exhibits robust generalization capabilities and can be seamlessly integrated into other mainstream models.
<div id='section'>Paperid: <span id='pid'>1567, <a href='https://arxiv.org/pdf/2407.17157.pdf' target='_blank'>https://arxiv.org/pdf/2407.17157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhang Nan, Yong Ding, Hao Quan, Deliang Li, Lisha Li, Guanghong Zhao, Xiaoyu Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17157">Establishing Causal Relationship Between Whole Slide Image Predictions and Diagnostic Evidence Subregions in Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the lack of fine-grained annotation guidance, current Multiple Instance Learning (MIL) struggles to establish a robust causal relationship between Whole Slide Image (WSI) diagnosis and evidence sub-images, just like fully supervised learning. So many noisy images can undermine the network's prediction. The proposed Causal Inference Multiple Instance Learning (CI-MIL), uses out-of-distribution generalization to reduce the recognition confusion of sub-images by MIL network, without requiring pixelwise annotations. Specifically, feature distillation is introduced to roughly identify the feature representation of lesion patches. Then, in the random Fourier feature space, these features are re-weighted to minimize the cross-correlation, effectively correcting the feature distribution deviation. These processes reduce the uncertainty when tracing the prediction results back to patches. Predicted diagnoses are more direct and reliable because the causal relationship between them and diagnostic evidence images is more clearly recognized by the network. Experimental results demonstrate that CI-MIL outperforms state-of-the-art methods, achieving 92.25% accuracy and 95.28% AUC on the Camelyon16 dataset (breast cancer), while 94.29% accuracy and 98.07% AUC on the TCGA-NSCLC dataset (non-small cell lung cancer). Additionally, CI-MIL exhibits superior interpretability, as its selected regions demonstrate high consistency with ground truth annotations, promising more reliable diagnostic assistance for pathologists.
<div id='section'>Paperid: <span id='pid'>1568, <a href='https://arxiv.org/pdf/2407.15174.pdf' target='_blank'>https://arxiv.org/pdf/2407.15174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byeong Tak Lee, Joon-myoung Kwon, Yong-Yeon Jo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15174">TADA: Temporal Adversarial Data Augmentation for Time Series Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aim to train models to effectively perform on samples that are unseen and outside of the distribution. Adversarial data augmentation (ADA) is a widely used technique in domain generalization. It enhances the model robustness by including synthetic samples designed to simulate potential unseen scenarios into the training datasets, which is then used to train the model. However, in time series data, traditional ADA approaches often fail to address distribution shifts related to temporal characteristics. To address this limitation, we propose Temporal Adversarial Data Augmentation (TADA) for time series data, which incorporate time warping into ADA. Although time warping is inherently non-differentiable, ADA relies on generating samples through backpropagation. We resolve this issue by leveraging the duality between phase shifts in the frequency domain and time shifts in the time domain, thereby making the process differentiable. Our evaluations across various time series datasets demonstrate that TADA outperforms existing methods for domain generalization. In addition, using distribution visualization, we confirmed that the distribution shifts induced by TADA are clearly different from those induced by ADA, and together, they effectively simulate real-world distribution shifts.
<div id='section'>Paperid: <span id='pid'>1569, <a href='https://arxiv.org/pdf/2407.14920.pdf' target='_blank'>https://arxiv.org/pdf/2407.14920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqin Jiao, Hao Cheng, Claudio Persello, George Vosselman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14920">RoIPoly: Vectorized Building Outline Extraction Using Vertex and Logit Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Polygonal building outlines are crucial for geographic and cartographic applications. The existing approaches for outline extraction from aerial or satellite imagery are typically decomposed into subtasks, e.g., building masking and vectorization, or treat this task as a sequence-to-sequence prediction of ordered vertices. The former lacks efficiency, and the latter often generates redundant vertices, both resulting in suboptimal performance. To handle these issues, we propose a novel Region-of-Interest (RoI) query-based approach called RoIPoly. Specifically, we formulate each vertex as a query and constrain the query attention on the most relevant regions of a potential building, yielding reduced computational overhead and more efficient vertex level interaction. Moreover, we introduce a novel learnable logit embedding to facilitate vertex classification on the attention map; thus, no post-processing is needed for redundant vertex removal. We evaluated our method on the vectorized building outline extraction dataset CrowdAI and the 2D floorplan reconstruction dataset Structured3D. On the CrowdAI dataset, RoIPoly with a ResNet50 backbone outperforms existing methods with the same or better backbones on most MS-COCO metrics, especially on small buildings, and achieves competitive results in polygon quality and vertex redundancy without any post-processing. On the Structured3D dataset, our method achieves the second-best performance on most metrics among existing methods dedicated to 2D floorplan reconstruction, demonstrating our cross-domain generalization capability. The code will be released upon acceptance of this paper.
<div id='section'>Paperid: <span id='pid'>1570, <a href='https://arxiv.org/pdf/2407.13998.pdf' target='_blank'>https://arxiv.org/pdf/2407.13998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan Min, Vittorio Castelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13998">RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA's answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3% of the most competitive LLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research.
<div id='section'>Paperid: <span id='pid'>1571, <a href='https://arxiv.org/pdf/2407.13978.pdf' target='_blank'>https://arxiv.org/pdf/2407.13978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangqiang Li, M. Amine Atoui, Xiangshun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13978">Dual adversarial and contrastive network for single-source domain generalization in fault diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization achieves fault diagnosis on unseen modes. In process industrial systems, fault samples are limited, and it is quite common that the available fault data are from a single mode. Extracting domain-invariant features from single-mode data for unseen mode fault diagnosis poses challenges. Existing methods utilize a generator module to simulate samples of unseen modes. However, multi-mode samples contain complex spatiotemporal information, which brings significant difficulties to accurate sample generation. To solve this problem, this paper proposed a dual adversarial and contrastive network (DACN) for single-source domain generalization in fault diagnosis. The main idea of DACN is to generate diverse sample features and extract domain-invariant feature representations. An adversarial pseudo-sample feature generation strategy is developed to create fake unseen mode sample features with sufficient semantic information and diversity, leveraging adversarial learning between the feature transformer and domain-invariant feature extractor. An enhanced domain-invariant feature extraction strategy is designed to capture common feature representations across multi-modes, utilizing contrastive learning and adversarial learning between the domain-invariant feature extractor and the discriminator. Experiments on the Tennessee Eastman process and continuous stirred-tank reactor demonstrate that DACN achieves high classification accuracy on unseen modes while maintaining a small model size.
<div id='section'>Paperid: <span id='pid'>1572, <a href='https://arxiv.org/pdf/2407.13751.pdf' target='_blank'>https://arxiv.org/pdf/2407.13751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoontae Hwang, Stefan Zohren, Yongjae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13751">Temporal Representation Learning for Stock Similarities and Its Applications in Investment Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of rapid globalization and digitalization, accurate identification of similar stocks has become increasingly challenging due to the non-stationary nature of financial markets and the ambiguity in conventional regional and sector classifications. To address these challenges, we examine SimStock, a novel temporal self-supervised learning framework that combines techniques from self-supervised learning (SSL) and temporal domain generalization to learn robust and informative representations of financial time series data. The primary focus of our study is to understand the similarities between stocks from a broader perspective, considering the complex dynamics of the global financial landscape. We conduct extensive experiments on four real-world datasets with thousands of stocks and demonstrate the effectiveness of SimStock in finding similar stocks, outperforming existing methods. The practical utility of SimStock is showcased through its application to various investment strategies, such as pairs trading, index tracking, and portfolio optimization, where it leads to superior performance compared to conventional methods. Our findings empirically examine the potential of data-driven approach to enhance investment decision-making and risk management practices by leveraging the power of temporal self-supervised learning in the face of the ever-changing global financial landscape.
<div id='section'>Paperid: <span id='pid'>1573, <a href='https://arxiv.org/pdf/2407.08479.pdf' target='_blank'>https://arxiv.org/pdf/2407.08479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel F. Perez-Ramirez, Carlos PÃ©rez-Penichet, Nicolas Tsiftes, Dejan Kostic, Magnus Boman, Thiemo Voigt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08479">Robust Generalization of Graph Neural Networks for Carrier Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Battery-free sensor tags are devices that leverage backscatter techniques to communicate with standard IoT devices, thereby augmenting a network's sensing capabilities in a scalable way. For communicating, a sensor tag relies on an unmodulated carrier provided by a neighboring IoT device, with a schedule coordinating this provisioning across the network. Carrier scheduling--computing schedules to interrogate all sensor tags while minimizing energy, spectrum utilization, and latency--is an NP-Hard optimization problem. Recent work introduces learning-based schedulers that achieve resource savings over a carefully-crafted heuristic, generalizing to networks of up to 60 nodes. However, we find that their advantage diminishes in networks with hundreds of nodes, and degrades further in larger setups. This paper introduces RobustGANTT, a GNN-based scheduler that improves generalization (without re-training) to networks up to 1000 nodes (100x training topology sizes). RobustGANTT not only achieves better and more consistent generalization, but also computes schedules requiring up to 2x less resources than existing systems. Our scheduler exhibits average runtimes of hundreds of milliseconds, allowing it to react fast to changing network conditions. Our work not only improves resource utilization in large-scale backscatter networks, but also offers valuable insights in learning-based scheduling.
<div id='section'>Paperid: <span id='pid'>1574, <a href='https://arxiv.org/pdf/2407.08243.pdf' target='_blank'>https://arxiv.org/pdf/2407.08243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08243">Generalized Face Anti-spoofing via Finer Domain Partition and Disentangling Liveness-irrelevant Factors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face anti-spoofing techniques based on domain generalization have recently been studied widely. Adversarial learning and meta-learning techniques have been adopted to learn domain-invariant representations. However, prior approaches often consider the dataset gap as the primary factor behind domain shifts. This perspective is not fine-grained enough to reflect the intrinsic gap among the data accurately. In our work, we redefine domains based on identities rather than datasets, aiming to disentangle liveness and identity attributes. We emphasize ignoring the adverse effect of identity shift, focusing on learning identity-invariant liveness representations through orthogonalizing liveness and identity features. To cope with style shifts, we propose Style Cross module to expand the stylistic diversity and Channel-wise Style Attention module to weaken the sensitivity to style shifts, aiming to learn robust liveness representations. Furthermore, acknowledging the asymmetry between live and spoof samples, we introduce a novel contrastive loss, Asymmetric Augmented Instance Contrast. Extensive experiments on four public datasets demonstrate that our method achieves state-of-the-art performance under cross-dataset and limited source dataset scenarios. Additionally, our method has good scalability when expanding diversity of identities. The codes will be released soon.
<div id='section'>Paperid: <span id='pid'>1575, <a href='https://arxiv.org/pdf/2406.14797.pdf' target='_blank'>https://arxiv.org/pdf/2406.14797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbo Pei, Zhuqing Jiang, Aidong Men, Haiying Wang, Haiyong Luo, Shiping Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14797">Camera-Invariant Meta-Learning Network for Single-Camera-Training Person Re-identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-camera-training person re-identification (SCT re-ID) aims to train a re-ID model using SCT datasets where each person appears in only one camera. The main challenge of SCT re-ID is to learn camera-invariant feature representations without cross-camera same-person (CCSP) data as supervision. Previous methods address it by assuming that the most similar person should be found in another camera. However, this assumption is not guaranteed to be correct. In this paper, we propose a Camera-Invariant Meta-Learning Network (CIMN) for SCT re-ID. CIMN assumes that the camera-invariant feature representations should be robust to camera changes. To this end, we split the training data into meta-train set and meta-test set based on camera IDs and perform a cross-camera simulation via meta-learning strategy, aiming to enforce the representations learned from the meta-train set to be robust to the meta-test set. With the cross-camera simulation, CIMN can learn camera-invariant and identity-discriminative representations even there are no CCSP data. However, this simulation also causes the separation of the meta-train set and the meta-test set, which ignores some beneficial relations between them. Thus, we introduce three losses: meta triplet loss, meta classification loss, and meta camera alignment loss, to leverage the ignored relations. The experiment results demonstrate that our method achieves comparable performance with and without CCSP data, and outperforms the state-of-the-art methods on SCT re-ID benchmarks. In addition, it is also effective in improving the domain generalization ability of the model.
<div id='section'>Paperid: <span id='pid'>1576, <a href='https://arxiv.org/pdf/2406.04609.pdf' target='_blank'>https://arxiv.org/pdf/2406.04609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junru Zhang, Lang Feng, Zhidan Liu, Yuhan Wu, Yang He, Yabo Dong, Duanqing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04609">Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing domain generalization (DG) methods for cross-person generalization tasks often face challenges in capturing intra- and inter-domain style diversity, resulting in domain gaps with the target domain. In this study, we explore a novel perspective to tackle this problem, a process conceptualized as domain padding. This proposal aims to enrich the domain diversity by synthesizing intra- and inter-domain style data while maintaining robustness to class labels. We instantiate this concept using a conditional diffusion model and introduce a style-fused sampling strategy to enhance data generation diversity. In contrast to traditional condition-guided sampling, our style-fused sampling strategy allows for the flexible use of one or more random styles to guide data synthesis. This feature presents a notable advancement: it allows for the maximum utilization of possible permutations and combinations among existing styles to generate a broad spectrum of new style instances. Empirical evaluations on a broad range of datasets demonstrate that our generated data achieves remarkable diversity within the domain space. Both intra- and inter-domain generated data have proven to be significant and valuable, contributing to varying degrees of performance enhancements. Notably, our approach outperforms state-of-the-art DG methods in all human activity recognition tasks.
<div id='section'>Paperid: <span id='pid'>1577, <a href='https://arxiv.org/pdf/2405.15225.pdf' target='_blank'>https://arxiv.org/pdf/2405.15225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yajing Liu, Shijun Zhou, Xiyao Liu, Chunhui Hao, Baojie Fan, Jiandong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15225">Unbiased Faster R-CNN for Single-source Domain Generalized Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization (SDG) for object detection is a challenging yet essential task as the distribution bias of the unseen domain degrades the algorithm performance significantly. However, existing methods attempt to extract domain-invariant features, neglecting that the biased data leads the network to learn biased features that are non-causal and poorly generalizable. To this end, we propose an Unbiased Faster R-CNN (UFR) for generalizable feature learning. Specifically, we formulate SDG in object detection from a causal perspective and construct a Structural Causal Model (SCM) to analyze the data bias and feature bias in the task, which are caused by scene confounders and object attribute confounders. Based on the SCM, we design a Global-Local Transformation module for data augmentation, which effectively simulates domain diversity and mitigates the data bias. Additionally, we introduce a Causal Attention Learning module that incorporates a designed attention invariance loss to learn image-level features that are robust to scene confounders. Moreover, we develop a Causal Prototype Learning module with an explicit instance constraint and an implicit prototype constraint, which further alleviates the negative impact of object attribute confounders. Experimental results on five scenes demonstrate the prominent generalization ability of our method, with an improvement of 3.9% mAP on the Night-Clear scene.
<div id='section'>Paperid: <span id='pid'>1578, <a href='https://arxiv.org/pdf/2404.13848.pdf' target='_blank'>https://arxiv.org/pdf/2404.13848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Yang, Zuchao Li, Shuai Xie, Wei Yu, Shijun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13848">DSDRNet: Disentangling Representation and Reconstruct Network for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization faces challenges due to the distribution shift between training and testing sets, and the presence of unseen target domains. Common solutions include domain alignment, meta-learning, data augmentation, or ensemble learning, all of which rely on domain labels or domain adversarial techniques. In this paper, we propose a Dual-Stream Separation and Reconstruction Network, dubbed DSDRNet. It is a disentanglement-reconstruction approach that integrates features of both inter-instance and intra-instance through dual-stream fusion. The method introduces novel supervised signals by combining inter-instance semantic distance and intra-instance similarity. Incorporating Adaptive Instance Normalization (AdaIN) into a two-stage cyclic reconstruction process enhances self-disentangled reconstruction signals to facilitate model convergence. Extensive experiments on four benchmark datasets demonstrate that DSDRNet outperforms other popular methods in terms of domain generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1579, <a href='https://arxiv.org/pdf/2404.02785.pdf' target='_blank'>https://arxiv.org/pdf/2404.02785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arsham Gholamzadeh Khoee, Yinan Yu, Robert Feldt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02785">Domain Generalization through Meta-Learning: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution--an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the concept of meta-learning for domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies. Additionally, we present a decision graph to assist readers in navigating the taxonomy based on data availability and domain shifts, enabling them to select and develop a proper model tailored to their specific problem requirements. Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field. Our survey provides practical insights and an informed discussion on promising research directions.
<div id='section'>Paperid: <span id='pid'>1580, <a href='https://arxiv.org/pdf/2403.20002.pdf' target='_blank'>https://arxiv.org/pdf/2403.20002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelin Zhao, Fenglei Fan, Wenlong Liao, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20002">Grounding and Enhancing Grid-based Models for Neural Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical framework for grid-based models. This framework points out that these models' approximation and generalization behaviors are determined by grid tangent kernels (GTK), which are intrinsic properties of grid-based models. The proposed framework facilitates a consistent and systematic analysis of diverse grid-based models. Furthermore, the introduced framework motivates the development of a novel grid-based model named the Multiplicative Fourier Adaptive Grid (MulFAGrid). The numerical analysis demonstrates that MulFAGrid exhibits a lower generalization bound than its predecessors, indicating its robust generalization performance. Empirical studies reveal that MulFAGrid achieves state-of-the-art performance in various tasks, including 2D image fitting, 3D signed distance field (SDF) reconstruction, and novel view synthesis, demonstrating superior representation ability. The project website is available at https://sites.google.com/view/cvpr24-2034-submission/home.
<div id='section'>Paperid: <span id='pid'>1581, <a href='https://arxiv.org/pdf/2403.17676.pdf' target='_blank'>https://arxiv.org/pdf/2403.17676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songwei Liu, Yingyi Wen, Jingfang Pei, Yang Liu, Lekai Song, Pengyu Liu, Xiaoyue Fan, Wenchen Yang, Danmei Pan, Teng Ma, Yue Lin, Gang Wang, Guohua Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17676">Analysis on reservoir activation with the nonlinearity harnessed from solution-processed molybdenum disulfide</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reservoir computing is a recurrent neural network designed for approximating complex dynamics in, for instance, motion tracking, spatial-temporal pattern recognition, and chaotic attractor reconstruction. Its implementation demands intense computation for the nonlinear transformation of the reservoir input, i.e. activating the reservoir. Configuring physical nonlinear networks as the reservoir and employing the physical nonlinearity for the reservoir activation is an emergent solution to address the challenge. In this work, we analyze the feasibility of harnessing the nonlinearity from solution-processed molybdenum disulfide (MoS2) for reservoir activation. We fit the high-order nonlinearity, achieved by Stark modulation of MoS2, as the activation function to facilitate implementation of a reservoir computing model. Due to the high-order nonlinearity, the model can achieve long-term synchronization and robust generalization for complex dynamical system regression. As a potential application exploring this ability, we appoint the model to generate chaotic random numbers for secure data encryption. Given this reservoir activation capability, and the scalability of solution-processed MoS2, our results suggest the potential for realizing physical reservoir computing with solution-processed MoS2.
<div id='section'>Paperid: <span id='pid'>1582, <a href='https://arxiv.org/pdf/2403.16689.pdf' target='_blank'>https://arxiv.org/pdf/2403.16689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16689">SYNAPSE: SYmbolic Neural-Aided Preference Synthesis Engine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of preference learning, which aims to align robot behaviors through learning user specific preferences (e.g. "good pull-over location") from visual demonstrations. Despite its similarity to learning factual concepts (e.g. "red door"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a novel framework called SYNAPSE, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited data. SYNAPSE represents preferences as neuro-symbolic programs, facilitating inspection of individual parts for alignment, in a domain-specific language (DSL) that operates over images and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We perform extensive evaluations on various preferential concepts as well as user case studies demonstrating its ability to align well with dissimilar user preferences. Our method significantly outperforms baselines, especially when it comes to out of distribution generalization. We show the importance of the design choices in the framework through multiple ablation studies. Code, additional results, and supplementary material can be found on the website: https://amrl.cs.utexas.edu/synapse
<div id='section'>Paperid: <span id='pid'>1583, <a href='https://arxiv.org/pdf/2403.15605.pdf' target='_blank'>https://arxiv.org/pdf/2403.15605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khiem Le, Long Ho, Cuong Do, Danh Le-Phuoc, Kok-Seng Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15605">Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. Federated Domain Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the Federated Learning paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while retaining discrimination of those features. Then, we incorporate a simple yet effective regularizer to guide these models in directly capturing domain-invariant representations that the global model's classifier can leverage. Extensive experimental results on two benchmark datasets, i.e., PACS and Office-Home, and a real-world medical dataset, Camelyon17, indicate that our proposed method outperforms other existing methods in addressing this particular problem.
<div id='section'>Paperid: <span id='pid'>1584, <a href='https://arxiv.org/pdf/2403.12245.pdf' target='_blank'>https://arxiv.org/pdf/2403.12245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Lin, Glen Chou, Dmitry Berenson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12245">Improving Out-of-Distribution Generalization of Learned Dynamics by Learning Pseudometrics and Constraint Manifolds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method for improving the prediction accuracy of learned robot dynamics models on out-of-distribution (OOD) states. We achieve this by leveraging two key sources of structure often present in robot dynamics: 1) sparsity, i.e., some components of the state may not affect the dynamics, and 2) physical limits on the set of possible motions, in the form of nonholonomic constraints. Crucially, we do not assume this structure is known a priori, and instead learn it from data. We use contrastive learning to obtain a distance pseudometric that uncovers the sparsity pattern in the dynamics, and use it to reduce the input space when learning the dynamics. We then learn the unknown constraint manifold by approximating the normal space of possible motions from the data, which we use to train a Gaussian process (GP) representation of the constraint manifold. We evaluate our approach on a physical differential-drive robot and a simulated quadrotor, showing improved prediction accuracy on OOD data relative to baselines.
<div id='section'>Paperid: <span id='pid'>1585, <a href='https://arxiv.org/pdf/2403.08477.pdf' target='_blank'>https://arxiv.org/pdf/2403.08477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan Richard Schwarz, Ying Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08477">Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent successes suggest that parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-distribution (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient fine-tuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-distribution and out-of-distribution generalization. Our code is publicly available.
<div id='section'>Paperid: <span id='pid'>1586, <a href='https://arxiv.org/pdf/2403.07033.pdf' target='_blank'>https://arxiv.org/pdf/2403.07033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Chen, Xingjian Dong, Zhike Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07033">Interpreting What Typical Fault Signals Look Like via Prototype-matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks, with powerful nonlinear mapping and classification capabilities, are widely applied in mechanical fault diagnosis to ensure safety. However, being typical black-box models, their application is limited in high-reliability-required scenarios. To understand the classification logic and explain what typical fault signals look like, the prototype matching network (PMN) is proposed by combining the human-inherent prototype-matching with autoencoder (AE). The PMN matches AE-extracted feature with each prototype and selects the most similar prototype as the prediction result. It has three interpreting paths on classification logic, fault prototypes, and matching contributions. Conventional diagnosis and domain generalization experiments demonstrate its competitive diagnostic performance and distinguished advantages in representation learning. Besides, the learned typical fault signals (i.e., sample-level prototypes) showcase the ability for denoising and extracting subtle key features that experts find challenging to capture. This ability broadens human understanding and provides a promising solution from interpretability research to AI-for-Science.
<div id='section'>Paperid: <span id='pid'>1587, <a href='https://arxiv.org/pdf/2403.06537.pdf' target='_blank'>https://arxiv.org/pdf/2403.06537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yeeun Kim, Hyunseo Shin, Eunkyung Choi, Hongseok Oh, Hyunjun Kim, Wonseok Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06537">On the Consideration of AI Openness: Can Good Intent Be Abused?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open source is a driving force behind scientific advancement.However, this openness is also a double-edged sword, with the inherent risk that innovative technologies can be misused for purposes harmful to society. What is the likelihood that an open source AI model or dataset will be used to commit a real-world crime, and if a criminal does exploit it, will the people behind the technology be able to escape legal liability? To address these questions, we explore a legal domain where individual choices can have a significant impact on society. Specifically, we build the EVE-V1 dataset that comprises 200 question-answer pairs related to criminal offenses based on 200 Korean precedents first to explore the possibility of malicious models emerging. We further developed EVE-V2 using 600 fraud-related precedents to confirm the existence of malicious models that can provide harmful advice on a wide range of criminal topics to test the domain generalization ability. Remarkably, widely used open-source large-scale language models (LLMs) provide unethical and detailed information about criminal activities when fine-tuned with EVE. We also take an in-depth look at the legal issues that malicious language models and their builders could realistically face. Our findings highlight the paradoxical dilemma that open source accelerates scientific progress, but requires great care to minimize the potential for misuse. Warning: This paper contains content that some may find unethical.
<div id='section'>Paperid: <span id='pid'>1588, <a href='https://arxiv.org/pdf/2402.08221.pdf' target='_blank'>https://arxiv.org/pdf/2402.08221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohe Li, Feilong Huang, Zide Fan, Fangli Mou, Yingyan Hou, Chen Qian, Lijie Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08221">MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory prediction has garnered widespread attention in different fields, such as autonomous driving and robotic navigation. However, due to the significant variations in trajectory patterns across different scenarios, models trained in known environments often falter in unseen ones. To learn a generalized model that can directly handle unseen domains without requiring any model updating, we propose a novel meta-learning-based trajectory prediction method called MetaTra. This approach incorporates a Dual Trajectory Transformer (Dual-TT), which enables a thorough exploration of the individual intention and the interactions within group motion patterns in diverse scenarios. Building on this, we propose a meta-learning framework to simulate the generalization process between source and target domains. Furthermore, to enhance the stability of our prediction outcomes, we propose a Serial and Parallel Training (SPT) strategy along with a feature augmentation method named MetaMix. Experimental results on several real-world datasets confirm that MetaTra not only surpasses other state-of-the-art methods but also exhibits plug-and-play capabilities, particularly in the realm of domain generalization.
<div id='section'>Paperid: <span id='pid'>1589, <a href='https://arxiv.org/pdf/2402.07746.pdf' target='_blank'>https://arxiv.org/pdf/2402.07746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Douwe J. Spaanderman, Martijn P. A. Starmans, Gonnie C. M. van Erp, David F. Hanff, Judith H. Sluijter, Anne-Rose W. Schut, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. Grunhagen, Wiro J. Niessen, Jacob J. Visser, Stefan Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07746">Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI using Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segmentations are crucial in medical imaging to obtain morphological, volumetric, and radiomics biomarkers. Manual segmentation is accurate but not feasible in the radiologist's clinical workflow, while automatic segmentation generally obtains sub-par performance. We therefore developed a minimally interactive deep learning-based segmentation method for soft-tissue tumors (STTs) on CT and MRI. The method requires the user to click six points near the tumor's extreme boundaries. These six points are transformed into a distance map and serve, with the image, as input for a Convolutional Neural Network. For training and validation, a multicenter dataset containing 514 patients and nine STT types in seven anatomical locations was used, resulting in a Dice Similarity Coefficient (DSC) of 0.85$\pm$0.11 (mean $\pm$ standard deviation (SD)) for CT and 0.84$\pm$0.12 for T1-weighted MRI, when compared to manual segmentations made by expert radiologists. Next, the method was externally validated on a dataset including five unseen STT phenotypes in extremities, achieving 0.81$\pm$0.08 for CT, 0.84$\pm$0.09 for T1-weighted MRI, and 0.88\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI. In conclusion, our minimally interactive segmentation method effectively segments different types of STTs on CT and MRI, with robust generalization to previously unseen phenotypes and imaging modalities.
<div id='section'>Paperid: <span id='pid'>1590, <a href='https://arxiv.org/pdf/2402.05066.pdf' target='_blank'>https://arxiv.org/pdf/2402.05066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shathushan Sivashangaran, Apoorva Khairnar, Azim Eskandarian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05066">Exploration Without Maps via Zero-Shot Out-of-Distribution Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Operation of Autonomous Mobile Robots (AMRs) of all forms that include wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS denied environments without a-priori maps, exclusively using onboard sensors, is an unsolved problem that has potential to transform the economy, and vastly improve humanity's capabilities with improvements to agriculture, manufacturing, disaster response, military and space exploration. Conventional AMR automation approaches are modularized into perception, motion planning and control which is computationally inefficient, and requires explicit feature extraction and engineering, that inhibits generalization, and deployment at scale. Few works have focused on real-world end-to-end approaches that directly map sensor inputs to control outputs due to the large amount of well curated training data required for supervised Deep Learning (DL) which is time consuming and labor intensive to collect and label, and sample inefficiency and challenges to bridging the simulation to reality gap using Deep Reinforcement Learning (DRL). This paper presents a novel method to efficiently train DRL for robust end-to-end AMR exploration, in a constrained environment at physical limits in simulation, transferred zero-shot to the real-world. The representation learned in a compact parameter space with 2 fully connected layers with 64 nodes each is demonstrated to exhibit emergent behavior for out-of-distribution generalization to navigation in new environments that include unstructured terrain without maps, and dynamic obstacle avoidance. The learned policy outperforms conventional navigation algorithms while consuming a fraction of the computation resources, enabling execution on a range of AMR forms with varying embedded computer payloads.
<div id='section'>Paperid: <span id='pid'>1591, <a href='https://arxiv.org/pdf/2402.01203.pdf' target='_blank'>https://arxiv.org/pdf/2402.01203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Fu Wu, Minseung Lee, Sungjin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01203">Neural Language of Thought Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Language of Thought Hypothesis suggests that human cognition operates on a structured, language-like system of mental representations. While neural language models can naturally benefit from the compositional structure inherently and explicitly expressed in language data, learning such representations from non-linguistic general observations, like images, remains a challenge. In this work, we introduce the Neural Language of Thought Model (NLoTM), a novel approach for unsupervised learning of LoTH-inspired representation and generation. NLoTM comprises two key components: (1) the Semantic Vector-Quantized Variational Autoencoder, which learns hierarchical, composable discrete representations aligned with objects and their properties, and (2) the Autoregressive LoT Prior, an autoregressive transformer that learns to generate semantic concept tokens compositionally, capturing the underlying data distribution. We evaluate NLoTM on several 2D and 3D image datasets, demonstrating superior performance in downstream tasks, out-of-distribution generalization, and image generation quality compared to patch-based VQ-VAE and continuous object-centric representations. Our work presents a significant step towards creating neural networks exhibiting more human-like understanding by developing LoT-like representations and offers insights into the intersection of cognitive science and machine learning.
<div id='section'>Paperid: <span id='pid'>1592, <a href='https://arxiv.org/pdf/2401.05064.pdf' target='_blank'>https://arxiv.org/pdf/2401.05064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernardo Torres, Stefan Lattner, GaÃ«l Richard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05064">Singer Identity Representation Learning using Self-Supervised Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and trained models to facilitate further research on singing voice and related areas.
<div id='section'>Paperid: <span id='pid'>1593, <a href='https://arxiv.org/pdf/2401.04425.pdf' target='_blank'>https://arxiv.org/pdf/2401.04425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Sun, Panagiotis Kosmas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04425">Meta-forests: Domain generalization on random forests with meta-learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called "meta-forests", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly object recognition datasets and a glucose monitoring dataset that we have used in a previous study. Our results show that meta-forests outperforms state-of-the-art approaches in terms of generalization performance on both object recognition and glucose monitoring datasets.
<div id='section'>Paperid: <span id='pid'>1594, <a href='https://arxiv.org/pdf/2401.02287.pdf' target='_blank'>https://arxiv.org/pdf/2401.02287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Thomine, Hichem Snoussi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02287">Distillation-based fabric anomaly detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised texture anomaly detection has been a concerning topic in a vast amount of industrial processes. Patterned textures inspection, particularly in the context of fabric defect detection, is indeed a widely encountered use case. This task involves handling a diverse spectrum of colors and textile types, encompassing a wide range of fabrics. Given the extensive variability in colors, textures, and defect types, fabric defect detection poses a complex and challenging problem in the field of patterned textures inspection. In this article, we propose a knowledge distillation-based approach tailored specifically for addressing the challenge of unsupervised anomaly detection in textures resembling fabrics. Our method aims to redefine the recently introduced reverse distillation approach, which advocates for an encoder-decoder design to mitigate classifier bias and to prevent the student from reconstructing anomalies. In this study, we present a new reverse distillation technique for the specific task of fabric defect detection. Our approach involves a meticulous design selection that strategically highlights high-level features. To demonstrate the capabilities of our approach both in terms of performance and inference speed, we conducted a series of experiments on multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside conducting experiments on a dataset acquired from a textile manufacturing facility. The main contributions of this paper are the following: a robust texture anomaly detector utilizing a reverse knowledge-distillation technique suitable for both anomaly detection and domain generalization and a novel dataset encompassing a diverse range of fabrics and defects.
<div id='section'>Paperid: <span id='pid'>1595, <a href='https://arxiv.org/pdf/2312.05141.pdf' target='_blank'>https://arxiv.org/pdf/2312.05141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Inseop Chung, KiYoon Yoo, Nojun Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05141">Open Domain Generalization with a Single Network by Regularization Exploiting Pre-trained Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open Domain Generalization (ODG) is a challenging task as it not only deals with distribution shifts but also category shifts between the source and target datasets. To handle this task, the model has to learn a generalizable representation that can be applied to unseen domains while also identify unknown classes that were not present during training. Previous work has used multiple source-specific networks, which involve a high computation cost. Therefore, this paper proposes a method that can handle ODG using only a single network. The proposed method utilizes a head that is pre-trained by linear-probing and employs two regularization terms, each targeting the regularization of feature extractor and the classification head, respectively. The two regularization terms fully utilize the pre-trained features and collaborate to modify the head of the model without excessively altering the feature extractor. This ensures a smoother softmax output and prevents the model from being biased towards the source domains. The proposed method shows improved adaptability to unseen domains and increased capability to detect unseen classes as well. Extensive experiments show that our method achieves competitive performance in several benchmarks. We also justify our method with careful analysis of the effect on the logits, features, and the head.
<div id='section'>Paperid: <span id='pid'>1596, <a href='https://arxiv.org/pdf/2311.15906.pdf' target='_blank'>https://arxiv.org/pdf/2311.15906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Sun, Hao Zheng, Zhigang Hu, Liu Yang, Meiguang Zheng, Bo Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15906">MetaDefa: Meta-learning based on Domain Enhancement and Feature Alignment for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The single domain generalization(SDG) based on meta-learning has emerged as an effective technique for solving the domain-shift problem. However, the inadequate match of data distribution between source and augmented domains and difficult separation of domain-invariant features from domain-related features make SDG model hard to achieve great generalization. Therefore, a novel meta-learning method based on domain enhancement and feature alignment (MetaDefa) is proposed to improve the model generalization performance. First, the background substitution and visual corruptions techniques are used to generate diverse and effective augmented domains. Then, the multi-channel feature alignment module based on class activation maps and class agnostic activation maps is designed to effectively extract adequate transferability knowledge. In this module, domain-invariant features can be fully explored by focusing on similar target regions between source and augmented domains feature space and suppressing the feature representation of non-similar target regions. Extensive experiments on two publicly available datasets show that MetaDefa has significant generalization performance advantages in unknown multiple target domains.
<div id='section'>Paperid: <span id='pid'>1597, <a href='https://arxiv.org/pdf/2311.13612.pdf' target='_blank'>https://arxiv.org/pdf/2311.13612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Liao, Theodoros Tsiligkaridis, Brian Kulis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13612">Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past year, a large body of multimodal research has emerged around zero-shot evaluation using GPT descriptors. These studies boost the zero-shot accuracy of pretrained VL models with an ensemble of label-specific text generated by GPT. A recent study, WaffleCLIP, demonstrated that similar zero-shot accuracy can be achieved with an ensemble of random descriptors. However, both zero-shot methods are un-trainable and consequently sub-optimal when some few-shot out-of-distribution (OOD) training data is available. Inspired by these prior works, we present two more flexible methods called descriptor and word soups, which do not require an LLM at test time and can leverage training data to increase OOD target accuracy. Descriptor soup greedily selects a small set of textual descriptors using generic few-shot training data, then calculates robust class embeddings using the selected descriptors. Word soup greedily assembles a chain of words in a similar manner. Compared to existing few-shot soft prompt tuning methods, word soup requires fewer parameters by construction and less GPU memory, since it does not require backpropagation. Both soups outperform current published few-shot methods, even when combined with SoTA zero-shot methods, on cross-dataset and domain generalization benchmarks. Compared with SoTA prompt and descriptor ensembling methods, such as ProDA and WaffleCLIP, word soup achieves higher OOD accuracy with fewer ensemble members. Please checkout our code: github.com/Chris210634/word_soups
<div id='section'>Paperid: <span id='pid'>1598, <a href='https://arxiv.org/pdf/2309.13525.pdf' target='_blank'>https://arxiv.org/pdf/2309.13525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sina Malakouti, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13525">Semi-Supervised Domain Generalization for Object Detection via Language-Guided Feature Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing domain adaptation (DA) and generalization (DG) methods in object detection enforce feature alignment in the visual space but face challenges like object appearance variability and scene complexity, which make it difficult to distinguish between objects and achieve accurate detection. In this paper, we are the first to address the problem of semi-supervised domain generalization by exploring vision-language pre-training and enforcing feature alignment through the language space. We employ a novel Cross-Domain Descriptive Multi-Scale Learning (CDDMSL) aiming to maximize the agreement between descriptions of an image presented with different domain-specific characteristics in the embedding space. CDDMSL significantly outperforms existing methods, achieving 11.7% and 7.5% improvement in DG and DA settings, respectively. Comprehensive analysis and ablation studies confirm the effectiveness of our method, positioning CDDMSL as a promising approach for domain generalization in object detection tasks.
<div id='section'>Paperid: <span id='pid'>1599, <a href='https://arxiv.org/pdf/2309.10149.pdf' target='_blank'>https://arxiv.org/pdf/2309.10149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsu Kim, Walid Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10149">Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outperforms memory-based CL baselines across all environments while significantly improving the generalization performance on unseen target environments.
<div id='section'>Paperid: <span id='pid'>1600, <a href='https://arxiv.org/pdf/2309.06358.pdf' target='_blank'>https://arxiv.org/pdf/2309.06358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arijit Ghosh Chowdhury, Aman Chadha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06358">Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how "in-the-wild" generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets with generated data leads to better robustness towards natural distribution shifts.
<div id='section'>Paperid: <span id='pid'>1601, <a href='https://arxiv.org/pdf/2308.15618.pdf' target='_blank'>https://arxiv.org/pdf/2308.15618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Choudhary, Mosbah Aouad, Krishnakant Saboo, Angelina Hwang, Jacob Kechter, Blake Bordeaux, Puneet Bhullar, David DiCaudo, Steven Nelson, Nneka Comfere, Emma Johnson, Olayemi Sokumbi, Jason Sluzevich, Leah Swanson, Dennis Murphree, Aaron Mangold, Ravishankar Iyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15618">RACR-MIL: Rank-aware contextual reasoning for weakly supervised grading of squamous cell carcinoma using whole slide images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Squamous cell carcinoma (SCC) is the most common cancer subtype, with an increasing incidence and a significant impact on cancer-related mortality. SCC grading using whole slide images is inherently challenging due to the lack of a reliable protocol and substantial tissue heterogeneity. We propose RACR-MIL, the first weakly-supervised SCC grading approach achieving robust generalization across multiple anatomies (skin, head and neck, lung). RACR-MIL is an attention-based multiple-instance learning framework that enhances grade-relevant contextual representation learning and addresses tumor heterogeneity through two key innovations: (1) a hybrid WSI graph that captures both local tissue context and non-local phenotypical dependencies between tumor regions, and (2) a rank-ordering constraint in the attention mechanism that consistently prioritizes higher-grade tumor regions, aligning with pathologists diagnostic process. Our model achieves state-of-the-art performance across multiple SCC datasets, achieving 3-9% higher grading accuracy, resilience to class imbalance, and up to 16% improved tumor localization. In a pilot study, pathologists reported that RACR-MIL improved grading efficiency in 60% of cases, underscoring its potential as a clinically viable cancer diagnosis and grading assistant.
<div id='section'>Paperid: <span id='pid'>1602, <a href='https://arxiv.org/pdf/2308.06624.pdf' target='_blank'>https://arxiv.org/pdf/2308.06624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Berker Demirel, Erchan Aptoula, Huseyin Ozkan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06624">ADRMX: Additive Disentanglement of Domain Features with Remix Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original additive disentanglement strategy. Moreover, a new data augmentation technique is introduced to further support the generalization capacity of ADRMX, where samples from different domains are mixed within the latent space. Through extensive experiments conducted on DomainBed under fair conditions, ADRMX is shown to achieve state-of-the-art performance. Code will be made available at GitHub after the revision process.
<div id='section'>Paperid: <span id='pid'>1603, <a href='https://arxiv.org/pdf/2308.03936.pdf' target='_blank'>https://arxiv.org/pdf/2308.03936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Sikaroudi, Maryam Hosseini, Shahryar Rahnamayan, H. R. Tizhoosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03936">ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an exhaustive methodology that leverages all levels of feature abstraction, targeting an enhancement in the generalizability of image classification to unobserved hospitals. Our approach incorporates augmentation-based self-supervision with common distribution shifts in histopathology scenarios serving as the pretext task. This enables us to derive invariant features from training images without relying on training labels, thereby covering different abstraction levels. Moving onto the subsequent abstraction level, we employ a domain alignment module to facilitate further extraction of invariant features across varying training hospitals. To represent the highly specific features of participating hospitals, an encoder is trained to classify hospital labels, independent of their diagnostic labels. The features from each of these encoders are subsequently disentangled to minimize redundancy and segregate the features. This representation, which spans a broad spectrum of semantic information, enables the development of a model demonstrating increased robustness to unseen images from disparate distributions. Experimental results from the PACS dataset (a domain generalization benchmark), a synthetic dataset created by applying histopathology-specific jitters to the MHIST dataset (defining different domains with varied distribution shifts), and a Renal Cell Carcinoma dataset derived from four image repositories from TCGA, collectively indicate that our proposed model is adept at managing varying levels of image granularity. Thus, it shows improved generalizability when faced with new, out-of-distribution hospital images.
<div id='section'>Paperid: <span id='pid'>1604, <a href='https://arxiv.org/pdf/2306.10089.pdf' target='_blank'>https://arxiv.org/pdf/2306.10089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Thomine, Hichem Snoussi, Mahmoud Soua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10089">FABLE : Fabric Anomaly Detection Automation Process</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised anomaly in industry has been a concerning topic and a stepping stone for high performance industrial automation process. The vast majority of industry-oriented methods focus on learning from good samples to detect anomaly notwithstanding some specific industrial scenario requiring even less specific training and therefore a generalization for anomaly detection. The obvious use case is the fabric anomaly detection, where we have to deal with a really wide range of colors and types of textile and a stoppage of the production line for training could not be considered. In this paper, we propose an automation process for industrial fabric texture defect detection with a specificity-learning process during the domain-generalized anomaly detection. Combining the ability to generalize and the learning process offer a fast and precise anomaly detection and segmentation. The main contributions of this paper are the following: A domain-generalization texture anomaly detection method achieving the state-of-the-art performances, a fast specific training on good samples extracted by the proposed method, a self-evaluation method based on custom defect creation and an automatic detection of already seen fabric to prevent re-training.
<div id='section'>Paperid: <span id='pid'>1605, <a href='https://arxiv.org/pdf/2306.01706.pdf' target='_blank'>https://arxiv.org/pdf/2306.01706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinman Park, Francois Barnard, Saad Hossain, Sirisha Rambhatla, Paul Fieguth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01706">Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised domain adaptation (UDA) aims to bridge the gap between source and target domains in the absence of target domain labels using two main techniques: input-level alignment (such as generative modeling and stylization) and feature-level alignment (which matches the distribution of the feature maps, e.g. gradient reversal layers). Motivated from the success of generative modeling for image classification, stylization-based methods were recently proposed for regression tasks, such as pose estimation. However, use of input-level alignment via generative modeling and stylization incur additional overhead and computational complexity which limit their use in real-world DA tasks. To investigate the role of input-level alignment for DA, we ask the following question: Is generative modeling-based stylization necessary for visual domain adaptation in regression? Surprisingly, we find that input-alignment has little effect on regression tasks as compared to classification. Based on these insights, we develop a non-parametric feature-level domain alignment method -- Implicit Stylization (ImSty) -- which results in consistent improvements over SOTA regression task, without the need for computationally intensive stylization and generative modeling. Our work conducts a critical evaluation of the role of generative modeling and stylization, at a time when these are also gaining popularity for domain generalization.
<div id='section'>Paperid: <span id='pid'>1606, <a href='https://arxiv.org/pdf/2306.00879.pdf' target='_blank'>https://arxiv.org/pdf/2306.00879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kimathi Kaai, Saad Hossain, Sirisha Rambhatla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00879">Domain Generalization for Domain-Linked Classes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) focuses on transferring domain-invariant knowledge from multiple source domains (available at train time) to an, a priori, unseen target domain(s). This requires a class to be expressed in multiple domains for the learning algorithm to break the spurious correlations between domain and class. However, in the real-world, classes may often be domain-linked, i.e. expressed only in a specific domain, which leads to extremely poor generalization performance for these classes. In this work, we aim to learn generalizable representations for these domain-linked classes by transferring domain-invariant knowledge from classes expressed in multiple source domains (domain-shared classes). To this end, we introduce this task to the community and propose a Fair and cONtrastive feature-space regularization algorithm for Domain-linked DG, FOND. Rigorous and reproducible experiments with baselines across popular DG tasks demonstrate our method and its variants' ability to accomplish state-of-the-art DG results for domain-linked classes. We also provide practical insights on data conditions that increase domain-linked class generalizability to tackle real-world data scarcity.
<div id='section'>Paperid: <span id='pid'>1607, <a href='https://arxiv.org/pdf/2305.18417.pdf' target='_blank'>https://arxiv.org/pdf/2305.18417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanka Subhra Mondal, Steven Frankland, Taylor Webb, Jonathan D. Cohen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18417">Determinantal Point Process Attention Over Grid Cell Code Supports Out of Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks have made tremendous gains in emulating human-like intelligence, and have been used increasingly as ways of understanding how the brain may solve the complex computational problems on which this relies. However, these still fall short of, and therefore fail to provide insight into how the brain supports strong forms of generalization of which humans are capable. One such case is out-of-distribution (OOD) generalization-successful performance on test examples that lie outside the distribution of the training set. Here, we identify properties of processing in the brain that may contribute to this ability. We describe a two-part algorithm that draws on specific features of neural computation to achieve OOD generalization, and provide a proof of concept by evaluating performance on two challenging cognitive tasks. First we draw on the fact that the mammalian brain represents metric spaces using grid cell code (e.g., in the entorhinal cortex): abstract representations of relational structure, organized in recurring motifs that cover the representational space. Second, we propose an attentional mechanism that operates over the grid cell code using Determinantal Point Process (DPP), that we call DPP attention (DPP-A) -- a transformation that ensures maximum sparseness in the coverage of that space. We show that a loss function that combines standard task-optimized error with DPP-A can exploit the recurring motifs in the grid cell code, and can be integrated with common architectures to achieve strong OOD generalization performance on analogy and arithmetic tasks. This provides both an interpretation of how the grid cell code in the mammalian brain may contribute to generalization performance, and at the same time a potential means for improving such capabilities in artificial neural networks.
<div id='section'>Paperid: <span id='pid'>1608, <a href='https://arxiv.org/pdf/2305.07677.pdf' target='_blank'>https://arxiv.org/pdf/2305.07677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinglun Cai, Monica Sunkara, Xilai Li, Anshu Bhatia, Xiao Pan, Sravan Bodapati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.07677">Masked Audio Text Encoders are Effective Multi-Modal Rescorers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours), MATE achieves a WER reduction of 8%-23% over the first-pass baseline.
<div id='section'>Paperid: <span id='pid'>1609, <a href='https://arxiv.org/pdf/2304.03431.pdf' target='_blank'>https://arxiv.org/pdf/2304.03431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gauri Gupta, Ritvik Kapila, Keshav Gupta, Ramesh Raskar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03431">Domain Generalization In Robust Invariant Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a desirable property for training in resource-constrained settings.
<div id='section'>Paperid: <span id='pid'>1610, <a href='https://arxiv.org/pdf/2303.11034.pdf' target='_blank'>https://arxiv.org/pdf/2303.11034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haohao Sun, Yilong Zhang, Peng Chen, Haixia Wang, Ronghua Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11034">Internal Structure Attention Network for Fingerprint Presentation Attack Detection from Optical Coherence Tomography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a non-invasive optical imaging technique, optical coherence tomography (OCT) has proven promising for automatic fingerprint recognition system (AFRS) applications. Diverse approaches have been proposed for OCT-based fingerprint presentation attack detection (PAD). However, considering the complexity and variety of PA samples, it is extremely challenging to increase the generalization ability with the limited PA dataset. To solve the challenge, this paper presents a novel supervised learning-based PAD method, denoted as ISAPAD, which applies prior knowledge to guide network training and enhance the generalization ability. The proposed dual-branch architecture can not only learns global features from the OCT image, but also concentrate on layered structure feature which comes from the internal structure attention module (ISAM). The simple yet effective ISAM enables the proposed network to obtain layered segmentation features belonging only to Bonafide from noisy OCT volume data directly. Combined with effective training strategies and PAD score generation rules, ISAPAD obtains optimal PAD performance in limited training data. Domain generalization experiments and visualization analysis validate the effectiveness of the proposed method for OCT PAD.
<div id='section'>Paperid: <span id='pid'>1611, <a href='https://arxiv.org/pdf/2303.02513.pdf' target='_blank'>https://arxiv.org/pdf/2303.02513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rabiul Awal, Roy Ka-Wei Lee, Eshaan Tanwar, Tanmay Garg, Tanmoy Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02513">Model-Agnostic Meta-Learning for Multilingual Hate Speech Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hate speech in social media is a growing phenomenon, and detecting such toxic content has recently gained significant traction in the research community. Existing studies have explored fine-tuning language models (LMs) to perform hate speech detection, and these solutions have yielded significant performance. However, most of these studies are limited to detecting hate speech only in English, neglecting the bulk of hateful content that is generated in other languages, particularly in low-resource languages. Developing a classifier that captures hate speech and nuances in a low-resource language with limited data is extremely challenging. To fill the research gap, we propose HateMAML, a model-agnostic meta-learning-based framework that effectively performs hate speech detection in low-resource languages. HateMAML utilizes a self-supervision strategy to overcome the limitation of data scarcity and produces better LM initialization for fast adaptation to an unseen target language (i.e., cross-lingual transfer) or other hate speech datasets (i.e., domain generalization). Extensive experiments are conducted on five datasets across eight different low-resource languages. The results show that HateMAML outperforms the state-of-the-art baselines by more than 3% in the cross-domain multilingual transfer setting. We also conduct ablation studies to analyze the characteristics of HateMAML.
<div id='section'>Paperid: <span id='pid'>1612, <a href='https://arxiv.org/pdf/2302.03754.pdf' target='_blank'>https://arxiv.org/pdf/2302.03754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03754">Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora ("external memories"), with the option to "plug in" new memory at inference time. We develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. We instantiate the model in a zero-shot dense retrieval setting by augmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains strong zero-shot retrieval accuracy on the eighteen tasks included in the standard BEIR benchmark. It outperforms systems that seek generalization from increased model parameters and computation steps. Our analysis further illustrates the necessity of augmenting with mixture-of-memory for robust generalization, the benefits of augmentation learning, and how MoMA utilizes the plug-in memory at inference time without changing its parameters. We plan to open source our code.
<div id='section'>Paperid: <span id='pid'>1613, <a href='https://arxiv.org/pdf/2302.02302.pdf' target='_blank'>https://arxiv.org/pdf/2302.02302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianxin Luan, John Thompson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02302">Achieving Robust Generalization for Wireless Channel Estimation Neural Networks by Designed Training Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a method to design the training data that can support robust generalization of trained neural networks to unseen channels. The proposed design that improves the generalization is described and analysed. It avoids the requirement of online training for previously unseen channels, as this is a memory and processing intensive solution, especially for battery powered mobile terminals. To prove the validity of the proposed method, we use the channels modelled by different standards and fading modelling for simulation. We also use an attention-based structure and a convolutional neural network to evaluate the generalization results achieved. Simulation results show that the trained neural networks maintain almost identical performance on the unseen channels.
<div id='section'>Paperid: <span id='pid'>1614, <a href='https://arxiv.org/pdf/2302.00980.pdf' target='_blank'>https://arxiv.org/pdf/2302.00980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hwan Heo, Youngjin Oh, Jaewon Lee, Hyunwoo J. Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00980">Domain Generalization Emerges from Dreaming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have proven that DNNs, unlike human vision, tend to exploit texture information rather than shape. Such texture bias is one of the factors for the poor generalization performance of DNNs. We observe that the texture bias negatively affects not only in-domain generalization but also out-of-distribution generalization, i.e., Domain Generalization. Motivated by the observation, we propose a new framework to reduce the texture bias of a model by a novel optimization-based data augmentation, dubbed Stylized Dream. Our framework utilizes adaptive instance normalization (AdaIN) to augment the style of an original image yet preserve the content. We then adopt a regularization loss to predict consistent outputs between Stylized Dream and original images, which encourages the model to learn shape-based representations. Extensive experiments show that the proposed method achieves state-of-the-art performance in out-of-distribution settings on public benchmark datasets: PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet.
<div id='section'>Paperid: <span id='pid'>1615, <a href='https://arxiv.org/pdf/2301.09120.pdf' target='_blank'>https://arxiv.org/pdf/2301.09120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zining Chen, Weiqiu Wang, Zhicheng Zhao, Aidong Men
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.09120">Causality-based Dual-Contrastive Learning Framework for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) is essentially a sub-branch of out-of-distribution generalization, which trains models from multiple source domains and generalizes to unseen target domains. Recently, some domain generalization algorithms have emerged, but most of them were designed with non-transferable complex architecture. Additionally, contrastive learning has become a promising solution for simplicity and efficiency in DG. However, existing contrastive learning neglected domain shifts that caused severe model confusions. In this paper, we propose a Dual-Contrastive Learning (DCL) module on feature and prototype contrast. Moreover, we design a novel Causal Fusion Attention (CFA) module to fuse diverse views of a single image to attain prototype. Furthermore, we introduce a Similarity-based Hard-pair Mining (SHM) strategy to leverage information on diversity shift. Extensive experiments show that our method outperforms state-of-the-art algorithms on three DG datasets. The proposed algorithm can also serve as a plug-and-play module without usage of domain labels.
<div id='section'>Paperid: <span id='pid'>1616, <a href='https://arxiv.org/pdf/2301.00383.pdf' target='_blank'>https://arxiv.org/pdf/2301.00383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenan Huang, Jun Wen, Siheng Chen, Linchao Zhu, Nenggan Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00383">Discriminative Radial Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptation methods reduce domain shift typically by learning domain-invariant features. Most existing methods are built on distribution matching, e.g., adversarial domain adaptation, which tends to corrupt feature discriminability. In this paper, we propose Discriminative Radial Domain Adaptation (DRDA) which bridges source and target domains via a shared radial structure. It's motivated by the observation that as the model is trained to be progressively discriminative, features of different categories expand outwards in different directions, forming a radial structure. We show that transferring such an inherently discriminative structure would enable to enhance feature transferability and discriminability simultaneously. Specifically, we represent each domain with a global anchor and each category a local anchor to form a radial structure and reduce domain shift via structure matching. It consists of two parts, namely isometric transformation to align the structure globally and local refinement to match each category. To enhance the discriminability of the structure, we further encourage samples to cluster close to the corresponding local anchors based on optimal-transport assignment. Extensively experimenting on multiple benchmarks, our method is shown to consistently outperforms state-of-the-art approaches on varied tasks, including the typical unsupervised domain adaptation, multi-source domain adaptation, domain-agnostic learning, and domain generalization.
<div id='section'>Paperid: <span id='pid'>1617, <a href='https://arxiv.org/pdf/2212.11237.pdf' target='_blank'>https://arxiv.org/pdf/2212.11237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhao Yuan, Francesco Pinto, Adam Davies, Philip Torr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.11237">Not Just Pretty Pictures: Toward Interventional Data Augmentation Using Text-to-Image Generators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural image classifiers are known to undergo severe performance degradation when exposed to inputs that are sampled from environmental conditions that differ from their training data. Given the recent progress in Text-to-Image (T2I) generation, a natural question is how modern T2I generators can be used to simulate arbitrary interventions over such environmental factors in order to augment training data and improve the robustness of downstream classifiers. We experiment across a diverse collection of benchmarks in single domain generalization (SDG) and reducing reliance on spurious features (RRSF), ablating across key dimensions of T2I generation, including interventional prompting strategies, conditioning mechanisms, and post-hoc filtering. Our extensive empirical findings demonstrate that modern T2I generators like Stable Diffusion can indeed be used as a powerful interventional data augmentation mechanism, outperforming previously state-of-the-art data augmentation techniques regardless of how each dimension is configured.
<div id='section'>Paperid: <span id='pid'>1618, <a href='https://arxiv.org/pdf/2211.08583.pdf' target='_blank'>https://arxiv.org/pdf/2211.08583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiroki Naganuma, Kartik Ahuja, Shiro Takagi, Tetsuya Motokawa, Rio Yokota, Kohta Ishikawa, Ikuro Sato, Ioannis Mitliagkas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.08583">Empirical Study on Optimizer Selection for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern deep learning systems do not generalize well when the test data distribution is slightly different to the training data distribution. While much promising work has been accomplished to address this fragility, a systematic study of the role of optimizers and their out-of-distribution generalization performance has not been undertaken. In this study, we examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We address this question for image and text classification using DomainBed, WILDS, and Backgrounds Challenge as testbeds for studying different types of shifts -- namely correlation and diversity shift. We search over a wide range of hyperparameters and examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following findings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g., Adam) perform worse than non-adaptive optimizers (e.g., SGD, momentum SGD) on out-of-distribution performance. In particular, even though there is no significant difference in in-distribution performance, we show a measurable difference in out-of-distribution performance. ii) in-distribution performance and out-of-distribution performance exhibit three types of behavior depending on the dataset -- linear returns, increasing returns, and diminishing returns. For example, in the training of natural language data using Adam, fine-tuning the performance of in-distribution performance does not significantly contribute to the out-of-distribution generalization performance.
<div id='section'>Paperid: <span id='pid'>1619, <a href='https://arxiv.org/pdf/2211.02475.pdf' target='_blank'>https://arxiv.org/pdf/2211.02475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sivaramakrishnan Rajaraman, Feng Yang, Ghada Zamzmi, Zhiyun Xue, Sameer Antani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.02475">Generalizability of Deep Adult Lung Segmentation Models to the Pediatric Population: A Retrospective Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lung segmentation in chest X-rays (CXRs) is an important prerequisite for improving the specificity of diagnoses of cardiopulmonary diseases in a clinical decision support system. Current deep learning models for lung segmentation are trained and evaluated on CXR datasets in which the radiographic projections are captured predominantly from the adult population. However, the shape of the lungs is reported to be significantly different across the developmental stages from infancy to adulthood. This might result in age-related data domain shifts that would adversely impact lung segmentation performance when the models trained on the adult population are deployed for pediatric lung segmentation. In this work, our goal is to (i) analyze the generalizability of deep adult lung segmentation models to the pediatric population and (ii) improve performance through a stage-wise, systematic approach consisting of CXR modality-specific weight initializations, stacked ensembles, and an ensemble of stacked ensembles. To evaluate segmentation performance and generalizability, novel evaluation metrics consisting of mean lung contour distance (MLCD) and average hash score (AHS) are proposed in addition to the multi-scale structural similarity index measure (MS-SSIM), the intersection of union (IoU), Dice score, 95% Hausdorff distance (HD95), and average symmetric surface distance (ASSD). Our results showed a significant improvement (p < 0.05) in cross-domain generalization through our approach. This study could serve as a paradigm to analyze the cross-domain generalizability of deep segmentation models for other medical imaging modalities and applications.
<div id='section'>Paperid: <span id='pid'>1620, <a href='https://arxiv.org/pdf/2207.02093.pdf' target='_blank'>https://arxiv.org/pdf/2207.02093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathan Ng, Neha Hulkund, Kyunghyun Cho, Marzyeh Ghassemi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.02093">Predicting Out-of-Domain Generalization with Neighborhood Invariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing and deploying machine learning models safely depends on the ability to characterize and compare their abilities to generalize to new environments. Although recent work has proposed a variety of methods that can directly predict or theoretically bound the generalization capacity of a model, they rely on strong assumptions such as matching train/test distributions and access to model gradients. In order to characterize generalization when these assumptions are not satisfied, we propose neighborhood invariance, a measure of a classifier's output invariance in a local transformation neighborhood. Specifically, we sample a set of transformations and given an input test point, calculate the invariance as the largest fraction of transformed points classified into the same class. Crucially, our measure is simple to calculate, does not depend on the test point's true label, makes no assumptions about the data distribution or model, and can be applied even in out-of-domain (OOD) settings where existing methods cannot, requiring only selecting a set of appropriate data transformations. In experiments on robustness benchmarks in image classification, sentiment analysis, and natural language inference, we demonstrate a strong and robust correlation between our neighborhood invariance measure and actual OOD generalization on over 4,600 models evaluated on over 100 unique train/test domain pairs.
<div id='section'>Paperid: <span id='pid'>1621, <a href='https://arxiv.org/pdf/2109.14196.pdf' target='_blank'>https://arxiv.org/pdf/2109.14196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Namyup Kim, Taeyoung Son, Jaehyun Pahk, Cuiling Lan, Wenjun Zeng, Suha Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.14196">WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect web-crawled images which present large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects styles of the web-crawled images into training images on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled images with their predicted pseudo labels for training to further enhance the capability of the network. Extensive experiments demonstrate that our method clearly outperforms existing domain generalization techniques.
<div id='section'>Paperid: <span id='pid'>1622, <a href='https://arxiv.org/pdf/2510.04441.pdf' target='_blank'>https://arxiv.org/pdf/2510.04441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Zhu, Naihao Deng, Naichen Shi, Aditya Gangrade, Clayton Scott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04441">Domain Generalization: A Tale of Two ERMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) is the problem of generalizing from several distributions (or domains), for which labeled training data are available, to a new test domain for which no labeled data is available. A common finding in the DG literature is that it is difficult to outperform empirical risk minimization (ERM) on the pooled training data. In this work, we argue that this finding has primarily been reported for datasets satisfying a \emph{covariate shift} assumption. When the dataset satisfies a \emph{posterior drift} assumption instead, we show that ``domain-informed ERM,'' wherein feature vectors are augmented with domain-specific information, outperforms pooling ERM. These claims are supported by a theoretical framework and experiments on language and vision tasks.
<div id='section'>Paperid: <span id='pid'>1623, <a href='https://arxiv.org/pdf/2510.01165.pdf' target='_blank'>https://arxiv.org/pdf/2510.01165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oussama Gabouj, Kamel Charaf, Ivan Zakazov, Nicolas Baldwin, Robert West
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01165">GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) achieve strong performance across diverse tasks, but their effectiveness often depends on the quality of the provided context. Retrieval-Augmented Generation (RAG) enriches prompts with external information, but its reliance on static databases constrains adaptability and can result in irrelevant demonstrations. In this work, we propose a Generative Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach where an LLM model is trained to generate input-specific concise demonstrations. By tailoring demonstrations to each input, our method offers better contextual support than traditional RAG approaches. We demonstrate the superiority of GRAD under budget constraints, where we limit both the number of tokens used per demonstration and the number of tokens used for the final output. Trained solely on a math dataset, GRAD consistently outperforms strong baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM questions, highlighting GRAD's robust generalization to out-of-distribution (OOD) domains such as physics, chemistry, and computer science. Furthermore, we show that demonstrations generated by trained smaller models can effectively guide larger target models, reducing training costs while maintaining competitive accuracy. Overall, this work introduces a scalable demonstration generator model presenting the first step toward a dynamic few-shot learning paradigm in resource-constrained settings. We release the code used for the project.
<div id='section'>Paperid: <span id='pid'>1624, <a href='https://arxiv.org/pdf/2510.00478.pdf' target='_blank'>https://arxiv.org/pdf/2510.00478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wang, Wonho Bae, Jiahong Chen, Wenxu Wang, Junhyug Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00478">Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work on latent diffusion models (LDMs) has focused almost exclusively on generative tasks, leaving their potential for discriminative transfer largely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a novel LDM-based framework for a more practical variant of source-free domain adaptation (SFDA): the source provider may share not only a pre-trained classifier but also an auxiliary latent diffusion module, trained once on the source data and never exposing raw source samples. DVD encodes each source feature's label information into its latent vicinity by fitting a Gaussian prior over its k-nearest neighbors and training the diffusion network to drift noisy samples back to label-consistent representations. During adaptation, we sample from each target feature's latent vicinity, apply the frozen diffusion module to generate source-like cues, and use a simple InfoNCE loss to align the target encoder to these cues, explicitly transferring decision boundaries without source access. Across standard SFDA benchmarks, DVD outperforms state-of-the-art methods. We further show that the same latent diffusion module enhances the source classifier's accuracy on in-domain data and boosts performance in supervised classification and domain generalization experiments. DVD thus reinterprets LDMs as practical, privacy-preserving bridges for explicit knowledge transfer, addressing a core challenge in source-free domain adaptation that prior methods have yet to solve.
<div id='section'>Paperid: <span id='pid'>1625, <a href='https://arxiv.org/pdf/2509.14195.pdf' target='_blank'>https://arxiv.org/pdf/2509.14195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalima Binta Manir, Tim Oates
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14195">Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mental representation, characterized by structured internal models mirroring external environments, is fundamental to advanced cognition but remains challenging to investigate empirically. Existing theory hypothesizes that second-order learning -- learning mechanisms that adapt first-order learning (i.e., learning about the task/domain) -- promotes the emergence of such environment-cognition isomorphism. In this paper, we empirically validate this hypothesis by proposing a hierarchical architecture comprising a Graph Convolutional Network (GCN) as a first-order learner and an MLP controller as a second-order learner. The GCN directly maps node-level features to predictions of optimal navigation paths, while the MLP dynamically adapts the GCN's parameters when confronting structurally novel maze environments. We demonstrate that second-order learning is particularly effective when the cognitive system develops an internal mental map structurally isomorphic to the environment. Quantitative and qualitative results highlight significant performance improvements and robust generalization on unseen maze tasks, providing empirical support for the pivotal role of structured mental representations in maximizing the effectiveness of second-order learning.
<div id='section'>Paperid: <span id='pid'>1626, <a href='https://arxiv.org/pdf/2509.13442.pdf' target='_blank'>https://arxiv.org/pdf/2509.13442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnab Kumar Roy, Hemant Kumar Kathania, Paban Sapkota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13442">Enhancing Speaker-Independent Dysarthric Speech Severity Classification with DSSCNet and Cross-Corpus Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dysarthric speech severity classification is crucial for objective clinical assessment and progress monitoring in individuals with motor speech disorders. Although prior methods have addressed this task, achieving robust generalization in speaker-independent (SID) scenarios remains challenging. This work introduces DSSCNet, a novel deep neural architecture that combines Convolutional, Squeeze-Excitation (SE), and Residual network, helping it extract discriminative representations of dysarthric speech from mel spectrograms. The addition of SE block selectively focuses on the important features of the dysarthric speech, thereby minimizing loss and enhancing overall model performance. We also propose a cross-corpus fine-tuning framework for severity classification, adapted from detection-based transfer learning approaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora: TORGO and UA-Speech under speaker-independent evaluation protocols: One-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols. DSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and 64.18% under LOSO setting on TORGO and UA-Speech respectively outperforming existing state-of-the-art methods. Upon fine-tuning, the performance improves substantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25% on UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. These results demonstrate the effectiveness and generalizability of DSSCNet for fine-grained severity classification across diverse dysarthric speech datasets.
<div id='section'>Paperid: <span id='pid'>1627, <a href='https://arxiv.org/pdf/2509.09262.pdf' target='_blank'>https://arxiv.org/pdf/2509.09262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seung Gyu Jeong, Seong Eun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09262">Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this technical report, we describe our submission for Task 1, Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025 Challenge. Our work tackles the dual challenges of strict complexity constraints and robust generalization to both seen and unseen devices, while also leveraging the new rule allowing the use of device labels at test time. Our proposed system is based on a knowledge distillation framework where an efficient CP-MobileNet student learns from a compact, specialized two-teacher ensemble. This ensemble combines a baseline PaSST teacher, trained with standard cross-entropy, and a 'generalization expert' teacher. This expert is trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted from prior work, which explicitly structures the feature space for device robustness. To capitalize on the availability of test-time device labels, the distilled student model then undergoes a final device-specific fine-tuning stage. Our proposed system achieves a final accuracy of 57.93\% on the development set, demonstrating a significant improvement over the official baseline, particularly on unseen devices.
<div id='section'>Paperid: <span id='pid'>1628, <a href='https://arxiv.org/pdf/2509.08705.pdf' target='_blank'>https://arxiv.org/pdf/2509.08705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalima Binta Manir, Tim Oates
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08705">One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel Theory of Mind (ToM) framework inspired by dual-process theories from cognitive science, integrating a fast, habitual graph-based reasoning system (System 1), implemented via graph convolutional networks (GCNs), and a slower, context-sensitive meta-adaptive learning system (System 2), driven by meta-learning techniques. Our model dynamically balances intuitive and deliberative reasoning through a learned context gate mechanism. We validate our architecture on canonical false-belief tasks and systematically explore its capacity to replicate hallmark cognitive biases associated with dual-process theory, including anchoring, cognitive-load fatigue, framing effects, and priming effects. Experimental results demonstrate that our dual-process approach closely mirrors human adaptive behavior, achieves robust generalization to unseen contexts, and elucidates cognitive mechanisms underlying reasoning biases. This work bridges artificial intelligence and cognitive theory, paving the way for AI systems exhibiting nuanced, human-like social cognition and adaptive decision-making capabilities.
<div id='section'>Paperid: <span id='pid'>1629, <a href='https://arxiv.org/pdf/2509.07381.pdf' target='_blank'>https://arxiv.org/pdf/2509.07381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sichao Wu, Jiang Wu, Xingyu Cao, Fawang Zhang, Guangyuan Yu, Junjie Zhao, Yue Qu, Fei Ma, Jingliang Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07381">TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional online Model Predictive Control (MPC) methods often suffer from excessive computational complexity, limiting their practical deployment. Explicit MPC mitigates online computational load by pre-computing control policies offline; however, existing explicit MPC methods typically rely on simplified system dynamics and cost functions, restricting their accuracy for complex systems. This paper proposes TransMPC, a novel Transformer-based explicit MPC algorithm capable of generating highly accurate control sequences in real-time for complex dynamic systems. Specifically, we formulate the MPC policy as an encoder-only Transformer leveraging bidirectional self-attention, enabling simultaneous inference of entire control sequences in a single forward pass. This design inherently accommodates variable prediction horizons while ensuring low inference latency. Furthermore, we introduce a direct policy optimization framework that alternates between sampling and learning phases. Unlike imitation-based approaches dependent on precomputed optimal trajectories, TransMPC directly optimizes the true finite-horizon cost via automatic differentiation. Random horizon sampling combined with a replay buffer provides independent and identically distributed (i.i.d.) training samples, ensuring robust generalization across varying states and horizon lengths. Extensive simulations and real-world vehicle control experiments validate the effectiveness of TransMPC in terms of solution accuracy, adaptability to varying horizons, and computational efficiency.
<div id='section'>Paperid: <span id='pid'>1630, <a href='https://arxiv.org/pdf/2509.02983.pdf' target='_blank'>https://arxiv.org/pdf/2509.02983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02983">DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous underwater navigation remains a challenging problem due to limited sensing capabilities and the difficulty of constructing accurate maps in underwater environments. In this paper, we propose a Diffusion-based Underwater Visual Navigation policy via knowledge-transferred depth features, named DUViN, which enables vision-based end-to-end 4-DoF motion control for underwater vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles and maintain a safe and perception awareness altitude relative to the terrain without relying on pre-built maps. To address the difficulty of collecting large-scale underwater navigation datasets, we propose a method that ensures robust generalization under domain shifts from in-air to underwater environments by leveraging depth features and introducing a novel model transfer strategy. Specifically, our training framework consists of two phases: we first train the diffusion-based visual navigation policy on in-air datasets using a pre-trained depth feature extractor. Secondly, we retrain the extractor on an underwater depth estimation task and integrate the adapted extractor into the trained navigation policy from the first step. Experiments in both simulated and real-world underwater environments demonstrate the effectiveness and generalization of our approach. The experimental videos are available at https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.
<div id='section'>Paperid: <span id='pid'>1631, <a href='https://arxiv.org/pdf/2509.01642.pdf' target='_blank'>https://arxiv.org/pdf/2509.01642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian P. Oppelt, Andreas Foltyn, Nadine R. Lang-Richter, Bjoern M. Eskofier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01642">REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task load detection is essential for optimizing human performance across diverse applications, yet current models often lack generalizability beyond narrow experimental domains. While prior research has focused on individual tasks and limited modalities, there remains a gap in evaluating model robustness and transferability in real-world scenarios. This paper addresses these limitations by introducing a new multimodal dataset that extends established cognitive load detection benchmarks with a real-world gaming application, using the $n$-back test as a scientific foundation. Task load annotations are derived from objective performance, subjective NASA-TLX ratings, and task-level design, enabling a comprehensive evaluation framework. State-of-the-art end-to-end model, including xLSTM, ConvNeXt, and Transformer architectures are systematically trained and evaluated on multiple modalities and application domains to assess their predictive performance and cross-domain generalization. Results demonstrate that multimodal approaches consistently outperform unimodal baselines, with specific modalities and model architectures showing varying impact depending on the application subset. Importantly, models trained on one domain exhibit reduced performance when transferred to novel applications, underscoring remaining challenges for universal cognitive load estimation. These findings provide robust baselines and actionable insights for developing more generalizable cognitive load detection systems, advancing both research and practical implementation in human-computer interaction and adaptive systems.
<div id='section'>Paperid: <span id='pid'>1632, <a href='https://arxiv.org/pdf/2508.20534.pdf' target='_blank'>https://arxiv.org/pdf/2508.20534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frederik Rajiv Manichand, Robin Deuber, Robert Jakob, Steve Swerling, Jamie Rosen, Elgar Fleisch, Patrick Langer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20534">Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating Body Mass Index (BMI) from camera images with machine learning models enables rapid weight assessment when traditional methods are unavailable or impractical, such as in telehealth or emergency scenarios. Existing computer vision approaches have been limited to datasets of up to 14,500 images. In this study, we present a deep learning-based BMI estimation method trained on our WayBED dataset, a large proprietary collection of 84,963 smartphone images from 25,353 individuals. We introduce an automatic filtering method that uses posture clustering and person detection to curate the dataset by removing low-quality images, such as those with atypical postures or incomplete views. This process retained 71,322 high-quality images suitable for training. We achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test set (WayBED data) using full-body images, the lowest value in the published literature to the best of our knowledge. Further, we achieve a MAPE of 13% on the completely unseen~(during training) VisualBodyToBMI dataset, comparable with state-of-the-art approaches trained on it, demonstrating robust generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the full pipeline, including image filtering and BMI estimation, on Android devices using the CLAID framework. We release our complete code for model training, filtering, and the CLAID package for mobile deployment as open-source contributions.
<div id='section'>Paperid: <span id='pid'>1633, <a href='https://arxiv.org/pdf/2508.10026.pdf' target='_blank'>https://arxiv.org/pdf/2508.10026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhao, Yanjun Zhao, Jiaming Song, Shien He, Lusheng Zhang, Qiang Zhang, Tianjiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10026">SABER: Switchable and Balanced Training for Efficient LLM Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems. We propose SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. SABER first profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. In parallel, we incorporate no-think examples to ensure the model remains reliable even when explicit reasoning is turned off. SABER further supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling flexible trade-offs between latency and reasoning depth. Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.
<div id='section'>Paperid: <span id='pid'>1634, <a href='https://arxiv.org/pdf/2508.08549.pdf' target='_blank'>https://arxiv.org/pdf/2508.08549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Li, Pengcheng Zhou, Linye Ma, Wenyi Zhao, Huihua Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08549">Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Both limited annotation and domain shift are significant challenges frequently encountered in medical image segmentation, leading to derivative scenarios like semi-supervised medical (SSMIS), semi-supervised medical domain generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA). Conventional methods are generally tailored to specific tasks in isolation, the error accumulation hinders the effective utilization of unlabeled data and limits further improvements, resulting in suboptimal performance when these issues occur. In this paper, we aim to develop a generic framework that masters all three tasks. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data and increasing the diversity of the model. To tackle this issue, we employ a Diverse Teaching and Label Propagation Network (DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation. Our DTLP-Net involves a single student model and two diverse teacher models, which can generate reliable pseudo-labels for the student model. The first teacher model decouple the training process with labeled and unlabeled data, The second teacher is momentum-updated periodically, thus generating reliable yet divers pseudo-labels. To fully utilize the information within the data, we adopt inter-sample and intra-sample data augmentation to learn the global and local knowledge. In addition, to further capture the voxel-level correlations, we propose label propagation to enhance the model robust. We evaluate our proposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG tasks. The results showcase notable improvements compared to state-of-the-art methods across all five settings, indicating the potential of our framework to tackle more challenging SSL scenarios.
<div id='section'>Paperid: <span id='pid'>1635, <a href='https://arxiv.org/pdf/2508.06248.pdf' target='_blank'>https://arxiv.org/pdf/2508.06248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrii Yermakov, Jan Cech, Jiri Matas, Mario Fritz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06248">Deepfake Detection that Generalizes Across Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization of deepfake detectors to unseen manipulation techniques remains a challenge for practical deployment. Although many approaches adapt foundation models by introducing significant architectural complexity, this work demonstrates that robust generalization is achievable through a parameter-efficient adaptation of a pre-trained CLIP vision encoder. The proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters (0.03% of the total) and enhances generalization by enforcing a hyperspherical feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from 2019 to 2025. The proposed method achieves state-of-the-art performance, outperforming more complex, recent approaches in average cross-dataset AUROC. Our analysis yields two primary findings for the field: 1) training on paired real-fake data from the same source video is essential for mitigating shortcut learning and improving generalization, and 2) detection difficulty on academic datasets has not strictly increased over time, with models trained on older, diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method, proving that state-of-the-art generalization is attainable by making targeted, minimal changes to a pre-trained CLIP model. The code will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>1636, <a href='https://arxiv.org/pdf/2508.06248.pdf' target='_blank'>https://arxiv.org/pdf/2508.06248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrii Yermakov, Jan Cech, Jiri Matas, Mario Fritz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06248">Deepfake Detection that Generalizes Across Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization of deepfake detectors to unseen manipulation techniques remains a challenge for practical deployment. Although many approaches adapt foundation models by introducing significant architectural complexity, this work demonstrates that robust generalization is achievable through a parameter-efficient adaptation of one of the foundational pre-trained vision encoders. The proposed method, GenD, fine-tunes only the Layer Normalization parameters (0.03% of the total) and enhances generalization by enforcing a hyperspherical feature manifold using L2 normalization and metric learning on it. We conducted an extensive evaluation on 14 benchmark datasets spanning from 2019 to 2025. The proposed method achieves state-of-the-art performance, outperforming more complex, recent approaches in average cross-dataset AUROC. Our analysis yields two primary findings for the field: 1) training on paired real-fake data from the same source video is essential for mitigating shortcut learning and improving generalization, and 2) detection difficulty on academic datasets has not strictly increased over time, with models trained on older, diverse datasets showing strong generalization capabilities. This work delivers a computationally efficient and reproducible method, proving that state-of-the-art generalization is attainable by making targeted, minimal changes to a pre-trained foundational image encoder model. The code will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>1637, <a href='https://arxiv.org/pdf/2507.22092.pdf' target='_blank'>https://arxiv.org/pdf/2507.22092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Carloni, Biagio Brattoli, Seongho Keum, Jongchan Park, Taebum Lee, Chang Ho Ahn, Sergio Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22092">Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational pathology (CPath) has shown great potential in mining actionable insights from Whole Slide Images (WSIs). Deep Learning (DL) has been at the center of modern CPath, and while it delivers unprecedented performance, it is also known that DL may be affected by irrelevant details, such as those introduced during scanning by different commercially available scanners. This may lead to scanner bias, where the model outputs for the same tissue acquired by different scanners may vary. In turn, it hinders the trust of clinicians in CPath-based tools and their deployment in real-world clinical practices. Recent pathology Foundation Models (FMs) promise to provide better domain generalization capabilities. In this paper, we benchmark FMs using a multi-scanner dataset and show that FMs still suffer from scanner bias. Following this observation, we propose ScanGen, a contrastive loss function applied during task-specific fine-tuning that mitigates scanner bias, thereby enhancing the models' robustness to scanner variations. Our approach is applied to the Multiple Instance Learning task of Epidermal Growth Factor Receptor (EGFR) mutation prediction from H\&E-stained WSIs in lung cancer. We observe that ScanGen notably enhances the ability to generalize across scanners, while retaining or improving the performance of EGFR mutation prediction.
<div id='section'>Paperid: <span id='pid'>1638, <a href='https://arxiv.org/pdf/2507.20028.pdf' target='_blank'>https://arxiv.org/pdf/2507.20028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dhruv Sarkar, Aprameyo Chakrabartty, Bibhudatta Bhanja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20028">TAPS : Frustratingly Simple Test Time Active Learning for VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-Time Optimization enables models to adapt to new data during inference by updating parameters on-the-fly. Recent advances in Vision-Language Models (VLMs) have explored learning prompts at test time to improve performance in downstream tasks. In this work, we extend this idea by addressing a more general and practical challenge: Can we effectively utilize an oracle in a continuous data stream where only one sample is available at a time, requiring an immediate query decision while respecting latency and memory constraints? To tackle this, we propose a novel Test-Time Active Learning (TTAL) framework that adaptively queries uncertain samples and updates prompts dynamically. Unlike prior methods that assume batched data or multiple gradient updates, our approach operates in a real-time streaming scenario with a single test sample per step. We introduce a dynamically adjusted entropy threshold for active querying, a class-balanced replacement strategy for memory efficiency, and a class-aware distribution alignment technique to enhance adaptation. The design choices are justified using careful theoretical analysis. Extensive experiments across 10 cross-dataset transfer benchmarks and 4 domain generalization datasets demonstrate consistent improvements over state-of-the-art methods while maintaining reasonable latency and memory overhead. Our framework provides a practical and effective solution for real-world deployment in safety-critical applications such as autonomous systems and medical diagnostics.
<div id='section'>Paperid: <span id='pid'>1639, <a href='https://arxiv.org/pdf/2507.13207.pdf' target='_blank'>https://arxiv.org/pdf/2507.13207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Etienne Le Naour, Tahar Nabil, Ghislain Agoua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13207">MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a growing interest for time series foundation models, with a strong emphasis on the forecasting task. Yet, the crucial task of out-of-domain imputation of missing values remains largely underexplored. We propose a first step to fill this gap by leveraging implicit neural representations (INRs). INRs model time series as continuous functions and naturally handle various missing data scenarios and sampling rates. While they have shown strong performance within specific distributions, they struggle under distribution shifts. To address this, we introduce MoTM (Mixture of Timeflow Models), a step toward a foundation model for time series imputation. Building on the idea that a new time series is a mixture of previously seen patterns, MoTM combines a basis of INRs, each trained independently on a distinct family of time series, with a ridge regressor that adapts to the observed context at inference. We demonstrate robust in-domain and out-of-domain generalization across diverse imputation scenarios (e.g., block and pointwise missingness, variable sampling rates), paving the way for adaptable foundation imputation models.
<div id='section'>Paperid: <span id='pid'>1640, <a href='https://arxiv.org/pdf/2507.12060.pdf' target='_blank'>https://arxiv.org/pdf/2507.12060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun-Hsiang Lin, Yu-Wen Tseng, Kang-Yang Huang, Jhih-Ciang Wu, Wen-Huang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12060">InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face anti-spoofing (FAS) aims to construct a robust system that can withstand diverse attacks. While recent efforts have concentrated mainly on cross-domain generalization, two significant challenges persist: limited semantic understanding of attack types and training redundancy across domains. We address the first by integrating vision-language models (VLMs) to enhance the perception of visual input. For the second challenge, we employ a meta-domain strategy to learn a unified model that generalizes well across multiple domains. Our proposed InstructFLIP is a novel instruction-tuned framework that leverages VLMs to enhance generalization via textual guidance trained solely on a single domain. At its core, InstructFLIP explicitly decouples instructions into content and style components, where content-based instructions focus on the essential semantics of spoofing, and style-based instructions consider variations related to the environment and camera characteristics. Extensive experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA models in accuracy and substantially reducing training redundancy across diverse domains in FAS. Project website is available at https://kunkunlin1221.github.io/InstructFLIP.
<div id='section'>Paperid: <span id='pid'>1641, <a href='https://arxiv.org/pdf/2507.09681.pdf' target='_blank'>https://arxiv.org/pdf/2507.09681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Osher Rafaeli, Tal Svoray, Ariel Nahlieli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09681">Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-resolution elevation estimations are essential to understand catchment and hillslope hydrology, study urban morphology and dynamics, and monitor the growth, decline, and mortality of terrestrial ecosystems. Various deep learning approaches (e.g., super-resolution techniques, monocular depth estimation) have been developed to create high-resolution Digital Elevation Models (DEMs). However, super-resolution techniques are limited by the upscaling factor, and monocular depth estimation lacks global elevation context, making its conversion to a seamless DEM restricted. The recently introduced technique of prompt-based monocular depth estimation has opened new opportunities to extract estimates of absolute elevation in a global context. We present here a framework for the estimation of high-resolution DEMs as a new paradigm for absolute global elevation mapping. It is exemplified using low-resolution Shuttle Radar Topography Mission (SRTM) elevation data as prompts and high-resolution RGB imagery from the National Agriculture Imagery Program (NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived DEMs and employs a versatile prompting strategy, enabling tasks such as DEM estimation, void filling, and updating. Our framework achieves a 100x resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of magnitude. Evaluations across three diverse U.S. landscapes show robust generalization, capturing urban structures and fine-scale terrain features with < 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological analysis confirms suitability for hazard and environmental studies. We demonstrate scalability by applying the framework to large regions in the U.S. and Israel. All code and pretrained models are publicly available at: https://osherr1996.github.io/prompt2dem_propage/.
<div id='section'>Paperid: <span id='pid'>1642, <a href='https://arxiv.org/pdf/2507.04494.pdf' target='_blank'>https://arxiv.org/pdf/2507.04494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niels Leadholm, Viviane Clay, Scott Knudstrup, Hojae Lee, Jeff Hawkins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04494">Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current AI systems achieve impressive performance on many tasks, yet they lack core attributes of biological intelligence, including rapid, continual learning, representations grounded in sensorimotor interactions, and structured knowledge that enables efficient generalization. Neuroscience theory suggests that mammals evolved flexible intelligence through the replication of a semi-independent, sensorimotor module, a functional unit known as a cortical column. To address the disparity between biological and artificial intelligence, thousand-brains systems were proposed as a means of mirroring the architecture of cortical columns and their interactions.
  In the current work, we evaluate the unique properties of Monty, the first implementation of a thousand-brains system. We focus on 3D object perception, and in particular, the combined task of object recognition and pose estimation. Utilizing the YCB dataset of household objects, we first assess Monty's use of sensorimotor learning to build structured representations, finding that these enable robust generalization. These representations include an emphasis on classifying objects by their global shape, as well as a natural ability to detect object symmetries. We then explore Monty's use of model-free and model-based policies to enable rapid inference by supporting principled movements. We find that such policies complement Monty's modular architecture, a design that can accommodate communication between modules to further accelerate inference speed via a novel `voting' algorithm. Finally, we examine Monty's use of associative, Hebbian-like binding to enable rapid, continual, and computationally efficient learning, properties that compare favorably to current deep learning architectures. While Monty is still in a nascent stage of development, these findings support thousand-brains systems as a powerful and promising new approach to AI.
<div id='section'>Paperid: <span id='pid'>1643, <a href='https://arxiv.org/pdf/2506.23577.pdf' target='_blank'>https://arxiv.org/pdf/2506.23577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanning Hou, Yanran Ruan, Junfa Li, Shanshan Wang, Jianfeng Qiu, Ke Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23577">StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing the alignment between text and image features in the CLIP model is a critical challenge in zero-shot industrial anomaly detection tasks. Recent studies predominantly utilize specific category prompts during pretraining, which can cause overfitting to the training categories and limit model generalization. To address this, we propose a method that transforms category names through multicategory name stacking to create stacked prompts, forming the basis of our StackCLIP model. Our approach introduces two key components. The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts by stacking semantically analogous categories, while utilizing multi-object textual feature fusion to amplify discriminative anomalies among similar objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific linear layers tailored for each stack cluster and adaptively integrates them based on the attributes of test categories. These modules work together to deliver superior training speed, stability, and convergence, significantly boosting anomaly segmentation performance. Additionally, our stacked prompt framework offers robust generalization across classification tasks. To further improve performance, we introduce the Regulating Prompt Learning (RPL) module, which leverages the generalization power of stacked prompts to refine prompt learning, elevating results in anomaly detection classification tasks. Extensive testing on seven industrial anomaly detection datasets demonstrates that our method achieves state-of-the-art performance in both zero-shot anomaly detection and segmentation tasks.
<div id='section'>Paperid: <span id='pid'>1644, <a href='https://arxiv.org/pdf/2506.17761.pdf' target='_blank'>https://arxiv.org/pdf/2506.17761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiheng Liang, Ziru Yu, Zujie Xie, Yuchen Guo, Yulan Guo, Xiangyang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17761">Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the limitations of current spectral analysis methods-such as reliance on single-modality data, limited generalizability, and poor interpretability-we propose a novel multi-modal spectral analysis framework that integrates prior knowledge graphs with Large Language Models. Our method explicitly bridges physical spectral measurements and chemical structural semantics by representing them in a unified Textual Graph format, enabling flexible, interpretable, and generalizable spectral understanding. Raw spectra are first transformed into TAGs, where nodes and edges are enriched with textual attributes describing both spectral properties and chemical context. These are then merged with relevant prior knowledge-including functional groups and molecular graphs-to form a Task Graph that incorporates "Prompt Nodes" supporting LLM-based contextual reasoning. A Graph Neural Network further processes this structure to complete downstream tasks. This unified design enables seamless multi-modal integration and automated feature decoding with minimal manual annotation. Our framework achieves consistently high performance across multiple spectral analysis tasks, including node-level, edge-level, and graph-level classification. It demonstrates robust generalization in both zero-shot and few-shot settings, highlighting its effectiveness in learning from limited data and supporting in-context reasoning. This work establishes a scalable and interpretable foundation for LLM-driven spectral analysis, unifying physical and chemical modalities for scientific applications.
<div id='section'>Paperid: <span id='pid'>1645, <a href='https://arxiv.org/pdf/2506.05292.pdf' target='_blank'>https://arxiv.org/pdf/2506.05292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Declan A. Norton, Yuanzhao Zhang, Michelle Girvan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05292">Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning techniques offer an effective approach to modeling dynamical systems solely from observed data. However, without explicit structural priors -- built-in assumptions about the underlying dynamics -- these techniques typically struggle to generalize to aspects of the dynamics that are poorly represented in the training data. Here, we demonstrate that reservoir computing -- a simple, efficient, and versatile machine learning framework often used for data-driven modeling of dynamical systems -- can generalize to unexplored regions of state space without explicit structural priors. First, we describe a multiple-trajectory training scheme for reservoir computers that supports training across a collection of disjoint time series, enabling effective use of available training data. Then, applying this training scheme to multistable dynamical systems, we show that RCs trained on trajectories from a single basin of attraction can achieve out-of-domain generalization by capturing system behavior in entirely unobserved basins.
<div id='section'>Paperid: <span id='pid'>1646, <a href='https://arxiv.org/pdf/2506.05292.pdf' target='_blank'>https://arxiv.org/pdf/2506.05292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Declan A. Norton, Yuanzhao Zhang, Michelle Girvan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05292">Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning techniques offer an effective approach to modeling dynamical systems solely from observed data. However, without explicit structural priors -- built-in assumptions about the underlying dynamics -- these techniques typically struggle to generalize to aspects of the dynamics that are poorly represented in the training data. Here, we demonstrate that reservoir computing -- a simple, efficient, and versatile machine learning framework often used for data-driven modeling of dynamical systems -- can generalize to unexplored regions of state space without explicit structural priors. First, we describe a multiple-trajectory training scheme for reservoir computers that supports training across a collection of disjoint time series, enabling effective use of available training data. Then, applying this training scheme to multistable dynamical systems, we show that RCs trained on trajectories from a single basin of attraction can achieve out-of-domain generalization by capturing system behavior in entirely unobserved basins.
<div id='section'>Paperid: <span id='pid'>1647, <a href='https://arxiv.org/pdf/2506.03972.pdf' target='_blank'>https://arxiv.org/pdf/2506.03972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guohua Wu, Shengqi Chen, Pengchao Deng, Wenting Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03972">MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complete blood cell detection holds significant value in clinical diagnostics. Conventional manual microscopy methods suffer from time inefficiency and diagnostic inaccuracies. Existing automated detection approaches remain constrained by high deployment costs and suboptimal accuracy. While deep learning has introduced powerful paradigms to this field, persistent challenges in detecting overlapping cells and multi-scale objects hinder practical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a blood cell detection model based on the YOLOv11 framework, incorporating three key architectural innovations to enhance detection performance. Specifically, the multi-scale dilated residual module (MS-DRM) replaces the original C3K2 modules to improve multi-scale discriminability; the dynamic cross-path feature enhancement module (DCFEM) enables the fusion of hierarchical features from the backbone with aggregated features from the neck to enhance feature representations; and the light adaptive-weight downsampling module (LADS) improves feature downsampling through adaptive spatial weighting while reducing computational complexity. Experimental results on the CBC benchmark demonstrate that MS-YOLO achieves precise detection of overlapping cells and multi-scale objects, particularly small targets such as platelets, achieving an mAP@50 of 97.4% that outperforms existing models. Further validation on the supplementary WBCDD dataset confirms its robust generalization capability. Additionally, with a lightweight architecture and real-time inference efficiency, MS-YOLO meets clinical deployment requirements, providing reliable technical support for standardized blood pathology assessment.
<div id='section'>Paperid: <span id='pid'>1648, <a href='https://arxiv.org/pdf/2505.19493.pdf' target='_blank'>https://arxiv.org/pdf/2505.19493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Zhao, Xueliang Zhang, Zhong-Qiu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19493">Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Acoustic echo cancellation (AEC) is an important speech signal processing technology that can remove echoes from microphone signals to enable natural-sounding full-duplex speech communication. While single-channel AEC is widely adopted, multi-channel AEC can leverage spatial cues afforded by multiple microphones to achieve better performance. Existing multi-channel AEC approaches typically combine beamforming with deep neural networks (DNN). This work proposes a two-stage algorithm that enhances multi-channel AEC by incorporating sound source directional cues. Specifically, a lightweight DNN is first trained to predict the sound source directions, and then the predicted directional information, multi-channel microphone signals, and single-channel far-end signal are jointly fed into an AEC network to estimate the near-end signal. Evaluation results show that the proposed algorithm outperforms baseline approaches and exhibits robust generalization across diverse acoustic environments.
<div id='section'>Paperid: <span id='pid'>1649, <a href='https://arxiv.org/pdf/2505.17692.pdf' target='_blank'>https://arxiv.org/pdf/2505.17692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziteng Yang, Jingzehua Xu, Yanshu Li, Zepeng Li, Yeqiang Wang, Xinghui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17692">ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any target domain training samples, relying solely on external auxiliary data. Existing CLIP-based methods attempt to activate the model's ZSAD potential via handcrafted or static learnable prompts. The former incur high engineering costs and limited semantic coverage, whereas the latter apply identical descriptions across diverse anomaly types, thus fail to adapt to complex variations. Furthermore, since CLIP is originally pretrained on large-scale classification tasks, its anomaly segmentation quality is highly sensitive to the exact wording of class names, severely constraining prompting strategies that depend on class labels. To address these challenges, we introduce ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local visual context to adaptively generate fine-grained textual prompts, eliminating manual templates and class-name priors. This design enables our model to focus on precise abnormal regions, making it particularly valuable when category labels are ambiguous or privacy-constrained. Extensive experiments on 15 industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves state-of-the-art performance and robust cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>1650, <a href='https://arxiv.org/pdf/2505.17692.pdf' target='_blank'>https://arxiv.org/pdf/2505.17692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziteng Yang, Jingzehua Xu, Yanshu Li, Zepeng Li, Yeqiang Wang, Xinghui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17692">ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any target domain training samples, relying solely on external auxiliary data. Existing CLIP-based methods attempt to activate the model's ZSAD potential via handcrafted or static learnable prompts. The former incur high engineering costs and limited semantic coverage, whereas the latter apply identical descriptions across diverse anomaly types, thus fail to adapt to complex variations. Furthermore, since CLIP is originally pretrained on large-scale classification tasks, its anomaly segmentation quality is highly sensitive to the exact wording of class names, severely constraining prompting strategies that depend on class labels. To address these challenges, we introduce ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local visual context to adaptively generate fine-grained textual prompts, eliminating manual templates and class-name priors. This design enables our model to focus on precise abnormal regions, making it particularly valuable when category labels are ambiguous or privacy-constrained. Extensive experiments on 15 industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves state-of-the-art performance and robust cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>1651, <a href='https://arxiv.org/pdf/2505.11654.pdf' target='_blank'>https://arxiv.org/pdf/2505.11654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Liu, Yingxue Zhang, Xin Zhang, Ling Tian, Yanhua Li, Jun Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11654">UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting urban dynamics is crucial for managing transportation systems, optimizing urban planning, and enhancing public services. While neural network-based approaches have achieved success, they often rely on task-specific architectures and large volumes of data, limiting their ability to generalize across diverse urban scenarios. Meanwhile, Large Language Models (LLMs) offer strong reasoning and generalization capabilities, yet their application to spatial-temporal urban dynamics remains underexplored. Existing LLM-based methods struggle to effectively integrate multifaceted spatial-temporal data and fail to address distributional shifts between training and testing data, limiting their predictive reliability in real-world applications. To bridge this gap, we propose UrbanMind, a novel spatial-temporal LLM framework for multifaceted urban dynamics prediction that ensures both accurate forecasting and robust generalization. At its core, UrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with specialized masking strategies that capture intricate spatial-temporal dependencies and intercorrelations among multifaceted urban dynamics. Additionally, we design a semantic-aware prompting and fine-tuning strategy that encodes spatial-temporal contextual details into prompts, enhancing LLMs' ability to reason over spatial-temporal patterns. To further improve generalization, we introduce a test time adaptation mechanism with a test data reconstructor, enabling UrbanMind to dynamically adjust to unseen test data by reconstructing LLM-generated embeddings. Extensive experiments on real-world urban datasets across multiple cities demonstrate that UrbanMind consistently outperforms state-of-the-art baselines, achieving high accuracy and robust generalization, even in zero-shot settings.
<div id='section'>Paperid: <span id='pid'>1652, <a href='https://arxiv.org/pdf/2505.02124.pdf' target='_blank'>https://arxiv.org/pdf/2505.02124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samidha Verma, Arushi Goyal, Ananya Mathur, Ankit Anand, Sayan Ranu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02124">GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a program that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.
<div id='section'>Paperid: <span id='pid'>1653, <a href='https://arxiv.org/pdf/2504.20498.pdf' target='_blank'>https://arxiv.org/pdf/2504.20498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhong Han, Yupei Wang, Liang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20498">Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization (SDG) in object detection aims to develop a detector using only source domain data that generalizes well to unseen target domains. Existing methods are primarily CNN-based and improve robustness through data augmentation combined with feature alignment. However, these methods are limited, as augmentation is only effective when the synthetic distribution approximates that of unseen domains, thus failing to ensure generalization across diverse scenarios.
  While DEtection TRansformer (DETR) has shown strong generalization in domain adaptation due to global context modeling, its potential for SDG remains underexplored. To this end, we propose Style-Adaptive DEtection TRansformer (SA-DETR), a DETR-based detector tailored for SDG. SA-DETR introduces an online domain style adapter that projects the style representation of unseen domains into the source domain via a dynamic memory bank. This bank self-organizes into diverse style prototypes and is continuously updated under a test-time adaptation framework, enabling effective style rectification.
  Additionally, we design an object-aware contrastive learning module to promote extraction of domain-invariant features. By applying gating masks that constrain contrastive learning in both spatial and semantic dimensions, this module facilitates instance-level cross-domain contrast and enhances generalization.
  Extensive experiments across five distinct weather scenarios demonstrate that SA-DETR consistently outperforms existing methods in both detection accuracy and domain generalization capability.
<div id='section'>Paperid: <span id='pid'>1654, <a href='https://arxiv.org/pdf/2504.19882.pdf' target='_blank'>https://arxiv.org/pdf/2504.19882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runhui Zhang, Sijin Zhou, Zhuang Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19882">Federated Out-of-Distribution Generalization: A Causal Augmentation View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning aims to collaboratively model by integrating multi-source information to obtain a model that can generalize across all client data. Existing methods often leverage knowledge distillation or data augmentation to mitigate the negative impact of data bias across clients. However, the limited performance of teacher models on out-of-distribution samples and the inherent quality gap between augmented and original data hinder their effectiveness and they typically fail to leverage the advantages of incorporating rich contextual information. To address these limitations, this paper proposes a Federated Causal Augmentation method, termed FedCAug, which employs causality-inspired data augmentation to break the spurious correlation between attributes and categories. Specifically, it designs a causal region localization module to accurately identify and decouple the background and objects in the image, providing rich contextual information for causal data augmentation. Additionally, it designs a causality-inspired data augmentation module that integrates causal features and within-client context to generate counterfactual samples. This significantly enhances data diversity, and the entire process does not require any information sharing between clients, thereby contributing to the protection of data privacy. Extensive experiments conducted on three datasets reveal that FedCAug markedly reduces the model's reliance on background to predict sample labels, achieving superior performance compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1655, <a href='https://arxiv.org/pdf/2504.07415.pdf' target='_blank'>https://arxiv.org/pdf/2504.07415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyoyun Choi, Byungmu Yoon, Soobum Kim, Jonggwon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07415">Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated radiology report generation (RRG) holds potential to reduce radiologists' workload, especially as recent advancements in large language models (LLMs) enable the development of multimodal models for chest X-ray (CXR) report generation. However, multimodal LLMs (MLLMs) are resource-intensive, requiring vast datasets and substantial computational cost for training. To address these challenges, we propose a retrieval-augmented generation approach that leverages multimodal retrieval and LLMs to generate radiology reports while mitigating hallucinations and reducing computational demands. Our method uses LLMs to extract key phrases from radiology reports, effectively focusing on essential diagnostic information. Through exploring effective training strategies, including image encoder structure search, adding noise to text embeddings, and additional training objectives, we combine complementary pre-trained image encoders and adopt contrastive learning between text and semantic image embeddings. We evaluate our approach on MIMIC-CXR dataset, achieving state-of-the-art results on CheXbert metrics and competitive RadGraph F1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method demonstrates robust generalization for multi-view RRG, making it suitable for comprehensive clinical applications.
<div id='section'>Paperid: <span id='pid'>1656, <a href='https://arxiv.org/pdf/2503.23612.pdf' target='_blank'>https://arxiv.org/pdf/2503.23612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Belkadi, Steve Hong, Marian Chen, Miruna Cretu, Charles Harris, Pietro Lio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23612">Diffusion-Free Graph Generation with Next-Scale Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive models excel in efficiency and plug directly into the transformer ecosystem, delivering robust generalization, predictable scalability, and seamless workflows such as fine-tuning and parallelized training. However, they require an explicit sequence order, which contradicts the unordered nature of graphs. In contrast, diffusion models maintain permutation invariance and enable one-shot generation but require up to thousands of denoising steps and additional features for expressivity, leading to high computational costs. Inspired by recent breakthroughs in image generation, especially the success of visual autoregressive methods, we propose MAG, a novel diffusion-free graph generation framework based on next-scale prediction. By leveraging a hierarchy of latent representations, the model progressively generates scales of the entire graph without the need for explicit node ordering. Experiments on both generic and molecular graph datasets demonstrated the potential of this method, achieving inference speedups of up to three orders of magnitude over state-of-the-art methods, while preserving high-quality generation.
<div id='section'>Paperid: <span id='pid'>1657, <a href='https://arxiv.org/pdf/2503.23430.pdf' target='_blank'>https://arxiv.org/pdf/2503.23430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjun Song, Youngsik Hwang, Jonghun Lee, Heechang Lee, Dong-Young Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23430">DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn models that perform well on unseen target domains by training on multiple source domains. Sharpness-Aware Minimization (SAM), known for finding flat minima that improve generalization, has therefore been widely adopted in DG. However, our analysis reveals that SAM in DG may converge to \textit{fake flat minima}, where the total loss surface appears flat in terms of global sharpness but remains sharp with respect to individual source domains. To understand this phenomenon more precisely, we formalize the average worst-case domain risk as the maximum loss under domain distribution shifts within a bounded divergence, and derive a generalization bound that reveals the limitations of global sharpness-aware minimization. In contrast, we show that individual sharpness provides a valid upper bound on this risk, making it a more suitable proxy for robust domain generalization. Motivated by these insights, we shift the DG paradigm toward minimizing individual sharpness across source domains. We propose \textit{Decreased-overhead Gradual SAM (DGSAM)}, which applies gradual domain-wise perturbations in a computationally efficient manner to consistently reduce individual sharpness. Extensive experiments demonstrate that DGSAM not only improves average accuracy but also reduces performance variance across domains, while incurring less computational overhead than SAM.
<div id='section'>Paperid: <span id='pid'>1658, <a href='https://arxiv.org/pdf/2503.22728.pdf' target='_blank'>https://arxiv.org/pdf/2503.22728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Fu, Ziqi Ni, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22728">Dual Audio-Centric Modality Coupling for Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of audio-driven talking head videos is a key challenge in computer vision and graphics, with applications in virtual avatars and digital media. Traditional approaches often struggle with capturing the complex interaction between audio and facial dynamics, leading to lip synchronization and visual quality issues. In this paper, we propose a novel NeRF-based framework, Dual Audio-Centric Modality Coupling (DAMC), which effectively integrates content and dynamic features from audio inputs. By leveraging a dual encoder structure, DAMC captures semantic content through the Content-Aware Encoder and ensures precise visual synchronization through the Dynamic-Sync Encoder. These features are fused using a Cross-Synchronized Fusion Module (CSFM), enhancing content representation and lip synchronization. Extensive experiments show that our method outperforms existing state-of-the-art approaches in key metrics such as lip synchronization accuracy and image quality, demonstrating robust generalization across various audio inputs, including synthetic speech from text-to-speech (TTS) systems. Our results provide a promising solution for high-quality, audio-driven talking head generation and present a scalable approach for creating realistic talking heads.
<div id='section'>Paperid: <span id='pid'>1659, <a href='https://arxiv.org/pdf/2503.20839.pdf' target='_blank'>https://arxiv.org/pdf/2503.20839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amr Mousa, Neil Karavis, Michele Caprio, Wei Pan, Richard Allmendinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20839">TAR: Teacher-Aligned Representations via Contrastive Learning for Quadrupedal Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed using the teacher-student paradigm, where a privileged teacher guides a proprioceptive student policy. However, key challenges such as representation misalignment between privileged teacher and proprioceptive-only student, covariate shift due to behavioral cloning, and lack of deployable adaptation; lead to poor generalization in real-world scenarios. We propose Teacher-Aligned Representations via Contrastive Learning (TAR), a framework that leverages privileged information with self-supervised contrastive learning to bridge this gap. By aligning representations to a privileged teacher in simulation via contrastive objectives, our student policy learns structured latent spaces and exhibits robust generalization to Out-of-Distribution (OOD) scenarios, surpassing the fully privileged "Teacher". Results showed accelerated training by 2x compared to state-of-the-art baselines to achieve peak performance. OOD scenarios showed better generalization by 40% on average compared to existing methods. Moreover, TAR transitions seamlessly into learning during deployment without requiring privileged states, setting a new benchmark in sample-efficient, adaptive locomotion and enabling continual fine-tuning in real-world scenarios. Open-source code and videos are available at https://amrmousa.com/TARLoco/.
<div id='section'>Paperid: <span id='pid'>1660, <a href='https://arxiv.org/pdf/2503.06759.pdf' target='_blank'>https://arxiv.org/pdf/2503.06759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Q. Vo, Samira Zare, Son T. Ly, Lin Wang, Chika F. Ezeana, Xiaohui Yu, Kelvin K. Wong, Stephen T. C. Wong, Hien V. Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06759">Revisiting Invariant Learning for Out-of-Domain Generalization on Multi-Site Mammogram Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in robust deep learning techniques for mammogram breast cancer classification, their reliability in real-world clinical development settings remains uncertain. The translation of these models to clinical practice faces challenges due to variations in medical centers, imaging protocols, and patient populations. To enhance their robustness, invariant learning methods have been proposed, prioritizing causal factors over misleading features. However, their effectiveness in clinical development and impact on mammogram classification require investigation. This paper reassesses the application of invariant learning for breast cancer risk estimation based on mammograms. Utilizing diverse multi-site public datasets, it represents the first study in this area. The objective is to evaluate invariant learning's benefits in developing robust models. Invariant learning methods, including Invariant Risk Minimization and Variance Risk Extrapolation, are compared quantitatively against Empirical Risk Minimization. Evaluation metrics include accuracy, average precision, and area under the curve. Additionally, interpretability is examined through class activation maps and visualization of learned representations. This research examines the advantages, limitations, and challenges of invariant learning for mammogram classification, guiding future studies to develop generalized methods for breast cancer prediction on whole mammograms in out-of-domain scenarios.
<div id='section'>Paperid: <span id='pid'>1661, <a href='https://arxiv.org/pdf/2503.06698.pdf' target='_blank'>https://arxiv.org/pdf/2503.06698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Thomas, Deepti Ghadiyaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06698">What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training.
<div id='section'>Paperid: <span id='pid'>1662, <a href='https://arxiv.org/pdf/2503.06472.pdf' target='_blank'>https://arxiv.org/pdf/2503.06472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Luo, Jiaqi Tang, Chenyi Huang, Feiyang Hao, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06472">CalliReader: Contextualizing Chinese Calligraphy via an Embedding-Aligned Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chinese calligraphy, a UNESCO Heritage, remains computationally challenging due to visual ambiguity and cultural complexity. Existing AI systems fail to contextualize their intricate scripts, because of limited annotated data and poor visual-semantic alignment. We propose CalliReader, a vision-language model (VLM) that solves the Chinese Calligraphy Contextualization (CC$^2$) problem through three innovations: (1) character-wise slicing for precise character extraction and sorting, (2) CalliAlign for visual-text token compression and alignment, (3) embedding instruction tuning (e-IT) for improving alignment and addressing data scarcity. We also build CalliBench, the first benchmark for full-page calligraphic contextualization, addressing three critical issues in previous OCR and VQA approaches: fragmented context, shallow reasoning, and hallucination. Extensive experiments including user studies have been conducted to verify our CalliReader's \textbf{superiority to other state-of-the-art methods and even human professionals in page-level calligraphy recognition and interpretation}, achieving higher accuracy while reducing hallucination. Comparisons with reasoning models highlight the importance of accurate recognition as a prerequisite for reliable comprehension. Quantitative analyses validate CalliReader's efficiency; evaluations on document and real-world benchmarks confirm its robust generalization ability.
<div id='section'>Paperid: <span id='pid'>1663, <a href='https://arxiv.org/pdf/2502.20249.pdf' target='_blank'>https://arxiv.org/pdf/2502.20249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Vuillecard, Jean-Marc Odobez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20249">Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D gaze estimation in unconstrained real-world environments remains a significant challenge due to variations in appearance, head pose, occlusion, and the limited availability of in-the-wild 3D gaze datasets. To address these challenges, we introduce a novel Self-Training Weakly-Supervised Gaze Estimation framework (ST-WSGE). This two-stage learning framework leverages diverse 2D gaze datasets, such as gaze-following data, which offer rich variations in appearances, natural scenes, and gaze distributions, and proposes an approach to generate 3D pseudo-labels and enhance model generalization. Furthermore, traditional modality-specific models, designed separately for images or videos, limit the effective use of available training data. To overcome this, we propose the Gaze Transformer (GaT), a modality-agnostic architecture capable of simultaneously learning static and dynamic gaze information from both image and video datasets. By combining 3D video datasets with 2D gaze target labels from gaze following tasks, our approach achieves the following key contributions: (i) Significant state-of-the-art improvements in within-domain and cross-domain generalization on unconstrained benchmarks like Gaze360 and GFIE, with notable cross-modal gains in video gaze estimation; (ii) Superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360 compared to frontal face methods. Code and pre-trained models will be released to the community.
<div id='section'>Paperid: <span id='pid'>1664, <a href='https://arxiv.org/pdf/2502.20144.pdf' target='_blank'>https://arxiv.org/pdf/2502.20144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Pignet, John Klein, Genevieve Robin, Antoine Olivier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20144">Robust sensitivity control in digital pathology via tile score distribution matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying digital pathology models across medical centers is challenging due to distribution shifts. Recent advances in domain generalization improve model transferability in terms of aggregated performance measured by the Area Under Curve (AUC). However, clinical regulations often require to control the transferability of other metrics, such as prescribed sensitivity levels. We introduce a novel approach to control the sensitivity of whole slide image (WSI) classification models, based on optimal transport and Multiple Instance Learning (MIL). Validated across multiple cohorts and tasks, our method enables robust sensitivity control with only a handful of calibration samples, providing a practical solution for reliable deployment of computational pathology systems.
<div id='section'>Paperid: <span id='pid'>1665, <a href='https://arxiv.org/pdf/2502.18735.pdf' target='_blank'>https://arxiv.org/pdf/2502.18735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Harvey Chapman, Feras Dayoub, Will Browne, Christopher Lehnert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18735">QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural Language Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A domain shift exists between the large-scale, internet data used to train a Vision-Language Model (VLM) and the raw image streams collected by a robot. Existing adaptation strategies require the definition of a closed-set of classes, which is impractical for a robot that must respond to diverse natural language queries. In response, we present QueryAdapter; a novel framework for rapidly adapting a pre-trained VLM in response to a natural language query. QueryAdapter leverages unlabelled data collected during previous deployments to align VLM features with semantic classes related to the query. By optimising learnable prompt tokens and actively selecting objects for training, an adapted model can be produced in a matter of minutes. We also explore how objects unrelated to the query should be dealt with when using real-world data for adaptation. In turn, we propose the use of object captions as negative class labels, helping to produce better calibrated confidence scores during adaptation. Extensive experiments on ScanNet++ demonstrate that QueryAdapter significantly enhances object retrieval performance compared to state-of-the-art unsupervised VLM adapters and 3D scene graph methods. Furthermore, the approach exhibits robust generalization to abstract affordance queries and other datasets, such as Ego4D.
<div id='section'>Paperid: <span id='pid'>1666, <a href='https://arxiv.org/pdf/2502.14376.pdf' target='_blank'>https://arxiv.org/pdf/2502.14376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14376">A Similarity Paradigm Through Textual Regularization Without Forgetting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods.
<div id='section'>Paperid: <span id='pid'>1667, <a href='https://arxiv.org/pdf/2502.10838.pdf' target='_blank'>https://arxiv.org/pdf/2502.10838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janne Laakkonen, Ivan Kukanov, Ville HautamÃ¤ki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10838">Generalizable speech deepfake detection via meta-learned LoRA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable detection of speech deepfakes (spoofs) must remain effective when the distribution of spoofing attacks shifts. We frame the task as domain generalization and show that inserting Low-Rank Adaptation (LoRA) adapters into every attention head of a self-supervised (SSL) backbone, then training only those adapters with Meta-Learning Domain Generalization (MLDG), yields strong zero-shot performance. The resulting model updates about 3.6 million parameters, roughly 1.1% of the 318 million updated in full fine-tuning, yet surpasses a fully fine-tuned counterpart on five of six evaluation corpora. A first-order MLDG loop encourages the adapters to focus on cues that persist across attack types, lowering the average EER from 8.84% for the fully fine-tuned model to 5.30% with our best MLDG-LoRA configuration. Our findings show that combining meta-learning with parameter-efficient adaptation offers an effective method for zero-shot, distribution-shift-aware speech deepfake detection.
<div id='section'>Paperid: <span id='pid'>1668, <a href='https://arxiv.org/pdf/2502.08757.pdf' target='_blank'>https://arxiv.org/pdf/2502.08757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Hasanzadeh Karkan, Ahmed Ibrahim, Jean-FranÃ§ois Frigon, FranÃ§ois Leduc-Primeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08757">A Low-Complexity Plug-and-Play Deep Learning Model for Massive MIMO Precoding Across Sites</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Massive multiple-input multiple-output (mMIMO) technology has transformed wireless communication by enhancing spectral efficiency and network capacity. This paper proposes a novel deep learning-based mMIMO precoder to tackle the complexity challenges of existing approaches, such as weighted minimum mean square error (WMMSE), while leveraging meta-learning domain generalization and a teacher-student architecture to improve generalization across diverse communication environments. When deployed to a previously unseen site, the proposed model achieves excellent sum-rate performance while maintaining low computational complexity by avoiding matrix inversions and by using a simpler neural network structure. The model is trained and tested on a custom ray-tracing dataset composed of several base station locations. The experimental results indicate that our method effectively balances computational efficiency with high sum-rate performance while showcasing strong generalization performance in unseen environments. Furthermore, with fine-tuning, the proposed model outperforms WMMSE across all tested sites and SNR conditions while reducing complexity by at least 73$\times$.
<div id='section'>Paperid: <span id='pid'>1669, <a href='https://arxiv.org/pdf/2502.04289.pdf' target='_blank'>https://arxiv.org/pdf/2502.04289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thorben Prein, Elton Pan, Sami Haddouti, Marco Lorenz, Janik Jehkul, Tymoteusz Wilk, Cansu Moran, Menelaos Panagiotis Fotiadis, Artur P. Toshev, Elsa Olivetti, Jennifer L. M. Rupp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04289">Retro-Rank-In: A Ranking-Based Approach for Inorganic Materials Synthesis Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrosynthesis strategically plans the synthesis of a chemical target compound from simpler, readily available precursor compounds. This process is critical for synthesizing novel inorganic materials, yet traditional methods in inorganic chemistry continue to rely on trial-and-error experimentation. Emerging machine-learning approaches struggle to generalize to entirely new reactions due to their reliance on known precursors, as they frame retrosynthesis as a multi-label classification task. To address these limitations, we propose Retro-Rank-In, a novel framework that reformulates the retrosynthesis problem by embedding target and precursor materials into a shared latent space and learning a pairwise ranker on a bipartite graph of inorganic compounds. We evaluate Retro-Rank-In's generalizability on challenging retrosynthesis dataset splits designed to mitigate data duplicates and overlaps. For instance, for Cr2AlB2, it correctly predicts the verified precursor pair CrB + Al despite never seeing them in training, a capability absent in prior work. Extensive experiments show that Retro-Rank-In sets a new state-of-the-art, particularly in out-of-distribution generalization and candidate set ranking, offering a powerful tool for accelerating inorganic material synthesis.
<div id='section'>Paperid: <span id='pid'>1670, <a href='https://arxiv.org/pdf/2502.00545.pdf' target='_blank'>https://arxiv.org/pdf/2502.00545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotong Tu, Chenyu Ma, Qingyao Wu, Yinhao Liu, Hongyang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00545">Integrating Frequency Guidance into Multi-source Domain Generalization for Bearing Fault Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent generalizable fault diagnosis researches have effectively tackled the distributional shift between unseen working conditions. Most of them mainly focus on learning domain-invariant representation through feature-level methods. However, the increasing numbers of unseen domains may lead to domain-invariant features contain instance-level spurious correlations, which impact the previous models' generalizable ability. To address the limitations, we propose the Fourier-based Augmentation Reconstruction Network, namely FARNet.The methods are motivated by the observation that the Fourier phase component and amplitude component preserve different semantic information of the signals, which can be employed in domain augmentation techniques. The network comprises an amplitude spectrum sub-network and a phase spectrum sub-network, sequentially reducing the discrepancy between the source and target domains. To construct a more robust generalized model, we employ a multi-source domain data augmentation strategy in the frequency domain. Specifically, a Frequency-Spatial Interaction Module (FSIM) is introduced to handle global information and local spatial features, promoting representation learning between the two sub-networks. To refine the decision boundary of our model output compared to conventional triplet loss, we propose a manifold triplet loss to contribute to generalization. Through extensive experiments on the CWRU and SJTU datasets, FARNet demonstrates effective performance and achieves superior results compared to current cross-domain approaches on the benchmarks.
<div id='section'>Paperid: <span id='pid'>1671, <a href='https://arxiv.org/pdf/2501.09527.pdf' target='_blank'>https://arxiv.org/pdf/2501.09527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oleg Somov, Elena Tutubalina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09527">Confidence Estimation for Error Detection in Text-to-SQL Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-SQL enables users to interact with databases through natural language, simplifying the retrieval and synthesis of information. Despite the success of large language models (LLMs) in converting natural language questions into SQL queries, their broader adoption is limited by two main challenges: achieving robust generalization across diverse queries and ensuring interpretative confidence in their predictions. To tackle these issues, our research investigates the integration of selective classifiers into Text-to-SQL systems. We analyse the trade-off between coverage and risk using entropy based confidence estimation with selective classifiers and assess its impact on the overall performance of Text-to-SQL models. Additionally, we explore the models' initial calibration and improve it with calibration techniques for better model alignment between confidence and accuracy. Our experimental results show that encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and decoder-only Llama 3, thus the designated external entropy-based selective classifier has better performance. The study also reveal that, in terms of error detection, selective classifier with a higher probability detects errors associated with irrelevant questions rather than incorrect query generations.
<div id='section'>Paperid: <span id='pid'>1672, <a href='https://arxiv.org/pdf/2501.07378.pdf' target='_blank'>https://arxiv.org/pdf/2501.07378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Deng, Zhe Xu, Tsuyoshi Isshiki, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07378">FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation is challenging due to the diversity of medical images and the lack of labeled data, which motivates recent developments in federated semi-supervised learning (FSSL) to leverage a large amount of unlabeled data from multiple centers for model training without sharing raw data. However, what remains under-explored in FSSL is the domain shift problem which may cause suboptimal model aggregation and low effectivity of the utilization of unlabeled data, eventually leading to unsatisfactory performance in unseen domains. In this paper, we explore this previously ignored scenario, namely domain generalized federated semi-supervised learning (FedSemiDG), which aims to learn a model in a distributed manner from multiple domains with limited labeled data and abundant unlabeled data such that the model can generalize well to unseen domains. We present a novel framework, Federated Generalization-Aware SemiSupervised Learning (FGASL), to address the challenges in FedSemiDG by effectively tackling critical issues at both global and local levels. Globally, we introduce Generalization-Aware Aggregation (GAA), assigning adaptive weights to local models based on their generalization performance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement (DR) strategy to combine global and domain-specific knowledge, generating more reliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA) enforces feature consistency under perturbations, promoting domain-invariant learning. Extensive experiments on four medical segmentation tasks (cardiac MRI, spine MRI, bladder cancer MRI and colorectal polyp) demonstrate that our method significantly outperforms state-of-the-art FSSL and domain generalization approaches, achieving robust generalization on unseen domains.
<div id='section'>Paperid: <span id='pid'>1673, <a href='https://arxiv.org/pdf/2412.12469.pdf' target='_blank'>https://arxiv.org/pdf/2412.12469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingquan Feng, Zhijie Chen, Yixin Huang, Yizhou Liu, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12469">Optimal Control Operator Perspective and a Neural Adaptive Spectral Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optimal control problems (OCPs) involve finding a control function for a dynamical system such that a cost functional is optimized. It is central to physical systems in both academia and industry. In this paper, we propose a novel instance-solution control operator perspective, which solves OCPs in a one-shot manner without direct dependence on the explicit expression of dynamics or iterative optimization processes. The control operator is implemented by a new neural operator architecture named Neural Adaptive Spectral Method (NASM), a generalization of classical spectral methods. We theoretically validate the perspective and architecture by presenting the approximation error bounds of NASM for the control operator. Experiments on synthetic environments and a real-world dataset verify the effectiveness and efficiency of our approach, including substantial speedup in running time, and high-quality in- and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1674, <a href='https://arxiv.org/pdf/2412.10089.pdf' target='_blank'>https://arxiv.org/pdf/2412.10089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Cao, Songcan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10089">Guidance Not Obstruction: A Conjugate Consistent Enhanced Strategy for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization addresses domain shift in real-world applications. Most approaches adopt a domain angle, seeking invariant representation across domains by aligning their marginal distributions, irrespective of individual classes, naturally leading to insufficient exploration of discriminative information. Switching to a class angle, we find that multiple domain-related peaks or clusters within the same individual classes must emerge due to distribution shift. In other words, marginal alignment does not guarantee conditional alignment, leading to suboptimal generalization. Therefore, we argue that acquiring discriminative generalization between classes within domains is crucial. In contrast to seeking distribution alignment, we endeavor to safeguard domain-related between-class discrimination. To this end, we devise a novel Conjugate Consistent Enhanced Module, namely Con2EM, based on a distribution over domains, i.e., a meta-distribution. Specifically, we employ a novel distribution-level Universum strategy to generate supplementary diverse domain-related class-conditional distributions, thereby enhancing generalization. This allows us to resample from these generated distributions to provide feedback to the primordial instance-level classifier, further improving its adaptability to the target-agnostic. To ensure generation accuracy, we establish an additional distribution-level classifier to regularize these conditional distributions. Extensive experiments have been conducted to demonstrate its effectiveness and low computational cost compared to SOTAs.
<div id='section'>Paperid: <span id='pid'>1675, <a href='https://arxiv.org/pdf/2412.02029.pdf' target='_blank'>https://arxiv.org/pdf/2412.02029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ihab Tabbara, Hussein Sibai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02029">Learning Ensembles of Vision-based Safety Control Filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety filters in control systems correct nominal controls that violate safety constraints. Designing such filters as functions of visual observations in uncertain and complex environments is challenging. Several deep learning-based approaches to tackle this challenge have been proposed recently. However, formally verifying that the learned filters satisfy critical properties that enable them to guarantee the safety of the system is currently beyond reach. Instead, in this work, motivated by the success of ensemble methods in reinforcement learning, we empirically investigate the efficacy of ensembles in enhancing the accuracy and the out-of-distribution generalization of such filters, as a step towards more reliable ones. We experiment with diverse pre-trained vision representation models as filter backbones, training approaches, and output aggregation techniques. We compare the performance of ensembles with different configurations against each other, their individual member models, and large single-model baselines in distinguishing between safe and unsafe states and controls in the DeepAccident dataset. Our results show that diverse ensembles have better state and control classification accuracies compared to individual models.
<div id='section'>Paperid: <span id='pid'>1676, <a href='https://arxiv.org/pdf/2411.17332.pdf' target='_blank'>https://arxiv.org/pdf/2411.17332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos Garrido-Munoz, Jorge Calvo-Zaragoza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17332">On the Generalization of Handwritten Text Recognition Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Handwritten Text Recognition (HTR) have led to significant reductions in transcription errors on standard benchmarks under the i.i.d. assumption, thus focusing on minimizing in-distribution (ID) errors. However, this assumption does not hold in real-world applications, which has motivated HTR research to explore Transfer Learning and Domain Adaptation techniques. In this work, we investigate the unaddressed limitations of HTR models in generalizing to out-of-distribution (OOD) data. We adopt the challenging setting of Domain Generalization, where models are expected to generalize to OOD data without any prior access. To this end, we analyze 336 OOD cases from eight state-of-the-art HTR models across seven widely used datasets, spanning five languages. Additionally, we study how HTR models leverage synthetic data to generalize. We reveal that the most significant factor for generalization lies in the textual divergence between domains, followed by visual divergence. We demonstrate that the error of HTR models in OOD scenarios can be reliably estimated, with discrepancies falling below 10 points in 70\% of cases. We identify the underlying limitations of HTR models, laying the foundation for future research to address this challenge.
<div id='section'>Paperid: <span id='pid'>1677, <a href='https://arxiv.org/pdf/2410.20088.pdf' target='_blank'>https://arxiv.org/pdf/2410.20088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atula Tejaswi, Yoonsang Lee, Sujay Sanghavi, Eunsol Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20088">RARe: Retrieval Augmented Retrieval with In-Context Examples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space.
<div id='section'>Paperid: <span id='pid'>1678, <a href='https://arxiv.org/pdf/2410.14821.pdf' target='_blank'>https://arxiv.org/pdf/2410.14821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mansoor Ali Teevno, Gilberto Ochoa-Ruiz, Sharib Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14821">Tackling domain generalization for out-of-distribution endoscopic imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent advances in deep learning (DL) for surgical scene segmentation have yielded promising results on single-center and single-imaging modality data, these methods usually do not generalize well to unseen distributions or modalities. Even though human experts can identify visual appearances, DL methods often fail to do so when data samples do not follow a similar distribution. Current literature addressing domain gaps in modality changes has focused primarily on natural scene data. However, these methods cannot be directly applied to endoscopic data, as visual cues in such data are more limited compared to natural scenes. In this work, we exploit both style and content information in images by performing instance normalization and feature covariance mapping techniques to preserve robust and generalizable feature representations. Additionally, to avoid the risk of removing salient feature representations associated with objects of interest, we introduce a restitution module within the feature-learning ResNet backbone that retains useful task-relevant features. Our proposed method shows a 13.7% improvement over the baseline DeepLabv3+ and nearly an 8% improvement over recent state-of-the-art (SOTA) methods for the target (different modality) set of the EndoUDA polyp dataset. Similarly, our method achieved a 19% improvement over the baseline and 6% over the best-performing SOTA method on the EndoUDA Barrett's esophagus (BE) dataset.
<div id='section'>Paperid: <span id='pid'>1679, <a href='https://arxiv.org/pdf/2410.09409.pdf' target='_blank'>https://arxiv.org/pdf/2410.09409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Jiang, Xinlong Wan, Kaiying Zhu, Xihe Qiu, Zhijun Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09409">Distribution-aware Noisy-label Crack Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road crack segmentation is critical for robotic systems tasked with the inspection, maintenance, and monitoring of road infrastructures. Existing deep learning-based methods for crack segmentation are typically trained on specific datasets, which can lead to significant performance degradation when applied to unseen real-world scenarios. To address this, we introduce the SAM-Adapter, which incorporates the general knowledge of the Segment Anything Model (SAM) into crack segmentation, demonstrating enhanced performance and generalization capabilities. However, the effectiveness of the SAM-Adapter is constrained by noisy labels within small-scale training sets, including omissions and mislabeling of cracks. In this paper, we present an innovative joint learning framework that utilizes distribution-aware domain-specific semantic knowledge to guide the discriminative learning process of the SAM-Adapter. To our knowledge, this is the first approach that effectively minimizes the adverse effects of noisy labels on the supervised learning of the SAM-Adapter. Our experimental results on two public pavement crack segmentation datasets confirm that our method significantly outperforms existing state-of-the-art techniques. Furthermore, evaluations on the completely unseen CFD dataset demonstrate the high cross-domain generalization capability of our model, underscoring its potential for practical applications in crack segmentation.
<div id='section'>Paperid: <span id='pid'>1680, <a href='https://arxiv.org/pdf/2410.08972.pdf' target='_blank'>https://arxiv.org/pdf/2410.08972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michalis Korakakis, Andreas Vlachos, Adrian Weller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08972">ALVIN: Active Learning Via INterpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active Learning aims to minimize annotation effort by selecting the most useful instances from a pool of unlabeled data. However, typical active learning methods overlook the presence of distinct example groups within a class, whose prevalence may vary, e.g., in occupation classification datasets certain demographics are disproportionately represented in specific classes. This oversight causes models to rely on shortcuts for predictions, i.e., spurious correlations between input attributes and labels occurring in well-represented groups. To address this issue, we propose Active Learning Via INterpolation (ALVIN), which conducts intra-class interpolations between examples from under-represented and well-represented groups to create anchors, i.e., artificial points situated between the example groups in the representation space. By selecting instances close to the anchors for annotation, ALVIN identifies informative examples exposing the model to regions of the representation space that counteract the influence of shortcuts. Crucially, since the model considers these examples to be of high certainty, they are likely to be ignored by typical active learning methods. Experimental results on six datasets encompassing sentiment analysis, natural language inference, and paraphrase detection demonstrate that ALVIN outperforms state-of-the-art active learning methods in both in-distribution and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1681, <a href='https://arxiv.org/pdf/2410.06977.pdf' target='_blank'>https://arxiv.org/pdf/2410.06977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyue Li, Shuoyi Chen, Mang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06977">Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wildlife ReID involves utilizing visual technology to identify specific individuals of wild animals in different scenarios, holding significant importance for wildlife conservation, ecological research, and environmental monitoring. Existing wildlife ReID methods are predominantly tailored to specific species, exhibiting limited applicability. Although some approaches leverage extensively studied person ReID techniques, they struggle to address the unique challenges posed by wildlife. Therefore, in this paper, we present a unified, multi-species general framework for wildlife ReID. Given that high-frequency information is a consistent representation of unique features in various species, significantly aiding in identifying contours and details such as fur textures, we propose the Adaptive High-Frequency Transformer model with the goal of enhancing high-frequency information learning. To mitigate the inevitable high-frequency interference in the wilderness environment, we introduce an object-aware high-frequency selection strategy to adaptively capture more valuable high-frequency components. Notably, we unify the experimental settings of multiple wildlife datasets for ReID, achieving superior performance over state-of-the-art ReID methods. In domain generalization scenarios, our approach demonstrates robust generalization to unknown species.
<div id='section'>Paperid: <span id='pid'>1682, <a href='https://arxiv.org/pdf/2409.17899.pdf' target='_blank'>https://arxiv.org/pdf/2409.17899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Sun, Zeyu Zhao, Korin Richmond, Yuanchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17899">Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.
<div id='section'>Paperid: <span id='pid'>1683, <a href='https://arxiv.org/pdf/2409.12450.pdf' target='_blank'>https://arxiv.org/pdf/2409.12450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mansoor Ali Teevno, Rafael Martinez-Garcia-Pena, Gilberto Ochoa-Ruiz, Sharib Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12450">Domain Generalization for Endoscopic Image Segmentation by Disentangling Style-Content Information and SuperPixel Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Frequent monitoring is necessary to stratify individuals based on their likelihood of developing gastrointestinal (GI) cancer precursors. In clinical practice, white-light imaging (WLI) and complementary modalities such as narrow-band imaging (NBI) and fluorescence imaging are used to assess risk areas. However, conventional deep learning (DL) models show degraded performance due to the domain gap when a model is trained on one modality and tested on a different one. In our earlier approach, we used a superpixel-based method referred to as "SUPRA" to effectively learn domain-invariant information using color and space distances to generate groups of pixels. One of the main limitations of this earlier work is that the aggregation does not exploit structural information, making it suboptimal for segmentation tasks, especially for polyps and heterogeneous color distributions. Therefore, in this work, we propose an approach for style-content disentanglement using instance normalization and instance selective whitening (ISW) for improved domain generalization when combined with SUPRA. We evaluate our approach on two datasets: EndoUDA Barrett's Esophagus and EndoUDA polyps, and compare its performance with three state-of-the-art (SOTA) methods. Our findings demonstrate a notable enhancement in performance compared to both baseline and SOTA methods across the target domain data. Specifically, our approach exhibited improvements of 14%, 10%, 8%, and 18% over the baseline and three SOTA methods on the polyp dataset. Additionally, it surpassed the second-best method (EndoUDA) on the Barrett's Esophagus dataset by nearly 2%.
<div id='section'>Paperid: <span id='pid'>1684, <a href='https://arxiv.org/pdf/2409.12011.pdf' target='_blank'>https://arxiv.org/pdf/2409.12011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Du, Tong Niu, Rong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12011">Mixture of Prompt Learning for Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As powerful pre-trained vision-language models (VLMs) like CLIP gain prominence, numerous studies have attempted to combine VLMs for downstream tasks. Among these, prompt learning has been validated as an effective method for adapting to new tasks, which only requiring a small number of parameters. However, current prompt learning methods face two challenges: first, a single soft prompt struggles to capture the diverse styles and patterns within a dataset; second, fine-tuning soft prompts is prone to overfitting. To address these challenges, we propose a mixture of soft prompt learning method incorporating a routing module. This module is able to capture a dataset's varied styles and dynamically selects the most suitable prompts for each instance. Additionally, we introduce a novel gating mechanism to ensure the router selects prompts based on their similarity to hard prompt templates, which both retaining knowledge from hard prompts and improving selection accuracy. We also implement semantically grouped text-level supervision, initializing each soft prompt with the token embeddings of manually designed templates from its group and applied a contrastive loss between the resulted text feature and hard prompt encoded text feature. This supervision ensures that the text features derived from soft prompts remain close to those from their corresponding hard prompts, preserving initial knowledge and mitigating overfitting. Our method has been validated on 11 datasets, demonstrating evident improvements in few-shot learning, domain generalization, and base-to-new generalization scenarios compared to existing baselines. The code will be available at \url{https://anonymous.4open.science/r/mocoop-6387}
<div id='section'>Paperid: <span id='pid'>1685, <a href='https://arxiv.org/pdf/2409.09832.pdf' target='_blank'>https://arxiv.org/pdf/2409.09832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Nanduri, Rama Chellappa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09832">Template-based Multi-Domain Face Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable performance of deep neural networks for face detection and recognition tasks in the visible spectrum, their performance on more challenging non-visible domains is comparatively still lacking. While significant research has been done in the fields of domain adaptation and domain generalization, in this paper we tackle scenarios in which these methods have limited applicability owing to the lack of training data from target domains. We focus on the problem of single-source (visible) and multi-target (SWIR, long-range/remote, surveillance, and body-worn) face recognition task. We show through experiments that a good template generation algorithm becomes crucial as the complexity of the target domain increases. In this context, we introduce a template generation algorithm called Norm Pooling (and a variant known as Sparse Pooling) and show that it outperforms average pooling across different domains and networks, on the IARPA JANUS Benchmark Multi-domain Face (IJB-MDF) dataset.
<div id='section'>Paperid: <span id='pid'>1686, <a href='https://arxiv.org/pdf/2409.08587.pdf' target='_blank'>https://arxiv.org/pdf/2409.08587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Damiano, Thomas Dietzen, Toon van Waterschoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08587">Frequency Tracking Features for Data-Efficient Deep Siren Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The identification of siren sounds in urban soundscapes is a crucial safety aspect for smart vehicles and has been widely addressed by means of neural networks that ensure robustness to both the diversity of siren signals and the strong and unstructured background noise characterizing traffic. Convolutional neural networks analyzing spectrogram features of incoming signals achieve state-of-the-art performance when enough training data capturing the diversity of the target acoustic scenes is available. In practice, data is usually limited and algorithms should be robust to adapt to unseen acoustic conditions without requiring extensive datasets for re-training. In this work, given the harmonic nature of siren signals, characterized by a periodically evolving fundamental frequency, we propose a low-complexity feature extraction method based on frequency tracking using a single-parameter adaptive notch filter. The features are then used to design a small-scale convolutional network suitable for training with limited data. The evaluation results indicate that the proposed model consistently outperforms the traditional spectrogram-based model when limited training data is available, achieves better cross-domain generalization and has a smaller size.
<div id='section'>Paperid: <span id='pid'>1687, <a href='https://arxiv.org/pdf/2409.04768.pdf' target='_blank'>https://arxiv.org/pdf/2409.04768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Qiao, Wenyu Wang, Meixia Qu, Kun Su, Bin Jiang, Qiang Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04768">Medical Image Segmentation via Single-Source Domain Generalization with Random Amplitude Spectrum Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of medical image segmentation is challenged by domain generalization (DG) due to domain shifts in clinical datasets. The DG challenge is exacerbated by the scarcity of medical data and privacy concerns. Traditional single-source domain generalization (SSDG) methods primarily rely on stacking data augmentation techniques to minimize domain discrepancies. In this paper, we propose Random Amplitude Spectrum Synthesis (RASS) as a training augmentation for medical images. RASS enhances model generalization by simulating distribution changes from a frequency perspective. This strategy introduces variability by applying amplitude-dependent perturbations to ensure broad coverage of potential domain variations. Furthermore, we propose random mask shuffle and reconstruction components, which can enhance the ability of the backbone to process structural information and increase resilience intra- and cross-domain changes. The proposed Random Amplitude Spectrum Synthesis for Single-Source Domain Generalization (RAS^4DG) is validated on 3D fetal brain images and 2D fundus photography, and achieves an improved DG segmentation performance compared to other SSDG models.
<div id='section'>Paperid: <span id='pid'>1688, <a href='https://arxiv.org/pdf/2409.03509.pdf' target='_blank'>https://arxiv.org/pdf/2409.03509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chamuditha Jayanaga Galappaththige, Zachary Izzo, Xilin He, Honglu Zhou, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03509">Domain-Guided Weight Modulation for Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unarguably, deep learning models capable of generalizing to unseen domain data while leveraging a few labels are of great practical significance due to low developmental costs. In search of this endeavor, we study the challenging problem of semi-supervised domain generalization (SSDG), where the goal is to learn a domain-generalizable model while using only a small fraction of labeled data and a relatively large fraction of unlabeled data. Domain generalization (DG) methods show subpar performance under the SSDG setting, whereas semi-supervised learning (SSL) methods demonstrate relatively better performance, however, they are considerably poor compared to the fully-supervised DG methods. Towards handling this new, but challenging problem of SSDG, we propose a novel method that can facilitate the generation of accurate pseudo-labels under various domain shifts. This is accomplished by retaining the domain-level specialism in the classifier during training corresponding to each source domain. Specifically, we first create domain-level information vectors on the fly which are then utilized to learn a domain-aware mask for modulating the classifier's weights. We provide a mathematical interpretation for the effect of this modulation procedure on both pseudo-labeling and model training. Our method is plug-and-play and can be readily applied to different SSL baselines for SSDG. Extensive experiments on six challenging datasets in two different SSDG settings show that our method provides visible gains over the various strong SSL-based SSDG baselines.
<div id='section'>Paperid: <span id='pid'>1689, <a href='https://arxiv.org/pdf/2408.06017.pdf' target='_blank'>https://arxiv.org/pdf/2408.06017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Zheng, Dennis M. Kochmann, Siddhant Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06017">HyperCAN: Hypernetwork-Driven Deep Parameterized Constitutive Models for Metamaterials</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HyperCAN, a machine learning framework that utilizes hypernetworks to construct adaptable constitutive artificial neural networks for a wide range of beam-based metamaterials exhibiting diverse mechanical behavior under finite deformations. HyperCAN integrates an input convex network that models the nonlinear stress-strain map of a truss lattice, while ensuring adherence to fundamental mechanics principles, along with a hypernetwork that dynamically adjusts the parameters of the convex network as a function of the lattice topology and geometry. This unified framework demonstrates robust generalization in predicting the mechanical behavior of previously unseen metamaterial designs and loading scenarios well beyond the training domain. We show how HyperCAN can be integrated into multiscale simulations to accurately capture the highly nonlinear responses of large-scale truss metamaterials, closely matching fully resolved simulations while significantly reducing computational costs. This offers new efficient opportunities for the multiscale design and optimization of truss metamaterials.
<div id='section'>Paperid: <span id='pid'>1690, <a href='https://arxiv.org/pdf/2407.16067.pdf' target='_blank'>https://arxiv.org/pdf/2407.16067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Shi, Gautam Gare, Jinjin Tian, Siqi Chai, Zhiqiu Lin, Arun Vasudevan, Di Feng, Francesco Ferroni, Shu Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16067">LCA-on-the-Line: Benchmarking Out-of-Distribution Generalization with Class Taxonomies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the challenge of predicting models' Out-of-Distribution (OOD) performance using in-distribution (ID) measurements without requiring OOD data. Existing evaluations with "Effective Robustness", which use ID accuracy as an indicator of OOD accuracy, encounter limitations when models are trained with diverse supervision and distributions, such as class labels (Vision Models, VMs, on ImageNet) and textual descriptions (Visual-Language Models, VLMs, on LAION). VLMs often generalize better to OOD data than VMs despite having similar or lower ID performance. To improve the prediction of models' OOD performance from ID measurements, we introduce the Lowest Common Ancestor (LCA)-on-the-Line framework. This approach revisits the established concept of LCA distance, which measures the hierarchical distance between labels and predictions within a predefined class hierarchy, such as WordNet. We assess 75 models using ImageNet as the ID dataset and five significantly shifted OOD variants, uncovering a strong linear correlation between ID LCA distance and OOD top-1 accuracy. Our method provides a compelling alternative for understanding why VLMs tend to generalize better. Additionally, we propose a technique to construct a taxonomic hierarchy on any dataset using K-means clustering, demonstrating that LCA distance is robust to the constructed taxonomic hierarchy. Moreover, we demonstrate that aligning model predictions with class taxonomies, through soft labels or prompt engineering, can enhance model generalization. Open source code in our Project Page: https://elvishelvis.github.io/papers/lca/.
<div id='section'>Paperid: <span id='pid'>1691, <a href='https://arxiv.org/pdf/2407.15138.pdf' target='_blank'>https://arxiv.org/pdf/2407.15138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duo Su, Junjie Hou, Weizhi Gao, Yingjie Tian, Bowen Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15138">D$^4$M: Dataset Distillation via Disentangled Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dataset distillation offers a lightweight synthetic dataset for fast network training with promising test accuracy. To imitate the performance of the original dataset, most approaches employ bi-level optimization and the distillation space relies on the matching architecture. Nevertheless, these approaches either suffer significant computational costs on large-scale datasets or experience performance decline on cross-architectures. We advocate for designing an economical dataset distillation framework that is independent of the matching architectures. With empirical observations, we argue that constraining the consistency of the real and synthetic image spaces will enhance the cross-architecture generalization. Motivated by this, we introduce Dataset Distillation via Disentangled Diffusion Model (D$^4$M), an efficient framework for dataset distillation. Compared to architecture-dependent methods, D$^4$M employs latent diffusion model to guarantee consistency and incorporates label information into category prototypes. The distilled datasets are versatile, eliminating the need for repeated generation of distinct datasets for various architectures. Through comprehensive experiments, D$^4$M demonstrates superior performance and robust generalization, surpassing the SOTA methods across most aspects.
<div id='section'>Paperid: <span id='pid'>1692, <a href='https://arxiv.org/pdf/2407.14719.pdf' target='_blank'>https://arxiv.org/pdf/2407.14719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Radwan, Islam Osman, Mohamed S. Shehata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14719">Universal Medical Imaging Model for Domain Generalization with Data Privacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving domain generalization in medical imaging poses a significant challenge, primarily due to the limited availability of publicly labeled datasets in this domain. This limitation arises from concerns related to data privacy and the necessity for medical expertise to accurately label the data. In this paper, we propose a federated learning approach to transfer knowledge from multiple local models to a global model, eliminating the need for direct access to the local datasets used to train each model. The primary objective is to train a global model capable of performing a wide variety of medical imaging tasks. This is done while ensuring the confidentiality of the private datasets utilized during the training of these models. To validate the effectiveness of our approach, extensive experiments were conducted on eight datasets, each corresponding to a different medical imaging application. The client's data distribution in our experiments varies significantly as they originate from diverse domains. Despite this variation, we demonstrate a statistically significant improvement over a state-of-the-art baseline utilizing masked image modeling over a diverse pre-training dataset that spans different body parts and scanning types. This improvement is achieved by curating information learned from clients without accessing any labeled dataset on the server.
<div id='section'>Paperid: <span id='pid'>1693, <a href='https://arxiv.org/pdf/2406.14259.pdf' target='_blank'>https://arxiv.org/pdf/2406.14259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaozhe Hu, Jia-Li Yin, Bin Chen, Luojun Lin, Bo-Hao Chen, Ximeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14259">MEAT: Median-Ensemble Adversarial Training for Improving Robustness and Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-ensemble adversarial training methods improve model robustness by ensembling models at different training epochs, such as model weight averaging (WA). However, previous research has shown that self-ensemble defense methods in adversarial training (AT) still suffer from robust overfitting, which severely affects the generalization performance. Empirically, in the late phases of training, the AT becomes more overfitting to the extent that the individuals for weight averaging also suffer from overfitting and produce anomalous weight values, which causes the self-ensemble model to continue to undergo robust overfitting due to the failure in removing the weight anomalies. To solve this problem, we aim to tackle the influence of outliers in the weight space in this work and propose an easy-to-operate and effective Median-Ensemble Adversarial Training (MEAT) method to solve the robust overfitting phenomenon existing in self-ensemble defense from the source by searching for the median of the historical model weights. Experimental results show that MEAT achieves the best robustness against the powerful AutoAttack and can effectively allievate the robust overfitting. We further demonstrate that most defense methods can improve robust generalization and robustness by combining with MEAT.
<div id='section'>Paperid: <span id='pid'>1694, <a href='https://arxiv.org/pdf/2406.12423.pdf' target='_blank'>https://arxiv.org/pdf/2406.12423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David BergstrÃ¶m, Mattias Tiger, Fredrik Heintz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12423">Deep Temporal Deaggregation: Large-Scale Spatio-Temporal Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many of today's data is time-series data originating from various sources, such as sensors, transaction systems, or production systems. Major challenges with such data include privacy and business sensitivity. Generative time-series models have the potential to overcome these problems, allowing representative synthetic data, such as people's movement in cities, to be shared openly and be used to the benefit of society at large. However, contemporary approaches are limited to prohibitively short sequences and small scales. Aside from major memory limitations, the models generate less accurate and less representative samples the longer the sequences are. This issue is further exacerbated by the lack of a comprehensive and accessible benchmark. Furthermore, a common need in practical applications is what-if analysis and dynamic adaptation to data distribution changes, for usage in decision making and to manage a changing world: What if this road is temporarily blocked or another road is added? The focus of this paper is on mobility data, such as people's movement in cities, requiring all these issues to be addressed. To this end, we propose a transformer-based diffusion model, TDDPM, for time-series which outperforms and scales substantially better than state-of-the-art. This is evaluated in a new comprehensive benchmark across several sequence lengths, standard datasets, and evaluation measures. We also demonstrate how the model can be conditioned on a prior over spatial occupancy frequency information, allowing the model to generate mobility data for previously unseen environments and for hypothetical scenarios where the underlying road network and its usage changes. This is evaluated by training on mobility data from part of a city. Then, using only aggregate spatial information as prior, we demonstrate out-of-distribution generalization to the unobserved remainder of the city.
<div id='section'>Paperid: <span id='pid'>1695, <a href='https://arxiv.org/pdf/2406.05980.pdf' target='_blank'>https://arxiv.org/pdf/2406.05980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Xu, Chaojie Ji, Yankai Cao, Ye Li, Ruxin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05980">Causality-inspired Latent Feature Augmentation for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization (Single-DG) intends to develop a generalizable model with only one single training domain to perform well on other unknown target domains. Under the domain-hungry configuration, how to expand the coverage of source domain and find intrinsic causal features across different distributions is the key to enhancing the models' generalization ability. Existing methods mainly depend on the meticulous design of finite image-level transformation techniques and learning invariant features across domains based on statistical correlation between samples and labels in source domain. This makes it difficult to capture stable semantics between source and target domains, which hinders the improvement of the model's generalization performance. In this paper, we propose a novel causality-inspired latent feature augmentation method for Single-DG by learning the meta-knowledge of feature-level transformation based on causal learning and interventions. Instead of strongly relying on the finite image-level transformation, with the learned meta-knowledge, we can generate diverse implicit feature-level transformations in latent space based on the consistency of causal features and diversity of non-causal features, which can better compensate for the domain-hungry defect and reduce the strong reliance on initial finite image-level transformations and capture more stable domain-invariant causal features for generalization. Extensive experiments on several open-access benchmarks demonstrate the outstanding performance of our model over other state-of-the-art single domain generalization and also multi-source domain generalization methods.
<div id='section'>Paperid: <span id='pid'>1696, <a href='https://arxiv.org/pdf/2406.00764.pdf' target='_blank'>https://arxiv.org/pdf/2406.00764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Yang, Xiaobing Pei, Kai Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00764">IENE: Identifying and Extrapolating the Node Environment for Out-of-Distribution Generalization on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the performance degradation of graph neural networks (GNNs) under distribution shifts, the work on out-of-distribution (OOD) generalization on graphs has received widespread attention. A novel perspective involves distinguishing potential confounding biases from different environments through environmental identification, enabling the model to escape environmentally-sensitive correlations and maintain stable performance under distribution shifts. However, in graph data, confounding factors not only affect the generation process of node features but also influence the complex interaction between nodes. We observe that neglecting either aspect of them will lead to a decrease in performance. In this paper, we propose IENE, an OOD generalization method on graphs based on node-level environmental identification and extrapolation techniques. It strengthens the model's ability to extract invariance from two granularities simultaneously, leading to improved generalization. Specifically, to identify invariance in features, we utilize the disentangled information bottleneck framework to achieve mutual promotion between node-level environmental estimation and invariant feature learning. Furthermore, we extrapolate topological environments through graph augmentation techniques to identify structural invariance. We implement the conceptual method with specific algorithms and provide theoretical analysis and proofs for our approach. Extensive experimental evaluations on two synthetic and four real-world OOD datasets validate the superiority of IENE, which outperforms existing techniques and provides a flexible framework for enhancing the generalization of GNNs.
<div id='section'>Paperid: <span id='pid'>1697, <a href='https://arxiv.org/pdf/2405.19525.pdf' target='_blank'>https://arxiv.org/pdf/2405.19525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Islam Osman, Mohamed S. Shehata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19525">Lifelong Learning Using a Dynamically Growing Tree of Sub-networks for Domain Generalization in Video Object Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art video object segmentation models have achieved great success using supervised learning with massive labeled training datasets. However, these models are trained using a single source domain and evaluated using videos sampled from the same source domain. When these models are evaluated using videos sampled from a different target domain, their performance degrades significantly due to poor domain generalization, i.e., their inability to learn from multi-domain sources simultaneously using traditional supervised learning. In this paper, We propose a dynamically growing tree of sub-networks (DGT) to learn effectively from multi-domain sources. DGT uses a novel lifelong learning technique that allows the model to continuously and effectively learn from new domains without forgetting the previously learned domains. Hence, the model can generalize to out-of-domain videos. The proposed work is evaluated using single-source in-domain (traditional video object segmentation), multi-source in-domain, and multi-source out-of-domain video object segmentation. The results of DGT show a single source in-domain performance gain of 0.2% and 3.5% on the DAVIS16 and DAVIS17 datasets, respectively. However, when DGT is evaluated using in-domain multi-sources, the results show superior performance compared to state-of-the-art video object segmentation and other lifelong learning techniques with an average performance increase in the F-score of 6.9% with minimal catastrophic forgetting. Finally, in the out-of-domain experiment, the performance of DGT is 2.7% and 4% better than state-of-the-art in 1 and 5-shots, respectively.
<div id='section'>Paperid: <span id='pid'>1698, <a href='https://arxiv.org/pdf/2405.16027.pdf' target='_blank'>https://arxiv.org/pdf/2405.16027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Tan, Huei Zhou, Yinxiang Huang, Zeming Zheng, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16027">Feature Protection For Out-of-distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the availability of large pre-trained models, a modern workflow for building real-world machine learning solutions is to fine-tune such models on a downstream task with a relatively small domain-specific dataset. In such applications, one major challenge is that the small fine-tuning dataset does not have sufficient coverage of the distribution encountered when the model is deployed. It is thus important to design fine-tuning methods that are robust to out-of-distribution (OOD) data that are under-represented by the training data. This paper compares common fine-tuning methods to investigate their OOD performance and demonstrates that standard methods will result in a significant change to the pre-trained model so that the fine-tuned features overfit the fine-tuning dataset. However, this causes deteriorated OOD performance. To overcome this issue, we show that protecting pre-trained features leads to a fine-tuned model more robust to OOD generalization. We validate the feature protection methods with extensive experiments of fine-tuning CLIP on ImageNet and DomainNet.
<div id='section'>Paperid: <span id='pid'>1699, <a href='https://arxiv.org/pdf/2405.11154.pdf' target='_blank'>https://arxiv.org/pdf/2405.11154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yang, Mingxuan Xia, Sangzhou Xia, Chicheng Ma, Hui Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11154">Revisiting the Robust Generalization of Adversarial Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the vulnerability of large-scale pre-trained vision-language models like CLIP against adversarial attacks is key to ensuring zero-shot generalization capacity on various downstream tasks. State-of-the-art defense mechanisms generally adopt prompt learning strategies for adversarial fine-tuning to improve the adversarial robustness of the pre-trained model while keeping the efficiency of adapting to downstream tasks. Such a setup leads to the problem of over-fitting which impedes further improvement of the model's generalization capacity on both clean and adversarial examples. In this work, we propose an adaptive Consistency-guided Adversarial Prompt Tuning (i.e., CAPT) framework that utilizes multi-modal prompt learning to enhance the alignment of image and text features for adversarial examples and leverage the strong generalization of pre-trained CLIP to guide the model-enhancing its robust generalization on adversarial examples while maintaining its accuracy on clean ones. We also design a novel adaptive consistency objective function to balance the consistency of adversarial inputs and clean inputs between the fine-tuning model and the pre-trained model. We conduct extensive experiments across 14 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show the superiority of CAPT over other state-of-the-art adaption methods. CAPT demonstrated excellent performance in terms of the in-distribution performance and the generalization under input distribution shift and across datasets.
<div id='section'>Paperid: <span id='pid'>1700, <a href='https://arxiv.org/pdf/2404.19094.pdf' target='_blank'>https://arxiv.org/pdf/2404.19094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Merler, Katsiaryna Haitsiukevich, Nicola Dainese, Pekka Marttinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19094">In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State of the art Symbolic Regression (SR) methods currently build specialized models, while the application of Large Language Models (LLMs) remains largely unexplored. In this work, we introduce the first comprehensive framework that utilizes LLMs for the task of SR. We propose In-Context Symbolic Regression (ICSR), an SR method which iteratively refines a functional form with an LLM and determines its coefficients with an external optimizer. ICSR leverages LLMs' strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors. Our findings reveal that LLMs are able to successfully find symbolic equations that fit the given data, matching or outperforming the overall performance of the best SR baselines on four popular benchmarks, while yielding simpler equations with better out of distribution generalization.
<div id='section'>Paperid: <span id='pid'>1701, <a href='https://arxiv.org/pdf/2404.15242.pdf' target='_blank'>https://arxiv.org/pdf/2404.15242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Ling, Liwei Tan, Wenjun Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15242">A Hybrid Kernel-Free Boundary Integral Method with Operator Learning for Solving Parametric Partial Differential Equations In Complex Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Kernel-Free Boundary Integral (KFBI) method presents an iterative solution to boundary integral equations arising from elliptic partial differential equations (PDEs). This method effectively addresses elliptic PDEs on irregular domains, including the modified Helmholtz, Stokes, and elasticity equations. The rapid evolution of neural networks and deep learning has invigorated the exploration of numerical PDEs. An increasing interest is observed in deep learning approaches that seamlessly integrate mathematical principles for investigating numerical PDEs. We propose a hybrid KFBI method, integrating the foundational principles of the KFBI method with the capabilities of deep learning. This approach, within the framework of the boundary integral method, designs a network to approximate the solution operator for the corresponding integral equations by mapping the parameters, inhomogeneous terms and boundary information of PDEs to the boundary density functions, which can be regarded as the solution of the integral equations. The models are trained using data generated by the Cartesian grid-based KFBI algorithm, exhibiting robust generalization capabilities. It accurately predicts density functions across diverse boundary conditions and parameters within the same class of equations. Experimental results demonstrate that the trained model can directly infer the boundary density function with satisfactory precision, obviating the need for iterative steps in solving boundary integral equations. Furthermore, applying the inference results of the model as initial values for iterations is also reasonable; this approach can retain the inherent second-order accuracy of the KFBI method while accelerating the traditional KFBI approach by reducing about 50% iterations.
<div id='section'>Paperid: <span id='pid'>1702, <a href='https://arxiv.org/pdf/2404.13278.pdf' target='_blank'>https://arxiv.org/pdf/2404.13278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmadreza Eslaminia, Yuquan Meng, Klara Nahrstedt, Chenhui Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13278">Federated Transfer Learning with Task Personalization for Condition Monitoring in Ultrasonic Metal Welding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultrasonic metal welding (UMW) is a key joining technology with widespread industrial applications. Condition monitoring (CM) capabilities are critically needed in UMW applications because process anomalies significantly deteriorate the joining quality. Recently, machine learning models emerged as a promising tool for CM in many manufacturing applications due to their ability to learn complex patterns. Yet, the successful deployment of these models requires substantial training data that may be expensive and time-consuming to collect. Additionally, many existing machine learning models lack generalizability and cannot be directly applied to new process configurations (i.e., domains). Such issues may be potentially alleviated by pooling data across manufacturers, but data sharing raises critical data privacy concerns. To address these challenges, this paper presents a Federated Transfer Learning with Task Personalization (FTL-TP) framework that provides domain generalization capabilities in distributed learning while ensuring data privacy. By effectively learning a unified representation from feature space, FTL-TP can adapt CM models for clients working on similar tasks, thereby enhancing their overall adaptability and performance jointly. To demonstrate the effectiveness of FTL-TP, we investigate two distinct UMW CM tasks, tool condition monitoring and workpiece surface condition classification. Compared with state-of-the-art FL algorithms, FTL-TP achieves a 5.35%--8.08% improvement of accuracy in CM in new target domains. FTL-TP is also shown to perform excellently in challenging scenarios involving unbalanced data distributions and limited client fractions. Furthermore, by implementing the FTL-TP method on an edge-cloud architecture, we show that this method is both viable and efficient in practice. The FTL-TP framework is readily extensible to various other manufacturing applications.
<div id='section'>Paperid: <span id='pid'>1703, <a href='https://arxiv.org/pdf/2404.12999.pdf' target='_blank'>https://arxiv.org/pdf/2404.12999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lisheng Wu, Ke Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12999">Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploration efficiency poses a significant challenge in goal-conditioned reinforcement learning (GCRL) tasks, particularly those with long horizons and sparse rewards. A primary limitation to exploration efficiency is the agent's inability to leverage environmental structural patterns. In this study, we introduce a novel framework, GEASD, designed to capture these patterns through an adaptive skill distribution during the learning process. This distribution optimizes the local entropy of achieved goals within a contextual horizon, enhancing goal-spreading behaviors and facilitating deep exploration in states containing familiar structural patterns. Our experiments reveal marked improvements in exploration efficiency using the adaptive skill distribution compared to a uniform skill distribution. Additionally, the learned skill distribution demonstrates robust generalization capabilities, achieving substantial exploration progress in unseen tasks containing similar local structures.
<div id='section'>Paperid: <span id='pid'>1704, <a href='https://arxiv.org/pdf/2404.02493.pdf' target='_blank'>https://arxiv.org/pdf/2404.02493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Cui, Kai Jiang, Shi Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02493">A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber and Heterogeneous Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a deep learning-enhanced multigrid solver for high-frequency and heterogeneous Helmholtz equations. By applying spectral analysis, we categorize the iteration error into characteristic and non-characteristic components. We eliminate the non-characteristic components by a multigrid wave cycle, which employs carefully selected smoothers on each grid. We diminish the characteristic components by a learned phase function and the approximate solution of an advection-diffusion-reaction (ADR) equation, which is solved using another multigrid V-cycle on a coarser scale, referred to as the ADR cycle. The resulting solver, termed Wave-ADR-NS, enables the handling of error components with varying frequencies and overcomes constraints on the number of grid points per wavelength on coarse grids. Furthermore, we provide an efficient implementation using differentiable programming, making Wave-ADR-NS an end-to-end Helmholtz solver that incorporates parameters learned through a semi-supervised training. Wave-ADR-NS demonstrates robust generalization capabilities for both in-distribution and out-of-distribution velocity fields of varying difficulty. Comparative experiments with other multigrid methods validate its superior performance in solving heterogeneous 2D Helmholtz equations with wavenumbers exceeding 2000.
<div id='section'>Paperid: <span id='pid'>1705, <a href='https://arxiv.org/pdf/2403.16175.pdf' target='_blank'>https://arxiv.org/pdf/2403.16175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arindam Majee, Avisek Gupta, Sourav Raha, Swagatam Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16175">Enhancing MRI-Based Classification of Alzheimer's Disease with Explainable 3D Hybrid Compact Convolutional Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Alzheimer's disease (AD), characterized by progressive cognitive decline and memory loss, presents a formidable global health challenge, underscoring the critical importance of early and precise diagnosis for timely interventions and enhanced patient outcomes. While MRI scans provide valuable insights into brain structures, traditional analysis methods often struggle to discern intricate 3D patterns crucial for AD identification. Addressing this challenge, we introduce an alternative end-to-end deep learning model, the 3D Hybrid Compact Convolutional Transformers 3D (HCCT). By synergistically combining convolutional neural networks (CNNs) and vision transformers (ViTs), the 3D HCCT adeptly captures both local features and long-range relationships within 3D MRI scans. Extensive evaluations on prominent AD benchmark dataset, ADNI, demonstrate the 3D HCCT's superior performance, surpassing state of the art CNN and transformer-based methods in classification accuracy. Its robust generalization capability and interpretability marks a significant stride in AD classification from 3D MRI scans, promising more accurate and reliable diagnoses for improved patient care and superior clinical outcomes.
<div id='section'>Paperid: <span id='pid'>1706, <a href='https://arxiv.org/pdf/2403.13728.pdf' target='_blank'>https://arxiv.org/pdf/2403.13728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Sun, Nutan Chen, Alexej Gossmann, Matteo Wohlrapp, Yu Xing, Carla Feistner, Emilio Dorigatt, Felix Drost, Daniele Scarcella, Lisa Beer, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13728">M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A probabilistic graphical model is proposed, modeling the joint model parameter and multiplier evolution, with a hypervolume based likelihood, promoting multi-objective descent in structural risk minimization. We address multi-objective model parameter optimization via a surrogate single objective penalty loss with time-varying multipliers, equivalent to online scheduling of loss landscape. The multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems with shrinking bounds according to Pareto dominance. The bound serves as setpoint for the low-level multiplier controller to schedule loss landscapes via output feedback of each loss term. Our method forms closed loop of model parameter dynamic, circumvents excessive memory requirements and extra computational burden of existing multi-objective deep learning methods, and is robust against controller hyperparameter variation, demonstrated on domain generalization tasks with multi-dimensional regularization losses.
<div id='section'>Paperid: <span id='pid'>1707, <a href='https://arxiv.org/pdf/2402.18447.pdf' target='_blank'>https://arxiv.org/pdf/2402.18447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deng Li, Aming Wu, Yaowei Wang, Yahong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18447">Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic object-centric perception network based on prompt learning, aiming to adapt to the variations in image complexity. Specifically, we propose an object-centric gating module based on prompt learning to focus attention on the object-centric features guided by the various scene prompts. Then, with the object-centric gating masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive object-centric relevant features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and object detection. The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.
<div id='section'>Paperid: <span id='pid'>1708, <a href='https://arxiv.org/pdf/2402.17773.pdf' target='_blank'>https://arxiv.org/pdf/2402.17773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaniv Cohen, Tomer Gafni, Ronen Greenberg, Kobi Cohen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17773">SINR-Aware Deep Reinforcement Learning for Distributed Dynamic Channel Allocation in Cognitive Interference Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of dynamic channel allocation (DCA) in cognitive communication networks with the goal of maximizing a global signal-to-interference-plus-noise ratio (SINR) measure under a specified target quality of service (QoS)-SINR for each network. The shared bandwidth is partitioned into K channels with frequency separation. In contrast to the majority of existing studies that assume perfect orthogonality or a one- to-one user-channel allocation mapping, this paper focuses on real-world systems experiencing inter-carrier interference (ICI) and channel reuse by multiple large-scale networks. This realistic scenario significantly increases the problem dimension, rendering existing algorithms inefficient. We propose a novel multi-agent reinforcement learning (RL) framework for distributed DCA, named Channel Allocation RL To Overlapped Networks (CARLTON). The CARLTON framework is based on the Centralized Training with Decentralized Execution (CTDE) paradigm, utilizing the DeepMellow value-based RL algorithm. To ensure robust performance in the interference-laden environment we address, CARLTON employs a low-dimensional representation of observations, generating a QoS-type measure while maximizing a global SINR measure and ensuring the target QoS-SINR for each network. Our results demonstrate exceptional performance and robust generalization, showcasing superior efficiency compared to alternative state-of-the-art methods, while achieving a marginally diminished performance relative to a fully centralized approach.
<div id='section'>Paperid: <span id='pid'>1709, <a href='https://arxiv.org/pdf/2402.14318.pdf' target='_blank'>https://arxiv.org/pdf/2402.14318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SÅawomir Dadas, MaÅgorzata GrÄbowiec
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14318">Assessing generalization capability of text ranking models in Polish</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) is becoming an increasingly popular technique for integrating internal knowledge bases with large language models. In a typical RAG pipeline, three models are used, responsible for the retrieval, reranking, and generation stages. In this article, we focus on the reranking problem for the Polish language, examining the performance of rerankers and comparing their results with available retrieval models. We conduct a comprehensive evaluation of existing models and those trained by us, utilizing a benchmark of 41 diverse information retrieval tasks for the Polish language. The results of our experiments show that most models struggle with out-of-domain generalization. However, a combination of effective optimization method and a large training dataset allows for building rerankers that are both compact in size and capable of generalization. The best of our models establishes a new state-of-the-art for reranking in the Polish language, outperforming existing models with up to 30 times more parameters.
<div id='section'>Paperid: <span id='pid'>1710, <a href='https://arxiv.org/pdf/2402.09891.pdf' target='_blank'>https://arxiv.org/pdf/2402.09891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vivian Y. Nastl, Moritz Hardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09891">Do causal predictors generalize better to new domains?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. In addition, we show that recent causal machine learning methods for domain generalization do not perform better in our evaluation than standard predictors trained on the set of causal features. Likewise, causal discovery algorithms either fail to run or select causal variables that perform no better than our selection. Extensive robustness checks confirm that our findings are stable under variable misclassification.
<div id='section'>Paperid: <span id='pid'>1711, <a href='https://arxiv.org/pdf/2402.09613.pdf' target='_blank'>https://arxiv.org/pdf/2402.09613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Niss, Kevin Vogt-Lowell, Theodoros Tsiligkaridis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09613">Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundations models are presented as generalists that often perform well over a myriad of tasks. Fine-tuning these models, even on limited data, provides an additional boost in task-specific performance but often at the cost of their wider generalization, an effect termed catastrophic forgetting. In this paper, we analyze the relation between task difficulty in the CLIP model and the performance of several simple parameter-efficient fine-tuning methods through the lens of domain generalization and catastrophic forgetting. We provide evidence that the silhouette score of the zero-shot image and text embeddings is a better measure of task difficulty than the average cosine similarity of correct image/label embeddings, and discuss observable relationships between task difficulty, fine-tuning method, domain generalization, and catastrophic forgetting. Additionally, the averaged results across tasks and performance measures demonstrate that a simplified method that trains only a subset of attention weights, which we call A-CLIP, yields a balance between domain generalization and catastrophic forgetting.
<div id='section'>Paperid: <span id='pid'>1712, <a href='https://arxiv.org/pdf/2402.06315.pdf' target='_blank'>https://arxiv.org/pdf/2402.06315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxuan Zhang, Quan Pan, Salvador GarcÃ­a
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06315">Multisource Semisupervised Adversarial Domain Generalization Network for Cross-Scene Sea-Land Clutter Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning (DL)-based sea\textendash land clutter classification for sky-wave over-the-horizon-radar (OTHR) has become a novel research topic. In engineering applications, real-time predictions of sea\textendash land clutter with existing distribution discrepancies are crucial. To solve this problem, this article proposes a novel Multisource Semisupervised Adversarial Domain Generalization Network (MSADGN) for cross-scene sea\textendash land clutter classification. MSADGN can extract domain-invariant and domain-specific features from one labeled source domain and multiple unlabeled source domains, and then generalize these features to an arbitrary unseen target domain for real-time prediction of sea\textendash land clutter. Specifically, MSADGN consists of three modules: domain-related pseudolabeling module, domain-invariant module, and domain-specific module. The first module introduces an improved pseudolabel method called domain-related pseudolabel, which is designed to generate reliable pseudolabels to fully exploit unlabeled source domains. The second module utilizes a generative adversarial network (GAN) with a multidiscriminator to extract domain-invariant features, to enhance the model's transferability in the target domain. The third module employs a parallel multiclassifier branch to extract domain-specific features, to enhance the model's discriminability in the target domain. The effectiveness of our method is validated in twelve domain generalizations (DG) scenarios. Meanwhile, we selected 10 state-of-the-art DG methods for comparison. The experimental results demonstrate the superiority of our method.
<div id='section'>Paperid: <span id='pid'>1713, <a href='https://arxiv.org/pdf/2402.04967.pdf' target='_blank'>https://arxiv.org/pdf/2402.04967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piush Aggarwal, Jawar Mehrabanian, Weigang Huang, Ãzge Alacam, Torsten Zesch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04967">Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with an average $Î$F1 of 0.18.
<div id='section'>Paperid: <span id='pid'>1714, <a href='https://arxiv.org/pdf/2402.04875.pdf' target='_blank'>https://arxiv.org/pdf/2402.04875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Ahuja, Amin Mansouri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04875">On Provable Length and Compositional Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution generalization capabilities of sequence-to-sequence models can be studied from the lens of two crucial forms of generalization: length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization: the ability to generalize to token combinations not seen during training. In this work, we provide first provable guarantees on length and compositional generalization for common sequence-to-sequence models -- deep sets, transformers, state space models, and recurrent neural nets -- trained to minimize the prediction error. We show that \emph{limited capacity} versions of these different architectures achieve both length and compositional generalization provided the training distribution is sufficiently diverse. In the first part, we study structured limited capacity variants of different architectures and arrive at the generalization guarantees with limited diversity requirements on the training distribution. In the second part, we study limited capacity variants with less structural assumptions and arrive at generalization guarantees but with more diversity requirements on the training distribution. Further, we also show that chain-of-thought supervision enables length generalization in higher capacity counterparts of the different architectures we study.
<div id='section'>Paperid: <span id='pid'>1715, <a href='https://arxiv.org/pdf/2401.16520.pdf' target='_blank'>https://arxiv.org/pdf/2401.16520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyan Li, Andrew M. Sayer, Ian T. Carroll, Xin Huang, Jianwu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16520">MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and Attention-based Regression for Cloud Property Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of Earth science, effective cloud property retrieval, encompassing cloud masking, cloud phase classification, and cloud optical thickness (COT) prediction, remains pivotal. Traditional methodologies necessitate distinct models for each sensor instrument due to their unique spectral characteristics. Recent strides in Earth Science research have embraced machine learning and deep learning techniques to extract features from satellite datasets' spectral observations. However, prevailing approaches lack novel architectures accounting for hierarchical relationships among retrieval tasks. Moreover, considering the spectral diversity among existing sensors, the development of models with robust generalization capabilities over different sensor datasets is imperative. Surprisingly, there is a dearth of methodologies addressing the selection of an optimal model for diverse datasets. In response, this paper introduces MT-HCCAR, an end-to-end deep learning model employing multi-task learning to simultaneously tackle cloud masking, cloud phase retrieval (classification tasks), and COT prediction (a regression task). The MT-HCCAR integrates a hierarchical classification network (HC) and a classification-assisted attention-based regression network (CAR), enhancing precision and robustness in cloud labeling and COT prediction. Additionally, a comprehensive model selection method rooted in K-fold cross-validation, one standard error rule, and two introduced performance scores is proposed to select the optimal model over three simulated satellite datasets OCI, VIIRS, and ABI. The experiments comparing MT-HCCAR with baseline methods, the ablation studies, and the model selection affirm the superiority and the generalization capabilities of MT-HCCAR.
<div id='section'>Paperid: <span id='pid'>1716, <a href='https://arxiv.org/pdf/2401.15323.pdf' target='_blank'>https://arxiv.org/pdf/2401.15323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haesun Joung, Kyogu Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15323">Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Music auto-tagging is crucial for enhancing music discovery and recommendation. Existing models in Music Information Retrieval (MIR) struggle with real-world noise such as environmental and speech sounds in multimedia content. This study proposes a method inspired by speech-related tasks to enhance music auto-tagging performance in noisy settings. The approach integrates Domain Adversarial Training (DAT) into the music domain, enabling robust music representations that withstand noise. Unlike previous research, this approach involves an additional pretraining phase for the domain classifier, to avoid performance degradation in the subsequent phase. Adding various synthesized noisy music data improves the model's generalization across different noise levels. The proposed architecture demonstrates enhanced performance in music auto-tagging by effectively utilizing unlabeled noisy music data. Additional experiments with supplementary unlabeled data further improves the model's performance, underscoring its robust generalization capabilities and broad applicability.
<div id='section'>Paperid: <span id='pid'>1717, <a href='https://arxiv.org/pdf/2401.13652.pdf' target='_blank'>https://arxiv.org/pdf/2401.13652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Della Santa, Sandra Pieraccini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13652">Graph-Instructed Neural Networks for Sparse Grid-Based Discontinuity Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Instructed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization properties of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
<div id='section'>Paperid: <span id='pid'>1718, <a href='https://arxiv.org/pdf/2401.05578.pdf' target='_blank'>https://arxiv.org/pdf/2401.05578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Chen, Zhenya Zang, Xingda Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05578">Fast Cerebral Blood Flow Analysis via Extreme Learning Machine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a rapid and precise analytical approach for analyzing cerebral blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the application of the Extreme Learning Machine (ELM). Our evaluation of ELM and existing algorithms involves a comprehensive set of metrics. We assess these algorithms using synthetic datasets for both semi-infinite and multi-layer models. The results demonstrate that ELM consistently achieves higher fidelity across various noise levels and optical parameters, showcasing robust generalization ability and outperforming iterative fitting algorithms. Through a comparison with a computationally efficient neural network, ELM attains comparable accuracy with reduced training and inference times. Notably, the absence of a back-propagation process in ELM during training results in significantly faster training speeds compared to existing neural network approaches. This proposed strategy holds promise for edge computing applications with online training capabilities.
<div id='section'>Paperid: <span id='pid'>1719, <a href='https://arxiv.org/pdf/2401.03765.pdf' target='_blank'>https://arxiv.org/pdf/2401.03765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhimin Zhang, Xiang Gao, Wei Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03765">InvariantOODG: Learning Invariant Features of Point Clouds for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The convenience of 3D sensors has led to an increase in the use of 3D point clouds in various applications. However, the differences in acquisition devices or scenarios lead to divergence in the data distribution of point clouds, which requires good generalization of point cloud representation learning methods. While most previous methods rely on domain adaptation, which involves fine-tuning pre-trained models on target domain data, this may not always be feasible in real-world scenarios where target domain data may be unavailable. To address this issue, we propose InvariantOODG, which learns invariability between point clouds with different distributions using a two-branch network to extract local-to-global features from original and augmented point clouds. Specifically, to enhance local feature learning of point clouds, we define a set of learnable anchor points that locate the most useful local regions and two types of transformations to augment the input point clouds. The experimental results demonstrate the effectiveness of the proposed model on 3D domain generalization benchmarks.
<div id='section'>Paperid: <span id='pid'>1720, <a href='https://arxiv.org/pdf/2312.17463.pdf' target='_blank'>https://arxiv.org/pdf/2312.17463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Eyre, Elliot Creager, David Madras, Vardan Papyan, Richard Zemel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17463">Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing deep neural network classifiers that perform robustly on distributions differing from the available training data is an active area of machine learning research. However, out-of-distribution generalization for regression-the analogous problem for modeling continuous targets-remains relatively unexplored. To tackle this problem, we return to first principles and analyze how the closed-form solution for Ordinary Least Squares (OLS) regression is sensitive to covariate shift. We characterize the out-of-distribution risk of the OLS model in terms of the eigenspectrum decomposition of the source and target data. We then use this insight to propose a method for adapting the weights of the last layer of a pre-trained neural regression model to perform better on input data originating from a different distribution. We demonstrate how this lightweight spectral adaptation procedure can improve out-of-distribution performance for synthetic and real-world datasets.
<div id='section'>Paperid: <span id='pid'>1721, <a href='https://arxiv.org/pdf/2312.10900.pdf' target='_blank'>https://arxiv.org/pdf/2312.10900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yemin Yu, Luotian Yuan, Ying Wei, Hanyu Gao, Xinhai Ye, Zhihua Wang, Fei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10900">RetroOOD: Understanding Out-of-Distribution Generalization in Retrosynthesis Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning-assisted retrosynthesis prediction models have been gaining widespread adoption, though their performances oftentimes degrade significantly when deployed in real-world applications embracing out-of-distribution (OOD) molecules or reactions. Despite steady progress on standard benchmarks, our understanding of existing retrosynthesis prediction models under the premise of distribution shifts remains stagnant. To this end, we first formally sort out two types of distribution shifts in retrosynthesis prediction and construct two groups of benchmark datasets. Next, through comprehensive experiments, we systematically compare state-of-the-art retrosynthesis prediction models on the two groups of benchmarks, revealing the limitations of previous in-distribution evaluation and re-examining the advantages of each model. More remarkably, we are motivated by the above empirical insights to propose two model-agnostic techniques that can improve the OOD generalization of arbitrary off-the-shelf retrosynthesis prediction algorithms. Our preliminary experiments show their high potential with an average performance improvement of 4.6%, and the established benchmarks serve as a foothold for further retrosynthesis prediction research towards OOD generalization.
<div id='section'>Paperid: <span id='pid'>1722, <a href='https://arxiv.org/pdf/2312.09004.pdf' target='_blank'>https://arxiv.org/pdf/2312.09004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Sabanza Gil, Andres M. Bran, Malte Franke, Remi Schlama, Jeremy S. Luterbacher, Philippe Schwaller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09004">Holistic chemical evaluation reveals pitfalls in reaction prediction models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prediction of chemical reactions has gained significant interest within the machine learning community in recent years, owing to its complexity and crucial applications in chemistry. However, model evaluation for this task has been mostly limited to simple metrics like top-k accuracy, which obfuscates fine details of a model's limitations. Inspired by progress in other fields, we propose a new assessment scheme that builds on top of current approaches, steering towards a more holistic evaluation. We introduce the following key components for this goal: CHORISO, a curated dataset along with multiple tailored splits to recreate chemically relevant scenarios, and a collection of metrics that provide a holistic view of a model's advantages and limitations. Application of this method to state-of-the-art models reveals important differences on sensitive fronts, especially stereoselectivity and chemical out-of-distribution generalization. Our work paves the way towards robust prediction models that can ultimately accelerate chemical discovery.
<div id='section'>Paperid: <span id='pid'>1723, <a href='https://arxiv.org/pdf/2311.10845.pdf' target='_blank'>https://arxiv.org/pdf/2311.10845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangzhi Li, Lei Ma, Xingyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10845">Domain Generalization of 3D Object Detection by Density-Resampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point-cloud-based 3D object detection suffers from performance degradation when encountering data with novel domain gaps. To tackle it, the single-domain generalization (SDG) aims to generalize the detection model trained in a limited single source domain to perform robustly on unexplored domains. In this paper, we propose an SDG method to improve the generalizability of 3D object detection to unseen target domains. Unlike prior SDG works for 3D object detection solely focusing on data augmentation, our work introduces a novel data augmentation method and contributes a new multi-task learning strategy in the methodology. Specifically, from the perspective of data augmentation, we design a universal physical-aware density-based data augmentation (PDDA) method to mitigate the performance loss stemming from diverse point densities. From the learning methodology viewpoint, we develop a multi-task learning for 3D object detection: during source training, besides the main standard detection task, we leverage an auxiliary self-supervised 3D scene restoration task to enhance the comprehension of the encoder on background and foreground details for better recognition and detection of objects. Furthermore, based on the auxiliary self-supervised task, we propose the first test-time adaptation method for domain generalization of 3D object detection, which efficiently adjusts the encoder's parameters to adapt to unseen target domains during testing time, to further bridge domain gaps. Extensive cross-dataset experiments covering "Car", "Pedestrian", and "Cyclist" detections, demonstrate our method outperforms state-of-the-art SDG methods and even overpass unsupervised domain adaptation methods under some circumstances.
<div id='section'>Paperid: <span id='pid'>1724, <a href='https://arxiv.org/pdf/2311.08503.pdf' target='_blank'>https://arxiv.org/pdf/2311.08503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aveen Dayal, Vimal K. B., Linga Reddy Cenkeramaddi, C. Krishna Mohan, Abhinav Kumar, Vineeth N Balasubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08503">MADG: Margin-based Adversarial Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\mathcal{H}Î\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, MADG, motivated by a margin loss-based discrepancy metric. The proposed MADG model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed MADG model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the MADG model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.
<div id='section'>Paperid: <span id='pid'>1725, <a href='https://arxiv.org/pdf/2311.05861.pdf' target='_blank'>https://arxiv.org/pdf/2311.05861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Steven Korevaar, Ruwan Tennakoon, Ricky O'Brien, Dwarikanath Mahapatra, Alireza Bab-Hadiasha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05861">Domain Generalization by Learning from Privileged Medical Imaging Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning the ability to generalize knowledge between similar contexts is particularly important in medical imaging as data distributions can shift substantially from one hospital to another, or even from one machine to another. To strengthen generalization, most state-of-the-art techniques inject knowledge of the data distribution shifts by enforcing constraints on learned features or regularizing parameters. We offer an alternative approach: Learning from Privileged Medical Imaging Information (LPMII). We show that using some privileged information such as tumor shape or location leads to stronger domain generalization ability than current state-of-the-art techniques. This paper demonstrates that by using privileged information to predict the severity of intra-layer retinal fluid in optical coherence tomography scans, the classification accuracy of a deep learning model operating on out-of-distribution data improves from $0.911$ to $0.934$. This paper provides a strong starting point for using privileged information in other medical problems requiring generalization.
<div id='section'>Paperid: <span id='pid'>1726, <a href='https://arxiv.org/pdf/2310.11991.pdf' target='_blank'>https://arxiv.org/pdf/2310.11991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Floris Holstege, Bram Wouters, Noud van Giersbergen, Cees Diks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11991">Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
<div id='section'>Paperid: <span id='pid'>1727, <a href='https://arxiv.org/pdf/2310.07361.pdf' target='_blank'>https://arxiv.org/pdf/2310.07361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mateusz Michalkiewicz, Masoud Faraki, Xiang Yu, Manmohan Chandraker, Mahsa Baktashmotlagh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07361">Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Overfitting to the source domain is a common issue in gradient-based training of deep neural networks. To compensate for the over-parameterized models, numerous regularization techniques have been introduced such as those based on dropout. While these methods achieve significant improvements on classical benchmarks such as ImageNet, their performance diminishes with the introduction of domain shift in the test set i.e. when the unseen data comes from a significantly different distribution. In this paper, we move away from the classical approach of Bernoulli sampled dropout mask construction and propose to base the selection on gradient-signal-to-noise ratio (GSNR) of network's parameters. Specifically, at each training step, parameters with high GSNR will be discarded. Furthermore, we alleviate the burden of manually searching for the optimal dropout ratio by leveraging a meta-learning approach. We evaluate our method on standard domain generalization benchmarks and achieve competitive results on classification and face anti-spoofing problems.
<div id='section'>Paperid: <span id='pid'>1728, <a href='https://arxiv.org/pdf/2310.06182.pdf' target='_blank'>https://arxiv.org/pdf/2310.06182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiancong Xiao, Ruoyu Sun, Zhi- Quan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06182">PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is found empirically that adversarially robust generalization is crucial in establishing defense algorithms against adversarial attacks. Therefore, it is interesting to study the theoretical guarantee of robust generalization. This paper focuses on norm-based complexity, based on a PAC-Bayes approach (Neyshabur et al., 2017). The main challenge lies in extending the key ingredient, which is a weight perturbation bound in standard settings, to the robust settings. Existing attempts heavily rely on additional strong assumptions, leading to loose bounds. In this paper, we address this issue and provide a spectrally-normalized robust generalization bound for DNNs. Compared to existing bounds, our bound offers two significant advantages: Firstly, it does not depend on additional assumptions. Secondly, it is considerably tighter, aligning with the bounds of standard generalization. Therefore, our result provides a different perspective on understanding robust generalization: The mismatch terms between standard and robust generalization bounds shown in previous studies do not contribute to the poor robust generalization. Instead, these disparities solely due to mathematical issues. Finally, we extend the main result to adversarial robustness against general non-$\ell_p$ attacks and other neural network architectures.
<div id='section'>Paperid: <span id='pid'>1729, <a href='https://arxiv.org/pdf/2310.02473.pdf' target='_blank'>https://arxiv.org/pdf/2310.02473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sepidehsadat Hosseini, Mengyao Zhai, Hossein Hajimirsadegh, Frederick Tung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02473">Prompting-based Temporal Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning traditionally assumes that the training and testing data are distributed independently and identically. However, in many real-world settings, the data distribution can shift over time, leading to poor generalization of trained models in future time periods. This paper presents a novel prompting-based approach to temporal domain generalization that is parameter-efficient, time-efficient, and does not require access to future data during training. Our method adapts a trained model to temporal drift by learning global prompts, domain-specific prompts, and drift-aware prompts that capture underlying temporal dynamics. Experiments on classification, regression, and time series forecasting tasks demonstrate the generality of the proposed approach. The code repository will be publicly shared.
<div id='section'>Paperid: <span id='pid'>1730, <a href='https://arxiv.org/pdf/2308.15856.pdf' target='_blank'>https://arxiv.org/pdf/2308.15856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ozan Sener, Vladlen Koltun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15856">Domain Generalization without Excess Empirical Risk</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given data from diverse sets of distinct distributions, domain generalization aims to learn models that generalize to unseen distributions. A common approach is designing a data-driven surrogate penalty to capture generalization and minimize the empirical risk jointly with the penalty. We argue that a significant failure mode of this recipe is an excess risk due to an erroneous penalty or hardness in joint optimization. We present an approach that eliminates this problem. Instead of jointly minimizing empirical risk with the penalty, we minimize the penalty under the constraint of optimality of the empirical risk. This change guarantees that the domain generalization penalty cannot impair optimization of the empirical risk, i.e., in-distribution performance. To solve the proposed optimization problem, we demonstrate an exciting connection to rate-distortion theory and utilize its tools to design an efficient method. Our approach can be applied to any penalty-based domain generalization method, and we demonstrate its effectiveness by applying it to three examplar methods from the literature, showing significant improvements.
<div id='section'>Paperid: <span id='pid'>1731, <a href='https://arxiv.org/pdf/2308.15050.pdf' target='_blank'>https://arxiv.org/pdf/2308.15050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taotao Jing, Lichen Wang, Naji Khosravan, Zhiqiang Wan, Zachary Bessinger, Zhengming Ding, Sing Bing Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15050">iBARLE: imBalance-Aware Room Layout Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Room layout estimation predicts layouts from a single panorama. It requires datasets with large-scale and diverse room shapes to train the models. However, there are significant imbalances in real-world datasets including the dimensions of layout complexity, camera locations, and variation in scene appearance. These issues considerably influence the model training performance. In this work, we propose the imBalance-Aware Room Layout Estimation (iBARLE) framework to address these issues. iBARLE consists of (1) Appearance Variation Generation (AVG) module, which promotes visual appearance domain generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances generalizability w.r.t. room structure, and (3) a gradient-based layout objective function, which allows more effective accounting for occlusions in complex layouts. All modules are jointly trained and help each other to achieve the best performance. Experiments and ablation studies based on ZInD~\cite{cruz2021zillow} dataset illustrate that iBARLE has state-of-the-art performance compared with other layout estimation baselines.
<div id='section'>Paperid: <span id='pid'>1732, <a href='https://arxiv.org/pdf/2308.14909.pdf' target='_blank'>https://arxiv.org/pdf/2308.14909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyungchan Yoon, Changhwan Kim, Eunwoo Song, Hyun-Wook Yoon, Hong-Goo Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14909">Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For personalized speech generation, a neural text-to-speech (TTS) model must be successfully implemented with limited data from a target speaker. To this end, the baseline TTS model needs to be amply generalized to out-of-domain data (i.e., target speaker's speech). However, approaches to address this out-of-domain generalization problem in TTS have yet to be thoroughly studied. In this work, we propose an effective pruning method for a transformer known as sparse attention, to improve the TTS model's generalization abilities. In particular, we prune off redundant connections from self-attention layers whose attention weights are below the threshold. To flexibly determine the pruning strength for searching optimal degree of generalization, we also propose a new differentiable pruning method that allows the model to automatically learn the thresholds. Evaluations on zero-shot multi-speaker TTS verify the effectiveness of our method in terms of voice quality and speaker similarity.
<div id='section'>Paperid: <span id='pid'>1733, <a href='https://arxiv.org/pdf/2308.13331.pdf' target='_blank'>https://arxiv.org/pdf/2308.13331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan-Aike TermÃ¶hlen, Timo Bartels, Tim Fingscheidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13331">A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of semantic segmentation requires a model to assign semantic labels to each pixel of an image. However, the performance of such models degrades when deployed in an unseen domain with different data distributions compared to the training domain. We present a new augmentation-driven approach to domain generalization for semantic segmentation using a re-parameterized vision transformer (ReVT) with weight averaging of multiple models after training. We evaluate our approach on several benchmark datasets and achieve state-of-the-art mIoU performance of 47.3% (prior art: 46.3%) for small models and of 50.1% (prior art: 47.8%) for midsized models on commonly used benchmark datasets. At the same time, our method requires fewer parameters and reaches a higher frame rate than the best prior art. It is also easy to implement and, unlike network ensembles, does not add any computational complexity during inference.
<div id='section'>Paperid: <span id='pid'>1734, <a href='https://arxiv.org/pdf/2308.03321.pdf' target='_blank'>https://arxiv.org/pdf/2308.03321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikai Zhou, Shuo Zhang, Ziruo Wang, Huanran Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03321">AFN: Adaptive Fusion Normalization via an Encoder-Decoder Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of deep learning is inseparable from normalization layers. Researchers have proposed various normalization functions, and each of them has both advantages and disadvantages. In response, efforts have been made to design a unified normalization function that combines all normalization procedures and mitigates their weaknesses. We also proposed a new normalization function called Adaptive Fusion Normalization. Through experiments, we demonstrate AFN outperforms the previous normalization techniques in domain generalization and image classification tasks.
<div id='section'>Paperid: <span id='pid'>1735, <a href='https://arxiv.org/pdf/2306.10809.pdf' target='_blank'>https://arxiv.org/pdf/2306.10809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Xu, Yuwang Wang, Xuejin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10809">Shape Guided Gradient Voting for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to address the domain shift between training and testing data. To learn the domain invariant representations, the model is usually trained on multiple domains. It has been found that the gradients of network weight relative to a specific task loss can characterize the task itself. In this work, with the assumption that the gradients of a specific domain samples under the classification task could also reflect the property of the domain, we propose a Shape Guided Gradient Voting (SGGV) method for domain generalization. Firstly, we introduce shape prior via extra inputs of the network to guide gradient descending towards a shape-biased direction for better generalization. Secondly, we propose a new gradient voting strategy to remove the outliers for robust optimization in the presence of shape guidance. To provide shape guidance, we add edge/sketch extracted from the training data as an explicit way, and also use texture augmented images as an implicit way. We conduct experiments on several popular domain generalization datasets in image classification task, and show that our shape guided gradient updating strategy brings significant improvement of the generalization.
<div id='section'>Paperid: <span id='pid'>1736, <a href='https://arxiv.org/pdf/2306.03984.pdf' target='_blank'>https://arxiv.org/pdf/2306.03984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abishek Komma, Nagesh Panyam Chandrasekarasastry, Timothy Leffel, Anuj Goyal, Angeliki Metallinou, Spyros Matsoukas, Aram Galstyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03984">Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Measurement of interaction quality is a critical task for the improvement of spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the proposed evaluation model shows better domain generalization ability compared to the baselines. On the basis of these results, we argue that having high-quality human-annotated data is an important component of evaluating interaction quality for large industrial-scale voice assistant platforms.
<div id='section'>Paperid: <span id='pid'>1737, <a href='https://arxiv.org/pdf/2306.03607.pdf' target='_blank'>https://arxiv.org/pdf/2306.03607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingchen Ma, Christos Tzamos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03607">Buying Information for Stochastic Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic optimization is one of the central problems in Machine Learning and Theoretical Computer Science. In the standard model, the algorithm is given a fixed distribution known in advance. In practice though, one may acquire at a cost extra information to make better decisions. In this paper, we study how to buy information for stochastic optimization and formulate this question as an online learning problem. Assuming the learner has an oracle for the original optimization problem, we design a $2$-competitive deterministic algorithm and a $e/(e-1)$-competitive randomized algorithm for buying information. We show that this ratio is tight as the problem is equivalent to a robust generalization of the ski-rental problem, which we call super-martingale stopping.
  We also consider an adaptive setting where the learner can choose to buy information after taking some actions for the underlying optimization problem. We focus on the classic optimization problem, Min-Sum Set Cover, where the goal is to quickly find an action that covers a given request drawn from a known distribution. We provide an $8$-competitive algorithm running in polynomial time that chooses actions and decides when to buy information about the underlying request.
<div id='section'>Paperid: <span id='pid'>1738, <a href='https://arxiv.org/pdf/2305.14600.pdf' target='_blank'>https://arxiv.org/pdf/2305.14600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Li, Ghazaleh Kazeminejad, Susan W. Brown, Martha Palmer, Vivek Srikumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14600">Learning Semantic Role Labeling from Compatible Label Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic role labeling (SRL) has multiple disjoint label sets, e.g., VerbNet and PropBank. Creating these datasets is challenging, therefore a natural question is how to use each one to help the other. Prior work has shown that cross-task interaction helps, but only explored multitask learning so far. A common issue with multi-task setup is that argument sequences are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like Semlink). In this paper, we eliminate such issue with a framework that jointly models VerbNet and PropBank labels as one sequence. In this setup, we show that enforcing Semlink constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from given PropBank arguments with over 99 F1. For learning, we propose a constrained marginal model that learns with knowledge defined in Semlink to further benefit from the large amounts of PropBank-only data. On the joint benchmark based on CoNLL05, our models achieve state-of-the-art F1's, outperforming the prior best in-domain model by 3.5 (VerbNet) and 0.8 (PropBank). For out-of-domain generalization, our models surpass the prior best by 3.4 (VerbNet) and 0.2 (PropBank).
<div id='section'>Paperid: <span id='pid'>1739, <a href='https://arxiv.org/pdf/2305.01754.pdf' target='_blank'>https://arxiv.org/pdf/2305.01754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aik Rui Tan, Shingo Urata, Samuel Goldman, Johannes C. B. Dietschreit, Rafael GÃ³mez-Bombarelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01754">Single-model uncertainty quantification in neural network potentials does not consistently outperform model ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks (NNs) often assign high confidence to their predictions, even for points far out-of-distribution, making uncertainty quantification (UQ) a challenge. When they are employed to model interatomic potentials in materials systems, this problem leads to unphysical structures that disrupt simulations, or to biased statistics and dynamics that do not reflect the true physics. Differentiable UQ techniques can find new informative data and drive active learning loops for robust potentials. However, a variety of UQ techniques, including newly developed ones, exist for atomistic simulations and there are no clear guidelines for which are most effective or suitable for a given case. In this work, we examine multiple UQ schemes for improving the robustness of NN interatomic potentials (NNIPs) through active learning. In particular, we compare incumbent ensemble-based methods against strategies that use single, deterministic NNs: mean-variance estimation, deep evidential regression, and Gaussian mixture models. We explore three datasets ranging from in-domain interpolative learning to more extrapolative out-of-domain generalization challenges: rMD17, ammonia inversion, and bulk silica glass. Performance is measured across multiple metrics relating model error to uncertainty. Our experiments show that none of the methods consistently outperformed each other across the various metrics. Ensembling remained better at generalization and for NNIP robustness; MVE only proved effective for in-domain interpolation, while GMM was better out-of-domain; and evidential regression, despite its promise, was not the preferable alternative in any of the cases. More broadly, cost-effective, single deterministic models cannot yet consistently match or outperform ensembling for uncertainty quantification in NNIPs.
<div id='section'>Paperid: <span id='pid'>1740, <a href='https://arxiv.org/pdf/2304.01959.pdf' target='_blank'>https://arxiv.org/pdf/2304.01959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taehoon Kim, Bohyung Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01959">Randomized Adversarial Style Perturbations for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel domain generalization technique, referred to as Randomized Adversarial Style Perturbation (RASP), which is motivated by the observation that the characteristics of each domain are captured by the feature statistics corresponding to style. The proposed algorithm perturbs the style of a feature in an adversarial direction towards a randomly selected class, and makes the model learn against being misled by the unexpected styles observed in unseen target domains. While RASP is effective to handle domain shifts, its naive integration into the training procedure might degrade the capability of learning knowledge from source domains because it has no restriction on the perturbations of representations. This challenge is alleviated by Normalized Feature Mixup (NFM), which facilitates the learning of the original features while achieving robustness to perturbed representations via their mixup during training. We evaluate the proposed algorithm via extensive experiments on various benchmarks and show that our approach improves domain generalization performance, especially in large-scale benchmarks.
<div id='section'>Paperid: <span id='pid'>1741, <a href='https://arxiv.org/pdf/2303.07771.pdf' target='_blank'>https://arxiv.org/pdf/2303.07771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rao Muhammad Umer, Armin Gruber, Sayedali Shetab Boushehri, Christian Metak, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07771">Imbalanced Domain Generalization for Robust Single Cell Classification in Hematological Cytomorphology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate morphological classification of white blood cells (WBCs) is an important step in the diagnosis of leukemia, a disease in which nonfunctional blast cells accumulate in the bone marrow. Recently, deep convolutional neural networks (CNNs) have been successfully used to classify leukocytes by training them on single-cell images from a specific domain. Most CNN models assume that the distributions of the training and test data are similar, i.e., the data are independently and identically distributed. Therefore, they are not robust to different staining procedures, magnifications, resolutions, scanners, or imaging protocols, as well as variations in clinical centers or patient cohorts. In addition, domain-specific data imbalances affect the generalization performance of classifiers. Here, we train a robust CNN for WBC classification by addressing cross-domain data imbalance and domain shifts. To this end, we use two loss functions and demonstrate their effectiveness in out-of-distribution (OOD) generalization. Our approach achieves the best F1 macro score compared to other existing methods and is able to consider rare cell types. This is the first demonstration of imbalanced domain generalization in hematological cytomorphology and paves the way for robust single cell classification methods for the application in laboratories and clinics.
<div id='section'>Paperid: <span id='pid'>1742, <a href='https://arxiv.org/pdf/2302.05110.pdf' target='_blank'>https://arxiv.org/pdf/2302.05110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Spandan Dey, Md Sahidullah, Goutam Saha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05110">Cross-Corpora Spoken Language Identification with Domain Diversification and Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the cross-corpora generalization issue for the low-resourced spoken language identification (LID) problem. We have conducted the experiments in the context of Indian LID and identified strikingly poor cross-corpora generalization due to corpora-dependent non-lingual biases. Our contribution to this work is twofold. First, we propose domain diversification, which diversifies the limited training data using different audio data augmentation methods. We then propose the concept of maximally diversity-aware cascaded augmentations and optimize the augmentation fold-factor for effective diversification of the training data. Second, we introduce the idea of domain generalization considering the augmentation methods as pseudo-domains. Towards this, we investigate both domain-invariant and domain-aware approaches. Our LID system is based on the state-of-the-art emphasized channel attention, propagation, and aggregation based time delay neural network (ECAPA-TDNN) architecture. We have conducted extensive experiments with three widely used corpora for Indian LID research. In addition, we conduct a final blind evaluation of our proposed methods on the Indian subset of VoxLingua107 corpus collected in the wild. Our experiments demonstrate that the proposed domain diversification is more promising over commonly used simple augmentation methods. The study also reveals that domain generalization is a more effective solution than domain diversification. We also notice that domain-aware learning performs better for same-corpora LID, whereas domain-invariant learning is more suitable for cross-corpora generalization. Compared to basic ECAPA-TDNN, its proposed domain-invariant extensions improve the cross-corpora EER up to 5.23%. In contrast, the proposed domain-aware extensions also improve performance for same-corpora test scenarios.
<div id='section'>Paperid: <span id='pid'>1743, <a href='https://arxiv.org/pdf/2302.01497.pdf' target='_blank'>https://arxiv.org/pdf/2302.01497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byounggyu Lew, Donghyun Son, Buru Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01497">Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to build generalized models that perform well on unseen domains when only source domains are available for model optimization. Recent studies have shown that large-scale pre-trained models can enhance domain generalization by leveraging their generalization power. However, these pre-trained models lack target task-specific knowledge yet due to discrepancies between the pre-training objectives and the target task. Although the task-specific knowledge could be learned from source domains by fine-tuning, this hurts the generalization power of pre-trained models due to gradient bias toward the source domains. To alleviate this problem, we propose a new domain generalization method that estimates unobservable gradients that reduce potential risks in unseen domains using a large-scale pre-trained model. These estimated unobservable gradients allow the pre-trained model to learn task-specific knowledge further while preserving its generalization ability by relieving the gradient bias. Our experimental results show that our method outperforms baseline methods on DomainBed, a standard benchmark in domain generalization. We also provide extensive analyses to demonstrate that the pre-trained model can learn task-specific knowledge without sacrificing its generalization power.
<div id='section'>Paperid: <span id='pid'>1744, <a href='https://arxiv.org/pdf/2301.11779.pdf' target='_blank'>https://arxiv.org/pdf/2301.11779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Penghao Jiang, Ke Xin, Zifeng Wang, Chunxi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11779">Invariant Meta Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern deep learning techniques have illustrated their excellent capabilities in many areas, but relies on large training data. Optimization-based meta-learning train a model on a variety tasks, such that it can solve new learning tasks using only a small number of training samples.However, these methods assumes that training and test dataare identically and independently distributed. To overcome such limitation, in this paper, we propose invariant meta learning for out-of-distribution tasks. Specifically, invariant meta learning find invariant optimal meta-initialization,and fast adapt to out-of-distribution tasks with regularization penalty. Extensive experiments demonstrate the effectiveness of our proposed invariant meta learning on out-of-distribution few-shot tasks.
<div id='section'>Paperid: <span id='pid'>1745, <a href='https://arxiv.org/pdf/2301.05499.pdf' target='_blank'>https://arxiv.org/pdf/2301.05499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vidit Vidit, Martin Engilberge, Mathieu Salzmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05499">CLIP the Gap: A Single Domain Generalization Approach for Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Domain Generalization (SDG) tackles the problem of training a model on a single source domain so that it generalizes to any unseen target domain. While this has been well studied for image classification, the literature on SDG object detection remains almost non-existent. To address the challenges of simultaneously learning robust object localization and representation, we propose to leverage a pre-trained vision-language model to introduce semantic domain concepts via textual prompts. We achieve this via a semantic augmentation strategy acting on the features extracted by the detector backbone, as well as a text-based classification loss. Our experiments evidence the benefits of our approach, outperforming by 10% the only existing SDG object detection method, Single-DGOD [49], on their own diverse weather-driving benchmark.
<div id='section'>Paperid: <span id='pid'>1746, <a href='https://arxiv.org/pdf/2212.02573.pdf' target='_blank'>https://arxiv.org/pdf/2212.02573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Du, Jiankang Deng, Miaojing Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02573">Domain-General Crowd Counting in Unseen Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift across crowd data severely hinders crowd counting models to generalize to unseen scenarios. Although domain adaptive crowd counting approaches close this gap to a certain extent, they are still dependent on the target domain data to adapt (e.g. finetune) their models to the specific domain. In this paper, we aim to train a model based on a single source domain which can generalize well on any unseen domain. This falls into the realm of domain generalization that remains unexplored in crowd counting. We first introduce a dynamic sub-domain division scheme which divides the source domain into multiple sub-domains such that we can initiate a meta-learning framework for domain generalization. The sub-domain division is dynamically refined during the meta-learning. Next, in order to disentangle domain-invariant information from domain-specific information in image features, we design the domain-invariant and -specific crowd memory modules to re-encode image features. Two types of losses, i.e. feature reconstruction and orthogonal losses, are devised to enable this disentanglement. Extensive experiments on several standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show the strong generalizability of our method.
<div id='section'>Paperid: <span id='pid'>1747, <a href='https://arxiv.org/pdf/2212.01168.pdf' target='_blank'>https://arxiv.org/pdf/2212.01168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yeongwoo Song, Hawoong Jeong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.01168">Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. While effective, these methods are limited to the system domain, where the type of system remains consistent and thus cannot ensure the adaptation to new, or unseen physical systems governed by different laws. For instance, a neural network trained on a mass-spring system cannot guarantee accurate predictions for the behavior of a two-body system or any other system with different physical laws. In this work, we take a significant leap forward by targeting cross domain generalization within the field of Hamiltonian dynamics. We model our system with a graph neural network (GNN) and employ a meta learning algorithm to enable the model to gain experience over a distribution of systems and make it adapt to new physics. Our approach aims to learn a unified Hamiltonian representation that is generalizable across multiple system domains, thereby overcoming the limitations of system-specific models. We demonstrate that the meta-trained model captures the generalized Hamiltonian representation that is consistent across different physical domains. Overall, through the use of meta learning, we offer a framework that achieves cross domain generalization, providing a step towards a unified model for understanding a wide array of dynamical systems via deep learning.
<div id='section'>Paperid: <span id='pid'>1748, <a href='https://arxiv.org/pdf/2211.08253.pdf' target='_blank'>https://arxiv.org/pdf/2211.08253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingang Qu, Thibault Faney, Ze Wang, Patrick Gallinari, Soleiman Yousef, Jean-Charles de Hemptinne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.08253">HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to domain shifts, machine learning systems typically struggle to generalize well to new domains that differ from those of training data, which is what domain generalization (DG) aims to address. Although a variety of DG methods have been proposed, most of them fall short in interpretability and require domain labels, which are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shifts. HMOE employs hypernetworks taking vectors as input to generate the weights of experts, which promotes knowledge sharing among experts and enables the exploration of their similarities in a low-dimensional vector space. We benchmark HMOE against other DG methods under a fair evaluation framework -- DomainBed. Our extensive experiments show that HMOE can effectively separate mixed-domain data into distinct clusters that are surprisingly more consistent with human intuition than original domain labels. Using self-learned domain information, HMOE achieves state-of-the-art results on most datasets and significantly surpasses other DG methods in average accuracy across all datasets.
<div id='section'>Paperid: <span id='pid'>1749, <a href='https://arxiv.org/pdf/2210.15206.pdf' target='_blank'>https://arxiv.org/pdf/2210.15206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashvin Nair, Brian Zhu, Gokul Narayanan, Eugen Solowjow, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.15206">Learning on the Job: Self-Rewarding Offline-to-Online Finetuning for Industrial Insertion of Novel Connectors from Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based methods in robotics hold the promise of generalization, but what can be done if a learned policy does not generalize to a new situation? In principle, if an agent can at least evaluate its own success (i.e., with a reward classifier that generalizes well even when the policy does not), it could actively practice the task and finetune the policy in this situation. We study this problem in the setting of industrial insertion tasks, such as inserting connectors in sockets and setting screws. Existing algorithms rely on precise localization of the connector or socket and carefully managed physical setups, such as assembly lines, to succeed at the task. But in unstructured environments such as homes or even some industrial settings, robots cannot rely on precise localization and may be tasked with previously unseen connectors. Offline reinforcement learning on a variety of connector insertion tasks is a potential solution, but what if the robot is tasked with inserting previously unseen connector? In such a scenario, we will still need methods that can robustly solve such tasks with online practice. One of the main observations we make in this work is that, with a suitable representation learning and domain generalization approach, it can be significantly easier for the reward function to generalize to a new but structurally similar task (e.g., inserting a new type of connector) than for the policy. This means that a learned reward function can be used to facilitate the finetuning of the robot's policy in situations where the policy fails to generalize in zero shot, but the reward function generalizes successfully. We show that such an approach can be instantiated in the real world, pretrained on 50 different connectors, and successfully finetuned to new connectors via the learned reward function. Videos can be viewed at https://sites.google.com/view/learningonthejob
<div id='section'>Paperid: <span id='pid'>1750, <a href='https://arxiv.org/pdf/2210.10636.pdf' target='_blank'>https://arxiv.org/pdf/2210.10636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parikshit Bansal, Yashoteja Prabhu, Emre Kiciman, Amit Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.10636">Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a user's input text, text-matching recommender systems output relevant items by comparing the input text to available items' description, such as product-to-product recommendation on e-commerce platforms. As users' interests and item inventory are expected to change, it is important for a text-matching system to generalize to data shifts, a task known as out-of-distribution (OOD) generalization. However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization. For a product recommendation task, fine-tuning obtains worse accuracy than the base model when recommending items in a new category or for a future time period. To explain this generalization failure, we consider an intervention-based importance metric, which shows that a fine-tuned model captures spurious correlations and fails to learn the causal features that determine the relevance between any two text inputs. Moreover, standard methods for causal regularization do not apply in this setting, because unlike in images, there exist no universally spurious features in a text-matching task (the same token may be spurious or causal depending on the text it is being matched to). For OOD generalization on text inputs, therefore, we highlight a different goal: avoiding high importance scores for certain features. We do so using an intervention-based regularizer that constraints the causal effect of any token on the model's relevance score to be similar to the base model. Results on Amazon product and 3 question recommendation datasets show that our proposed regularizer improves generalization for both in-distribution and OOD evaluation, especially in difficult scenarios when the base model is not accurate.
<div id='section'>Paperid: <span id='pid'>1751, <a href='https://arxiv.org/pdf/2210.04802.pdf' target='_blank'>https://arxiv.org/pdf/2210.04802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Hajipour, Ning Yu, Cristian-Alexandru Staicu, Mario Fritz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04802">SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on four state-of-the-art pretrained models and applied to two code generation tasks, exposes multiple failure modes attributed to OOD generalization issues. Additionally, our analysis uncovers that LoRA fine-tuning consistently exhibits significantly better OOD generalization performance than full fine-tuning across various scenarios.
<div id='section'>Paperid: <span id='pid'>1752, <a href='https://arxiv.org/pdf/2208.09449.pdf' target='_blank'>https://arxiv.org/pdf/2208.09449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deepak Maurya, Adarsh Barik, Jean Honorio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.09449">A Novel Plug-and-Play Approach for Adversarially Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a robust framework that employs adversarially robust training to safeguard the ML models against perturbed testing data. Our contributions can be seen from both computational and statistical perspectives. Firstly, from a computational/optimization point of view, we derive the ready-to-use exact solution for several widely used loss functions with a variety of norm constraints on adversarial perturbation for various supervised and unsupervised ML problems, including regression, classification, two-layer neural networks, graphical models, and matrix completion. The solutions are either in closed-form, or an easily tractable optimization problem such as 1-D convex optimization, semidefinite programming, difference of convex programming or a sorting-based algorithm. Secondly, from statistical/generalization viewpoint, using some of these results, we derive novel bounds of the adversarial Rademacher complexity for various problems, which entails new generalization bounds. Thirdly, we perform some sanity-check experiments on real-world datasets for supervised problems such as regression and classification, as well as for unsupervised problems such as matrix completion and learning graphical models, with very little computational overhead.
<div id='section'>Paperid: <span id='pid'>1753, <a href='https://arxiv.org/pdf/2208.07798.pdf' target='_blank'>https://arxiv.org/pdf/2208.07798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Deng, Kui Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.07798">Counterfactual Supervision-based Information Bottleneck for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning invariant (causal) features for out-of-distribution (OOD) generalization has attracted extensive attention recently, and among the proposals invariant risk minimization (IRM) is a notable solution. In spite of its theoretical promise for linear regression, the challenges of using IRM in linear classification problems remain. By introducing the information bottleneck (IB) principle into the learning of IRM, IB-IRM approach has demonstrated its power to solve these challenges. In this paper, we further improve IB-IRM from two aspects. First, we show that the key assumption of support overlap of invariant features used in IB-IRM is strong for the guarantee of OOD generalization and it is still possible to achieve the optimal solution without this assumption. Second, we illustrate two failure modes that IB-IRM (and IRM) could fail for learning the invariant features, and to address such failures, we propose a \textit{Counterfactual Supervision-based Information Bottleneck (CSIB)} learning algorithm that provably recovers the invariant features. By requiring counterfactual inference, CSIB works even when accessing data from a single environment. Empirical experiments on several datasets verify our theoretical results.
<div id='section'>Paperid: <span id='pid'>1754, <a href='https://arxiv.org/pdf/2205.09739.pdf' target='_blank'>https://arxiv.org/pdf/2205.09739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre RamÃ©, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, Matthieu Cord
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.09739">Diverse Weight Averaging for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Standard neural networks struggle to generalize under distribution shifts in computer vision. Fortunately, combining multiple networks can consistently improve out-of-distribution generalization. In particular, weight averaging (WA) strategies were shown to perform best on the competitive DomainBed benchmark; they directly average the weights of multiple networks despite their nonlinearities. In this paper, we propose Diverse Weight Averaging (DiWA), a new WA strategy whose main motivation is to increase the functional diversity across averaged models. To this end, DiWA averages weights obtained from several independent training runs: indeed, models obtained from different runs are more diverse than those collected along a single run thanks to differences in hyperparameters and training procedures. We motivate the need for diversity by a new bias-variance-covariance-locality decomposition of the expected error, exploiting similarities between WA and standard functional ensembling. Moreover, this decomposition highlights that WA succeeds when the variance term dominates, which we show occurs when the marginal distribution changes at test time. Experimentally, DiWA consistently improves the state of the art on DomainBed without inference overhead.
<div id='section'>Paperid: <span id='pid'>1755, <a href='https://arxiv.org/pdf/2202.13216.pdf' target='_blank'>https://arxiv.org/pdf/2202.13216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramchandran Muthukumar, Jeremias Sulam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.13216">Adversarial robustness of sparse local Lipschitz predictors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work studies the adversarial robustness of parametric functions composed of a linear predictor and a non-linear representation map. % that satisfies certain stability condition. Our analysis relies on \emph{sparse local Lipschitzness} (SLL), an extension of local Lipschitz continuity that better captures the stability and reduced effective dimensionality of predictors upon local perturbations. SLL functions preserve a certain degree of structure, given by the sparsity pattern in the representation map, and include several popular hypothesis classes, such as piece-wise linear models, Lasso and its variants, and deep feed-forward \relu networks. % are sparse local Lipschitz. We provide a tighter robustness certificate on the minimal energy of an adversarial example, as well as tighter data-dependent non-uniform bounds on the robust generalization error of these predictors. We instantiate these results for the case of deep neural networks and provide numerical evidence that supports our results, shedding new insights into natural regularization strategies to increase the robustness of these models.
<div id='section'>Paperid: <span id='pid'>1756, <a href='https://arxiv.org/pdf/2201.07916.pdf' target='_blank'>https://arxiv.org/pdf/2201.07916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Drew Penney, Bin Li, Jaroslaw Sydir, Lizhong Chen, Charlie Tai, Stefan Lee, Eoin Walsh, Thomas Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.07916">PROMPT: Learning Dynamic Resource Allocation Policies for Network Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A growing number of service providers are exploring methods to improve server utilization and reduce power consumption by co-scheduling high-priority latency-critical workloads with best-effort workloads. This practice requires strict resource allocation between workloads to reduce contention and maintain Quality-of-Service (QoS) guarantees. Prior work demonstrated promising opportunities to dynamically allocate resources based on workload demand, but may fail to meet QoS objectives in more stringent operating environments due to the presence of resource allocation cliffs, transient fluctuations in workload performance, and rapidly changing resource demand. We therefore propose PROMPT, a novel resource allocation framework using proactive QoS prediction to guide a reinforcement learning controller. PROMPT enables more precise resource optimization, more consistent handling of transient behaviors, and more robust generalization when co-scheduling new best-effort workloads not encountered during policy training. Evaluation shows that the proposed method incurs 4.2x fewer QoS violations, reduces severity of QoS violations by 12.7x, improves best-effort workload performance, and improves overall power efficiency over prior work.
<div id='section'>Paperid: <span id='pid'>1757, <a href='https://arxiv.org/pdf/2111.01996.pdf' target='_blank'>https://arxiv.org/pdf/2111.01996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Sun, Mingjie Li, Zhouchen Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.01996">Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial robustness, which primarily comprises sensitivity-based robustness and spatial robustness, plays an integral part in achieving robust generalization. In this paper, we endeavor to design strategies to achieve universal adversarial robustness. To achieve this, we first investigate the relatively less-explored realm of spatial robustness. Then, we integrate the existing spatial robustness methods by incorporating both local and global spatial vulnerability into a unified spatial attack and adversarial training approach. Furthermore, we present a comprehensive relationship between natural accuracy, sensitivity-based robustness, and spatial robustness, supported by strong evidence from the perspective of robust representation. Crucially, to reconcile the interplay between the mutual impacts of various robustness components into one unified framework, we incorporate the \textit{Pareto criterion} into the adversarial robustness analysis, yielding a novel strategy called Pareto Adversarial Training for achieving universal robustness. The resulting Pareto front, which delineates the set of optimal solutions, provides an optimal balance between natural accuracy and various adversarial robustness. This sheds light on solutions for achieving universal robustness in the future. To the best of our knowledge, we are the first to consider universal adversarial robustness via multi-objective optimization.
<div id='section'>Paperid: <span id='pid'>1758, <a href='https://arxiv.org/pdf/2510.08132.pdf' target='_blank'>https://arxiv.org/pdf/2510.08132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kodai Kawamura, Yuta Goto, Rintaro Yanagi, Hirokatsu Kataoka, Go Irie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08132">Approximate Domain Unlearning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on class unlearning, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize real cars while avoiding misrecognition of illustrated cars depicted in roadside advertisements as real cars, which could be hazardous. In this paper, we introduce Approximate Domain Unlearning (ADU), a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., illustration) while preserving accuracy for other domains (e.g., real). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments show that our approach outperforms baselines built upon VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code: https://kodaikawamura.github.io/Domain_Unlearning/.
<div id='section'>Paperid: <span id='pid'>1759, <a href='https://arxiv.org/pdf/2510.04688.pdf' target='_blank'>https://arxiv.org/pdf/2510.04688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joann Ching, Gerhard Widmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04688">A Study on the Data Distribution Gap in Music Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Music Emotion Recognition (MER) is a task deeply connected to human perception, relying heavily on subjective annotations collected from contributors. Prior studies tend to focus on specific musical styles rather than incorporating a diverse range of genres, such as rock and classical, within a single framework. In this paper, we address the task of recognizing emotion from audio content by investigating five datasets with dimensional emotion annotations -- EmoMusic, DEAM, PMEmo, WTC, and WCMED -- which span various musical styles. We demonstrate the problem of out-of-distribution generalization in a systematic experiment. By closely looking at multiple data and feature sets, we provide insight into genre-emotion relationships in existing data and examine potential genre dominance and dataset biases in certain feature representations. Based on these experiments, we arrive at a simple yet effective framework that combines embeddings extracted from the Jukebox model with chroma features and demonstrate how, alongside a combination of several diverse training sets, this permits us to train models with substantially improved cross-dataset generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1760, <a href='https://arxiv.org/pdf/2510.00329.pdf' target='_blank'>https://arxiv.org/pdf/2510.00329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarmad Mehrdad, Maxime Sabbah, Vincent Bonnet, Ludovic Righetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00329">Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the application of Minimal Observation Inverse Reinforcement Learning (MO-IRL) to model and predict human arm-reaching movements with time-varying cost weights. Using a planar two-link biomechanical model and high-resolution motion-capture data from subjects performing a pointing task, we segment each trajectory into multiple phases and learn phase-specific combinations of seven candidate cost functions. MO-IRL iteratively refines cost weights by scaling observed and generated trajectories in the maximum entropy IRL formulation, greatly reducing the number of required demonstrations and convergence time compared to classical IRL approaches. Training on ten trials per posture yields average joint-angle Root Mean Squared Errors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight divisions, respectively, versus 10.4 deg using a single static weight. Cross-validation on remaining trials and, for the first time, inter-subject validation on an unseen subject's 20 trials, demonstrates comparable predictive accuracy, around 8 deg RMSE, indicating robust generalization. Learned weights emphasize joint acceleration minimization during movement onset and termination, aligning with smoothness principles observed in biological motion. These results suggest that MO-IRL can efficiently uncover dynamic, subject-independent cost structures underlying human motor control, with potential applications for humanoid robots.
<div id='section'>Paperid: <span id='pid'>1761, <a href='https://arxiv.org/pdf/2509.24980.pdf' target='_blank'>https://arxiv.org/pdf/2509.24980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuang Liang, Jing He, Chuanmeizhi Wang, Lejun Liao, Guo Zhang, Yingcong Chen, Yuan Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24980">SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained diffusion models provide rich multi-scale latent features and are emerging as powerful vision backbones. While recent works such as Marigold~\citep{ke2024repurposing} and Lotus~\citep{he2024lotus} adapt diffusion priors for dense prediction with strong cross-domain generalization, their potential for structured outputs (e.g., human pose estimation) remains underexplored. In this paper, we propose \textbf{SDPose}, a fine-tuning framework built upon Stable Diffusion to fully exploit pre-trained diffusion priors for human pose estimation. First, rather than modifying cross-attention modules or introducing learnable embeddings, we directly predict keypoint heatmaps in the SD U-Net's image latent space to preserve the original generative priors. Second, we map these latent features into keypoint heatmaps through a lightweight convolutional pose head, which avoids disrupting the pre-trained backbone. Finally, to prevent overfitting and enhance out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction branch that preserves domain-transferable generative semantics. To evaluate robustness under domain shift, we further construct \textbf{COCO-OOD}, a style-transferred variant of COCO with preserved annotations. With just one-fifth of the training schedule used by Sapiens on COCO, SDPose attains parity with Sapiens-1B/2B on the COCO validation set and establishes a new state of the art on the cross-domain benchmarks HumanArt and COCO-OOD. Furthermore, we showcase SDPose as a zero-shot pose annotator for downstream controllable generation tasks, including ControlNet-based image synthesis and video generation, where it delivers qualitatively superior pose guidance.
<div id='section'>Paperid: <span id='pid'>1762, <a href='https://arxiv.org/pdf/2509.21234.pdf' target='_blank'>https://arxiv.org/pdf/2509.21234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abi Aryan, Zac Liu, Aaron Childress
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21234">AbideGym: Turning Static RL Worlds into Adaptive Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agents trained with reinforcement learning often develop brittle policies that fail when dynamics shift, a problem amplified by static benchmarks. AbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and scalable complexity to enforce intra-episode adaptation. By exposing weaknesses in static policies and promoting resilience, AbideGym provides a modular, reproducible evaluation framework for advancing research in curriculum learning, continual learning, and robust generalization.
<div id='section'>Paperid: <span id='pid'>1763, <a href='https://arxiv.org/pdf/2509.19233.pdf' target='_blank'>https://arxiv.org/pdf/2509.19233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Leyli-abadi, Antoine Marot, Jérôme Picault
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19233">Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of the energy transition, with increasing integration of renewable sources and cross-border electricity exchanges, power grids are encountering greater uncertainty and operational risk. Maintaining grid stability under varying conditions is a complex task, and power flow simulators are commonly used to support operators by evaluating potential actions before implementation. However, traditional physical solvers, while accurate, are often too slow for near real-time use. Machine learning models have emerged as fast surrogates, and to improve their adherence to physical laws (e.g., Kirchhoff's laws), they are often trained with embedded constraints which are also known as physics-informed or hybrid models. This paper presents an ablation study to demystify hybridization strategies, ranging from incorporating physical constraints as regularization terms or unsupervised losses, and exploring model architectures from simple multilayer perceptrons to advanced graph-based networks enabling the direct optimization of physics equations. Using our custom benchmarking pipeline for hybrid models called LIPS, we evaluate these models across four dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization. The results highlight how integrating physical knowledge impacts performance across these criteria. All the implementations are reproducible and provided in the corresponding Github page.
<div id='section'>Paperid: <span id='pid'>1764, <a href='https://arxiv.org/pdf/2509.17845.pdf' target='_blank'>https://arxiv.org/pdf/2509.17845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhang, Siming Sun, Zhengyu Fan, Qinmin Yang, Xuejun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17845">Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series analysis faces significant challenges in handling variable-length data and achieving robust generalization. While Transformer-based models have advanced time series tasks, they often struggle with feature redundancy and limited generalization capabilities. Drawing inspiration from classical CNN architectures' pyramidal structure, we propose a Multi-Scale Representation Learning Framework based on a Conv-like ScaleFusion Transformer. Our approach introduces a temporal convolution-like structure that combines patching operations with multi-head attention, enabling progressive temporal dimension compression and feature channel expansion. We further develop a novel cross-scale attention mechanism for effective feature fusion across different temporal scales, along with a log-space normalization method for variable-length sequences. Extensive experiments demonstrate that our framework achieves superior feature independence, reduced redundancy, and better performance in forecasting and classification tasks compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1765, <a href='https://arxiv.org/pdf/2509.16935.pdf' target='_blank'>https://arxiv.org/pdf/2509.16935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lavish Ramchandani, Gunjan Deotale, Dev Kumar Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16935">Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated with tumor aggressiveness and poor prognosis. Their detection remains a significant challenge due to subtle morphological cues, class imbalance, and inter-observer variability among pathologists. The MIDOG 2025 challenge introduced a dedicated track for atypical mitosis classification, enabling systematic evaluation of deep learning methods. In this study, we investigated the use of large vision foundation models, including Virchow, Virchow2, and UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We conducted extensive experiments with different LoRA ranks, as well as random and group-based data splits, to analyze robustness under varied conditions. Our best approach, Virchow with LoRA rank 8 and ensemble of three-fold cross-validation, achieved a balanced accuracy of 88.37% on the preliminary test set, ranking joint 9th in the challenge leaderboard. These results highlight the promise of foundation models with efficient adaptation strategies for the classification of atypical mitosis, while underscoring the need for improvements in specificity and domain generalization.
<div id='section'>Paperid: <span id='pid'>1766, <a href='https://arxiv.org/pdf/2509.16897.pdf' target='_blank'>https://arxiv.org/pdf/2509.16897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuewan He, Jielei Wang, Zihan Cheng, Yuchen Su, Shiyue Huang, Guoming Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16897">PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access to the real in-distribution (ID) data. While existing methods perform well on small-scale images, they suffer from mode collapse when synthesizing large-scale images, resulting in limited knowledge transfer. Recently, leveraging advanced generative models to synthesize photorealistic images has emerged as a promising alternative. Nevertheless, directly using off-the-shelf diffusion to generate datasets faces the precision-recall challenges: 1) ensuring synthetic data aligns with the real distribution, and 2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a precision-recall informed synthesis method. Specifically, we introduce Energy-guided Distribution Alignment to avoid the generation of out-of-distribution samples, and design the Diversified Prompt Engineering to enhance coverage of the real ID manifold. Extensive experiments on various large-scale image datasets demonstrate the superiority of PRISM. Moreover, we demonstrate that models trained with PRISM exhibit strong domain generalization.
<div id='section'>Paperid: <span id='pid'>1767, <a href='https://arxiv.org/pdf/2509.16531.pdf' target='_blank'>https://arxiv.org/pdf/2509.16531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junghwan Kim, Haotian Zhang, David Jurgens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16531">Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Authorship representation (AR) learning, which models an author's unique writing style, has demonstrated strong performance in authorship attribution tasks. However, prior research has primarily focused on monolingual settings-mostly in English-leaving the potential benefits of multilingual AR models underexplored. We introduce a novel method for multilingual AR learning that incorporates two key innovations: probabilistic content masking, which encourages the model to focus on stylistically indicative words rather than content-specific words, and language-aware batching, which improves contrastive learning by reducing cross-lingual interference. Our model is trained on over 4.5 million authors across 36 languages and 13 domains. It consistently outperforms monolingual baselines in 21 out of 22 non-English languages, achieving an average Recall@8 improvement of 4.85%, with a maximum gain of 15.91% in a single language. Furthermore, it exhibits stronger cross-lingual and cross-domain generalization compared to a monolingual model trained solely on English. Our analysis confirms the effectiveness of both proposed techniques, highlighting their critical roles in the model's improved performance.
<div id='section'>Paperid: <span id='pid'>1768, <a href='https://arxiv.org/pdf/2509.06464.pdf' target='_blank'>https://arxiv.org/pdf/2509.06464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erez Posner, Ore Shtalrid, Oded Erell, Daniel Noy, Moshe Bouhnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06464">A Statistical 3D Stomach Shape Model for Anatomical Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic and parameterized 3D models of human anatomy have become invaluable in research, diagnostics, and surgical planning. However, the development of detailed models for internal organs, such as the stomach, has been limited by data availability and methodological challenges. In this paper, we propose a novel pipeline for the generation of synthetic 3D stomach models, enabling the creation of anatomically diverse morphologies informed by established studies on stomach shape variability. Using this pipeline, we construct a dataset of synthetic stomachs. Building on this dataset, we develop a 3D statistical shape model of the stomach, trained to capture natural anatomical variability in a low-dimensional shape space. The model is further refined using CT meshes derived from publicly available datasets through a semi-supervised alignment process, enhancing its ability to generalize to unseen anatomical variations. We evaluated the model on a held-out test set of real stomach CT scans, demonstrating robust generalization and fit accuracy. We make the statistical shape model along with the synthetic dataset publicly available on GitLab: https://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research. This work introduces the first statistical 3D shape model of the stomach, with applications ranging from surgical simulation and pre-operative planning to medical education and computational modeling. By combining synthetic data generation, parametric modeling, and real-world validation, our approach represents a significant advancement in organ modeling and opens new possibilities for personalized healthcare solutions.
<div id='section'>Paperid: <span id='pid'>1769, <a href='https://arxiv.org/pdf/2509.06011.pdf' target='_blank'>https://arxiv.org/pdf/2509.06011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhai Weng, Xinjie Li, Can Wu, Weijie He, Jianfeng Lv, Dong Zhou, Zhongliang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06011">Light-Weight Cross-Modal Enhancement Method with Benchmark Construction for UAV-based Open-Vocabulary Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Object Detection (OVD) faces severe performance degradation when applied to UAV imagery due to the domain gap from ground-level datasets. To address this challenge, we propose a complete UAV-oriented solution that combines both dataset construction and model innovation. First, we design a refined UAV-Label Engine, which efficiently resolves annotation redundancy, inconsistency, and ambiguity, enabling the generation of largescale UAV datasets. Based on this engine, we construct two new benchmarks: UAVDE-2M, with over 2.4M instances across 1,800+ categories, and UAVCAP-15K, providing rich image-text pairs for vision-language pretraining. Second, we introduce the Cross-Attention Gated Enhancement (CAGE) module, a lightweight dual-path fusion design that integrates cross-attention, adaptive gating, and global FiLM modulation for robust textvision alignment. By embedding CAGE into the YOLO-World-v2 framework, our method achieves significant gains in both accuracy and efficiency, notably improving zero-shot detection on VisDrone by +5.3 mAP while reducing parameters and GFLOPs, and demonstrating strong cross-domain generalization on SIMD. Extensive experiments and real-world UAV deployment confirm the effectiveness and practicality of our proposed solution for UAV-based OVD
<div id='section'>Paperid: <span id='pid'>1770, <a href='https://arxiv.org/pdf/2509.03614.pdf' target='_blank'>https://arxiv.org/pdf/2509.03614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungho Choe, Xiaoli Qin, Abubakr Shafique, Amanda Dy, Susan Done, Dimitrios Androutsos, April Khademi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03614">Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Counting mitotic figures is time-intensive for pathologists and leads to inter-observer variability. Artificial intelligence (AI) promises a solution by automatically detecting mitotic figures while maintaining decision consistency. However, AI tools are susceptible to domain shift, where a significant drop in performance can occur due to differences in the training and testing sets, including morphological diversity between organs, species, and variations in staining protocols. Furthermore, the number of mitoses is much less than the count of normal nuclei, which introduces severely imbalanced data for the detection task. In this work, we formulate mitosis detection as a pixel-level segmentation and propose a teacher-student model that simultaneously addresses mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our method is based on a UNet segmentation backbone that integrates domain generalization modules, namely contrastive representation learning and domain-adversarial training. A teacher-student strategy is employed to generate pixel-level pseudo-masks not only for annotated mitoses and hard negatives but also for normal nuclei, thereby enhancing feature discrimination and improving robustness against domain shift. For the classification task, we introduce a multi-scale CNN classifier that leverages feature maps from the segmentation model within a multi-task learning paradigm. On the preliminary test set, the algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of 0.8414 in Track 2, demonstrating the effectiveness of integrating segmentation-based detection and classification into a unified framework for robust mitosis analysis.
<div id='section'>Paperid: <span id='pid'>1771, <a href='https://arxiv.org/pdf/2509.02601.pdf' target='_blank'>https://arxiv.org/pdf/2509.02601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Giedziun, Jan Sołtysik, Mateusz Górczany, Norbert Ropiak, Marcin Przymus, Piotr Krajewski, Jarosław Kwiecień, Artur Bartczak, Izabela Wasiak, Mateusz Maniewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02601">Foundation Model-Driven Classification of Atypical Mitotic Figures with Domain-Aware Training Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a solution for the MIDOG 2025 Challenge Track~2, addressing binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs). The approach leverages pathology-specific foundation model H-optimus-0, selected based on recent cross-domain generalization benchmarks and our empirical testing, with Low-Rank Adaptation (LoRA) fine-tuning and MixUp augmentation. Implementation includes soft labels based on multi-expert consensus, hard negative mining, and adaptive focal loss, metric learning and domain adaptation. The method demonstrates both the promise and challenges of applying foundation models to this complex classification task, achieving reasonable performance in the preliminary evaluation phase.
<div id='section'>Paperid: <span id='pid'>1772, <a href='https://arxiv.org/pdf/2509.02585.pdf' target='_blank'>https://arxiv.org/pdf/2509.02585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoyan Shen, Esther Bär, Maria Hawkins, Konstantin Bräutigam, Charles-Antoine Collins-Fekete
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02585">Pan-Cancer mitotic figures detection and domain generalization: MIDOG 2025 Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report details our submission to the Mitotic Domain Generalization (MIDOG) 2025 challenge, which addresses the critical task of mitotic figure detection in histopathology for cancer prognostication. Following the "Bitter Lesson"\cite{sutton2019bitterlesson} principle that emphasizes data scale over algorithmic novelty, we have publicly released two new datasets to bolster training data for both conventional \cite{Shen2024framework} and atypical mitoses \cite{shen_2025_16780587}. Besides, we implement up-to-date training methodologies for both track and reach a Track-1 F1-Score of 0.8407 on our test set, as well as a Track-2 balanced accuracy of 0.9107 for atypical mitotic cell classification.
<div id='section'>Paperid: <span id='pid'>1773, <a href='https://arxiv.org/pdf/2509.01299.pdf' target='_blank'>https://arxiv.org/pdf/2509.01299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Ni, Qingshan Liu, Xiaonan Niu, Danfeng Hong, Lingli Zhao, Haiyan Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01299">Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations over Time Intervals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain few-shot segmentation (CD-FSS) not only enables the segmentation of unseen categories with very limited samples, but also improves cross-domain generalization ability within the few-shot segmentation framework. Currently, existing CD-FSS studies typically design multiple independent modules to enhance the cross-domain generalization ability of feature representations. However, the independence among these modules hinders the effective flow of knowledge, making it difficult to fully leverage their collective potential. In contrast, this paper proposes an all-in-one module based on ordinary differential equations and Fourier transform, resulting in a structurally concise method--Few-Shot Segmentation over Time Intervals (FSS-TIs). FSS-TIs assumes the existence of an ODE relationship between the spectra (including amplitude and phase spectra) of domain-specific features and domain-agnostic features. This ODE formulation yields an iterative transformation process along a sequence of time intervals, while simultaneously applying affine transformations with randomized perturbations to the spectra. In doing so, the exploration of domain-agnostic feature representation spaces and the simulation of diverse potential target-domain distributions are reformulated as an optimization process over the intrinsic parameters of the ODE. Moreover, we strictly constrain the support-sample selection during target-domain fine-tuning so that it is consistent with the requirements of real-world few-shot segmentation tasks. For evaluation, we introduce five datasets from substantially different domains and define two sets of cross-domain few-shot segmentation tasks to comprehensively analyze the performance of FSS-TIs. Experimental results demonstrate the superiority of FSS-TIs over existing CD-FSS methods, and in-depth ablation studies further validate the cross-domain adaptability of FSS-TIs.
<div id='section'>Paperid: <span id='pid'>1774, <a href='https://arxiv.org/pdf/2508.21730.pdf' target='_blank'>https://arxiv.org/pdf/2508.21730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrizio Fagiolo, Nicolo' Vescera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21730">Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we present a variational algorithm for the Traveling Salesman Problem (TSP) that combines (i) a compact encoding of permutations, which reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy: where the circuit topology (``Ansatz'') is first optimized on a training instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel instances, limited to a rapid re-optimization of only the circuit parameters. This pipeline eliminates costly structural research in testing, making the procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$ cities, the resulting Ansatz achieves an average optimal trip sampling probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for 6 city cases. With 7 cities the success rate drops markedly to an average of $\sim 20\%$, revealing the onset of scalability limitations of the proposed method.
  The results show robust generalization ability for moderate problem sizes and indicate how freezing the Ansatz can dramatically reduce time-to-solution without degrading solution quality. The paper also discusses scalability limitations, the impact of ``warm-start'' initialization of parameters, and prospects for extension to more complex problems, such as Vehicle Routing and Job-Shop Scheduling.
<div id='section'>Paperid: <span id='pid'>1775, <a href='https://arxiv.org/pdf/2508.21730.pdf' target='_blank'>https://arxiv.org/pdf/2508.21730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrizio Fagiolo, Nicolo' Vescera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21730">Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we present a variational algorithm for the Traveling Salesman Problem (TSP) that combines (i) a compact encoding of permutations, which reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy: where the circuit topology (``Ansatz'') is first optimized on a training instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel instances, limited to a rapid re-optimization of only the circuit parameters. This pipeline eliminates costly structural research in testing, making the procedure immediately implementable on NISQ hardware. On a set of $40$ randomly generated symmetric instances that span $4 - 7$ cities, the resulting Ansatz achieves an average optimal trip sampling probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for 6 city cases. With 7 cities the success rate drops markedly to an average of $\sim 20\%$, revealing the onset of scalability limitations of the proposed method. The results show robust generalization ability for moderate problem sizes and indicate how freezing the Ansatz can dramatically reduce time-to-solution without degrading solution quality. The paper also discusses scalability limitations, the impact of ``warm-start'' initialization of parameters, and prospects for extension to more complex problems, such as Vehicle Routing and Job-Shop Scheduling.
<div id='section'>Paperid: <span id='pid'>1776, <a href='https://arxiv.org/pdf/2508.21035.pdf' target='_blank'>https://arxiv.org/pdf/2508.21035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gennaro Percannella, Mattia Sarno, Francesco Tortorella, Mario Vento
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21035">A multi-task neural network for atypical mitosis recognition under domain shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing atypical mitotic figures in histopathology images allows physicians to correctly assess tumor aggressiveness. Although machine learning models could be exploited for automatically performing such a task, under domain shift these models suffer from significative performance drops. In this work, an approach based on multi-task learning is proposed for addressing this problem. By exploiting auxiliary tasks, correlated to the main classification task, the proposed approach, submitted to the track 2 of the MItosis DOmain Generalization (MIDOG) challenge, aims to aid the model to focus only on the object to classify, ignoring the domain varying background of the image. The proposed approach shows promising performance in a preliminary evaluation conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25 challenge.
<div id='section'>Paperid: <span id='pid'>1777, <a href='https://arxiv.org/pdf/2508.21033.pdf' target='_blank'>https://arxiv.org/pdf/2508.21033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gennaro Percannella, Mattia Sarno, Francesco Tortorella, Mario Vento
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21033">Mitosis detection in domain shift scenarios: a Mamba-based approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitosis detection in histopathology images plays a key role in tumor assessment. Although machine learning algorithms could be exploited for aiding physicians in accurately performing such a task, these algorithms suffer from significative performance drop when evaluated on images coming from domains that are different from the training ones. In this work, we propose a Mamba-based approach for mitosis detection under domain shift, inspired by the promising performance demonstrated by Mamba in medical imaging segmentation tasks. Specifically, our approach exploits a VM-UNet architecture for carrying out the addressed task, as well as stain augmentation operations for further improving model robustness against domain shift. Our approach has been submitted to the track 1 of the MItosis DOmain Generalization (MIDOG) challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show large room for improvement for the proposed method.
<div id='section'>Paperid: <span id='pid'>1778, <a href='https://arxiv.org/pdf/2508.20294.pdf' target='_blank'>https://arxiv.org/pdf/2508.20294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank RÃ¶der, Jan Benad, Manfred Eppe, Pradeep Kr. Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20294">Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI's latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.
<div id='section'>Paperid: <span id='pid'>1779, <a href='https://arxiv.org/pdf/2508.19604.pdf' target='_blank'>https://arxiv.org/pdf/2508.19604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhe Fan, Chaoyu Liu, Zhonghua Qiao, Xiaoqin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19604">IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalized Semantic Segmentation (DGSS) focuses on training a model using labeled data from a source domain, with the goal of achieving robust generalization to unseen target domains during inference. A common approach to improve generalization is to augment the source domain with synthetic data generated by diffusion models (DMs). However, the generated images often contain structural or semantic defects due to training imperfections. Training segmentation models with such flawed data can lead to performance degradation and error accumulation. To address this issue, we propose to integrate inverse evolution layers (IELs) into the generative process. IELs are designed to highlight spatial discontinuities and semantic inconsistencies using Laplacian-based priors, enabling more effective filtering of undesirable generative patterns. Based on this mechanism, we introduce IELDM, an enhanced diffusion-based data augmentation framework that can produce higher-quality images. Furthermore, we observe that the defect-suppression capability of IELs can also benefit the segmentation network by suppressing artifact propagation. Based on this insight, we embed IELs into the decoder of the DGSS model and propose IELFormer to strengthen generalization capability in cross-domain scenarios. To further strengthen the model's semantic consistency across scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module, which performs frequency-domain analysis to achieve structured integration of multi-resolution features, thereby improving cross-scale coherence. Extensive experiments on benchmark datasets demonstrate that our approach achieves superior generalization performance compared to existing methods.
<div id='section'>Paperid: <span id='pid'>1780, <a href='https://arxiv.org/pdf/2508.18749.pdf' target='_blank'>https://arxiv.org/pdf/2508.18749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunlong Wu, Zhibo Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18749">Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in prompt optimization, exemplified by methods such as TextGrad, enable automatic, gradient-like refinement of textual prompts to enhance the performance of large language models (LLMs) on specific downstream tasks. However, current approaches are typically stateless and operate independently across optimization runs, lacking mechanisms to preserve and leverage historical optimization experience. Furthermore, they are susceptible to overfitting, often yielding prompt updates that generalize poorly beyond the immediate task context.
  To address these limitations, we propose Reflection-Enhanced Meta-Optimization (REMO), a novel framework that integrates (1) a memory-augmented Reflection Retrieval-Augmented Generation (RAG) module - structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer, implemented via an LLM-driven meta-controller that synthesizes epoch-level reflective insights to iteratively improve system-level prompting strategies. This architecture enables not only local, fine-grained prompt tuning akin to TextGrad, but also the systematic accumulation and reuse of cross-run optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode - without explicit chain-of-thought prompting - and evaluate its efficacy on the GSM8K benchmark for mathematical reasoning. Experimental results demonstrate that, compared to a TextGrad baseline, REMO achieves more stable and robust generalization, albeit at the cost of increased computational overhead. We provide a detailed exposition of the algorithmic design, conduct a qualitative and quantitative analysis of optimization dynamics, and present a comprehensive ablation study to elucidate the contributions of each component.
<div id='section'>Paperid: <span id='pid'>1781, <a href='https://arxiv.org/pdf/2508.18726.pdf' target='_blank'>https://arxiv.org/pdf/2508.18726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiroaki Aizawa, Yoshikazu Hayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18726">Flatness-aware Curriculum Learning via Adversarial Difficulty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks trained by empirical risk minimization often suffer from overfitting, especially to specific samples or domains, which leads to poor generalization. Curriculum Learning (CL) addresses this issue by selecting training samples based on the difficulty. From the optimization perspective, methods such as Sharpness-Aware Minimization (SAM) improve robustness and generalization by seeking flat minima. However, combining CL with SAM is not straightforward. In flat regions, both the loss values and the gradient norms tend to become uniformly small, which makes it difficult to evaluate sample difficulty and design an effective curriculum. To overcome this problem, we propose the Adversarial Difficulty Measure (ADM), which quantifies adversarial vulnerability by leveraging the robustness properties of models trained toward flat minima. Unlike loss- or gradient-based measures, which become ineffective as training progresses into flatter regions, ADM remains informative by measuring the normalized loss gap between original and adversarial examples. We incorporate ADM into CL-based training with SAM to dynamically assess sample difficulty. We evaluated our approach on image classification tasks, fine-grained recognition, and domain generalization. The results demonstrate that our method preserves the strengths of both CL and SAM while outperforming existing curriculum-based and flatness-aware training strategies.
<div id='section'>Paperid: <span id='pid'>1782, <a href='https://arxiv.org/pdf/2508.18612.pdf' target='_blank'>https://arxiv.org/pdf/2508.18612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumen Ghosh, Christine Jestin Hannan, Rajat Vashistha, Parveen Kundu, Sandra Brosda, Lauren G. Aoude, James Lonie, Andrew Nathanson, Jessica Ng, Andrew P. Barbour, Viktor Vegh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18612">Stress-testing cross-cancer generalizability of 3D nnU-Net for PET-CT tumor segmentation: multi-cohort evaluation with novel oesophageal and lung cancer datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust generalization is essential for deploying deep learning based tumor segmentation in clinical PET-CT workflows, where anatomical sites, scanners, and patient populations vary widely. This study presents the first cross cancer evaluation of nnU-Net on PET-CT, introducing two novel, expert-annotated whole-body datasets. 279 patients with oesophageal cancer (Australian cohort) and 54 with lung cancer (Indian cohort). These cohorts complement the public AutoPET dataset and enable systematic stress-testing of cross domain performance. We trained and tested 3D nnUNet models under three paradigms. Target only (oesophageal), public only (AutoPET), and combined training. For the tested sets, the oesophageal only model achieved the best in-domain accuracy (mean DSC, 57.8) but failed on external Indian lung cohort (mean DSC less than 3.4), indicating severe overfitting. The public only model generalized more broadly (mean DSC, 63.5 on AutoPET, 51.6 on Indian lung cohort) but underperformed in oesophageal Australian cohort (mean DSC, 26.7). The combined approach provided the most balanced results (mean DSC, lung (52.9), oesophageal (40.7), AutoPET (60.9)), reducing boundary errors and improving robustness across all cohorts. These findings demonstrate that dataset diversity, particularly multi demographic, multi center and multi cancer integration, outweighs architectural novelty as the key driver of robust generalization. This work presents the demography based cross cancer deep learning segmentation evaluation and highlights dataset diversity, rather than model complexity, as the foundation for clinically robust segmentation.
<div id='section'>Paperid: <span id='pid'>1783, <a href='https://arxiv.org/pdf/2508.17102.pdf' target='_blank'>https://arxiv.org/pdf/2508.17102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjie Jiang, Yunqi Zhou, Jiafeng Yan, Jing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17102">GRASP: Geospatial pixel Reasoning viA Structured Policy learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geospatial pixel reasoning is a nascent remote-sensing task that aims to generate segmentation masks directly from natural-language instructions. Prevailing MLLM-based systems co-train a language model and a mask decoder with dense pixel supervision, which is expensive and often weak on out-of-domain (OOD) data. We introduce GRASP, a structured policy-learning framework. In our design, a multimodal large language model first emits task-relevant bounding boxes and positive points from a vision-language instruction. These outputs are then passed to a pre-trained segmentation model, which consumes them as prompts to generate the final mask. Instead of supervised fine-tuning, we optimize the system purely with reinforcement learning: the model is trained solely with GRPO, guided by format rewards and accuracy rewards computed on boxes and points (no mask supervision). This leverages strong priors in foundation models, minimizes trainable parameters, and enables learning from inexpensive annotations. We additionally curate GRASP-1k, which contains reasoning-intensive queries, detailed reasoning traces, and fine-grained segmentation annotations. Evaluations on both in-domain and out-of-domain test sets show state-of-the-art results: about 4% improvement in-domain and up to 54% on OOD benchmarks. The experiment results evidence our model's robust generalization and demonstrate that complex geospatial segmentation behaviors can be learned via RL from weak spatial cues. Code and the dataset will be released open-source.
<div id='section'>Paperid: <span id='pid'>1784, <a href='https://arxiv.org/pdf/2508.16012.pdf' target='_blank'>https://arxiv.org/pdf/2508.16012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Circe Hsu, Claire Schlesinger, Karan Mudaliar, Jordan Leung, Robin Walters, Peter Schindler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16012">FIRE-GNN: Force-informed, Relaxed Equivariance Graph Neural Network for Rapid and Accurate Prediction of Surface Properties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The work function and cleavage energy of a surface are critical properties that determine the viability of materials in electronic emission applications, semiconductor devices, and heterogeneous catalysis. While first principles calculations are accurate in predicting these properties, their computational expense combined with the vast search space of surfaces make a comprehensive screening approach with density functional theory (DFT) infeasible. Here, we introduce FIRE-GNN (Force-Informed, Relaxed Equivariance Graph Neural Network), which integrates surface-normal symmetry breaking and machine learning interatomic potential (MLIP)-derived force information, achieving a twofold reduction in mean absolute error (down to 0.065 eV) over the previous state-of-the-art for work function prediction. We additionally benchmark recent invariant and equivariant architectures, analyze the impact of symmetry breaking, and evaluate out-of-distribution generalization, demonstrating that FIRE-GNN consistently outperforms competing models for work function predictions. This model enables accurate and rapid predictions of the work function and cleavage energy across a vast chemical space and facilitates the discovery of materials with tuned surface properties
<div id='section'>Paperid: <span id='pid'>1785, <a href='https://arxiv.org/pdf/2508.14094.pdf' target='_blank'>https://arxiv.org/pdf/2508.14094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Pikus, Pratyush Ranjan Tiwari, Burton Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14094">Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collecting high-quality training examples for language model fine-tuning is expensive, with practical budgets limiting the amount of data that can be procured. We investigate whether example difficulty affects GRPO training effectiveness by comparing selection strategies (easy, medium, hard, random) across multiple models and reasoning tasks. Training on the hardest 10\% of examples (those where the base model fails most often) yields dramatic performance gains up to 47\%, while easy examples produce minimal improvements of 3-15\%. This occurs because GRPO requires outcome variance to generate learning signals; hard examples maintain mixed success/failure outcomes throughout training while easy examples quickly converge to consistent success, eliminating learning opportunities. Moreover, models trained on hard examples show superior out-of-distribution generalization, with only hard-trained models achieving meaningful gains on the AIME2025 benchmark. Our findings provide clear guidance: when budget-constrained, prioritize collecting and annotating examples where your base model struggles, as these drive nearly all learning value in GRPO fine-tuning
<div id='section'>Paperid: <span id='pid'>1786, <a href='https://arxiv.org/pdf/2508.10351.pdf' target='_blank'>https://arxiv.org/pdf/2508.10351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhentai Zhang, Danyi Weng, Guibin Zhang, Xiang Chen, Kaixing Long, Jian Geng, Yanmeng Lu, Lei Zhang, Zhitao Zhou, Lei Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10351">Glo-UMF: A Unified Multi-model Framework for Automated Morphometry of Glomerular Ultrastructural Characterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background and Objective: To address the inability of single-model architectures to perform simultaneous analysis of complex glomerular ultrastructures, we developed Glo-UMF, a unified multi-model framework integrating segmentation, classification, and detection to systematically quantify key ultrastructural features. Methods: Glo-UMF decouples quantification tasks by constructing three dedicated deep models: an ultrastructure segmentation model, a glomerular filtration barrier (GFB) region classification model, and an electron-dense deposits (EDD) detection model. Their outputs are integrated through a post-processing workflow with adaptive GFB cropping and measurement location screening, enhancing measurement reliability and providing comprehensive quantitative results that overcome the limitations of traditional grading. Results: Trained on 372 electron microscopy images, Glo-UMF enables simultaneous quantification of glomerular basement membrane (GBM) thickness, the degree of foot process effacement (FPE), and EDD location. In 115 test cases spanning 9 renal pathological types, the automated quantification results showed strong agreement with pathological reports, with an average processing time of 4.23$\pm$0.48 seconds per case on a CPU environment. Conclusions: The modular design of Glo-UMF allows for flexible extensibility, supporting the joint quantification of multiple features. This framework ensures robust generalization and clinical applicability, demonstrating significant potential as an efficient auxiliary tool in glomerular pathological analysis.
<div id='section'>Paperid: <span id='pid'>1787, <a href='https://arxiv.org/pdf/2508.04316.pdf' target='_blank'>https://arxiv.org/pdf/2508.04316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Gui, Hongliang Ren, Shang Shi, Jin Lu, Changqiu Yu, Quanjun Cao, Guomin Gu, Qi Xuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04316">A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distributed Acoustic Sensing (DAS) technology finds growing applications across various domains. However, data distribution disparities due to heterogeneous sensing environments pose challenges for data-driven artificial intelligence (AI) models, limiting cross-domain generalization and facing a shortage of labeled training data. To address these issues, this study proposes a foundational model for DAS signal recognition based on a Masked Autoencoder, named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples, encompassing DAS gait spatiotemporal signals, 2D GASF images for perimeter security, 2D time-frequency images for pipeline leakage, and open-dataset signals including whale vocalizations and seismic activities, using a self-supervised mask reconstruction task to capture deep semantic features of DAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognition tasks. This method freezes the pretrained backbone parameters and fine-tunes only a small set of learnable visual prompt vectors inserted into the Transformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Super platform validate MAEPD using indoor gait recognition as a downstream task. The VPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322% of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT) method by 0.61% and reducing training time by 45%. The model also exhibits robust performance in pipeline leakage detection, confirming the generality, efficiency, and scalability of MAEPD as a foundational model. This approach offers a novel paradigm for addressing the limited generalization of signal recognition models in the DAS domain.
<div id='section'>Paperid: <span id='pid'>1788, <a href='https://arxiv.org/pdf/2508.03190.pdf' target='_blank'>https://arxiv.org/pdf/2508.03190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bronya Roni Chernyak, Yael Segal, Yosi Shrem, Joseph Keshet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03190">PatchDSU: Uncertainty Modeling for Out of Distribution Generalization in Keyword Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models excel at many tasks but rely on the assumption that training and test data follow the same distribution. This assumption often does not hold in real-world speech systems, where distribution shifts are common due to varying environments, recording conditions, and speaker diversity.
  The method of Domain Shifts with Uncertainty (DSU) augments the input of each neural network layer based on the input feature statistics. It addresses the problem of out-of-domain generalization by assuming feature statistics follow a multivariate Gaussian distribution and substitutes the input with sampled features from this distribution. While effective for computer vision, applying DSU to speech presents challenges due to the nature of the data. Unlike static visual data, speech is a temporal signal commonly represented by a spectrogram - the change of frequency over time. This representation cannot be treated as a simple image, and the resulting sparsity can lead to skewed feature statistics when applied to the entire input.
  To tackle out-of-distribution issues in keyword spotting, we propose PatchDSU, which extends DSU by splitting the input into patches and independently augmenting each patch. We evaluated PatchDSU and DSU alongside other methods on the Google Speech Commands, Librispeech, and TED-LIUM. Additionally, we evaluated performance under white Gaussian and MUSAN music noise conditions. We also explored out-of-domain generalization by analyzing model performance on datasets they were not trained on. Overall, in most cases, both PatchDSU and DSU outperform other methods. Notably, PatchDSU demonstrates more consistent improvements across the evaluated scenarios compared to other approaches.
<div id='section'>Paperid: <span id='pid'>1789, <a href='https://arxiv.org/pdf/2508.02995.pdf' target='_blank'>https://arxiv.org/pdf/2508.02995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Zhang Xinyu, Timothy Putra Prasetio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02995">VCNet: Recreating High-Level Visual Cortex Principles for Robust Artificial Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their success in image classification, modern convolutional neural networks (CNNs) exhibit fundamental limitations, including data inefficiency, poor out-of-distribution generalization, and vulnerability to adversarial perturbations. The primate visual system, in contrast, demonstrates superior efficiency and robustness, suggesting that its architectural principles may offer a blueprint for more capable artificial vision systems. This paper introduces Visual Cortex Network (VCNet), a novel neural network architecture whose design is informed by the macro-scale organization of the primate visual cortex. VCNet emulates key biological mechanisms, including hierarchical processing across distinct cortical areas, dual-stream information segregation, and top-down predictive feedback. We evaluate VCNet on two specialized benchmarks: the Spots-10 animal pattern dataset and a light field image classification task. Our results show that VCNet achieves a classification accuracy of 92.1\% on Spots-10 and 74.4\% on the light field dataset, surpassing contemporary models of comparable size. This work demonstrates that integrating neuroscientific principles into network design can lead to more efficient and robust models, providing a promising direction for addressing long-standing challenges in machine learning.
<div id='section'>Paperid: <span id='pid'>1790, <a href='https://arxiv.org/pdf/2508.02995.pdf' target='_blank'>https://arxiv.org/pdf/2508.02995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Zhang Xinyu, Timothy Putra Prasetio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02995">The Geometry of Cortical Computation: Manifold Disentanglement and Predictive Dynamics in VCNet</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their success, modern convolutional neural networks (CNNs) exhibit fundamental limitations, including data inefficiency, poor out-of-distribution generalization, and vulnerability to adversarial perturbations. These shortcomings can be traced to a lack of inductive biases that reflect the inherent geometric structure of the visual world. The primate visual system, in contrast, demonstrates superior efficiency and robustness, suggesting that its architectural and computational principles,which evolved to internalize these structures,may offer a blueprint for more capable artificial vision. This paper introduces Visual Cortex Network (VCNet), a novel neural network architecture whose design is informed by the macro-scale organization of the primate visual cortex. VCNet is framed as a geometric framework that emulates key biological mechanisms, including hierarchical processing across distinct cortical areas, dual-stream information segregation for learning disentangled representations, and top-down predictive feedback for representation refinement. We interpret these mechanisms through the lens of geometry and dynamical systems, positing that they guide the learning of structured, low-dimensional neural manifolds. We evaluate VCNet on two specialized benchmarks: the Spots-10 animal pattern dataset, which probes sensitivity to natural textures, and a light field image classification task, which requires processing higher-dimensional visual data. Our results show that VCNet achieves state-of-the-art accuracy of 92.1\% on Spots-10 and 74.4\% on the light field dataset, surpassing contemporary models of comparable size. This work demonstrates that integrating high-level neuroscientific principles, viewed through a geometric lens, can lead to more efficient and robust models, providing a promising direction for addressing long-standing challenges in machine learning.
<div id='section'>Paperid: <span id='pid'>1791, <a href='https://arxiv.org/pdf/2508.01137.pdf' target='_blank'>https://arxiv.org/pdf/2508.01137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeduo Zhang, Yalda Mohsenzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01137">Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To develop a domain-agnostic, semi-supervised anomaly detection framework that integrates deep reinforcement learning (DRL) to address challenges such as large-scale data, overfitting, and class imbalance, focusing on brain MRI volumes. This retrospective study used publicly available brain MRI datasets collected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and 578 T2-weighted MRI volumes (from healthy subjects) for training, while the BraTS 2021 dataset provided 251 volumes for validation and 1000 for testing (unhealthy subjects with Glioblastomas). Preprocessing included normalization, skull-stripping, and co-registering to a uniform voxel size. Experiments were conducted on both T1- and T2-weighted modalities. Additional experiments and ablation analyses were also carried out on the industrial datasets. The proposed method integrates DRL with feature representations to handle label scarcity, large-scale data and overfitting. Statistical analysis was based on several detection and segmentation metrics including AUROC and Dice score. The proposed method achieved an AUROC of 88.7% (pixel-level) and 96.7% (image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA) methods. On industrial surface datasets, the model also showed competitive performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset, indicating strong cross-domain generalization. Studies on anomaly sample size showed a monotonic increase in AUROC as more anomalies were seen, without evidence of overfitting or additional computational cost. The domain-agnostic semi-supervised approach using DRL shows significant promise for MRI anomaly detection, achieving strong performance on both medical and industrial datasets. Its robustness, generalizability and efficiency highlight its potential for real-world clinical applications.
<div id='section'>Paperid: <span id='pid'>1792, <a href='https://arxiv.org/pdf/2507.18542.pdf' target='_blank'>https://arxiv.org/pdf/2507.18542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Ruano, GonÃ§alo M. Correia, Leonor Barreiros, Afonso Mendes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18542">Effective Multi-Task Learning for Biomedical Named Entity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biomedical Named Entity Recognition presents significant challenges due to the complexity of biomedical terminology and inconsistencies in annotation across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER), a novel approach designed to handle nested named entities while integrating multiple datasets through an effective multi-task learning strategy. SRU-NER mitigates annotation gaps by dynamically adjusting loss computation to avoid penalizing predictions of entity types absent in a given dataset. Through extensive experiments, including a cross-corpus evaluation and human assessment of the model's predictions, SRU-NER achieves competitive performance in biomedical and general-domain NER tasks, while improving cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>1793, <a href='https://arxiv.org/pdf/2507.09440.pdf' target='_blank'>https://arxiv.org/pdf/2507.09440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Hill, Benjamin Eyre, Elliot Creager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09440">Transformers Don't In-Context Learn Least Squares Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-context learning (ICL) has emerged as a powerful capability of large pretrained transformers, enabling them to solve new tasks implicit in example input-output pairs without any gradient updates. Despite its practical success, the mechanisms underlying ICL remain largely mysterious. In this work we study synthetic linear regression to probe how transformers implement learning at inference time. Previous works have demonstrated that transformers match the performance of learning rules such as Ordinary Least Squares (OLS) regression or gradient descent and have suggested ICL is facilitated in transformers through the learned implementation of one of these techniques. In this work, we demonstrate through a suite of out-of-distribution generalization experiments that transformers trained for ICL fail to generalize after shifts in the prompt distribution, a behaviour that is inconsistent with the notion of transformers implementing algorithms such as OLS. Finally, we highlight the role of the pretraining corpus in shaping ICL behaviour through a spectral analysis of the learned representations in the residual stream. Inputs from the same distribution as the training data produce representations with a unique spectral signature: inputs from this distribution tend to have the same top two singular vectors. This spectral signature is not shared by out-of-distribution inputs, and a metric characterizing the presence of this signature is highly correlated with low loss.
<div id='section'>Paperid: <span id='pid'>1794, <a href='https://arxiv.org/pdf/2507.03026.pdf' target='_blank'>https://arxiv.org/pdf/2507.03026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhishek Verma, Nallarasan V, Balaraman Ravindran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03026">Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning in Reinforcement Learning (RL) enables agents to leverage knowledge from source tasks to accelerate learning in target tasks. While prior work, such as the Attend, Adapt, and Transfer (A2T) framework, addresses negative transfer and selective transfer, other critical challenges remain underexplored. This paper introduces the Generalized Adaptive Transfer Network (GATN), a deep RL architecture designed to tackle task generalization across domains, robustness to environmental changes, and computational efficiency in transfer. GATN employs a domain-agnostic representation module, a robustness-aware policy adapter, and an efficient transfer scheduler to achieve these goals. We evaluate GATN on diverse benchmarks, including Atari 2600, MuJoCo, and a custom chatbot dialogue environment, demonstrating superior performance in cross-domain generalization, resilience to dynamic environments, and reduced computational overhead compared to baselines. Our findings suggest GATN is a versatile framework for real-world RL applications, such as adaptive chatbots and robotic control.
<div id='section'>Paperid: <span id='pid'>1795, <a href='https://arxiv.org/pdf/2507.02365.pdf' target='_blank'>https://arxiv.org/pdf/2507.02365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Usama, Dong Eui Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02365">Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equalizer parameter optimization for signal integrity in high-speed Dynamic Random Access Memory systems is crucial but often computationally demanding or model-reliant. This paper introduces a data-driven framework employing learned latent signal representations for efficient signal integrity evaluation, coupled with a model-free Advantage Actor-Critic reinforcement learning agent for parameter optimization. The latent representation captures vital signal integrity features, offering a fast alternative to direct eye diagram analysis during optimization, while the reinforcement learning agent derives optimal equalizer settings without explicit system models. Applied to industry-standard Dynamic Random Access Memory waveforms, the method achieved significant eye-opening window area improvements: 42.7\% for cascaded Continuous-Time Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for Decision Feedback Equalizer-only configurations. These results demonstrate superior performance, computational efficiency, and robust generalization across diverse Dynamic Random Access Memory units compared to existing techniques. Core contributions include an efficient latent signal integrity metric for optimization, a robust model-free reinforcement learning strategy, and validated superior performance for complex equalizer architectures.
<div id='section'>Paperid: <span id='pid'>1796, <a href='https://arxiv.org/pdf/2506.16704.pdf' target='_blank'>https://arxiv.org/pdf/2506.16704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cynthia Dwork, Lunjia Hu, Han Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16704">How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a fundamental question of domain generalization: given a family of domains (i.e., data distributions), how many randomly sampled domains do we need to collect data from in order to learn a model that performs reasonably well on every seen and unseen domain in the family? We model this problem in the PAC framework and introduce a new combinatorial measure, which we call the domain shattering dimension. We show that this dimension characterizes the domain sample complexity. Furthermore, we establish a tight quantitative relationship between the domain shattering dimension and the classic VC dimension, demonstrating that every hypothesis class that is learnable in the standard PAC setting is also learnable in our setting.
<div id='section'>Paperid: <span id='pid'>1797, <a href='https://arxiv.org/pdf/2506.10245.pdf' target='_blank'>https://arxiv.org/pdf/2506.10245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iago Alves Brito, Julia Soares Dollis, Fernanda Bufon FÃ¤rber, Diogo Fernandes Costa Silva, Arlindo Rodrigues GalvÃ£o Filho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10245">ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ToxSyn-PT, the first large-scale Portuguese corpus that enables fine-grained hate-speech classification across nine legally protected minority groups. The dataset contains 53,274 synthetic sentences equally distributed between minorities groups and toxicity labels. ToxSyn-PT is created through a novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and (4) enrichment, plus additional neutral texts to curb overfitting to group-specific cues. The resulting corpus is class-balanced, stylistically diverse, and free from the social-media domain that dominate existing Portuguese datasets. Despite domain differences with traditional benchmarks, experiments on both binary and multi-label classification on the corpus yields strong results across five public Portuguese hate-speech datasets, demonstrating robust generalization even across domain boundaries. The dataset is publicly released to advance research on synthetic data and hate-speech detection in low-resource settings.
<div id='section'>Paperid: <span id='pid'>1798, <a href='https://arxiv.org/pdf/2506.08654.pdf' target='_blank'>https://arxiv.org/pdf/2506.08654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ciro Benito Raggio, Paolo Zaffino, Maria Francesca Spadea
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08654">A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise, limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield unit values and hindering direct dose calculation. Synthetic CT (sCT) generation from CBCT addresses these issues, especially using deep learning (DL) methods. Existing approaches are limited by institutional heterogeneity, scanner-dependent variations, and data privacy regulations that prevent multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region, extending our FedSynthCT framework. A conditional generative adversarial network was collaboratively trained on data from three European medical centers in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers, with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$ HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$, and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB. Notably, on an external validation dataset of 60 patients, comparable performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR: $33.52\pm2.06$ dB) without additional training, confirming robust generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT synthesis while preserving data privacy and offer a collaborative solution for developing generalizable models across institutions without centralized data sharing or site-specific fine-tuning.
<div id='section'>Paperid: <span id='pid'>1799, <a href='https://arxiv.org/pdf/2506.08379.pdf' target='_blank'>https://arxiv.org/pdf/2506.08379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yurun Yuan, Tengyang Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08379">Reinforce LLM Reasoning through Multi-Agent Reflection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1800, <a href='https://arxiv.org/pdf/2506.05443.pdf' target='_blank'>https://arxiv.org/pdf/2506.05443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyu Lin, Yan Wang, You Zhou, Xinye Ni, Jiahui Wu, Sen Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05443">UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a core mechanism of epigenetic regulation in eukaryotes, protein post-translational modifications (PTMs) require precise prediction to decipher dynamic life activity networks. To address the limitations of existing deep learning models in cross-modal feature fusion, domain generalization, and architectural optimization, this study proposes UniPTMs: the first unified framework for multi-type PTM prediction. The framework innovatively establishes a "Master-Slave" dual-path collaborative architecture: The master path dynamically integrates high-dimensional representations of protein sequences, structures, and evolutionary information through a Bidirectional Gated Cross-Attention (BGCA) module, while the slave path optimizes feature discrepancies and recalibration between structural and traditional features using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level feature integration across paths, the framework employs a Hierarchical Dynamic Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal features. Enhanced by a novel Hierarchical Contrastive loss function for feature consistency optimization, UniPTMs demonstrates significant performance improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art models across five modification types and transcends the Single-Type Prediction Paradigm. To strike a balance between model complexity and performance, we have also developed a lightweight variant named UniPTMs-mini.
<div id='section'>Paperid: <span id='pid'>1801, <a href='https://arxiv.org/pdf/2505.24067.pdf' target='_blank'>https://arxiv.org/pdf/2505.24067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu He, Ellen Vitercik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24067">Primal-Dual Neural Algorithmic Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Algorithmic Reasoning (NAR) trains neural networks to simulate classical algorithms, enabling structured and interpretable reasoning over complex data. While prior research has predominantly focused on learning exact algorithms for polynomial-time-solvable problems, extending NAR to harder problems remains an open challenge. In this work, we introduce a general NAR framework grounded in the primal-dual paradigm, a classical method for designing efficient approximation algorithms. By leveraging a bipartite representation between primal and dual variables, we establish an alignment between primal-dual algorithms and Graph Neural Networks. Furthermore, we incorporate optimal solutions from small instances to greatly enhance the model's reasoning capabilities. Our empirical results demonstrate that our model not only simulates but also outperforms approximation algorithms for multiple tasks, exhibiting robust generalization to larger and out-of-distribution graphs. Moreover, we highlight the framework's practical utility by integrating it with commercial solvers and applying it to real-world datasets.
<div id='section'>Paperid: <span id='pid'>1802, <a href='https://arxiv.org/pdf/2505.22284.pdf' target='_blank'>https://arxiv.org/pdf/2505.22284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Fan, Chuanlin Liao, Yi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22284">From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to achieve image restoration caused by multiple degradation patterns via a single model with unified parameters. Although existing AiOIR approaches obtain promising performance in closed and controlled scenarios, they still suffered from considerable performance reduction in real-world scenarios since the gap of data distributions between the training samples (source domain) and real-world test samples (target domain) can lead inferior degradation awareness ability. To address this issue, a Unified Domain-Adaptive Image Restoration (UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the learned knowledge from source domain to target domain. To improve the degradation identification, a codebook is designed to learn a group of discrete embeddings to denote the degradation patterns, and the cross-sample contrastive learning mechanism is further proposed to capture shared features from different samples of certain degradation. To bridge the data gap, a domain adaptation strategy is proposed to build the feature projection between the source and target domains by dynamically aligning their codebook embeddings, and a correlation alignment-based test-time adaptation mechanism is designed to fine-tune the alignment discrepancies by tightening the degradation embeddings to the corresponding cluster center in the source domain. Experimental results on 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art performance for the AiOIR task. Most importantly, the feature cluster validate the degradation identification under unknown conditions, and qualitative comparisons showcase robust generalization to real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1803, <a href='https://arxiv.org/pdf/2505.08392.pdf' target='_blank'>https://arxiv.org/pdf/2505.08392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ren Zhuang, Ben Wang, Shuifa Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08392">Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.
<div id='section'>Paperid: <span id='pid'>1804, <a href='https://arxiv.org/pdf/2505.08159.pdf' target='_blank'>https://arxiv.org/pdf/2505.08159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxiang Li, Junwei Feng, Jie Luo, Bowen Jiang, Xiangyu Zheng, Qigang Song, Jian Lv, Keith Butler, Hanyu Liu, Congwei Xie, Yu Xie, Yanming Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08159">Self-Optimizing Machine Learning Potential Assisted Automated Workflow for Highly Efficient Complex Systems Material Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning interatomic potentials have revolutionized complex materials design by enabling rapid exploration of material configurational spaces via crystal structure prediction with ab initio accuracy. However, critical challenges persist in ensuring robust generalization to unknown structures and minimizing the requirement for substantial expert knowledge and time-consuming manual interventions. Here, we propose an automated crystal structure prediction framework built upon the attention-coupled neural networks potential to address these limitations. The generalizability of the potential is achieved by sampling regions across the local minima of the potential energy surface, where the self-evolving pipeline autonomously refines the potential iteratively while minimizing human intervention. The workflow is validated on Mg-Ca-H ternary and Be-P-N-O quaternary systems by exploring nearly 10 million configurations, demonstrating substantial speedup compared to first-principles calculations. These results underscore the effectiveness of our approach in accelerating the exploration and discovery of complex multi-component functional materials.
<div id='section'>Paperid: <span id='pid'>1805, <a href='https://arxiv.org/pdf/2505.07165.pdf' target='_blank'>https://arxiv.org/pdf/2505.07165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Li, Hongzhang Zhu, Tao Chen, Xiaohua Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07165">Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, numerous pancreas segmentation methods have achieved promising performance on local single-source datasets. However, these methods don't adequately account for generalizability issues, and hence typically show limited performance and low stability on test data from other sources. Considering the limited availability of distinct data sources, we seek to improve the generalization performance of a pancreas segmentation model trained with a single-source dataset, i.e., the single source generalization task. In particular, we propose a dual self-supervised learning model that incorporates both global and local anatomical contexts. Our model aims to fully exploit the anatomical features of the intra-pancreatic and extra-pancreatic regions, and hence enhance the characterization of the high-uncertainty regions for more robust generalization. Specifically, we first construct a global-feature contrastive self-supervised learning module that is guided by the pancreatic spatial structure. This module obtains complete and consistent pancreatic features through promoting intra-class cohesion, and also extracts more discriminative features for differentiating between pancreatic and non-pancreatic tissues through maximizing inter-class separation. It mitigates the influence of surrounding tissue on the segmentation outcomes in high-uncertainty regions. Subsequently, a local-image restoration self-supervised learning module is introduced to further enhance the characterization of the high uncertainty regions. In this module, informative anatomical contexts are actually learned to recover randomly corrupted appearance patterns in those regions.
<div id='section'>Paperid: <span id='pid'>1806, <a href='https://arxiv.org/pdf/2505.06831.pdf' target='_blank'>https://arxiv.org/pdf/2505.06831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaoyun Zhao, Qiang Zhang, Chenrong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06831">Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving group-robust generalization in the presence of spurious correlations remains a significant challenge, particularly when bias annotations are unavailable. Recent studies on Class-Conditional Distribution Balancing (CCDB) reveal that spurious correlations often stem from mismatches between the class-conditional and marginal distributions of bias attributes. They achieve promising results by addressing this issue through simple distribution matching in a bias-agnostic manner. However, CCDB approximates each distribution using a single Gaussian, which is overly simplistic and rarely holds in real-world applications. To address this limitation, we propose a novel method called Bias Exploration via Overfitting (BEO), which captures each distribution in greater detail by modeling it as a mixture of latent groups. Building on these group-level descriptions, we introduce a fine-grained variant of CCDB, termed FG-CCDB, which performs more precise distribution matching and balancing within each group. Through group-level reweighting, FG-CCDB learns sample weights from a global perspective, achieving stronger mitigation of spurious correlations without incurring substantial storage or computational costs. Extensive experiments demonstrate that BEO serves as a strong proxy for ground-truth bias annotations and can be seamlessly integrated with bias-supervised methods. Moreover, when combined with FG-CCDB, our method performs on par with bias-supervised approaches on binary classification tasks and significantly outperforms them in highly biased multi-class scenarios.
<div id='section'>Paperid: <span id='pid'>1807, <a href='https://arxiv.org/pdf/2505.06690.pdf' target='_blank'>https://arxiv.org/pdf/2505.06690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianxin Zhang, Lianzi Jiang, Xinyu Han, Xiangrong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06690">A Causality- and Frequency-Aware Deep Learning Framework for Wave Elevation Prediction Behind Floating Breakwaters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the elevations of nonlinear wave fields behind floating breakwaters (FBs) is crucial for optimizing coastal engineering structures, enhancing safety, and improving design efficiency. Existing deep learning approaches exhibit limited generalization capability under unseen operating conditions. To address this challenge, this study proposes the Exogenous-to-Endogenous Frequency-Aware Network (E2E-FANet), a novel end-to-end neural network designed to model relationships between waves and structures. First, the Dual-Basis Frequency Mapping (DBFM) module leverages orthogonal cosine and sine bases to generate an adaptive time-frequency representation, enabling the model to effectively disentangle the evolving spectral components of wave signals. Second, the Exogenous-to-Endogenous Cross-Attention (E2ECA) module employs cross attention to explicitly model the unidirectional causal influence of floating breakwater motion on wave elevations. Additionally, a Temporal-wise Attention (TA) mechanism is incorporated that adaptively captures complex dependencies in endogenous variables. Extensive experiments, including generalization tests across diverse wave conditions and adaptability tests under varying relative water density (RW) conditions, demonstrate that E2E-FANet achieves superior predictive accuracy and robust generalization compared to mainstream models. This work emphasizes the importance of integrating causality and frequency-aware modeling in deep learning architectures for modeling nonlinear dynamics systems.
<div id='section'>Paperid: <span id='pid'>1808, <a href='https://arxiv.org/pdf/2504.16972.pdf' target='_blank'>https://arxiv.org/pdf/2504.16972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Ahmadi, Sajjad Emdadi Mahdimahalleh, Arman Farahat, Banafsheh Saffari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16972">Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of unlabeled time-series data in domains such as wireless communications, radar, biomedical engineering, and the Internet of Things (IoT) has driven advancements in unsupervised learning. This review synthesizes recent progress in applying autoencoders and vision transformers for unsupervised signal analysis, focusing on their architectures, applications, and emerging trends. We explore how these models enable feature extraction, anomaly detection, and classification across diverse signal types, including electrocardiograms, radar waveforms, and IoT sensor data. The review highlights the strengths of hybrid architectures and self-supervised learning, while identifying challenges in interpretability, scalability, and domain generalization. By bridging methodological innovations and practical applications, this work offers a roadmap for developing robust, adaptive models for signal intelligence.
<div id='section'>Paperid: <span id='pid'>1809, <a href='https://arxiv.org/pdf/2504.14707.pdf' target='_blank'>https://arxiv.org/pdf/2504.14707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ratna Kandala, Katie Hoemann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14707">Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores BERTopic's potential for modeling open-ended Belgian Dutch daily narratives, contrasting its performance with Latent Dirichlet Allocation (LDA) and KMeans. Although LDA scores well on certain automated metrics, human evaluations reveal semantically irrelevant co-occurrences, highlighting the limitations of purely statistic-based methods. In contrast, BERTopic's reliance on contextual embeddings yields culturally resonant themes, underscoring the importance of hybrid evaluation frameworks that account for morphologically rich languages. KMeans performed less coherently than prior research suggested, pointing to the unique challenges posed by personal narratives. Our findings emphasize the need for robust generalization in NLP models, especially in underrepresented linguistic contexts.
<div id='section'>Paperid: <span id='pid'>1810, <a href='https://arxiv.org/pdf/2504.14664.pdf' target='_blank'>https://arxiv.org/pdf/2504.14664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jixiang Sun, Fei Lei, Jiawei Zhang, Wenxiu Sun, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14664">Frequency-domain Learning with Kernel Prior for Blind Image Deblurring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While achieving excellent results on various datasets, many deep learning methods for image deblurring suffer from limited generalization capabilities with out-of-domain data. This limitation is likely caused by their dependence on certain domain-specific datasets. To address this challenge, we argue that it is necessary to introduce the kernel prior into deep learning methods, as the kernel prior remains independent of the image context. For effective fusion of kernel prior information, we adopt a rational implementation method inspired by traditional deblurring algorithms that perform deconvolution in the frequency domain. We propose a module called Frequency Integration Module (FIM) for fusing the kernel prior and combine it with a frequency-based deblurring Transfomer network. Experimental results demonstrate that our method outperforms state-of-the-art methods on multiple blind image deblurring tasks, showcasing robust generalization abilities. Source code will be available soon.
<div id='section'>Paperid: <span id='pid'>1811, <a href='https://arxiv.org/pdf/2504.12966.pdf' target='_blank'>https://arxiv.org/pdf/2504.12966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanmei Wang, Xiyao Liu, Fupeng Chu, Zhi Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12966">Vision and Language Integration for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims at training on source domains to uncover a domain-invariant feature space, allowing the model to perform robust generalization ability on unknown target domains. However, due to domain gaps, it is hard to find reliable common image feature space, and the reason for that is the lack of suitable basic units for images. Different from image in vision space, language has comprehensive expression elements that can effectively convey semantics. Inspired by the semantic completeness of language and intuitiveness of image, we propose VLCA, which combine language space and vision space, and connect the multiple image domains by using semantic space as the bridge domain. Specifically, in language space, by taking advantage of the completeness of language basic units, we tend to capture the semantic representation of the relations between categories through word vector distance. Then, in vision space, by taking advantage of the intuitiveness of image features, the common pattern of sample features with the same class is explored through low-rank approximation. In the end, the language representation is aligned with the vision representation through the multimodal space of text and image. Experiments demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1812, <a href='https://arxiv.org/pdf/2503.24111.pdf' target='_blank'>https://arxiv.org/pdf/2503.24111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur M. Faria, Ignacio F. GraÃ±a, Savvas Varsamopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24111">Inductive Graph Representation Learning with Quantum Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum Graph Neural Networks (QGNNs) present a promising approach for combining quantum computing with graph-structured data processing. While classical Graph Neural Networks (GNNs) are renowned for their scalability and robustness, existing QGNNs often lack flexibility due to graph-specific quantum circuit designs, limiting their applicability to a narrower range of graph-structured problems, falling short of real-world scenarios. To address these limitations, we propose a versatile QGNN framework inspired by the classical GraphSAGE approach, utilizing quantum models as aggregators. In this work, we integrate established techniques for inductive representation learning on graphs with parametrized quantum convolutional and pooling layers, effectively bridging classical and quantum paradigms. The convolutional layer is flexible, enabling tailored designs for specific problems. Benchmarked on a node regression task with the QM9 dataset, we demonstrate that our framework successfully models a non-trivial molecular dataset, achieving performance comparable to classical GNNs. In particular, we show that our quantum approach exhibits robust generalization across molecules with varying numbers of atoms without requiring circuit modifications, slightly outperforming classical GNNs. Furthermore, we numerically investigate the scalability of the QGNN framework. Specifically, we demonstrate the absence of barren plateaus in our architecture as the number of qubits increases, suggesting that the proposed quantum model can be extended to handle larger and more complex graph-based problems effectively.
<div id='section'>Paperid: <span id='pid'>1813, <a href='https://arxiv.org/pdf/2503.20897.pdf' target='_blank'>https://arxiv.org/pdf/2503.20897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Venuri Amarasinghe, Asini Jayakody, Isun Randila, Kalinga Bandara, Chamuditha Jayanga Galappaththige, Ranga Rodrigo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20897">Feature Modulation for Semi-Supervised Domain Generalization without Domain Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised domain generalization (SSDG) leverages a small fraction of labeled data alongside unlabeled data to enhance model generalization. Most of the existing SSDG methods rely on pseudo-labeling (PL) for unlabeled data, often assuming access to domain labels-a privilege not always available. However, domain shifts introduce domain noise, leading to inconsistent PLs that degrade model performance. Methods derived from FixMatch suffer particularly from lower PL accuracy, reducing the effectiveness of unlabeled data. To address this, we tackle the more challenging domain-label agnostic SSDG, where domain labels for unlabeled data are not available during training. First, we propose a feature modulation strategy that enhances class-discriminative features while suppressing domain-specific information. This modulation shifts features toward Similar Average Representations-a modified version of class prototypes-that are robust across domains, encouraging the classifier to distinguish between closely related classes and feature extractor to form tightly clustered, domain-invariant representations. Second, to mitigate domain noise and improve pseudo-label accuracy, we introduce a loss-scaling function that dynamically lowers the fixed confidence threshold for pseudo-labels, optimizing the use of unlabeled data. With these key innovations, our approach achieves significant improvements on four major domain generalization benchmarks-even without domain labels. We will make the code available.
<div id='section'>Paperid: <span id='pid'>1814, <a href='https://arxiv.org/pdf/2503.19152.pdf' target='_blank'>https://arxiv.org/pdf/2503.19152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shoffan Saifullah, RafaÅ DreÅ¼ewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19152">PSO-UNet: Particle Swarm-Optimized U-Net Framework for Precise Multimodal Brain Tumor Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation, particularly for brain tumor analysis, demands precise and computationally efficient models due to the complexity of multimodal MRI datasets and diverse tumor morphologies. This study introduces PSO-UNet, which integrates Particle Swarm Optimization (PSO) with the U-Net architecture for dynamic hyperparameter optimization. Unlike traditional manual tuning or alternative optimization approaches, PSO effectively navigates complex hyperparameter search spaces, explicitly optimizing the number of filters, kernel size, and learning rate. PSO-UNet substantially enhances segmentation performance, achieving Dice Similarity Coefficients (DSC) of 0.9578 and 0.9523 and Intersection over Union (IoU) scores of 0.9194 and 0.9097 on the BraTS 2021 and Figshare datasets, respectively. Moreover, the method reduces computational complexity significantly, utilizing only 7.8 million parameters and executing in approximately 906 seconds, markedly faster than comparable U-Net-based frameworks. These outcomes underscore PSO-UNet's robust generalization capabilities across diverse MRI modalities and tumor classifications, emphasizing its clinical potential and clear advantages over conventional hyperparameter tuning methods. Future research will explore hybrid optimization strategies and validate the framework against other bio-inspired algorithms to enhance its robustness and scalability.
<div id='section'>Paperid: <span id='pid'>1815, <a href='https://arxiv.org/pdf/2503.18567.pdf' target='_blank'>https://arxiv.org/pdf/2503.18567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biwen Meng, Xi Long, Wanrong Yang, Ruochen Liu, Yi Tian, Yalin Zheng, Jingxin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18567">Advancing Cross-Organ Domain Generalization with Test-Time Style Transfer and Diversity Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has made significant progress in addressing challenges in various fields including computational pathology (CPath). However, due to the complexity of the domain shift problem, the performance of existing models will degrade, especially when it comes to multi-domain or cross-domain tasks. In this paper, we propose a Test-time style transfer (T3s) that uses a bidirectional mapping mechanism to project the features of the source and target domains into a unified feature space, enhancing the generalization ability of the model. To further increase the style expression space, we introduce a Cross-domain style diversification module (CSDM) to ensure the orthogonality between style bases. In addition, data augmentation and low-rank adaptation techniques are used to improve feature alignment and sensitivity, enabling the model to adapt to multi-domain inputs effectively. Our method has demonstrated effectiveness on three unseen datasets.
<div id='section'>Paperid: <span id='pid'>1816, <a href='https://arxiv.org/pdf/2503.16271.pdf' target='_blank'>https://arxiv.org/pdf/2503.16271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Borges S. Carvalho, Alessandro Torcinovich, Victor Jimenez Rodriguez, Antonio E. CinÃ, Carlos Cotrini, Lea SchÃ¶nherr, Joachim M. Buhmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16271">Rethinking Robustness in Machine Learning: A Posterior Agreement Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The robustness of algorithms against covariate shifts is a fundamental problem with critical implications for the deployment of machine learning algorithms in the real world. Current evaluation methods predominantly match the robustness definition to that of standard generalization, relying on standard metrics like accuracy-based scores, which, while designed for performance assessment, lack a theoretical foundation encompassing their application in estimating robustness to distribution shifts. In this work, we set the desiderata for a robustness metric, and we propose a novel principled framework for the robustness assessment problem that directly follows the Posterior Agreement (PA) theory of model validation. Specifically, we extend the PA framework to the covariate shift setting by proposing a PA metric for robustness evaluation in supervised classification tasks. We assess the soundness of our metric in controlled environments and through an empirical robustness analysis in two different covariate shift scenarios: adversarial learning and domain generalization. We illustrate the suitability of PA by evaluating several models under different nature and magnitudes of shift, and proportion of affected observations. The results show that the PA metric provides a sensible and consistent analysis of the vulnerabilities in learning algorithms, even in the presence of few perturbed observations.
<div id='section'>Paperid: <span id='pid'>1817, <a href='https://arxiv.org/pdf/2503.10354.pdf' target='_blank'>https://arxiv.org/pdf/2503.10354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nevidu Jayatilleke, Ruvan Weerasinghe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10354">A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic patent summarization approaches that help in the patent analysis and comprehension procedure are in high demand due to the colossal growth of innovations. The development of natural language processing (NLP), text mining, and deep learning has notably amplified the efficacy of text summarization models for abundant types of documents. Summarizing patent text remains a pertinent challenge due to the labyrinthine writing style of these documents, which includes technical and legal intricacies. Additionally, these patent document contents are considerably lengthier than archetypal documents, which complicates the process of extracting pertinent information for summarization. Embodying extractive and abstractive text summarization methodologies into a hybrid framework, this study proposes a system for efficiently creating abstractive summaries of patent records. The procedure involves leveraging the LexRank graph-based algorithm to retrieve the important sentences from input parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART) model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for producing text summaries. This is accompanied by methodical testing and evaluation strategies. Furthermore, the author employed certain meta-learning techniques to achieve Domain Generalization (DG) of the abstractive component across multiple patent fields.
<div id='section'>Paperid: <span id='pid'>1818, <a href='https://arxiv.org/pdf/2503.09661.pdf' target='_blank'>https://arxiv.org/pdf/2503.09661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johnson Loh, Lyubov Dudchenko, Justus Viga, Tobias Gemmeke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09661">Towards Hardware Supported Domain Generalization in DNN-Based Edge Computing Devices for Health Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural network (DNN) models have shown remarkable success in many real-world scenarios, such as object detection and classification. Unfortunately, these models are not yet widely adopted in health monitoring due to exceptionally high requirements for model robustness and deployment in highly resource-constrained devices. In particular, the acquisition of biosignals, such as electrocardiogram (ECG), is subject to large variations between training and deployment, necessitating domain generalization (DG) for robust classification quality across sensors and patients. The continuous monitoring of ECG also requires the execution of DNN models in convenient wearable devices, which is achieved by specialized ECG accelerators with small form factor and ultra-low power consumption. However, combining DG capabilities with ECG accelerators remains a challenge. This article provides a comprehensive overview of ECG accelerators and DG methods and discusses the implication of the combination of both domains, such that multi-domain ECG monitoring is enabled with emerging algorithm-hardware co-optimized systems. Within this context, an approach based on correction layers is proposed to deploy DG capabilities on the edge. Here, the DNN fine-tuning for unknown domains is limited to a single layer, while the remaining DNN model remains unmodified. Thus, computational complexity (CC) for DG is reduced with minimal memory overhead compared to conventional fine-tuning of the whole DNN model. The DNN model-dependent CC is reduced by more than 2.5x compared to DNN fine-tuning at an average increase of F1 score by more than 20% on the generalized target domain. In summary, this article provides a novel perspective on robust DNN classification on the edge for health monitoring applications.
<div id='section'>Paperid: <span id='pid'>1819, <a href='https://arxiv.org/pdf/2503.02448.pdf' target='_blank'>https://arxiv.org/pdf/2503.02448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyi Wang, Yinning Shao, Yunlong Ma, Min Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02448">NodeNAS: Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural architecture search (GraphNAS) has demonstrated advantages in mitigating performance degradation of graph neural networks (GNNs) due to distribution shifts. Recent approaches introduce weight sharing across tailored architectures, generating unique GNN architectures for each graph end-to-end. However, existing GraphNAS methods do not account for distribution patterns across different graphs and heavily rely on extensive training data. With sparse or single training graphs, these methods struggle to discover optimal mappings between graphs and architectures, failing to generalize to out-of-distribution (OOD) data. In this paper, we propose node-specific graph neural architecture search(NodeNAS), which aims to tailor distinct aggregation methods for different nodes through disentangling node topology and graph distribution with limited datasets. We further propose adaptive aggregation attention based Multi-dim NodeNAS method(MNNAS), which learns an node-specific architecture customizer with good generalizability. Specifically, we extend the vertical depth of the search space, supporting simultaneous node-specific architecture customization across multiple dimensions. Moreover, we model the power-law distribution of node degrees under varying assortativity, encoding structure invariant information to guide architecture customization across each dimension. Extensive experiments across supervised and unsupervised tasks demonstrate that MNNAS surpasses state-of-the-art algorithms and achieves excellent OOD generalization.
<div id='section'>Paperid: <span id='pid'>1820, <a href='https://arxiv.org/pdf/2502.08155.pdf' target='_blank'>https://arxiv.org/pdf/2502.08155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhou, Yu Cheng, Songlin Li, Hongwang Zhang, Chenxu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08155">DGSense: A Domain Generalization Framework for Wireless Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wireless sensing is of great benefits to our daily lives. However, wireless signals are sensitive to the surroundings. Various factors, e.g. environments, locations, and individuals, may induce extra impact on wireless propagation. Such a change can be regarded as a domain, in which the data distribution shifts. A vast majority of the sensing schemes are learning-based. They are dependent on the training domains, resulting in performance degradation in unseen domains. Researchers have proposed various solutions to address this issue. But these solutions leverage either semi-supervised or unsupervised domain adaptation techniques. They still require some data in the target domains and do not perform well in unseen domains. In this paper, we propose a domain generalization framework DGSense, to eliminate the domain dependence problem in wireless sensing. The framework is a general solution working across diverse sensing tasks and wireless technologies. Once the sensing model is built, it can generalize to unseen domains without any data from the target domain. To achieve the goal, we first increase the diversity of the training set by a virtual data generator, and then extract the domain independent features via episodic training between the main feature extractor and the domain feature extractors. The feature extractors employ a pre-trained Residual Network (ResNet) with an attention mechanism for spatial features, and a 1D Convolutional Neural Network (1DCNN) for temporal features. To demonstrate the effectiveness and generality of DGSense, we evaluated on WiFi gesture recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall detection. All the systems exhibited high generalization capability to unseen domains, including new users, locations, and environments, free of new data and retraining.
<div id='section'>Paperid: <span id='pid'>1821, <a href='https://arxiv.org/pdf/2502.04034.pdf' target='_blank'>https://arxiv.org/pdf/2502.04034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Song, Yinpu Bai, Hui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04034">Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer Drug Response Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed FourierDrug, to address this challenge. Given the extracted feature from expression profile, we performed Fourier transforms and then introduced an asymmetric attention constraint that would cluster drug-sensitive samples into a compact group while drives resistant samples dispersed in the frequency domain. Our empirical experiments demonstrate that our model effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type. When evaluated on single-cell and patient-level drug response prediction tasks, FourierDrug--trained solely on in vitro cell line data without access to target-domain data--consistently outperforms or, at least, matched the performance of current state-of-the-art methods. These findings underscore the potential of our method for real-world clinical applications.
<div id='section'>Paperid: <span id='pid'>1822, <a href='https://arxiv.org/pdf/2501.17384.pdf' target='_blank'>https://arxiv.org/pdf/2501.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengpeng Xie, Yulong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17384">A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent's policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1823, <a href='https://arxiv.org/pdf/2501.15144.pdf' target='_blank'>https://arxiv.org/pdf/2501.15144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Yadav, Lingqiao Liu, Yuankai Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15144">Exploring Primitive Visual Measurement Understanding and the Role of Output Format in Learning in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the capabilities of current vision-language models (VLMs) in visual understanding and attribute measurement of primitive shapes using a benchmark focused on controlled 2D shape configurations with variations in spatial positioning, occlusion, rotation, size, and shape attributes such as type, quadrant, center-coordinates, rotation, occlusion status, and color as shown in Figure 1 and supplementary Figures S3-S81. We fine-tune state-of-the-art VLMs (2B-8B parameters) using Low-Rank Adaptation (LoRA) and validate them on multiple out-of-domain (OD) scenarios from our proposed benchmark. Our findings reveal that coherent sentence-based outputs outperform tuple formats, particularly in OD scenarios with large domain gaps. Additionally, we demonstrate that scaling numeric tokens during loss computation enhances numerical approximation capabilities, further improving performance on spatial and measurement tasks. These results highlight the importance of output format design, loss scaling strategies, and robust generalization techniques in enhancing the training and fine-tuning of VLMs, particularly for tasks requiring precise spatial approximations and strong OD generalization.
<div id='section'>Paperid: <span id='pid'>1824, <a href='https://arxiv.org/pdf/2501.13726.pdf' target='_blank'>https://arxiv.org/pdf/2501.13726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shi-Qi Yan, Zhen-Hua Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13726">RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing external knowledge, its generation process heavily depends on the quality and accuracy of the retrieved context. Large language models (LLMs) struggle to evaluate the correctness of non-parametric knowledge retrieved externally when it differs from internal memorization, leading to knowledge conflicts during response generation. To this end, we introduce the Retrieval Preference Optimization (RPO), a lightweight and effective alignment method to adaptively leverage multi-source knowledge based on retrieval relevance. An implicit representation of retrieval relevance is derived and incorporated into the reward model to integrate retrieval evaluation and response generation into a single model, solving the problem that previous methods necessitate the additional procedure to assess the retrieval quality. Notably, RPO is the only RAG-dedicated alignment approach that quantifies the awareness of retrieval relevance in training, overcoming mathematical obstacles. Experiments on four datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any extra component, exhibiting its robust generalization.
<div id='section'>Paperid: <span id='pid'>1825, <a href='https://arxiv.org/pdf/2501.00116.pdf' target='_blank'>https://arxiv.org/pdf/2501.00116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhou You, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00116">Text-to-Image GAN with Pretrained Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating desired images conditioned on given text descriptions has received lots of attention. Recently, diffusion models and autoregressive models have demonstrated their outstanding expressivity and gradually replaced GAN as the favored architectures for text-to-image synthesis. However, they still face some obstacles: slow inference speed and expensive training costs. To achieve more powerful and faster text-to-image synthesis under complex scenes, we propose TIGER, a text-to-image GAN with pretrained representations. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. (i) The vision-empowered discriminator absorbs the complex scene understanding ability and the domain generalization ability from pretrained vision models to enhance model performance. Unlike previous works, we explore stacking multiple pretrained models in our discriminator to collect multiple different representations. (ii) The high-capacity generator aims to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks (HFBlock). And the HFBlock contains several deep fusion modules and a global fusion module, which play different roles to benefit our model. Extensive experiments demonstrate the outstanding performance of our proposed TIGER both on standard and zero-shot text-to-image synthesis tasks. On the standard text-to-image synthesis task, TIGER achieves state-of-the-art performance on two challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On the zero-shot text-to-image synthesis task, we achieve comparable performance with fewer model parameters, smaller training data size and faster inference speed. Additionally, more experiments and analyses are conducted in the Supplementary Material.
<div id='section'>Paperid: <span id='pid'>1826, <a href='https://arxiv.org/pdf/2501.00050.pdf' target='_blank'>https://arxiv.org/pdf/2501.00050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fernando Martinez-Lopez, Lesther Santana, Mohamed Rahouti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00050">Learning in Multiple Spaces: Few-Shot Network Attack Detection with Metric-Fused Prototypical Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Network intrusion detection systems face significant challenges in identifying emerging attack patterns, especially when limited data samples are available. To address this, we propose a novel Multi-Space Prototypical Learning (MSPL) framework tailored for few-shot attack detection. The framework operates across multiple metric spaces-Euclidean, Cosine, Chebyshev, and Wasserstein distances-integrated through a constrained weighting scheme to enhance embedding robustness and improve pattern recognition. By leveraging Polyak-averaged prototype generation, the framework stabilizes the learning process and effectively adapts to rare and zero-day attacks. Additionally, an episodic training paradigm ensures balanced representation across diverse attack classes, enabling robust generalization. Experimental results on benchmark datasets demonstrate that MSPL outperforms traditional approaches in detecting low-profile and novel attack types, establishing it as a robust solution for zero-day attack detection.
<div id='section'>Paperid: <span id='pid'>1827, <a href='https://arxiv.org/pdf/2412.12449.pdf' target='_blank'>https://arxiv.org/pdf/2412.12449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongya Wu, Xin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12449">Adversarially robust generalization theory via Jacobian regularization for deep neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Powerful deep neural networks are vulnerable to adversarial attacks. To obtain adversarially robust models, researchers have separately developed adversarial training and Jacobian regularization techniques. There are abundant theoretical and empirical studies for adversarial training, but theoretical foundations for Jacobian regularization are still lacking. In this study, we show that Jacobian regularization is closely related to adversarial training in that $\ell_{2}$ or $\ell_{1}$ Jacobian regularized loss serves as an approximate upper bound on the adversarially robust loss under $\ell_{2}$ or $\ell_{\infty}$ adversarial attack respectively. Further, we establish the robust generalization gap for Jacobian regularized risk minimizer via bounding the Rademacher complexity of both the standard loss function class and Jacobian regularization function class. Our theoretical results indicate that the norms of Jacobian are related to both standard and robust generalization. We also perform experiments on MNIST data classification to demonstrate that Jacobian regularized risk minimization indeed serves as a surrogate for adversarially robust risk minimization, and that reducing the norms of Jacobian can improve both standard and robust generalization. This study promotes both theoretical and empirical understandings to adversarially robust generalization via Jacobian regularization.
<div id='section'>Paperid: <span id='pid'>1828, <a href='https://arxiv.org/pdf/2412.10031.pdf' target='_blank'>https://arxiv.org/pdf/2412.10031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jizhihui Liu, Qixun Teng, Qing Ma, Junjun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10031">FM2S: Towards Spatially-Correlated Noise Modeling in Zero-Shot Fluorescence Microscopy Image Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fluorescence microscopy image (FMI) denoising faces critical challenges due to the compound mixed Poisson-Gaussian noise with strong spatial correlation and the impracticality of acquiring paired noisy/clean data in dynamic biomedical scenarios. While supervised methods trained on synthetic noise (e.g., Gaussian/Poisson) suffer from out-of-distribution generalization issues, existing self-supervised approaches degrade under real FMI noise due to oversimplified noise assumptions and computationally intensive deep architectures. In this paper, we propose Fluorescence Micrograph to Self (FM2S), a zero-shot denoiser that achieves efficient FMI denoising through three key innovations: 1) A noise injection module that ensures training data sufficiency through adaptive Poisson-Gaussian synthesis while preserving spatial correlation and global statistics of FMI noise for robust model generalization; 2) A two-stage progressive learning strategy that first recovers structural priors via pre-denoised targets then refines high-frequency details through noise distribution alignment; 3) An ultra-lightweight network (3.5k parameters) enabling rapid convergence with 270$\times$ faster training and inference than SOTAs. Extensive experiments across FMI datasets demonstrate FM2S's superiority: It outperforms CVF-SID by 1.4dB PSNR on average while requiring 0.1% parameters of AP-BSN. Notably, FM2S maintains stable performance across varying noise levels, proving its practicality for microscopy platforms with diverse sensor characteristics. Code and datasets will be released.
<div id='section'>Paperid: <span id='pid'>1829, <a href='https://arxiv.org/pdf/2412.05572.pdf' target='_blank'>https://arxiv.org/pdf/2412.05572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Xu, Taiping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05572">From Deterministic to Probabilistic: A Novel Perspective on Domain Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional domain generalization methods often rely on domain alignment to reduce inter-domain distribution differences and learn domain-invariant representations. However, domain shifts are inherently difficult to eliminate, which limits model generalization. To address this, we propose an innovative framework that enhances data representation quality through probabilistic modeling and contrastive learning, reducing dependence on domain alignment and improving robustness under domain variations. Specifically, we combine deterministic features with uncertainty modeling to capture comprehensive feature distributions. Contrastive learning enforces distribution-level alignment by aligning the mean and covariance of feature distributions, enabling the model to dynamically adapt to domain variations and mitigate distribution shifts. Additionally, we design a frequency-domain-based structural enhancement strategy using discrete wavelet transforms to preserve critical structural details and reduce visual distortions caused by style variations. Experimental results demonstrate that the proposed framework significantly improves segmentation performance, providing a robust solution to domain generalization challenges in medical image segmentation.
<div id='section'>Paperid: <span id='pid'>1830, <a href='https://arxiv.org/pdf/2411.16959.pdf' target='_blank'>https://arxiv.org/pdf/2411.16959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ezra Ameperosa, Jeremy A. Collins, Mrinal Jain, Animesh Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16959">RoCoDA: Counterfactual Data Augmentation for Data-Efficient Robot Learning from Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning in robotics faces significant challenges in generalization due to the complexity of robotic environments and the high cost of data collection. We introduce RoCoDA, a novel method that unifies the concepts of invariance, equivariance, and causality within a single framework to enhance data augmentation for imitation learning. RoCoDA leverages causal invariance by modifying task-irrelevant subsets of the environment state without affecting the policy's output. Simultaneously, we exploit SE(3) equivariance by applying rigid body transformations to object poses and adjusting corresponding actions to generate synthetic demonstrations. We validate RoCoDA through extensive experiments on five robotic manipulation tasks, demonstrating improvements in policy performance, generalization, and sample efficiency compared to state-of-the-art data augmentation methods. Our policies exhibit robust generalization to unseen object poses, textures, and the presence of distractors. Furthermore, we observe emergent behavior such as re-grasping, indicating policies trained with RoCoDA possess a deeper understanding of task dynamics. By leveraging invariance, equivariance, and causality, RoCoDA provides a principled approach to data augmentation in imitation learning, bridging the gap between geometric symmetries and causal reasoning. Project Page: https://rocoda.github.io
<div id='section'>Paperid: <span id='pid'>1831, <a href='https://arxiv.org/pdf/2411.14883.pdf' target='_blank'>https://arxiv.org/pdf/2411.14883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Xu, Taiping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14883">Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain-invariant representation learning is a powerful method for domain generalization. Previous approaches face challenges such as high computational demands, training instability, and limited effectiveness with high-dimensional data, potentially leading to the loss of valuable features. To address these issues, we hypothesize that an ideal generalized representation should exhibit similar pattern responses within the same channel across cross-domain images. Based on this hypothesis, we use deep features from the source domain as queries, and deep features from the generated domain as keys and values. Through a cross-channel attention mechanism, the original deep features are reconstructed into robust regularization representations, forming an explicit constraint that guides the model to learn domain-invariant representations. Additionally, style augmentation is another common method. However, existing methods typically generate new styles through convex combinations of source domains, which limits the diversity of training samples by confining the generated styles to the original distribution. To overcome this limitation, we propose an Adaptive Feature Blending (AFB) method that generates out-of-distribution samples while exploring the in-distribution space, significantly expanding the domain range. Extensive experimental results demonstrate that our proposed methods achieve superior performance on two standard domain generalization benchmarks for medical image segmentation.
<div id='section'>Paperid: <span id='pid'>1832, <a href='https://arxiv.org/pdf/2410.16337.pdf' target='_blank'>https://arxiv.org/pdf/2410.16337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Deng, Baoxing Li, Xu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16337">Disambiguating Monocular Reconstruction of 3D Clothed Human with Spatial-Temporal Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D clothed humans from monocular camera data is highly challenging due to viewpoint limitations and image ambiguity. While implicit function-based approaches, combined with prior knowledge from parametric models, have made significant progress, there are still two notable problems. Firstly, the back details of human models are ambiguous due to viewpoint invisibility. The quality of the back details depends on the back normal map predicted by a convolutional neural network (CNN). However, the CNN lacks global information awareness for comprehending the back texture, resulting in excessively smooth back details. Secondly, a single image suffers from local ambiguity due to lighting conditions and body movement. However, implicit functions are highly sensitive to pixel variations in ambiguous regions. To address these ambiguities, we propose the Spatial-Temporal Transformer (STT) network for 3D clothed human reconstruction. A spatial transformer is employed to extract global information for normal map prediction. The establishment of global correlations facilitates the network in comprehending the holistic texture and shape of the human body. Simultaneously, to compensate for local ambiguity in images, a temporal transformer is utilized to extract temporal features from adjacent frames. The incorporation of temporal features can enhance the accuracy of input features in implicit networks. Furthermore, to obtain more accurate temporal features, joint tokens are employed to establish local correspondences between frames. Experimental results on the Adobe and MonoPerfCap datasets have shown that our method outperforms state-of-the-art methods and maintains robust generalization even under low-light outdoor conditions.
<div id='section'>Paperid: <span id='pid'>1833, <a href='https://arxiv.org/pdf/2410.07719.pdf' target='_blank'>https://arxiv.org/pdf/2410.07719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Xu, Xiao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07719">Understanding Adversarially Robust Generalization via Weight-Curvature Index</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite extensive research on adversarial examples, the underlying mechanisms of adversarially robust generalization, a critical yet challenging task for deep learning, remain largely unknown. In this work, we propose a novel perspective to decipher adversarially robust generalization through the lens of the Weight-Curvature Index (WCI). The proposed WCI quantifies the vulnerability of models to adversarial perturbations using the Frobenius norm of weight matrices and the trace of Hessian matrices. We prove generalization bounds based on PAC-Bayesian theory and second-order loss function approximations to elucidate the interplay between robust generalization gap, model parameters, and loss landscape curvature. Our theory and experiments show that WCI effectively captures the robust generalization performance of adversarially trained models. By offering a nuanced understanding of adversarial robustness based on the scale of model parameters and the curvature of the loss landscape, our work provides crucial insights for designing more resilient deep learning models, enhancing their reliability and security.
<div id='section'>Paperid: <span id='pid'>1834, <a href='https://arxiv.org/pdf/2410.07432.pdf' target='_blank'>https://arxiv.org/pdf/2410.07432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leyan Pan, Vijay Ganesh, Jacob Abernethy, Chris Esposo, Wenke Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07432">Can Transformers Reason Logically? A Study in SAT Solving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We formally study the logical reasoning capabilities of decoder-only Transformers in the context of the boolean satisfiability (SAT) problem. First, we prove by construction that decoder-only Transformers can decide 3-SAT, in a non-uniform model of computation, using backtracking and deduction via Chain-of-Thought (CoT). %We prove its correctness by showing trace equivalence to the well-known DPLL SAT-solving algorithm. Second, we implement our construction as a PyTorch model with a tool (PARAT) that we designed to empirically demonstrate its correctness and investigate its properties. Third, rather than \textit{programming} a transformer to reason, we evaluate empirically whether it can be \textit{trained} to do so by learning directly from algorithmic traces (``reasoning paths'') from our theoretical construction. The trained models demonstrate strong out-of-distribution generalization on problem sizes seen during training but has limited length generalization, which is consistent with the implications of our theoretical result
<div id='section'>Paperid: <span id='pid'>1835, <a href='https://arxiv.org/pdf/2409.16984.pdf' target='_blank'>https://arxiv.org/pdf/2409.16984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>P Aditya Sreekar, Sahil Verma, Suransh Chopra, Sarik Ghazarian, Abhishek Persad, Narayanan Sadagopan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16984">AXCEL: Automated eXplainable Consistency Evaluation using LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are widely used in both industry and academia for various tasks, yet evaluating the consistency of generated text responses continues to be a challenge. Traditional metrics like ROUGE and BLEU show a weak correlation with human judgment. More sophisticated metrics using Natural Language Inference (NLI) have shown improved correlations but are complex to implement, require domain-specific training due to poor cross-domain generalization, and lack explainability. More recently, prompt-based metrics using LLMs as evaluators have emerged; while they are easier to implement, they still lack explainability and depend on task-specific prompts, which limits their generalizability. This work introduces Automated eXplainable Consistency Evaluation using LLMs (AXCEL), a prompt-based consistency metric which offers explanations for the consistency scores by providing detailed reasoning and pinpointing inconsistent text spans. AXCEL is also a generalizable metric which can be adopted to multiple tasks without changing the prompt. AXCEL outperforms both non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting inconsistencies across summarization by 8.7%, free text generation by 6.2%, and data-to-text conversion tasks by 29.4%. We also evaluate the influence of underlying LLMs on prompt based metric performance and recalibrate the SOTA prompt-based metrics with the latest LLMs for fair comparison. Further, we show that AXCEL demonstrates strong performance using open source LLMs.
<div id='section'>Paperid: <span id='pid'>1836, <a href='https://arxiv.org/pdf/2409.16346.pdf' target='_blank'>https://arxiv.org/pdf/2409.16346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zhang, Roeland Wiersema, Juan Carrasquilla, Lukasz Cincio, Yong Baek Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16346">Scalable quantum dynamics compilation via quantum machine learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum dynamics compilation is an important task for improving quantum simulation efficiency: It aims to synthesize multi-qubit target dynamics into a circuit consisting of as few elementary gates as possible. Compared to deterministic methods such as Trotterization, variational quantum compilation (VQC) methods employ variational optimization to reduce gate costs while maintaining high accuracy. In this work, we explore the potential of a VQC scheme by making use of out-of-distribution generalization results in quantum machine learning (QML): By learning the action of a given many-body dynamics on a small data set of product states, we can obtain a unitary circuit that generalizes to highly entangled states such as the Haar random states. The efficiency in training allows us to use tensor network methods to compress such time-evolved product states by exploiting their low entanglement features. Our approach exceeds state-of-the-art compilation results in both system size and accuracy in one dimension ($1$D). For the first time, we extend VQC to systems on two-dimensional (2D) strips with a quasi-1D treatment, demonstrating a significant resource advantage over standard Trotterization methods, highlighting the method's promise for advancing quantum simulation tasks on near-term quantum processors.
<div id='section'>Paperid: <span id='pid'>1837, <a href='https://arxiv.org/pdf/2409.02146.pdf' target='_blank'>https://arxiv.org/pdf/2409.02146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dexin Duan, Peilin liu, Bingwei Hui, Fei Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02146">Brain-Inspired Online Adaptation for Remote Sensing with Spiking Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-device computing, or edge computing, is becoming increasingly important for remote sensing, particularly in applications like deep network-based perception on on-orbit satellites and unmanned aerial vehicles (UAVs). In these scenarios, two brain-like capabilities are crucial for remote sensing models: (1) high energy efficiency, allowing the model to operate on edge devices with limited computing resources, and (2) online adaptation, enabling the model to quickly adapt to environmental variations, weather changes, and sensor drift. This work addresses these needs by proposing an online adaptation framework based on spiking neural networks (SNNs) for remote sensing. Starting with a pretrained SNN model, we design an efficient, unsupervised online adaptation algorithm, which adopts an approximation of the BPTT algorithm and only involves forward-in-time computation that significantly reduces the computational complexity of SNN adaptation learning. Besides, we propose an adaptive activation scaling scheme to boost online SNN adaptation performance, particularly in low time-steps. Furthermore, for the more challenging remote sensing detection task, we propose a confidence-based instance weighting scheme, which substantially improves adaptation performance in the detection task. To our knowledge, this work is the first to address the online adaptation of SNNs. Extensive experiments on seven benchmark datasets across classification, segmentation, and detection tasks demonstrate that our proposed method significantly outperforms existing domain adaptation and domain generalization approaches under varying weather conditions. The proposed method enables energy-efficient and fast online adaptation on edge devices, and has much potential in applications such as remote perception on on-orbit satellites and UAV.
<div id='section'>Paperid: <span id='pid'>1838, <a href='https://arxiv.org/pdf/2408.12575.pdf' target='_blank'>https://arxiv.org/pdf/2408.12575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonyo Musabini, Ivan Novikov, Sana Soula, Christel Leonet, Lihao Wang, Rachid Benmokhtar, Fabian Burger, Thomas Boulay, Xavier Perrotton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12575">Enhanced Parking Perception by Multi-Task Fisheye Cross-view Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current parking area perception algorithms primarily focus on detecting vacant slots within a limited range, relying on error-prone homographic projection for both labeling and inference. However, recent advancements in Advanced Driver Assistance System (ADAS) require interaction with end-users through comprehensive and intelligent Human-Machine Interfaces (HMIs). These interfaces should present a complete perception of the parking area going from distinguishing vacant slots' entry lines to the orientation of other parked vehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT F-CVT), which leverages features from a four-camera fisheye Surround-view Camera System (SVCS) with multihead attentions to create a detailed Bird-Eye View (BEV) grid feature map. Features are processed by both a segmentation decoder and a Polygon-Yolo based object detection decoder for parking slots and vehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects within a 25m x 25m real open-road scenes with an average error of only 20 cm. Our larger model achieves an F-1 score of 0.89. Moreover the smaller model operates at 16 fps on an Nvidia Jetson Orin embedded board, with similar detection results to the larger one. MT F-CVT demonstrates robust generalization capability across different vehicles and camera rig configurations. A demo video from an unseen vehicle and camera rig is available at: https://streamable.com/jjw54x.
<div id='section'>Paperid: <span id='pid'>1839, <a href='https://arxiv.org/pdf/2408.09503.pdf' target='_blank'>https://arxiv.org/pdf/2408.09503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajun Song, Zhuoyan Xu, Yiqiao Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09503">Out-of-distribution generalization via composition: a lens through induction heads in Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) such as GPT-4 sometimes appear to be creative, solving novel tasks often with a few demonstrations in the prompt. These tasks require the models to generalize on distributions different from those from training data -- which is known as out-of-distribution (OOD) generalization. Despite the tremendous success of LLMs, how they approach OOD generalization remains an open and underexplored question. We examine OOD generalization in settings where instances are generated according to hidden rules, including in-context learning with symbolic reasoning. Models are required to infer the hidden rules behind input prompts without any fine-tuning.
  We empirically examined the training dynamics of Transformers on a synthetic example and conducted extensive experiments on a variety of pretrained LLMs, focusing on a type of components known as induction heads. We found that OOD generalization and composition are tied together -- models can learn rules by composing two self-attention layers, thereby achieving OOD generalization. Furthermore, a shared latent subspace in the embedding (or feature) space acts as a bridge for composition by aligning early layers and later layers, which we refer to as the common bridge representation hypothesis.
<div id='section'>Paperid: <span id='pid'>1840, <a href='https://arxiv.org/pdf/2407.18428.pdf' target='_blank'>https://arxiv.org/pdf/2407.18428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gina Wong, Joshua Gleason, Rama Chellappa, Yoav Wald, Anqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18428">Weighted Risk Invariance: Domain Generalization under Invariant Feature Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning models whose predictions are invariant under multiple environments is a promising approach for out-of-distribution generalization. Such models are trained to extract features $X_{\text{inv}}$ where the conditional distribution $Y \mid X_{\text{inv}}$ of the label given the extracted features does not change across environments. Invariant models are also supposed to generalize to shifts in the marginal distribution $p(X_{\text{inv}})$ of the extracted features $X_{\text{inv}}$, a type of shift we call an $\textit{invariant covariate shift}$. However, we show that proposed methods for learning invariant models underperform under invariant covariate shift, either failing to learn invariant models$\unicode{x2014}$even for data generated from simple and well-studied linear-Gaussian models$\unicode{x2014}$or having poor finite-sample performance. To alleviate these problems, we propose $\textit{weighted risk invariance}$ (WRI). Our framework is based on imposing invariance of the loss across environments subject to appropriate reweightings of the training examples. We show that WRI provably learns invariant models, i.e. discards spurious correlations, in linear-Gaussian settings. We propose a practical algorithm to implement WRI by learning the density $p(X_{\text{inv}})$ and the model parameters simultaneously, and we demonstrate empirically that WRI outperforms previous invariant learning methods under invariant covariate shift.
<div id='section'>Paperid: <span id='pid'>1841, <a href='https://arxiv.org/pdf/2407.14792.pdf' target='_blank'>https://arxiv.org/pdf/2407.14792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Radwan, Mohamed S. Shehata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14792">FedPartWhole: Federated domain generalization via consistent part-whole hierarchies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Domain Generalization (FedDG), aims to tackle the challenge of generalizing to unseen domains at test time while catering to the data privacy constraints that prevent centralized data storage from different domains originating at various clients. Existing approaches can be broadly categorized into four groups: domain alignment, data manipulation, learning strategies, and optimization of model aggregation weights. This paper proposes a novel approach to Federated Domain Generalization that tackles the problem from the perspective of the backbone model architecture. The core principle is that objects, even under substantial domain shifts and appearance variations, maintain a consistent hierarchical structure of parts and wholes. For instance, a photograph and a sketch of a dog share the same hierarchical organization, consisting of a head, body, limbs, and so on. The introduced architecture explicitly incorporates a feature representation for the image parse tree. To the best of our knowledge, this is the first work to tackle Federated Domain Generalization from a model architecture standpoint. Our approach outperforms a convolutional architecture of comparable size by over 12\%, despite utilizing fewer parameters. Additionally, it is inherently interpretable, contrary to the black-box nature of CNNs, which fosters trust in its predictions, a crucial asset in federated learning.
<div id='section'>Paperid: <span id='pid'>1842, <a href='https://arxiv.org/pdf/2406.12258.pdf' target='_blank'>https://arxiv.org/pdf/2406.12258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyojin Kim, Jiyoon Lee, Yonghyun Jeong, Haneol Jang, YoungJoon Yoo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12258">Advancing Cross-Domain Generalizability in Face Anti-Spoofing: Insights, Design, and Metrics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel perspective for enhancing anti-spoofing performance in zero-shot data domain generalization. Unlike traditional image classification tasks, face anti-spoofing datasets display unique generalization characteristics, necessitating novel zero-shot data domain generalization. One step forward to the previous frame-wise spoofing prediction, we introduce a nuanced metric calculation that aggregates frame-level probabilities for a video-wise prediction, to tackle the gap between the reported frame-wise accuracy and instability in real-world use-case. This approach enables the quantification of bias and variance in model predictions, offering a more refined analysis of model generalization. Our investigation reveals that simply scaling up the backbone of models does not inherently improve the mentioned instability, leading us to propose an ensembled backbone method from a Bayesian perspective. The probabilistically ensembled backbone both improves model robustness measured from the proposed metric and spoofing accuracy, and also leverages the advantages of measuring uncertainty, allowing for enhanced sampling during training that contributes to model generalization across new datasets. We evaluate the proposed method from the benchmark OMIC dataset and also the public CelebA-Spoof and SiW-Mv2. Our final model outperforms existing state-of-the-art methods across the datasets, showcasing advancements in Bias, Variance, HTER, and AUC metrics.
<div id='section'>Paperid: <span id='pid'>1843, <a href='https://arxiv.org/pdf/2406.10617.pdf' target='_blank'>https://arxiv.org/pdf/2406.10617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Akhavan Anvari, Rojina Kashefi, Vahid Reza Khazaie, Mohammad Khalooei, Mohammad Sabokrou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10617">Enhancing Anomaly Detection Generalization through Knowledge Exposure: The Dual Effects of Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection involves identifying instances within a dataset that deviate from the norm and occur infrequently. Current benchmarks tend to favor methods biased towards low diversity in normal data, which does not align with real-world scenarios. Despite advancements in these benchmarks, contemporary anomaly detection methods often struggle with out-of-distribution generalization, particularly in classifying samples with subtle transformations during testing. These methods typically assume that normal samples during test time have distributions very similar to those in the training set, while anomalies are distributed much further away. However, real-world test samples often exhibit various levels of distribution shift while maintaining semantic consistency. Therefore, effectively generalizing to samples that have undergone semantic-preserving transformations, while accurately detecting normal samples whose semantic meaning has changed after transformation as anomalies, is crucial for the trustworthiness and reliability of a model. For example, although it is clear that rotation shifts the meaning for a car in the context of anomaly detection but preserves the meaning for a bird, current methods are likely to detect both as abnormal. This complexity underscores the necessity for dynamic learning procedures rooted in the intrinsic concept of outliers. To address this issue, we propose new testing protocols and a novel method called Knowledge Exposure (KE), which integrates external knowledge to comprehend concept dynamics and differentiate transformations that induce semantic shifts. This approach enhances generalization by utilizing insights from a pre-trained CLIP model to evaluate the significance of anomalies for each concept. Evaluation on CIFAR-10, CIFAR-100, and SVHN with the new protocols demonstrates superior performance compared to previous methods.
<div id='section'>Paperid: <span id='pid'>1844, <a href='https://arxiv.org/pdf/2406.01825.pdf' target='_blank'>https://arxiv.org/pdf/2406.01825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunni Qu, James Wellnitz, Dzung Dinh, Bhargav Vaduri, Alexander Tropsha, Junier Oliva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01825">EXPLOR: Extrapolatory Pseudo-Label Matching for Out-of-distribution Uncertainty Based Rejection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>EXPLOR is a novel framework that utilizes support-expanding, extrapolatory pseudo-labeling to improve prediction and uncertainty-based rejection on out-of-distribution (OOD) points. EXPLOR utilizes a diverse set of base models as pseudo-labelers on the expansive augmented data to improve OOD performance through multiple MLP heads (one per base model) with shared embedding trained with a novel per-head matching loss. Unlike prior methods that rely on modality-specific augmentations or assume access to OOD data, EXPLOR introduces extrapolatory pseudo-labeling on latent-space augmentations, enabling robust OOD generalization with any real-valued vector data. In contrast to prior modality-agnostic methods with neural backbones, EXPLOR is model-agnostic, working effectively with methods from simple tree-based models to complex OOD generalization models. We demonstrate that EXPLOR achieves superior performance compared to state-of-the-art methods on diverse datasets in single-source domain generalization settings.
<div id='section'>Paperid: <span id='pid'>1845, <a href='https://arxiv.org/pdf/2405.06578.pdf' target='_blank'>https://arxiv.org/pdf/2405.06578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathan Ludlow, Yiwei Lyu, John Dolan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06578">Hierarchical Learned Risk-Aware Planning Framework for Human Driving Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to modeling human driving behavior, designed for use in evaluating autonomous vehicle control systems in a simulation environments. Our methodology leverages a hierarchical forward-looking, risk-aware estimation framework with learned parameters to generate human-like driving trajectories, accommodating multiple driver levels determined by model parameters. This approach is grounded in multimodal trajectory prediction, using a deep neural network with LSTM-based social pooling to predict the trajectories of surrounding vehicles. These trajectories are used to compute forward-looking risk assessments along the ego vehicle's path, guiding its navigation. Our method aims to replicate human driving behaviors by learning parameters that emulate human decision-making during driving. We ensure that our model exhibits robust generalization capabilities by conducting simulations, employing real-world driving data to validate the accuracy of our approach in modeling human behavior. The results reveal that our model effectively captures human behavior, showcasing its versatility in modeling human drivers in diverse highway scenarios.
<div id='section'>Paperid: <span id='pid'>1846, <a href='https://arxiv.org/pdf/2404.14701.pdf' target='_blank'>https://arxiv.org/pdf/2404.14701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Feng, Rui Yao, Stephane Hess, Ricardo A. Daziano, Timothy Brathwaite, Joan Walker, Shenhao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14701">Deep neural networks for choice analysis: Enhancing behavioral regularity with gradient regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) frequently present behaviorally irregular patterns, significantly limiting their practical potentials and theoretical validity in travel behavior modeling. This study proposes strong and weak behavioral regularities as novel metrics to evaluate the monotonicity of individual demand functions (known as the "law of demand"), and further designs a constrained optimization framework with six gradient regularizers to enhance DNNs' behavioral regularity. The proposed framework is applied to travel survey data from Chicago and London to examine the trade-off between predictive power and behavioral regularity for large vs. small sample scenarios and in-domain vs. out-of-domain generalizations. The results demonstrate that, unlike models with strong behavioral foundations such as the multinomial logit, the benchmark DNNs cannot guarantee behavioral regularity. However, gradient regularization (GR) increases DNNs' behavioral regularity by around 6 percentage points (pp) while retaining their relatively high predictive power. In the small sample scenario, GR is more effective than in the large sample scenario, simultaneously improving behavioral regularity by about 20 pp and log-likelihood by around 1.7%. Comparing with the in-domain generalization of DNNs, GR works more effectively in out-of-domain generalization: it drastically improves the behavioral regularity of poorly performing benchmark DNNs by around 65 pp, indicating the criticality of behavioral regularization for enhancing model transferability and application in forecasting. Moreover, the proposed framework is applicable to other NN-based choice models such as TasteNets. Future studies could use behavioral regularity as a metric along with log-likelihood in evaluating travel demand models, and investigate other methods to further enhance behavioral regularity when adopting complex machine learning models.
<div id='section'>Paperid: <span id='pid'>1847, <a href='https://arxiv.org/pdf/2404.11341.pdf' target='_blank'>https://arxiv.org/pdf/2404.11341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan L. Gamella, Jonas Peters, Peter BÃ¼hlmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11341">The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In some fields of AI, machine learning and statistics, the validation of new methods and algorithms is often hindered by the scarcity of suitable real-world datasets. Researchers must often turn to simulated data, which yields limited information about the applicability of the proposed methods to real problems. As a step forward, we have constructed two devices that allow us to quickly and inexpensively produce large datasets from non-trivial but well-understood physical systems. The devices, which we call causal chambers, are computer-controlled laboratories that allow us to manipulate and measure an array of variables from these physical systems, providing a rich testbed for algorithms from a variety of fields. We illustrate potential applications through a series of case studies in fields such as causal discovery, out-of-distribution generalization, change point detection, independent component analysis, and symbolic regression. For applications to causal inference, the chambers allow us to carefully perform interventions. We also provide and empirically validate a causal model of each chamber, which can be used as ground truth for different tasks. All hardware and software is made open source, and the datasets are publicly available at causalchamber.org or through the Python package causalchamber.
<div id='section'>Paperid: <span id='pid'>1848, <a href='https://arxiv.org/pdf/2404.00626.pdf' target='_blank'>https://arxiv.org/pdf/2404.00626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyoung Oh, Duhyun Kim, Jae-Young Sim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00626">Domain Generalizable Person Search Using Unreal Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collecting and labeling real datasets to train the person search networks not only requires a lot of time and effort, but also accompanies privacy issues. The weakly-supervised and unsupervised domain adaptation methods have been proposed to alleviate the labeling burden for target datasets, however, their generalization capability is limited. We introduce a novel person search method based on the domain generalization framework, that uses an automatically labeled unreal dataset only for training but is applicable to arbitrary unseen real datasets. To alleviate the domain gaps when transferring the knowledge from the unreal source dataset to the real target datasets, we estimate the fidelity of person instances which is then used to train the end-to-end network adaptively. Moreover, we devise a domain-invariant feature learning scheme to encourage the network to suppress the domain-related features. Experimental results demonstrate that the proposed method provides the competitive performance to existing person search methods even though it is applicable to arbitrary unseen datasets without any prior knowledge and re-training burdens.
<div id='section'>Paperid: <span id='pid'>1849, <a href='https://arxiv.org/pdf/2403.18295.pdf' target='_blank'>https://arxiv.org/pdf/2403.18295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongwei Zhou, Tiejun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18295">Dual Instruction Tuning with Large Language Models for Mathematical Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements highlight the success of instruction tuning with large language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual instruction tuning strategy to meticulously model mathematical reasoning from both forward and reverse directions. This involves introducing the Intermediate Reasoning State Prediction task (forward reasoning) and the Instruction Reconstruction task (reverse reasoning) to enhance the LLMs' understanding and execution of instructions. Training instances for these tasks are constructed based on existing mathematical instruction tuning datasets. Subsequently, LLMs undergo multi-task fine-tuning using both existing mathematical instructions and the newly created data. Comprehensive experiments validate the effectiveness and domain generalization of the dual instruction tuning strategy across various mathematical reasoning tasks.
<div id='section'>Paperid: <span id='pid'>1850, <a href='https://arxiv.org/pdf/2403.09101.pdf' target='_blank'>https://arxiv.org/pdf/2403.09101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daiwei Yu, Zhuorong Li, Lina Wei, Canghong Jin, Yun Zhang, Sixian Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09101">Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial training (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against adversarial attacks. However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in adversarial robustness between the training and testing curves. In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm. As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT. Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers. Empirical results demonstrate that our method can simultaneously boost the standard accuracy and robust performance across multiple benchmark datasets, attack types, and architectures. In addition, we also provide a set of analyses from the perspectives of information theory to dive into our method and suggest the importance of soft labels for robust generalization.
<div id='section'>Paperid: <span id='pid'>1851, <a href='https://arxiv.org/pdf/2403.04207.pdf' target='_blank'>https://arxiv.org/pdf/2403.04207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyudong Kim, Mehdi Ghasemi, Soroush Heidari, Seungryong Kim, Young Geun Kim, Sarma Vrudhula, Carole-Jean Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04207">HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device. In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations. Such fragmentation introduces a new type of data heterogeneity in FL, namely \textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations. In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance. We collect a dataset using heterogeneous devices with variations across vendors and performance tiers. By using this dataset, we demonstrate that \textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL. To address these challenges, we propose HeteroSwitch, which adaptively adopts generalization techniques (i.e., ISP transformation and SWAD) depending on the level of bias caused by varying HW and SW configurations. In our evaluation with a realistic FL dataset (FLAIR), HeteroSwitch reduces the variance of averaged precision by 6.3\% across device types.
<div id='section'>Paperid: <span id='pid'>1852, <a href='https://arxiv.org/pdf/2403.01622.pdf' target='_blank'>https://arxiv.org/pdf/2403.01622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Q. Tram, Nolan B. Gutierrez, William J. Beksi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01622">A Human-Centered Approach for Bootstrapping Causal Graph Creation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal inference, a cornerstone in disciplines such as economics, genomics, and medicine, is increasingly being recognized as fundamental to advancing the field of robotics. In particular, the ability to reason about cause and effect from observational data is crucial for robust generalization in robotic systems. However, the construction of a causal graphical model, a mechanism for representing causal relations, presents an immense challenge. Currently, a nuanced grasp of causal inference, coupled with an understanding of causal relationships, must be manually programmed into a causal graphical model. To address this difficulty, we present initial results towards a human-centered augmented reality framework for creating causal graphical models. Concretely, our system bootstraps the causal discovery process by involving humans in selecting variables, establishing relationships, performing interventions, generating counterfactual explanations, and evaluating the resulting causal graph at every step. We highlight the potential of our framework via a physical robot manipulator on a pick-and-place task.
<div id='section'>Paperid: <span id='pid'>1853, <a href='https://arxiv.org/pdf/2402.08971.pdf' target='_blank'>https://arxiv.org/pdf/2402.08971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minho Lee, Junghyun Min, Woochul Lee, Yeonsoo Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08971">Structured Language Generation Model for Robust Structure Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous work in structured prediction (e.g. NER, information extraction) using single model make use of explicit dataset information, which helps boost in-distribution performance but is orthogonal to robust generalization in real-world situations. To overcome this limitation, we propose the Structured Language Generation Model (SLGM), a framework that reduces sequence-to-sequence problems to classification problems via methodologies in loss calibration and decoding method. Our experimental results show that SLGM is able to maintain performance without explicit dataset information, follow and potentially replace dataset-specific fine-tuning.
<div id='section'>Paperid: <span id='pid'>1854, <a href='https://arxiv.org/pdf/2402.01555.pdf' target='_blank'>https://arxiv.org/pdf/2402.01555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Adebayo, Joost C. Dessing, SeÃ¡n McLoone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01555">SLYKLatent: A Learning Framework for Gaze Estimation Using Deep Facial Feature Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this research, we present SLYKLatent, a novel approach for enhancing gaze estimation by addressing appearance instability challenges in datasets due to aleatoric uncertainties, covariant shifts, and test domain generalization. SLYKLatent utilizes Self-Supervised Learning for initial training with facial expression datasets, followed by refinement with a patch-based tri-branch network and an inverse explained variance-weighted training loss function. Our evaluation on benchmark datasets achieves a 10.9% improvement on Gaze360, supersedes top MPIIFaceGaze results with 3.8%, and leads on a subset of ETH-XGaze by 11.6%, surpassing existing methods by significant margins. Adaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation studies confirm the effectiveness of SLYKLatent's novel components.
<div id='section'>Paperid: <span id='pid'>1855, <a href='https://arxiv.org/pdf/2401.13624.pdf' target='_blank'>https://arxiv.org/pdf/2401.13624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongjie Shi, Fanghui Liu, Yuan Cao, Johan A. K. Suykens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13624">Can overfitted deep neural networks in adversarial training generalize? -- An approximation viewpoint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial training is a widely used method to improve the robustness of deep neural networks (DNNs) over adversarial perturbations. However, it is empirically observed that adversarial training on over-parameterized networks often suffers from the \textit{robust overfitting}: it can achieve almost zero adversarial training error while the robust generalization performance is not promising. In this paper, we provide a theoretical understanding of the question of whether overfitted DNNs in adversarial training can generalize from an approximation viewpoint. Specifically, our main results are summarized into three folds: i) For classification, we prove by construction the existence of infinitely many adversarial training classifiers on over-parameterized DNNs that obtain arbitrarily small adversarial training error (overfitting), whereas achieving good robust generalization error under certain conditions concerning the data quality, well separated, and perturbation level. ii) Linear over-parameterization (meaning that the number of parameters is only slightly larger than the sample size) is enough to ensure such existence if the target function is smooth enough. iii) For regression, our results demonstrate that there also exist infinitely many overfitted DNNs with linear over-parameterization in adversarial training that can achieve almost optimal rates of convergence for the standard generalization error. Overall, our analysis points out that robust overfitting can be avoided but the required model capacity will depend on the smoothness of the target function, while a robust generalization gap is inevitable. We hope our analysis will give a better understanding of the mathematical foundations of robustness in DNNs from an approximation view.
<div id='section'>Paperid: <span id='pid'>1856, <a href='https://arxiv.org/pdf/2401.11734.pdf' target='_blank'>https://arxiv.org/pdf/2401.11734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wu, Fengmao Lv, Chenglizhao Chen, Aimin Hao, Shuo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11734">Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Colorectal polyp segmentation (CPS), an essential problem in medical image analysis, has garnered growing research attention. Recently, the deep learning-based model completely overwhelmed traditional methods in the field of CPS, and more and more deep CPS methods have emerged, bringing the CPS into the deep learning era. To help the researchers quickly grasp the main techniques, datasets, evaluation metrics, challenges, and trending of deep CPS, this paper presents a systematic and comprehensive review of deep-learning-based CPS methods from 2014 to 2023, a total of 115 technical papers. In particular, we first provide a comprehensive review of the current deep CPS with a novel taxonomy, including network architectures, level of supervision, and learning paradigm. More specifically, network architectures include eight subcategories, the level of supervision comprises six subcategories, and the learning paradigm encompasses 12 subcategories, totaling 26 subcategories. Then, we provided a comprehensive analysis the characteristics of each dataset, including the number of datasets, annotation types, image resolution, polyp size, contrast values, and polyp location. Following that, we summarized CPS's commonly used evaluation metrics and conducted a detailed analysis of 40 deep SOTA models, including out-of-distribution generalization and attribute-based performance analysis. Finally, we discussed deep learning-based CPS methods' main challenges and opportunities.
<div id='section'>Paperid: <span id='pid'>1857, <a href='https://arxiv.org/pdf/2401.08472.pdf' target='_blank'>https://arxiv.org/pdf/2401.08472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lidong Zeng, Zhedong Zheng, Yinwei Wei, Tat-seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08472">Instilling Multi-round Thinking to Text-guided Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper delves into the text-guided image editing task, focusing on modifying a reference image according to user-specified textual feedback to embody specific attributes. Despite recent advancements, a persistent challenge remains that the single-round generation often overlooks crucial details, particularly in the realm of fine-grained changes like shoes or sleeves. This issue compounds over multiple rounds of interaction, severely limiting customization quality. In an attempt to address this challenge, we introduce a new self-supervised regularization, \ie, multi-round regularization, which is compatible with existing methods. Specifically, the multi-round regularization encourages the model to maintain consistency across different modification orders. It builds upon the observation that the modification order generally should not affect the final result. Different from traditional one-round generation, the mechanism underpinning the proposed method is the error amplification of initially minor inaccuracies in capturing intricate details. Qualitative and quantitative experiments affirm that the proposed method achieves high-fidelity editing quality, especially the local modification, in both single-round and multiple-round generation, while also showcasing robust generalization to irregular text inputs. The effectiveness of our semantic alignment with textual feedback is further substantiated by the retrieval improvements on FahisonIQ and Fashion200k.
<div id='section'>Paperid: <span id='pid'>1858, <a href='https://arxiv.org/pdf/2312.17300.pdf' target='_blank'>https://arxiv.org/pdf/2312.17300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Padmaksha Roy, Tyler Cody, Himanshu Singhal, Kevin Choi, Ming Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17300">Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization focuses on leveraging knowledge from multiple related domains with ample training data and labels to enhance inference on unseen in-distribution (IN) and out-of-distribution (OOD) domains. In our study, we introduce a two-phase representation learning technique using multi-task learning. This approach aims to cultivate a latent space from features spanning multiple domains, encompassing both native and cross-domains, to amplify generalization to IN and OOD territories. Additionally, we attempt to disentangle the latent space by minimizing the mutual information between the prior and latent space, effectively de-correlating spurious feature correlations. Collectively, the joint optimization will facilitate domain-invariant feature learning. We assess the model's efficacy across multiple cybersecurity datasets, using standard classification metrics on both unseen IN and OOD sets, and juxtapose the results with contemporary domain generalization methods.
<div id='section'>Paperid: <span id='pid'>1859, <a href='https://arxiv.org/pdf/2312.09461.pdf' target='_blank'>https://arxiv.org/pdf/2312.09461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-Young Kim, Dong-Kyun Han, Seo-Hyeon Park, Geun-Deok Jang, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09461">Improving Generalization of Drowsiness State Classification by Domain-Specific Normalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Abnormal driver states, particularly have been major concerns for road safety, emphasizing the importance of accurate drowsiness detection to prevent accidents. Electroencephalogram (EEG) signals are recognized for their effectiveness in monitoring a driver's mental state by monitoring brain activities. However, the challenge lies in the requirement for prior calibration due to the variation of EEG signals among and within individuals. The necessity of calibration has made the brain-computer interface (BCI) less accessible. We propose a practical generalized framework for classifying driver drowsiness states to improve accessibility and convenience. We separate the normalization process for each driver, treating them as individual domains. The goal of developing a general model is similar to that of domain generalization. The framework considers the statistics of each domain separately since they vary among domains. We experimented with various normalization methods to enhance the ability to generalize across subjects, i.e. the model's generalization performance of unseen domains. The experiments showed that applying individual domain-specific normalization yielded an outstanding improvement in generalizability. Furthermore, our framework demonstrates the potential and accessibility by removing the need for calibration in BCI applications.
<div id='section'>Paperid: <span id='pid'>1860, <a href='https://arxiv.org/pdf/2311.18773.pdf' target='_blank'>https://arxiv.org/pdf/2311.18773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zitian Tang, Rohan Myer Krishnan, Zhiqiu Yu, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18773">Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding in Novel Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from (procedural) videos has increasingly served as a pathway for embodied agents to acquire skills from human demonstrations. To do this, video understanding models must be able to obtain structured understandings, such as the temporal segmentation of a demonstration into sequences of actions and skills, and to generalize the understandings to novel environments, tasks, and problem domains. In pursuit of this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1) step recognition and (2) video question answering, over a dataset of temporally segmented and labeled tasks in International Space Station spacewalk recordings. In tandem, the two tasks quantify a model's ability to: (1) generalize to novel domains; (2) utilize long temporal context and multimodal (e.g. visual and speech) information. Our extensive experimental analysis highlights the challenges of Spacewalk-18, but also suggests best practices for domain generalization and long-form understanding. Notably, we discover a promising adaptation via summarization technique that leads to significant performance improvement without model fine-tuning. The Spacewalk-18 benchmark is released at https://brown-palm.github.io/Spacewalk-18/.
<div id='section'>Paperid: <span id='pid'>1861, <a href='https://arxiv.org/pdf/2311.17593.pdf' target='_blank'>https://arxiv.org/pdf/2311.17593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rudra P. K. Poudel, Harit Pandya, Chao Zhang, Roberto Cipolla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17593">LanGWM: Language Grounded World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep reinforcement learning have showcased its potential in tackling complex tasks. However, experiments on visual control tasks have revealed that state-of-the-art reinforcement learning models struggle with out-of-distribution generalization. Conversely, expressing higher-level concepts and global contexts is relatively easy using language.
  Building upon recent success of the large language models, our main objective is to improve the state abstraction technique in reinforcement learning by leveraging language for robust action selection. Specifically, we focus on learning language-grounded visual features to enhance the world model learning, a model-based reinforcement learning technique.
  To enforce our hypothesis explicitly, we mask out the bounding boxes of a few objects in the image observation and provide the text prompt as descriptions for these masked objects. Subsequently, we predict the masked objects along with the surrounding regions as pixel reconstruction, similar to the transformer-based masked autoencoder approach.
  Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art performance in out-of-distribution test at the 100K interaction steps benchmarks of iGibson point navigation tasks. Furthermore, our proposed technique of explicit language-grounded visual representation learning has the potential to improve models for human-robot interaction because our extracted visual features are language grounded.
<div id='section'>Paperid: <span id='pid'>1862, <a href='https://arxiv.org/pdf/2311.16766.pdf' target='_blank'>https://arxiv.org/pdf/2311.16766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anuj Srivastava, Karm Patel, Pradeep Shenoy, Devarajan Sridharan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16766">Rescuing referral failures during automated diagnosis of domain-shifted medical images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of deep learning models deployed in the real world depends critically on their ability to generalize well across diverse data domains. Here, we address a fundamental challenge with selective classification during automated diagnosis with domain-shifted medical images. In this scenario, models must learn to avoid making predictions when label confidence is low, especially when tested with samples far removed from the training set (covariate shift). Such uncertain cases are typically referred to the clinician for further analysis and evaluation. Yet, we show that even state-of-the-art domain generalization approaches fail severely during referral when tested on medical images acquired from a different demographic or using a different technology. We examine two benchmark diagnostic medical imaging datasets exhibiting strong covariate shifts: i) diabetic retinopathy prediction with retinal fundus images and ii) multilabel disease prediction with chest X-ray images. We show that predictive uncertainty estimates do not generalize well under covariate shifts leading to non-monotonic referral curves, and severe drops in performance (up to 50%) at high referral rates (>70%). We evaluate novel combinations of robust generalization and post hoc referral approaches, that rescue these failures and achieve significant performance improvements, typically >10%, over baseline methods. Our study identifies a critical challenge with referral in domain-shifted medical images and finds key applications in reliable, automated disease diagnosis.
<div id='section'>Paperid: <span id='pid'>1863, <a href='https://arxiv.org/pdf/2311.14533.pdf' target='_blank'>https://arxiv.org/pdf/2311.14533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Altozano, Maria Eleonora Minissi, Mariano AlcaÃ±iz, Javier MarÃ­n-Morales
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14533">Introducing 3DCNN ResNets for ASD full-body kinematic assessment: a comparison with hand-crafted features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autism Spectrum Disorder (ASD) is characterized by challenges in social communication and restricted patterns, with motor abnormalities gaining traction for early detection. However, kinematic analysis in ASD is limited, often lacking robust validation and relying on hand-crafted features for single tasks, leading to inconsistencies across studies. End-to-end models have emerged as promising methods to overcome the need for feature engineering. Our aim is to propose a newly adapted 3DCNN ResNet from and compare it to widely used hand-crafted features for motor ASD assessment. Specifically, we developed a virtual reality environment with multiple motor tasks and trained models using both approaches. We prioritized a reliable validation framework with repeated cross-validation. Results show the proposed model achieves a maximum accuracy of 85$\pm$3%, outperforming state-of-the-art end-to-end models with short 1-to-3 minute samples. Our comparative analysis with hand-crafted features shows feature-engineered models outperformed our end-to-end model in certain tasks. However, our end-to-end model achieved a higher mean AUC of 0.80$\pm$0.03. Additionally, statistical differences were found in model variance, with our end-to-end model providing more consistent results with less variability across all VR tasks, demonstrating domain generalization and reliability. These findings show that end-to-end models enable less variable and context-independent ASD classification without requiring domain knowledge or task specificity. However, they also recognize the effectiveness of hand-crafted features in specific task scenarios.
<div id='section'>Paperid: <span id='pid'>1864, <a href='https://arxiv.org/pdf/2311.05418.pdf' target='_blank'>https://arxiv.org/pdf/2311.05418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eran Zvuloni, Leo Anthony Celi, Joachim A. Behar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05418">Generalization in medical AI: a perspective on developing scalable models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The scientific community is increasingly recognizing the importance of generalization in medical AI for translating research into practical clinical applications. A three-level scale is introduced to characterize out-of-distribution generalization performance of medical AI models. This scale addresses the diversity of real-world medical scenarios as well as whether target domain data and labels are available for model recalibration. It serves as a tool to help researchers characterize their development settings and determine the best approach to tackling the challenge of out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1865, <a href='https://arxiv.org/pdf/2310.17255.pdf' target='_blank'>https://arxiv.org/pdf/2310.17255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chamuditha Jayanga Galappaththige, Gayal Kuruppu, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17255">Generalizing to Unseen Domains in Diabetic Retinopathy Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic retinopathy (DR) is caused by long-standing diabetes and is among the fifth leading cause for visual impairments. The process of early diagnosis and treatments could be helpful in curing the disease, however, the detection procedure is rather challenging and mostly tedious. Therefore, automated diabetic retinopathy classification using deep learning techniques has gained interest in the medical imaging community. Akin to several other real-world applications of deep learning, the typical assumption of i.i.d data is also violated in DR classification that relies on deep learning. Therefore, developing DR classification methods robust to unseen distributions is of great value. In this paper, we study the problem of generalizing a model to unseen distributions or domains (a.k.a domain generalization) in DR classification. To this end, we propose a simple and effective domain generalization (DG) approach that achieves self-distillation in vision transformers (ViT) via a novel prediction softening mechanism. This prediction softening is an adaptive convex combination one-hot labels with the model's own knowledge. We perform extensive experiments on challenging open-source DR classification datasets under both multi-source and single-source DG settings with three different ViT backbones to establish the efficacy and applicability of our approach against competing methods. For the first time, we report the performance of several state-of-the-art DG methods on open-source DR classification datasets after conducting thorough experiments. Finally, our method is also capable of delivering improved calibration performance than other methods, showing its suitability for safety-critical applications, including healthcare. We hope that our contributions would investigate more DG research across the medical imaging community.
<div id='section'>Paperid: <span id='pid'>1866, <a href='https://arxiv.org/pdf/2310.11758.pdf' target='_blank'>https://arxiv.org/pdf/2310.11758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zong-Wei Hong, Yu-Chen Lin, Hsuan-Tung Liu, Yi-Ren Yeh, Chu-Song Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11758">Domain-Generalized Face Anti-Spoofing with Unknown Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although face anti-spoofing (FAS) methods have achieved remarkable performance on specific domains or attack types, few studies have focused on the simultaneous presence of domain changes and unknown attacks, which is closer to real application scenarios. To handle domain-generalized unknown attacks, we introduce a new method, DGUA-FAS, which consists of a Transformer-based feature extractor and a synthetic unknown attack sample generator (SUASG). The SUASG network simulates unknown attack samples to assist the training of the feature extractor. Experimental results show that our method achieves superior performance on domain generalization FAS with known or unknown attacks.
<div id='section'>Paperid: <span id='pid'>1867, <a href='https://arxiv.org/pdf/2310.03646.pdf' target='_blank'>https://arxiv.org/pdf/2310.03646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Sherborne, Naomi Saphra, Pradeep Dasigi, Hao Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03646">TRAM: Bridging Trust Regions and Sharpness Aware Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sharpness-aware minimization (SAM) reports improving domain generalization by reducing the loss surface curvature in the parameter space. However, generalization during fine-tuning is often more dependent on the transferability of representations in the function space. Trust-region methods (TR) target this goal by regularizing representation curvature to reduce catastrophic forgetting of pre-trained task-agnostic information while adopting task-specific skills. We consider unifying these strategies for low curvature in both parameter space and function space to improve out-of-domain (OOD) generalization. We propose Trust Region Aware Minimization (TRAM), a SAM algorithm fine-tuning for low parameter sharpness and smooth, informative representations preserving pre-trained structure. TRAM uses a trust region bound to inform the SAM adversarial neighborhood, introducing an awareness of function curvature within optimization for flatter minima. We empirically validate TRAM in vision (cross-dataset adaptation) and text (OOD language modeling, zero-shot cross-lingual transfer) tasks where robust domain transfer and representation generality are critical. TRAM outperforms SAM- and TR-based optimization across all tasks, notably surpassing competing methods for hard transfer between anticorrelated domains. TRAM establishes a novel standard in fine-tuning for domain-generalizable models with minimal additional computation over previous sharpness-aware methods.
<div id='section'>Paperid: <span id='pid'>1868, <a href='https://arxiv.org/pdf/2307.12622.pdf' target='_blank'>https://arxiv.org/pdf/2307.12622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengming Hu, Yeqian Du, Rui Wang, Hao Chen, Congcong Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12622">Phase Matching for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Fourier transform, an explicit decomposition method for visual signals, has been employed to explain the out-of-distribution generalization behaviors of Deep Neural Networks (DNNs). Previous studies indicate that the amplitude spectrum is susceptible to the disturbance caused by distribution shifts, whereas the phase spectrum preserves highly-structured spatial information that is crucial for robust visual representation learning. Inspired by this insight, this paper is dedicated to clarifying the relationships between Domain Generalization (DG) and the frequency components. Specifically, we provide distribution analysis and empirical experiments for the frequency components. Based on these observations, we propose a Phase Matching approach, termed PhaMa, to address DG problems. To this end, PhaMa introduces perturbations on the amplitude spectrum and establishes spatial relationships to match the phase components with patch contrastive learning. Experiments on multiple benchmarks demonstrate that our proposed method achieves state-of-the-art performance in domain generalization and out-of-distribution robustness tasks. Beyond vanilla analysis and experiments, we further clarify the relationships between the Fourier components and DG problems by introducing a Fourier-based Structural Causal Model (SCM).
<div id='section'>Paperid: <span id='pid'>1869, <a href='https://arxiv.org/pdf/2307.12459.pdf' target='_blank'>https://arxiv.org/pdf/2307.12459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunseung Lee, Youngjun Kwak, Jinho Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12459">Robust face anti-spoofing framework with Convolutional Vision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Owing to the advances in image processing technology and large-scale datasets, companies have implemented facial authentication processes, thereby stimulating increased focus on face anti-spoofing (FAS) against realistic presentation attacks. Recently, various attempts have been made to improve face recognition performance using both global and local learning on face images; however, to the best of our knowledge, this is the first study to investigate whether the robustness of FAS against domain shifts is improved by considering global information and local cues in face images captured using self-attention and convolutional layers. This study proposes a convolutional vision transformer-based framework that achieves robust performance for various unseen domain data. Our model resulted in 7.3%$p$ and 12.9%$p$ increases in FAS performance compared to models using only a convolutional neural network or vision transformer, respectively. It also shows the highest average rank in sub-protocols of cross-dataset setting over the other nine benchmark models for domain generalization.
<div id='section'>Paperid: <span id='pid'>1870, <a href='https://arxiv.org/pdf/2307.02712.pdf' target='_blank'>https://arxiv.org/pdf/2307.02712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emily Mu, John Guttag, Maggie Makar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02712">Multi-Similarity Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a similarity metric, contrastive methods learn a representation in which examples that are similar are pushed together and examples that are dissimilar are pulled apart. Contrastive learning techniques have been utilized extensively to learn representations for tasks ranging from image classification to caption generation. However, existing contrastive learning approaches can fail to generalize because they do not take into account the possibility of different similarity relations. In this paper, we propose a novel multi-similarity contrastive loss (MSCon), that learns generalizable embeddings by jointly utilizing supervision from multiple metrics of similarity. Our method automatically learns contrastive similarity weightings based on the uncertainty in the corresponding similarity, down-weighting uncertain tasks and leading to better out-of-domain generalization to new tasks. We show empirically that networks trained with MSCon outperform state-of-the-art baselines on in-domain and out-of-domain settings.
<div id='section'>Paperid: <span id='pid'>1871, <a href='https://arxiv.org/pdf/2307.00761.pdf' target='_blank'>https://arxiv.org/pdf/2307.00761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanhui Guo, Fangzhou Luo, Xiaolin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00761">Learning Degradation-Independent Representations for Camera ISP Pipelines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image signal processing (ISP) pipeline plays a fundamental role in digital cameras, which converts raw Bayer sensor data to RGB images. However, ISP-generated images usually suffer from imperfections due to the compounded degradations that stem from sensor noises, demosaicing noises, compression artifacts, and possibly adverse effects of erroneous ISP hyperparameter settings such as ISO and gamma values. In a general sense, these ISP imperfections can be considered as degradations. The highly complex mechanisms of ISP degradations, some of which are even unknown, pose great challenges to the generalization capability of deep neural networks (DNN) for image restoration and to their adaptability to downstream tasks. To tackle the issues, we propose a novel DNN approach to learn degradation-independent representations (DiR) through the refinement of a self-supervised learned baseline representation. The proposed DiR learning technique has remarkable domain generalization capability and consequently, it outperforms state-of-the-art methods across various downstream tasks, including blind image restoration, object detection, and instance segmentation, as verified in our experiments.
<div id='section'>Paperid: <span id='pid'>1872, <a href='https://arxiv.org/pdf/2306.17193.pdf' target='_blank'>https://arxiv.org/pdf/2306.17193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niklas Risse, Marcel BÃ¶hme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17193">Uncovering the Limits of Machine Learning for Automatic Vulnerability Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent results of machine learning for automatic vulnerability detection (ML4VD) have been very promising. Given only the source code of a function $f$, ML4VD techniques can decide if $f$ contains a security flaw with up to 70% accuracy. However, as evident in our own experiments, the same top-performing models are unable to distinguish between functions that contain a vulnerability and functions where the vulnerability is patched. So, how can we explain this contradiction and how can we improve the way we evaluate ML4VD techniques to get a better picture of their actual capabilities?
  In this paper, we identify overfitting to unrelated features and out-of-distribution generalization as two problems, which are not captured by the traditional approach of evaluating ML4VD techniques. As a remedy, we propose a novel benchmarking methodology to help researchers better evaluate the true capabilities and limits of ML4VD techniques. Specifically, we propose (i) to augment the training and validation dataset according to our cross-validation algorithm, where a semantic preserving transformation is applied during the augmentation of either the training set or the testing set, and (ii) to augment the testing set with code snippets where the vulnerabilities are patched.
  Using six ML4VD techniques and two datasets, we find (a) that state-of-the-art models severely overfit to unrelated features for predicting the vulnerabilities in the testing data, (b) that the performance gained by data augmentation does not generalize beyond the specific augmentations applied during training, and (c) that state-of-the-art ML4VD techniques are unable to distinguish vulnerable functions from their patches.
<div id='section'>Paperid: <span id='pid'>1873, <a href='https://arxiv.org/pdf/2306.11890.pdf' target='_blank'>https://arxiv.org/pdf/2306.11890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wolfgang M. Pernice, Michael Doron, Alex Quach, Aditya Pratapa, Sultan Kenjeyev, Nicholas De Veaux, Michio Hirano, Juan C. Caicedo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11890">Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world deployment of computer vision systems, including in the discovery processes of biomedical research, requires causal representations that are invariant to contextual nuisances and generalize to new data. Leveraging the internal replicate structure of two novel single-cell fluorescent microscopy datasets, we propose generally applicable tests to assess the extent to which models learn causal representations across increasingly challenging levels of OOD-generalization. We show that despite seemingly strong performance, as assessed by other established metrics, both naive and contemporary baselines designed to ward against confounding, collapse on these tests. We introduce a new method, Interventional Style Transfer (IST), that substantially improves OOD generalization by generating interventional training distributions in which spurious correlations between biological causes and nuisances are mitigated. We publish our code and datasets.
<div id='section'>Paperid: <span id='pid'>1874, <a href='https://arxiv.org/pdf/2306.09222.pdf' target='_blank'>https://arxiv.org/pdf/2306.09222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramnath Kumar, Kushal Majmundar, Dheeraj Nagaraj, Arun Sai Suggala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09222">Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Re-weighted Gradient Descent (RGD), a novel optimization technique that improves the performance of deep neural networks through dynamic sample re-weighting. Leveraging insights from distributionally robust optimization (DRO) with Kullback-Leibler divergence, our method dynamically assigns importance weights to training data during each optimization step. RGD is simple to implement, computationally efficient, and compatible with widely used optimizers such as SGD and Adam. We demonstrate the effectiveness of RGD on various learning tasks, including supervised learning, meta-learning, and out-of-domain generalization. Notably, RGD achieves state-of-the-art results on diverse benchmarks, with improvements of +0.7% on DomainBed, +1.44% on tabular classification, \textcolor{blue}+1.94% on GLUE with BERT, and +1.01% on ImageNet-1K with ViT.
<div id='section'>Paperid: <span id='pid'>1875, <a href='https://arxiv.org/pdf/2306.01271.pdf' target='_blank'>https://arxiv.org/pdf/2306.01271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binghui Li, Yuanzhi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01271">On the Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Similar to surprising performance in the standard deep learning, deep nets trained by adversarial training also generalize well for unseen clean data (natural data). However, despite adversarial training can achieve low robust training error, there exists a significant robust generalization gap. We call this phenomenon the Clean Generalization and Robust Overfitting (CGRO). In this work, we study the CGRO phenomenon in adversarial training from two views: representation complexity and training dynamics. Specifically, we consider a binary classification setting with $N$ separated training data points. First, we prove that, based on the assumption that we assume there is $\operatorname{poly}(D)$-size clean classifier (where $D$ is the data dimension), ReLU net with only $O(N D)$ extra parameters is able to leverages robust memorization to achieve the CGRO, while robust classifier still requires exponential representation complexity in worst case. Next, we focus on a structured-data case to analyze training dynamics, where we train a two-layer convolutional network with $O(N D)$ width against adversarial perturbation. We then show that a three-stage phase transition occurs during learning process and the network provably converges to robust memorization regime, which thereby results in the CGRO. Besides, we also empirically verify our theoretical analysis by experiments in real-image recognition datasets.
<div id='section'>Paperid: <span id='pid'>1876, <a href='https://arxiv.org/pdf/2305.19402.pdf' target='_blank'>https://arxiv.org/pdf/2305.19402.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Bao, Theofanis Karaletsos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19402">Contextual Vision Transformers for Robust Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Contextual Vision Transformers (ContextViT), a method designed to generate robust image representations for datasets experiencing shifts in latent factors across various groups. Derived from the concept of in-context learning, ContextViT incorporates an additional context token to encapsulate group-specific information. This integration allows the model to adjust the image representation in accordance with the group-specific context. Specifically, for a given input image, ContextViT maps images with identical group membership into this context token, which is appended to the input image tokens. Additionally, we introduce a context inference network to predict such tokens on-the-fly, given a batch of samples from the group. This enables ContextViT to adapt to new testing distributions during inference time. We demonstrate the efficacy of ContextViT across a wide range of applications. In supervised fine-tuning, we show that augmenting pre-trained ViTs with our proposed context conditioning mechanism results in consistent improvements in out-of-distribution generalization on iWildCam and FMoW. We also investigate self-supervised representation learning with ContextViT. Our experiments on the Camelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging benchmark demonstrate that ContextViT excels in learning stable image featurizations amidst distribution shift, consistently outperforming its ViT counterpart.
<div id='section'>Paperid: <span id='pid'>1877, <a href='https://arxiv.org/pdf/2303.14653.pdf' target='_blank'>https://arxiv.org/pdf/2303.14653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingda Guan, Zhengyang Feng, Huiying Chang, Kuo Du, Tingting Li, Min Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14653">SDTracker: Synthetic Data Based Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SDTracker, a method that harnesses the potential of synthetic data for multi-object tracking of real-world scenes in a domain generalization and semi-supervised fashion. First, we use the ImageNet dataset as an auxiliary to randomize the style of synthetic data. With out-of-domain data, we further enforce pyramid consistency loss across different "stylized" images from the same sample to learn domain invariant features. Second, we adopt the pseudo-labeling method to effectively utilize the unlabeled MOT17 training data. To obtain high-quality pseudo-labels, we apply proximal policy optimization (PPO2) algorithm to search confidence thresholds for each sequence. When using the unlabeled MOT17 training set, combined with the pure-motion tracking strategy upgraded via developed post-processing, we finally reach 61.4 HOTA.
<div id='section'>Paperid: <span id='pid'>1878, <a href='https://arxiv.org/pdf/2303.13462.pdf' target='_blank'>https://arxiv.org/pdf/2303.13462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Haug, M. S. Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13462">Generalization of Quantum Machine Learning Models Using Quantum Fisher Information Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization is the ability of machine learning models to make accurate predictions on new data by learning from training data. However, understanding generalization of quantum machine learning models has been a major challenge. Here, we introduce the data quantum Fisher information metric (DQFIM). It describes the capacity of variational quantum algorithms depending on variational ansatz, training data and their symmetries. We apply the DQFIM to quantify circuit parameters and training data needed to successfully train and generalize. Using the dynamical Lie algebra, we explain how to generalize using a low number of training states. Counter-intuitively, breaking symmetries of the training data can help to improve generalization. Finally, we find that out-of-distribution generalization, where training and testing data are drawn from different data distributions, can be better than using the same distribution. Our work provides a useful framework to explore the power of quantum machine learning models.
<div id='section'>Paperid: <span id='pid'>1879, <a href='https://arxiv.org/pdf/2303.06088.pdf' target='_blank'>https://arxiv.org/pdf/2303.06088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marin Scalbert, Maria Vakalopoulou, Florent CouziniÃ©-Devy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06088">Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Self-Supervised Learning (SSL), models are typically pretrained, fine-tuned, and evaluated on the same domains. However, they tend to perform poorly when evaluated on unseen domains, a challenge that Unsupervised Domain Generalization (UDG) seeks to address. Current UDG methods rely on domain labels, which are often challenging to collect, and domain-specific architectures that lack scalability when confronted with numerous domains, making the current methodology impractical and rigid. Inspired by contrastive-based UDG methods that mitigate spurious correlations by restricting comparisons to examples from the same domain, we hypothesize that eliminating style variability within a batch could provide a more convenient and flexible way to reduce spurious correlations without requiring domain labels. To verify this hypothesis, we introduce Batch Styles Standardization (BSS), a relatively simple yet powerful Fourier-based method to standardize the style of images in a batch specifically designed for integration with SSL methods to tackle UDG. Combining BSS with existing SSL methods offers serious advantages over prior UDG methods: (1) It eliminates the need for domain labels or domain-specific network components to enhance domain-invariance in SSL representations, and (2) offers flexibility as BSS can be seamlessly integrated with diverse contrastive-based but also non-contrastive-based SSL methods. Experiments on several UDG datasets demonstrate that it significantly improves downstream task performances on unseen domains, often outperforming or rivaling with UDG methods. Finally, this work clarifies the underlying mechanisms contributing to BSS's effectiveness in improving domain-invariance in SSL representations and performances on unseen domain.
<div id='section'>Paperid: <span id='pid'>1880, <a href='https://arxiv.org/pdf/2303.03084.pdf' target='_blank'>https://arxiv.org/pdf/2303.03084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephan Clémençon, Nathan Huet, Anne Sabourin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03084">On Regression in Extreme Regions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We establish a statistical learning theoretical framework aimed at extrapolation, or out-of-domain generalization, on the unobserved tails of covariates in continuous regression problems. Our strategy involves performing statistical regression on a subsample of observations with continuous labels that are the furthest away from the origin, focusing specifically on their angular components. The underlying assumptions of our approach are grounded in the theory of multivariate regular variation, a cornerstone of extreme value theory. We address the stylized problem of nonparametric least squares regression with predictors chosen from a Vapnik-Chervonenkis class. This work contributes to a broader initiative to develop statistical learning theoretical foundations for supervised learning strategies that enhance performance on the supposedly heavy tails of covariates. Previous efforts in this area have focused exclusively on binary classification on extreme covariates. Although the continuous target setting necessitates different techniques and regularity assumptions, our main results echo findings from earlier studies. We quantify the predictive performance on tail regions in terms of excess risk, presenting it as a finite sample risk bound with a clear bias-variance decomposition. Numerical experiments with simulated and real data illustrate our theoretical findings.
<div id='section'>Paperid: <span id='pid'>1881, <a href='https://arxiv.org/pdf/2302.09359.pdf' target='_blank'>https://arxiv.org/pdf/2302.09359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Honglin Wu, Xueqiong Li, Long Lan, Liyang Xu, Yuhua Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09359">Towards Radar Emitter Recognition in Changing Environments with Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analyzing radar signals from complex Electronic Warfare (EW) environment is a non-trivial task.However, in the real world, the changing EW environment results in inconsistent signal distribution, such as the pulse repetition interval (PRI) mismatch between different detected scenes.In this paper, we propose a novel domain generalization framework to improve the adaptability of signal recognition in changing environments.Specifically, we first design several noise generators to simulate varied scenes. Different from conventional augmentation methods, our introduced generators carefully enhance the diversity of the detected signals and meanwhile maintain the semantic features of the signals. Moreover, we propose a signal scene domain classifier that works in the manner of adversarial learning. The proposed classifier guarantees the signal predictor to generalize to different scenes. Extensive comparative experiments prove the proposed method's superiority.
<div id='section'>Paperid: <span id='pid'>1882, <a href='https://arxiv.org/pdf/2302.06874.pdf' target='_blank'>https://arxiv.org/pdf/2302.06874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankur Singh, Senthilnath Jayavelu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06874">Robust Representation Learning with Self-Distillation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent success of deep neural networks, there remains a need for effective methods to enhance domain generalization using vision transformers. In this paper, we propose a novel domain generalization technique called Robust Representation Learning with Self-Distillation (RRLD) comprising i) intermediate-block self-distillation and ii) augmentation-guided self-distillation to improve the generalization capabilities of transformer-based models on unseen domains. This approach enables the network to learn robust and general features that are invariant to different augmentations and domain shifts while effectively mitigating overfitting to source domains. To evaluate the effectiveness of our proposed method, we perform extensive experiments on PACS and OfficeHome benchmark datasets, as well as an industrial wafer semiconductor defect dataset. The results demonstrate that RRLD achieves robust and accurate generalization performance. We observe an average accuracy improvement in the range of 1.2% to 2.3% over the state-of-the-art on the three datasets.
<div id='section'>Paperid: <span id='pid'>1883, <a href='https://arxiv.org/pdf/2301.03313.pdf' target='_blank'>https://arxiv.org/pdf/2301.03313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darko Drakulic, Sofia Michel, Florian Mai, Arnaud Sors, Jean-Marc Andreoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.03313">BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the success of neural-based combinatorial optimization methods for end-to-end heuristic learning, out-of-distribution generalization remains a challenge. In this paper, we present a novel formulation of Combinatorial Optimization Problems (COPs) as Markov Decision Processes (MDPs) that effectively leverages common symmetries of COPs to improve out-of-distribution robustness. Starting from a direct MDP formulation of a constructive method, we introduce a generic way to reduce the state space, based on Bisimulation Quotienting (BQ) in MDPs. Then, for COPs with a recursive nature, we specialize the bisimulation and show how the reduced state exploits the symmetries of these problems and facilitates MDP solving. Our approach is principled and we prove that an optimal policy for the proposed BQ-MDP actually solves the associated COPs. We illustrate our approach on five classical problems: the Euclidean and Asymmetric Traveling Salesman, Capacitated Vehicle Routing, Orienteering and Knapsack Problems. Furthermore, for each problem, we introduce a simple attention-based policy network for the BQ-MDPs, which we train by imitation of (near) optimal solutions of small instances from a single distribution. We obtain new state-of-the-art results for the five COPs on both synthetic and realistic benchmarks. Notably, in contrast to most existing neural approaches, our learned policies show excellent generalization performance to much larger instances than seen during training, without any additional search procedure.
<div id='section'>Paperid: <span id='pid'>1884, <a href='https://arxiv.org/pdf/2212.14048.pdf' target='_blank'>https://arxiv.org/pdf/2212.14048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Furkan Luleci, F. Necati Catbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14048">Structural State Translation: Condition Transfer between Civil Structures Using Domain-Generalization for Structural Health Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using Structural Health Monitoring (SHM) systems with extensive sensing arrangements on every civil structure can be costly and impractical. Various concepts have been introduced to alleviate such difficulties, such as Population-based SHM (PBSHM). Nevertheless, the studies presented in the literature do not adequately address the challenge of accessing the information on different structural states (conditions) of dissimilar civil structures. The study herein introduces a novel framework named Structural State Translation (SST), which aims to estimate the response data of different civil structures based on the information obtained from a dissimilar structure. SST can be defined as Translating a state of one civil structure to another state after discovering and learning the domain-invariant representation in the source domains of a dissimilar civil structure. SST employs a Domain-Generalized Cycle-Generative (DGCG) model to learn the domain-invariant representation in the acceleration datasets obtained from a numeric bridge structure that is in two different structural conditions. In other words, the model is tested on three dissimilar numeric bridge models to translate their structural conditions. The evaluation results of SST via Mean Magnitude-Squared Coherence (MMSC) and modal identifiers showed that the translated bridge states (synthetic states) are significantly similar to the real ones. As such, the minimum and maximum average MMSC values of real and translated bridge states are 91.2% and 97.1%, the minimum and the maximum difference in natural frequencies are 5.71% and 0%, and the minimum and maximum Modal Assurance Criterion (MAC) values are 0.998 and 0.870. This study is critical for data scarcity and PBSHM, as it demonstrates that it is possible to obtain data from structures while the structure is actually in a different condition or state.
<div id='section'>Paperid: <span id='pid'>1885, <a href='https://arxiv.org/pdf/2209.14946.pdf' target='_blank'>https://arxiv.org/pdf/2209.14946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinglai Wei, Beiming Yuan, Diancheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14946">EiHi Net: Out-of-Distribution Generalization Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a new EiHi net to solve the out-of-distribution (OoD) generalization problem in deep learning. EiHi net is a model learning paradigm that can be blessed on any visual backbone. This paradigm can change the previous learning method of the deep model, namely find out correlations between inductive sample features and corresponding categories, which suffers from pseudo correlations between indecisive features and labels. We fuse SimCLR and VIC-Reg via explicitly and dynamically establishing the original - positive - negative sample pair as a minimal learning element, the deep model iteratively establishes a relationship close to the causal one between features and labels, while suppressing pseudo correlations. To further validate the proposed model, and strengthen the established causal relationships, we develop a human-in-the-loop strategy, with few guidance samples, to prune the representation space directly. Finally, it is shown that the developed EiHi net makes significant improvements in the most difficult and typical OoD dataset Nico, compared with the current SOTA results, without any domain ($e.g.$ background, irrelevant features) information.
<div id='section'>Paperid: <span id='pid'>1886, <a href='https://arxiv.org/pdf/2207.10792.pdf' target='_blank'>https://arxiv.org/pdf/2207.10792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minguk Jang, Sae-Young Chung, Hye Won Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.10792">Test-Time Adaptation via Self-Training with Nearest Neighbor Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time adaptation (TTA) aims to adapt a trained classifier using online unlabeled test data only, without any information related to the training procedure. Most existing TTA methods adapt the trained classifier using the classifier's prediction on the test data as pseudo-label. However, under test-time domain shift, accuracy of the pseudo labels cannot be guaranteed, and thus the TTA methods often encounter performance degradation at the adapted classifier. To overcome this limitation, we propose a novel test-time adaptation method, called Test-time Adaptation via Self-Training with nearest neighbor information (TAST), which is composed of the following procedures: (1) adds trainable adaptation modules on top of the trained feature extractor; (2) newly defines a pseudo-label distribution for the test data by using the nearest neighbor information; (3) trains these modules only a few times during test time to match the nearest neighbor-based pseudo label distribution and a prototype-based class distribution for the test data; and (4) predicts the label of test data using the average predicted class distribution from these modules. The pseudo-label generation is based on the basic intuition that a test data and its nearest neighbor in the embedding space are likely to share the same label under the domain shift. By utilizing multiple randomly initialized adaptation modules, TAST extracts useful information for the classification of the test data under the domain shift, using the nearest neighbor information. TAST showed better performance than the state-of-the-art TTA methods on two standard benchmark tasks, domain generalization, namely VLCS, PACS, OfficeHome, and TerraIncognita, and image corruption, particularly CIFAR-10/100C.
<div id='section'>Paperid: <span id='pid'>1887, <a href='https://arxiv.org/pdf/2207.00419.pdf' target='_blank'>https://arxiv.org/pdf/2207.00419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madeline C. Schiappa, Yogesh S. Rawat, Mubarak Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.00419">Self-Supervised Learning for Videos: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable success of deep learning in various domains relies on the availability of large-scale annotated datasets. However, obtaining annotations is expensive and requires great effort, which is especially challenging for videos. Moreover, the use of human-generated annotations leads to models with biased learning and poor domain generalization and robustness. As an alternative, self-supervised learning provides a way for representation learning which does not require annotations and has shown promise in both image and video domains. Different from the image domain, learning video representations are more challenging due to the temporal dimension, bringing in motion and other environmental dynamics. This also provides opportunities for video-exclusive ideas that advance self-supervised learning in the video and multimodal domain. In this survey, we provide a review of existing approaches on self-supervised learning focusing on the video domain. We summarize these methods into four different categories based on their learning objectives: 1) pretext tasks, 2) generative learning, 3) contrastive learning, and 4) cross-modal agreement. We further introduce the commonly used datasets, downstream evaluation tasks, insights into the limitations of existing works, and the potential future directions in this area.
<div id='section'>Paperid: <span id='pid'>1888, <a href='https://arxiv.org/pdf/2206.07837.pdf' target='_blank'>https://arxiv.org/pdf/2206.07837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jivat Neet Kaur, Emre Kiciman, Amit Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.07837">Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM), an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.
<div id='section'>Paperid: <span id='pid'>1889, <a href='https://arxiv.org/pdf/2111.10487.pdf' target='_blank'>https://arxiv.org/pdf/2111.10487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liling Zhang, Xinyu Lei, Yichun Shi, Hongyu Huang, Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.10487">Federated Learning with Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) enables a group of clients to jointly train a machine learning model with the help of a centralized server. Clients do not need to submit their local data to the server during training, and hence the local training data of clients is protected. In FL, distributed clients collect their local data independently, so the dataset of each client may naturally form a distinct source domain. In practice, the model trained over multiple source domains may have poor generalization performance on unseen target domains. To address this issue, we propose FedADG to equip federated learning with domain generalization capability. FedADG employs the federated adversarial learning approach to measure and align the distributions among different source domains via matching each distribution to a reference distribution. The reference distribution is adaptively generated (by accommodating all source domains) to minimize the domain shift distance during alignment. In FedADG, the alignment is fine-grained since each class is aligned independently. In this way, the learned feature representation is supposed to be universal, so it can generalize well on the unseen domains. Intensive experiments on various datasets demonstrate that FedADG has comparable performance with the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>1890, <a href='https://arxiv.org/pdf/2106.15453.pdf' target='_blank'>https://arxiv.org/pdf/2106.15453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Varsha Suresh, Gerard Yeo, Desmond C. Ong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.15453">Critically examining the Domain Generalizability of Facial Expression Recognition models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Facial Expression Recognition is a commercially-important application, but one under-appreciated limitation is that such applications require making predictions on out-of-sample distributions, where target images have different properties from the images the model was trained on. How well -- or how badly -- do facial expression recognition models do on unseen target domains? We provide a systematic and critical evaluation of transfer learning -- specifically, domain generalization -- in facial expression recognition. Using a state-of-the-art model with twelve datasets (six collected in-lab and six ``in-the-wild"), we conduct extensive round-robin-style experiments to evaluate classification accuracies when given new data from an unseen dataset. We also perform multi-source experiments to examine a model's ability to generalize from multiple source datasets, including (i) within-setting (e.g., lab to lab), (ii) cross-setting (e.g., in-the-wild to lab), and (iii) leave-one-out settings. Finally, we compare our results with three commercially-available software. We find sobering results: the accuracy of single- and multi-source domain generalization is only modest. Even for the best-performing multi-source settings, we observe average classification accuracies of 65.6% (range: 34.6%-88.6%; chance: 14.3%), corresponding to an average drop of 10.8 percentage points from the within-corpus classification performance (mean: 76.4%). We discuss the need for regular, systematic investigations into the generalizability of affective computing models and applications.
<div id='section'>Paperid: <span id='pid'>1891, <a href='https://arxiv.org/pdf/2106.07916.pdf' target='_blank'>https://arxiv.org/pdf/2106.07916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Duboudin, Emmanuel DellandrÃ©a, Corentin Abgrall, Gilles HÃ©naff, Liming Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.07916">Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for Better Single-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional deep learning algorithms often fail to generalize when they are tested outside of the domain of the training data. The issue can be mitigated by using unlabeled data from the target domain at training time, but because data distributions can change dynamically in real-life applications once a learned model is deployed, it is critical to create networks robust to unknown and unforeseen domain shifts. In this paper we focus on one of the reasons behind the inability of neural networks to be so: deep networks focus only on the most obvious, potentially spurious, clues to make their predictions and are blind to useful but slightly less efficient or more complex patterns. This behaviour has been identified and several methods partially addressed the issue. To investigate their effectiveness and limits, we first design a publicly available MNIST-based benchmark to precisely measure the ability of an algorithm to find the ''hidden'' patterns. Then, we evaluate state-of-the-art algorithms through our benchmark and show that the issue is largely unsolved. Finally, we propose a partially reversed contrastive loss to encourage intra-class diversity and find less strongly correlated patterns, whose efficiency is demonstrated by our experiments.
<div id='section'>Paperid: <span id='pid'>1892, <a href='https://arxiv.org/pdf/1911.00804.pdf' target='_blank'>https://arxiv.org/pdf/1911.00804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isabela Albuquerque, JoÃ£o Monteiro, Mohammad Darvishi, Tiago H. Falk, Ioannis Mitliagkas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1911.00804">Generalizing to unseen domains via distribution matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised learning results typically rely on assumptions of i.i.d. data. Unfortunately, those assumptions are commonly violated in practice. In this work, we tackle such problem by focusing on domain generalization: a formalization where the data generating process at test time may yield samples from never-before-seen domains (distributions). Our work relies on the following lemma: by minimizing a notion of discrepancy between all pairs from a set of given domains, we also minimize the discrepancy between any pairs of mixtures of domains. Using this result, we derive a generalization bound for our setting. We then show that low risk over unseen domains can be achieved by representing the data in a space where (i) the training distributions are indistinguishable, and (ii) relevant information for the task at hand is preserved. Minimizing the terms in our bound yields an adversarial formulation which estimates and minimizes pairwise discrepancies. We validate our proposed strategy on standard domain generalization benchmarks, outperforming a number of recently introduced methods. Notably, we tackle a real-world application where the underlying data corresponds to multi-channel electroencephalography time series from different subjects, each considered as a distinct domain.
<div id='section'>Paperid: <span id='pid'>1893, <a href='https://arxiv.org/pdf/2510.06257.pdf' target='_blank'>https://arxiv.org/pdf/2510.06257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangjun Mi, Frank Mueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06257">Toward Uncertainty-Aware and Generalizable Neural Decoding for Quantum LDPC Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum error correction (QEC) is essential for scalable quantum computing, yet decoding errors via conventional algorithms result in limited accuracy (i.e., suppression of logical errors) and high overheads, both of which can be alleviated by inference-based decoders. To date, such machine-learning (ML) decoders lack two key properties crucial for practical fault tolerance: reliable uncertainty quantification and robust generalization to previously unseen codes. To address this gap, we propose \textbf{QuBA}, a Bayesian graph neural decoder that integrates attention to both dot-product and multi-head, enabling expressive error-pattern recognition alongside calibrated uncertainty estimates. Building on QuBA, we further develop \textbf{SAGU }\textbf{(Sequential Aggregate Generalization under Uncertainty)}, a multi-code training framework with enhanced cross-domain robustness enabling decoding beyond the training set. Experiments on bivariate bicycle (BB) codes and their coprime variants demonstrate that (i) both QuBA and SAGU consistently outperform the classical baseline belief propagation (BP), achieving a reduction of on average \emph{one order of magnitude} in logical error rate (LER), and up to \emph{two orders of magnitude} under confident-decision bounds on the coprime BB code $[[154, 6, 16]]$; (ii) QuBA also surpasses state-of-the-art neural decoders, providing an advantage of roughly \emph{one order of magnitude} (e.g., for the larger BB code $[[756, 16, \leq34]]$) even when considering conservative (safe) decision bounds; (iii) SAGU achieves decoding performance comparable to or even outperforming QuBA's domain-specific training approach.
<div id='section'>Paperid: <span id='pid'>1894, <a href='https://arxiv.org/pdf/2510.03305.pdf' target='_blank'>https://arxiv.org/pdf/2510.03305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian Zheng, Subashree Venkatasubramanian, Shuolin Li, Amy Braverman, Xinyi Ke, Zhewen Hou, Peter Jin, Samarth Sanjay Agrawal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03305">Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning has been increasingly applied in climate modeling on system emulation acceleration, data-driven parameter inference, forecasting, and knowledge discovery, addressing challenges such as physical consistency, multi-scale coupling, data sparsity, robust generalization, and integration with scientific workflows. This paper analyzes a series of case studies from applied machine learning research in climate modeling, with a focus on design choices and workflow structure. Rather than reviewing technical details, we aim to synthesize workflow design patterns across diverse projects in ML-enabled climate modeling: from surrogate modeling, ML parameterization, probabilistic programming, to simulation-based inference, and physics-informed transfer learning. We unpack how these workflows are grounded in physical knowledge, informed by simulation data, and designed to integrate observations. We aim to offer a framework for ensuring rigor in scientific machine learning through more transparent model development, critical evaluation, informed adaptation, and reproducibility, and to contribute to lowering the barrier for interdisciplinary collaboration at the interface of data science and climate modeling.
<div id='section'>Paperid: <span id='pid'>1895, <a href='https://arxiv.org/pdf/2510.00347.pdf' target='_blank'>https://arxiv.org/pdf/2510.00347.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huitao Yang, Guanting Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00347">In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) continue to grow in capability, there is increasing interest in incorporating them into decision-making tasks. A common pipeline for this is Decision-Pretrained Transformers (DPTs). However, existing training methods for DPTs often struggle to generalize beyond their pretraining data distribution. To explore mitigation of this limitation, we propose in-context curiosity -- a lightweight, exploration-inspired regularizer for offline pretraining -- and introduce the Prediction-Powered Transformer (PPT) framework. PPT augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal to encourage broader exploration during training. In proof-of-concept experiments on Gaussian multi-armed bandits, PPT shows improved robustness: it moderates the performance degradation observed in DPT when test environments exhibit higher variance in reward, particularly when pretraining data has limited diversity. While the quality of offline data remain fundamental, our preliminary results suggest that curiosity-driven pretraining offers a promising direction for enhancing out-of-distribution generalization in in-context RL agents.
<div id='section'>Paperid: <span id='pid'>1896, <a href='https://arxiv.org/pdf/2510.00346.pdf' target='_blank'>https://arxiv.org/pdf/2510.00346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanbo Hou, Zhaoyi Liu, Xin Shen, Stephen Roberts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00346">Learning Domain-Robust Bioacoustic Representations for Mosquito Species Classification with Contrastive Learning and Distribution Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mosquito Species Classification (MSC) is crucial for vector surveillance and disease control. The collection of mosquito bioacoustic data is often limited by mosquito activity seasons and fieldwork. Mosquito recordings across regions, habitats, and laboratories often show non-biological variations from the recording environment, which we refer to as domain features. This study finds that models directly trained on audio recordings with domain features tend to rely on domain information rather than the species' acoustic cues for identification, resulting in illusory good performance while actually performing poor cross-domain generalization. To this end, we propose a Domain-Robust Bioacoustic Learning (DR-BioL) framework that combines contrastive learning with distribution alignment. Contrastive learning aims to promote cohesion within the same species and mitigate inter-domain discrepancies, and species-conditional distribution alignment further enhances cross-domain species representation. Experiments on a multi-domain mosquito bioacoustic dataset from diverse environments show that the DR-BioL improves the accuracy and robustness of baselines, highlighting its potential for reliable cross-domain MSC in the real world.
<div id='section'>Paperid: <span id='pid'>1897, <a href='https://arxiv.org/pdf/2509.25672.pdf' target='_blank'>https://arxiv.org/pdf/2509.25672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hasan Alp Caferoğlu, Mehmet Serhat Çelik, Özgür Ulusoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25672">SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating natural language questions into SQL has become a core challenge in enabling non-technical users to query databases. While recent work has explored large-scale synthetic data generation to improve model performance through post-training, most efforts emphasize cross-domain generalization. This leaves a gap for real-world enterprise scenarios, where models need to specialize to a single database schema and organizations require to be able to evaluate their Text-to-SQL systems on their own databases. To address this, we introduce SING-SQL, a fully automated two-stage framework for generating high-quality, high-coverage synthetic Text-to-SQL data for any target database, without relying on SQL logs or manual annotations. Our approach hierarchically partitions a database schema into sub-schemas, synthesizes SQL queries across multiple complexity levels, and applies a quality-aware pipeline that includes LLM-as-a-judge validation, executability checks, automatic repair, and column balancing. We further release SingSQL-LM, a family of compact language models fine-tuned on the synthetic data, achieving strong in-domain generalization. On the subset of the BIRD benchmark, SingSQL-LM-3B-R64 reaches 82.87% Soft F1 and 73.03% EX upper bound with 32 candidates, outperforming the best 3B-scale baseline by +16.21 in Soft F1 and +12.36 in EX. At the 1.5B scale, SingSQL-LM-1.5B-R64 improves over prior systems by +9.30 in Soft F1 and +4.49 in EX. On synthetic evaluation sets, SingSQL-LMs exceed prior systems by wide margins, establishing state-of-the-art performance among open models at comparable scales. Our study of context management strategies reveals that schema-free fine-tuning combined with schema-only inference provides the most robust results. These findings establish SING-SQL as a scalable, database-agnostic paradigm for producing and evaluating enterprise-grade Text-to-SQL systems.
<div id='section'>Paperid: <span id='pid'>1898, <a href='https://arxiv.org/pdf/2509.25637.pdf' target='_blank'>https://arxiv.org/pdf/2509.25637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kotaro Yoshida, Atsushi Nitanda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25637">How Does Preconditioning Guide Feature Learning in Deep Neural Networks?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preconditioning is widely used in machine learning to accelerate convergence on the empirical risk, yet its role on the expected risk remains underexplored. In this work, we investigate how preconditioning affects feature learning and generalization performance. We first show that the input information available to the model is conveyed solely through the Gram matrix defined by the preconditioner's metric, thereby inducing a controllable spectral bias on feature learning. Concretely, instantiating the preconditioner as the $p$-th power of the input covariance matrix and within a single-index teacher model, we prove that in generalization, the exponent $p$ and the alignment between the teacher and the input spectrum are crucial factors. We further investigate how the interplay between these factors influences feature learning from three complementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution generalization, and (iii) Forward knowledge transfer. Our results indicate that the learned feature representations closely mirror the spectral bias introduced by the preconditioner -- favoring components that are emphasized and exhibiting reduced sensitivity to those that are suppressed. Crucially, we demonstrate that generalization is significantly enhanced when this spectral bias is aligned with that of the teacher.
<div id='section'>Paperid: <span id='pid'>1899, <a href='https://arxiv.org/pdf/2509.25158.pdf' target='_blank'>https://arxiv.org/pdf/2509.25158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehimare Okoyomon, Arbel Yaniv, Christoph Goebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25158">Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Voltage prediction in distribution grids is a critical yet difficult task for maintaining power system stability. Machine learning approaches, particularly Graph Neural Networks (GNNs), offer significant speedups but suffer from poor generalization when trained on limited or incomplete data. In this work, we systematically investigate the role of inductive biases in improving a model's ability to reliably learn power flow. Specifically, we evaluate three physics-informed strategies: (i) power-flow-constrained loss functions, (ii) complex-valued neural networks, and (iii) residual-based task reformulation. Using the ENGAGE dataset, which spans multiple low- and medium-voltage grid configurations, we conduct controlled experiments to isolate the effect of each inductive bias and assess both standard predictive performance and out-of-distribution generalization. Our study provides practical insights into which model assumptions most effectively guide learning for reliable and efficient voltage prediction in modern distribution networks.
<div id='section'>Paperid: <span id='pid'>1900, <a href='https://arxiv.org/pdf/2509.23631.pdf' target='_blank'>https://arxiv.org/pdf/2509.23631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Yang, Changhao Zhao, Chen Wang, Jiansheng Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23631">DRIK: Distribution-Robust Inductive Kriging without Information Leakage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inductive kriging supports high-resolution spatio-temporal estimation with sparse sensor networks, but conventional training-evaluation setups often suffer from information leakage and poor out-of-distribution (OOD) generalization. We find that the common 2x2 spatio-temporal split allows test data to influence model selection through early stopping, obscuring the true OOD characteristics of inductive kriging. To address this issue, we propose a 3x3 partition that cleanly separates training, validation, and test sets, eliminating leakage and better reflecting real-world applications. Building on this redefined setting, we introduce DRIK, a Distribution-Robust Inductive Kriging approach designed with the intrinsic properties of inductive kriging in mind to explicitly enhance OOD generalization, employing a three-tier strategy at the node, edge, and subgraph levels. DRIK perturbs node coordinates to capture continuous spatial relationships, drops edges to reduce ambiguity in information flow and increase topological diversity, and adds pseudo-labeled subgraphs to strengthen domain generalization. Experiments on six diverse spatio-temporal datasets show that DRIK consistently outperforms existing methods, achieving up to 12.48% lower MAE while maintaining strong scalability.
<div id='section'>Paperid: <span id='pid'>1901, <a href='https://arxiv.org/pdf/2509.20785.pdf' target='_blank'>https://arxiv.org/pdf/2509.20785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jincai Song, Haipeng Chen, Jun Qin, Na Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20785">Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised domain generalization (SSDG) in medical image segmentation offers a promising solution for generalizing to unseen domains during testing, addressing domain shift challenges and minimizing annotation costs. However, conventional SSDG methods assume labeled and unlabeled data are available for each source domain in the training set, a condition that is not always met in practice. The coexistence of limited annotation and domain shift in the training set is a prevalent issue. Thus, this paper explores a more practical and challenging scenario, cross-domain semi-supervised domain generalization (CD-SSDG), where domain shifts occur between labeled and unlabeled training data, in addition to shifts between training and testing sets. Existing SSDG methods exhibit sub-optimal performance under such domain shifts because of inaccurate pseudolabels. To address this issue, we propose a novel dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG. Building upon the co-training paradigm with two sub-models offering cross pseudo supervision, our DAC framework integrates extra feature-level supervision and asymmetric auxiliary tasks for each sub-model. This feature-level supervision serves to address inaccurate pseudo supervision caused by domain shifts between labeled and unlabeled data, utilizing complementary supervision from the rich feature space. Additionally, two distinct auxiliary self-supervised tasks are integrated into each sub-model to enhance domain-invariant discriminative feature learning and prevent model collapse. Extensive experiments on real-world medical image segmentation datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust generalizability of the proposed DAC framework.
<div id='section'>Paperid: <span id='pid'>1902, <a href='https://arxiv.org/pdf/2509.20489.pdf' target='_blank'>https://arxiv.org/pdf/2509.20489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>D. Darankoum, C. Habermacher, J. Volle, S. Grudinin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20489">CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography signals (EEGs) contain rich multi-scale information crucial for understanding brain states, with potential applications in diagnosing and advancing the drug development landscape. However, extracting meaningful features from raw EEG signals while handling noise and channel variability remains a major challenge. This work proposes a novel end-to-end deep-learning framework that addresses these issues through several key innovations. First, we designed an encoder capable of explicitly capturing multi-scale frequency oscillations covering a wide range of features for different EEG-related tasks. Secondly, to model complex dependencies and handle the high temporal resolution of EEGs, we introduced an attention-based encoder that simultaneously learns interactions across EEG channels and within localized {\em patches} of individual channels. We integrated a dedicated gating network on top of the attention encoder to dynamically filter out noisy and non-informative channels, enhancing the reliability of EEG data. The entire encoding process is guided by a novel loss function, which leverages supervised and contrastive learning, significantly improving model generalization. We validated our approach in multiple applications, ranging from the classification of effects across multiple Central Nervous System (CNS) disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease. Our results demonstrate that the proposed learning paradigm can extract biologically meaningful patterns from raw EEG signals across different species, autonomously select high-quality channels, and achieve robust generalization through innovative architectural and loss design.
<div id='section'>Paperid: <span id='pid'>1903, <a href='https://arxiv.org/pdf/2509.19372.pdf' target='_blank'>https://arxiv.org/pdf/2509.19372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuzanna Dubanowska, Maciej Żelaszczyk, Michał Brzozowski, Paolo Mandica, Michał Karpowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19372">Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We critically assess the efficacy of the current SOTA in hallucination detection and find that its performance on the RAGTruth dataset is largely driven by a spurious correlation with data. Controlling for this effect, state-of-the-art performs no better than supervised linear probes, while requiring extensive hyperparameter tuning across datasets. Out-of-distribution generalization is currently out of reach, with all of the analyzed methods performing close to random. We propose a set of guidelines for hallucination detection and its evaluation.
<div id='section'>Paperid: <span id='pid'>1904, <a href='https://arxiv.org/pdf/2509.18901.pdf' target='_blank'>https://arxiv.org/pdf/2509.18901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Popovič, Michael Färber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18901">Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works in Natural Language Inference (NLI) and related tasks, such as automated fact-checking, employ atomic fact decomposition to enhance interpretability and robustness. For this, existing methods rely on resource-intensive generative large language models (LLMs) to perform decomposition. We propose JEDI, an encoder-only architecture that jointly performs extractive atomic fact decomposition and interpretable inference without requiring generative models during inference. To facilitate training, we produce a large corpus of synthetic rationales covering multiple NLI benchmarks. Experimental results demonstrate that JEDI achieves competitive accuracy in distribution and significantly improves robustness out of distribution and in adversarial settings over models based solely on extractive rationale supervision. Our findings show that interpretability and robust generalization in NLI can be realized using encoder-only architectures and synthetic rationales. Code and data available at https://jedi.nicpopovic.com
<div id='section'>Paperid: <span id='pid'>1905, <a href='https://arxiv.org/pdf/2509.16686.pdf' target='_blank'>https://arxiv.org/pdf/2509.16686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengge Cai, Haowen Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16686">EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it incurs significant memory overhead. Recent work on Multi-head Latent Attention (MLA) mitigates this by compressing KV representations into a shared latent space, achieving a better trade-off between performance and cache efficiency. While MLA already achieves significant KV cache reduction, the scope for further compression remains limited without performance loss. In this paper, we propose \textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel extension of MLA that further reduces KV cache size while enhancing representational expressiveness. EG-MLA introduces a token-specific embedding gating mechanism applied in the latent space, enabling fine-grained modulation of compressed KV vectors with minimal additional computation. Compared to MHA, EG-MLA achieves over 91.6\% reduction in KV cache size with negligible performance degradation. Relative to MLA, EG-MLA consistently improves task accuracy across diverse reasoning benchmarks while achieving up to 59.9\% additional memory savings. Our theoretical analysis highlights how embedding gating induces implicit high-order interactions, and empirical evaluations demonstrate robust generalization across model scales and compression regimes. Notably, we successfully scale EG-MLA to over 1 billion parameters, demonstrating its practical viability for large-scale LLM deployment. These results establish EG-MLA as a memory- and compute-efficient attention mechanism that enables scalable, high-performance inference in modern LLMs.
<div id='section'>Paperid: <span id='pid'>1906, <a href='https://arxiv.org/pdf/2509.10502.pdf' target='_blank'>https://arxiv.org/pdf/2509.10502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sujatha Kotte, Vangala Govindakrishnan Saipradeep, Vidushi Walia, Dhandapani Nandagopal, Thomas Joseph, Naveen Sivadasan, Bhagat Singh Lali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10502">MIDOG 2025 Track 2: A Deep Learning Model for Classification of Atypical and Normal Mitotic Figures under Class and Hardness Imbalances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivation: Accurate classification of mitotic figures into normal and atypical types is crucial for tumor prognostication in digital pathology. However, developing robust deep learning models for this task is challenging due to the subtle morphological differences, as well as significant class and hardness imbalances in real-world histopathology datasets. Methods: We propose a novel deep learning approach based on a ResNet backbone with specialized classification heads. Our architecture uniquely models both the mitotic figure phenotype and the instance difficulty simultaneously. This method is specifically designed to handle the challenges of diverse tissue types, scanner variability, and imbalanced data. We employed focal loss to effectively mitigate the pronounced class imbalance, and a comprehensive data augmentation pipeline was implemented to enhance the model's robustness and generalizability. Results: Our approach demonstrated strong and consistent performance. In a 5-fold cross-validation on the MIDOG 2025 Track 2 dataset, it achieved a mean balanced accuracy of 0.8744 +/- 0.0093 and an ROC AUC of 0.9505 +/- 0.029. The model showed robust generalization across preliminary leaderboard evaluations, achieving an overall balanced accuracy of 0.8736 +/- 0.0204. Conclusion: The proposed method offers a reliable and generalizable solution for the classification of atypical and normal mitotic figures. By addressing the inherent challenges of real world data, our approach has the potential to support precise prognostic assessments in clinical practice and improve consistency in pathological diagnosis.
<div id='section'>Paperid: <span id='pid'>1907, <a href='https://arxiv.org/pdf/2509.09251.pdf' target='_blank'>https://arxiv.org/pdf/2509.09251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanyang Wang, Yuxuan Yang, Hongjun Wang, Lihui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09251">Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intelligent fault diagnosis of rotating mechanical equipment usually requires a large amount of labeled sample data. However, in practical industrial applications, acquiring enough data is both challenging and expensive in terms of time and cost. Moreover, different types of rotating mechanical equipment with different unique mechanical properties, require separate training of diagnostic models for each case. To address the challenges of limited fault samples and the lack of generalizability in prediction models for practical engineering applications, we propose a Multi-Attention Meta Transformer method for few-shot unsupervised rotating machinery fault diagnosis (MMT-FD). This framework extracts potential fault representations from unlabeled data and demonstrates strong generalization capabilities, making it suitable for diagnosing faults across various types of mechanical equipment. The MMT-FD framework integrates a time-frequency domain encoder and a meta-learning generalization model. The time-frequency domain encoder predicts status representations generated through random augmentations in the time-frequency domain. These enhanced data are then fed into a meta-learning network for classification and generalization training, followed by fine-tuning using a limited amount of labeled data. The model is iteratively optimized using a small number of contrastive learning iterations, resulting in high efficiency. To validate the framework, we conducted experiments on a bearing fault dataset and rotor test bench data. The results demonstrate that the MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled sample data, exhibiting robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1908, <a href='https://arxiv.org/pdf/2509.03137.pdf' target='_blank'>https://arxiv.org/pdf/2509.03137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yi, Qian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03137">A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is widely adopted as a standard method for radionuclide quantification because of its inherent advantages such as high precision, self-calibrating capability, and independence from radioactive reference sources. However, multiradionuclide analysis via TDCR faces the challenges of limited automation and reliance on mixture-specific standards, which may not be easily available. Here, we present an Artificial Intelligence (AI) framework that combines numerical spectral simulation and deep learning for standard-free automated analysis. $β$ spectra for model training were generated using Geant4 simulations coupled with statistically modeled detector response sampling. A tailored neural network architecture, trained on this dataset covering various nuclei mix ratio and quenching scenarios, enables autonomous resolution of individual radionuclide activities and detecting efficiency through end-to-end learning paradigms. The model delivers consistent high accuracy across tasks: activity proportions (mean absolute error = 0.009), detection efficiencies (mean absolute error = 0.002), and spectral reconstruction (Structural Similarity Index = 0.9998), validating its physical plausibility for quenched $β$ spectroscopy. This AI-driven methodology exhibits significant potential for automated safety-compliant multiradionuclide analysis with robust generalization, real-time processing capabilities, and engineering feasibility, particularly in scenarios where reference materials are unavailable or rapid field analysis is required.
<div id='section'>Paperid: <span id='pid'>1909, <a href='https://arxiv.org/pdf/2509.02630.pdf' target='_blank'>https://arxiv.org/pdf/2509.02630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Euiseop Song, Jaeyoung Park, Jaewoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02630">Challenges and Lessons from MIDOG 2025: A Two-Stage Approach to Domain-Robust Mitotic Figure Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitotic figure detection remains a challenging task in computational pathology due to domain variability and morphological complexity. This paper describes our participation in the MIDOG 2025 challenge, focusing on robust mitotic figure detection across diverse tissue domains. We developed a two-stage pipeline combining Faster R-CNN for candidate detection with an ensemble of three classifiers (DenseNet-121, EfficientNet-v2, InceptionResNet-v2) for false positive reduction. Our best submission achieved F1-score 0.2237 (Recall: 0.9528, Precision: 0.1267) using a Faster R-CNN trained solely on MIDOG++ dataset. While our high recall demonstrates effective mitotic figure detection, the critically low precision (12.67%) reveals fundamental challenges in distinguishing true mitoses from morphologically similar imposters across diverse domains. Analysis of six submission variants showed that subsequent optimization attempts were counterproductive, highlighting the omplexity of domain generalization in histopathology. This work provides valuable insights into the practical challenges of developing robust mitotic figure detection algorithms and emphasizes the importance of effective false positive suppression strategies.
<div id='section'>Paperid: <span id='pid'>1910, <a href='https://arxiv.org/pdf/2509.02589.pdf' target='_blank'>https://arxiv.org/pdf/2509.02589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Qi, Dominic Labella, Thomas Sanford, Maxwell Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02589">Normal and Atypical Mitosis Image Classifier using Efficient Vision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle atypical versus normal mitosis classification in the MIDOG 2025 challenge using EfficientViT-L2, a hybrid CNN--ViT architecture optimized for accuracy and efficiency. A unified dataset of 13,938 nuclei from seven cancer types (MIDOG++ and AMi-Br) was used, with atypical mitoses comprising ~15. To assess domain generalization, we applied leave-one-cancer-type-out cross-validation with 5-fold ensembles, using stain-deconvolution for image augmentation. For challenge submissions, we trained an ensemble with the same 5-fold split but on all cancer types. In the preliminary evaluation phase, this model achieved balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy of 0.85, demonstrating competitive and well-balanced performance across metrics.
<div id='section'>Paperid: <span id='pid'>1911, <a href='https://arxiv.org/pdf/2509.02287.pdf' target='_blank'>https://arxiv.org/pdf/2509.02287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pushpendra Dhakara, Prachi Chachodhia, Vaibhav Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02287">SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unstructured urban environments present unique challenges for scene understanding and generalization due to their complex and diverse layouts. We introduce SynthGenNet, a self-supervised student-teacher architecture designed to enable robust test-time domain generalization using synthetic multi-source imagery. Our contributions include the novel ClassMix++ algorithm, which blends labeled data from various synthetic sources while maintaining semantic integrity, enhancing model adaptability. We further employ Grounded Mask Consistency Loss (GMC), which leverages source ground truth to improve cross-domain prediction consistency and feature alignment. The Pseudo-Label Guided Contrastive Learning (PLGCL) mechanism is integrated into the student network to facilitate domain-invariant feature learning through iterative knowledge distillation from the teacher network. This self-supervised strategy improves prediction accuracy, addresses real-world variability, bridges the sim-to-real domain gap, and reliance on labeled target data, even in complex urban areas. Outcomes show our model outperforms the state-of-the-art (relying on single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value on real-world datasets like Indian Driving Dataset (IDD).
<div id='section'>Paperid: <span id='pid'>1912, <a href='https://arxiv.org/pdf/2509.00010.pdf' target='_blank'>https://arxiv.org/pdf/2509.00010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuchang Liu, Paul A. O'Gorman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00010">CERA: A Framework for Improved Generalization of Machine Learning Models to Changed Climates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust generalization under climate change remains a major challenge for machine learning applications in climate science. Most existing approaches struggle to extrapolate beyond the climate they were trained on, leading to a strong dependence on training data from model simulations of warm climates. Use of climate-invariant inputs improves generalization but requires challenging manual feature engineering. Here, we present CERA (Climate-invariant Encoding through Representation Alignment), a machine learning framework consisting of an autoencoder with explicit latent-space alignment, followed by a predictor for downstream process estimation. We test CERA on the problem of parameterizing moist-physics processes. Without training on labeled data from a +4K climate, CERA leverages labeled control-climate data and unlabeled warmer-climate inputs to improve generalization to the warmer climate, outperforming both raw-input and physically informed baselines in predicting key moisture and energy tendencies. It captures not only the vertical and meridional structures of the moisture tendencies, but also shifts in the intensity distribution of precipitation including extremes. Ablation experiments show that latent alignment improves both accuracy and the robustness across random seeds used in training. While some reduced skill remains in the boundary layer, the framework offers a data-driven alternative to manual feature engineering of climate invariant inputs. Beyond parameterizations used in hybrid ML-physics systems, the approach holds promise for other climate applications such as statistical downscaling.
<div id='section'>Paperid: <span id='pid'>1913, <a href='https://arxiv.org/pdf/2508.19804.pdf' target='_blank'>https://arxiv.org/pdf/2508.19804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Marzahl, Brian Napora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19804">A bag of tricks for real-time Mitotic Figure detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitotic figure (MF) detection in histopathology images is challenging due to large variations in slide scanners, staining protocols, tissue types, and the presence of artifacts. This paper presents a collection of training techniques - a bag of tricks - that enable robust, real-time MF detection across diverse domains. We build on the efficient RTMDet single stage object detector to achieve high inference speed suitable for clinical deployment. Our method addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling, and careful augmentation. Additionally, we employ targeted, hard negative mining on necrotic and debris tissue to reduce false positives. In a grouped 5-fold cross-validation across multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025 challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81, outperforming larger models and demonstrating adaptability to new, unfamiliar domains. The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption.
<div id='section'>Paperid: <span id='pid'>1914, <a href='https://arxiv.org/pdf/2508.15452.pdf' target='_blank'>https://arxiv.org/pdf/2508.15452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>UÄurcan AkyÃ¼z, Deniz Katircioglu-ÃztÃ¼rk, Emre K. SÃ¼slÃ¼, Burhan KeleÅ, Mete C. Kaya, Gamze Durhan, Meltem G. AkpÄ±nar, Figen B. DemirkazÄ±k, GÃ¶zde B. Akar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15452">DoSReMC: Domain Shift Resilient Mammography Classification using Batch Normalization Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous deep learning-based solutions have been developed for the automatic recognition of breast cancer using mammography images. However, their performance often declines when applied to data from different domains, primarily due to domain shift -- the variation in data distributions between source and target domains. This performance drop limits the safe and equitable deployment of AI in real-world clinical settings. In this study, we present DoSReMC (Domain Shift Resilient Mammography Classification), a batch normalization (BN) adaptation framework designed to enhance cross-domain generalization without retraining the entire model. Using three large-scale full-field digital mammography (FFDM) datasets -- including HCTP, a newly introduced, pathologically confirmed in-house dataset -- we conduct a systematic cross-domain evaluation with convolutional neural networks (CNNs). Our results demonstrate that BN layers are a primary source of domain dependence: they perform effectively when training and testing occur within the same domain, and they significantly impair model generalization under domain shift. DoSReMC addresses this limitation by fine-tuning only the BN and fully connected (FC) layers, while preserving pretrained convolutional filters. We further integrate this targeted adaptation with an adversarial training scheme, yielding additional improvements in cross-domain generalizability. DoSReMC can be readily incorporated into existing AI pipelines and applied across diverse clinical environments, providing a practical pathway toward more robust and generalizable mammography classification systems.
<div id='section'>Paperid: <span id='pid'>1915, <a href='https://arxiv.org/pdf/2508.11354.pdf' target='_blank'>https://arxiv.org/pdf/2508.11354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyi Zhao, Muthu Rama Krishnan Mookiah, Emanuele Trucco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11354">Leveraging the RETFound foundation model for optic disc segmentation in retinal images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RETFound is a well-known foundation model (FM) developed for fundus camera and optical coherence tomography images. It has shown promising performance across multiple datasets in diagnosing diseases, both eye-specific and systemic, from retinal images. However, to our best knowledge, it has not been used for other tasks. We present the first adaptation of RETFound for optic disc segmentation, a ubiquitous and foundational task in retinal image analysis. The resulting segmentation system outperforms state-of-the-art, segmentation-specific baseline networks after training a head with only a very modest number of task-specific examples. We report and discuss results with four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private dataset, GoDARTS, achieving about 96% Dice consistently across all datasets. Overall, our method obtains excellent performance in internal verification, domain generalization and domain adaptation, and exceeds most of the state-of-the-art baseline results. We discuss the results in the framework of the debate about FMs as alternatives to task-specific architectures. The code is available at: [link to be added after the paper is accepted]
<div id='section'>Paperid: <span id='pid'>1916, <a href='https://arxiv.org/pdf/2508.08644.pdf' target='_blank'>https://arxiv.org/pdf/2508.08644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guiming Cao, Yuming Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08644">AME: Aligned Manifold Entropy for Robust Vision-Language Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation is a long-established technique for knowledge transfer, and has regained attention in the context of the recent emergence of large vision-language models (VLMs). However, vision-language knowledge distillation often requires sufficient training data to achieve robust generalization on amples with ambiguous or boundary-adjacent representations, which are associated with high predictive uncertainty. Critically, collecting such large-scale, task-specific data for training is often impractical in real-world scenarios. To address this major challenge arising from the entanglement of uncertainty and cross-modal feature representation, we propose Aligned Manifold Entropy for Robust Vision-Language Distillation (AME), aiming to achieve robust generalization under real-world conditions. AME applies entropy minimization over a reconfigured shared manifold, where multi-modal data (i.e., image and text) are bridged through a pair of projection functions, conducive to structural compression for cross-modal feature representations. This enables robust knowledge distillation under low-data regimes, while requiring no architectural modifications to the backbone. As a result, it can serve as a plug-and-play module compatible with a wide range of vision-language distillation frameworks. Notably, our theoretical analysis reveals that integrating knowledge distillation with entropy minimization over the shared manifold leads to a tighter generalization error bound. Extensive experiments across diverse distillation architectures and training settings demonstrate that AME consistently facilitates robust knowledge distillation, resulting in superior generalization performance across a wide spectrum of downstream tasks.
<div id='section'>Paperid: <span id='pid'>1917, <a href='https://arxiv.org/pdf/2508.03555.pdf' target='_blank'>https://arxiv.org/pdf/2508.03555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Chaffin, RaphaÃ«l Sourty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03555">PyLate: Flexible Training and Retrieval for Late Interaction Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural ranking has become a cornerstone of modern information retrieval. While single vector search remains the dominant paradigm, it suffers from the shortcoming of compressing all the information into a single vector. This compression leads to notable performance degradation in out-of-domain, long-context, and reasoning-intensive retrieval tasks. Multi-vector approaches pioneered by ColBERT aim to address these limitations by preserving individual token embeddings and computing similarity via the MaxSim operator. This architecture has demonstrated superior empirical advantages, including enhanced out-of-domain generalization, long-context handling, and performance in complex retrieval scenarios. Despite these compelling empirical results and clear theoretical advantages, the practical adoption and public availability of late interaction models remain low compared to their single-vector counterparts, primarily due to a lack of accessible and modular tools for training and experimenting with such models. To bridge this gap, we introduce PyLate, a streamlined library built on top of Sentence Transformers to support multi-vector architectures natively, inheriting its efficient training, advanced logging, and automated model card generation while requiring minimal code changes to code templates users are already familiar with. By offering multi-vector-specific features such as efficient indexes, PyLate aims to accelerate research and real-world application of late interaction models, thereby unlocking their full potential in modern IR systems. Finally, PyLate has already enabled the development of state-of-the-art models, including GTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility for both research and production environments.
<div id='section'>Paperid: <span id='pid'>1918, <a href='https://arxiv.org/pdf/2507.22485.pdf' target='_blank'>https://arxiv.org/pdf/2507.22485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Bochow, Philipp Hess, Alexander Robinson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22485">Physics-constrained generative machine learning-based high-resolution downscaling of Greenland's surface mass balance and surface temperature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate, high-resolution projections of the Greenland ice sheet's surface mass balance (SMB) and surface temperature are essential for understanding future sea-level rise, yet current approaches are either computationally demanding or limited to coarse spatial scales. Here, we introduce a novel physics-constrained generative modeling framework based on a consistency model (CM) to downscale low-resolution SMB and surface temperature fields by a factor of up to 32 (from 160 km to 5 km grid spacing) in a few sampling steps. The CM is trained on monthly outputs of the regional climate model MARv3.12 and conditioned on ice-sheet topography and insolation. By enforcing a hard conservation constraint during inference, we ensure approximate preservation of SMB and temperature sums on the coarse spatial scale as well as robust generalization to extreme climate states without retraining. On the test set, our constrained CM achieves a continued ranked probability score of 6.31 mmWE for the SMB and 0.1 K for the surface temperature, outperforming interpolation-based downscaling. Together with spatial power-spectral analysis, we demonstrate that the CM faithfully reproduces variability across spatial scales. We further apply bias-corrected outputs of the NorESM2 Earth System Model as inputs to our CM, to demonstrate the potential of our model to directly downscale ESM fields. Our approach delivers realistic, high-resolution climate forcing for ice-sheet simulations with fast inference and can be readily integrated into Earth-system and ice-sheet model workflows to improve projections of the future contribution to sea-level rise from Greenland and potentially other ice sheets and glaciers too.
<div id='section'>Paperid: <span id='pid'>1919, <a href='https://arxiv.org/pdf/2507.20913.pdf' target='_blank'>https://arxiv.org/pdf/2507.20913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialei Cui, Jianwei Du, Yanzhe Li, Lei Gao, Hui Jiang, Chenfu Bao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20913">HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of face manipulation techniques poses a critical challenge for face forgery detection: cross-domain generalization. Conventional methods, which rely on simple classification objectives, often fail to learn domain-invariant representations. We propose HAMLET-FFD, a cognitively inspired Hierarchical Adaptive Multi-modal Learning framework that tackles this challenge via bidirectional cross-modal reasoning. Building on contrastive vision-language models such as CLIP, HAMLET-FFD introduces a knowledge refinement loop that iteratively assesses authenticity by integrating visual evidence with conceptual cues, emulating expert forensic analysis. A key innovation is a bidirectional fusion mechanism in which textual authenticity embeddings guide the aggregation of hierarchical visual features, while modulated visual features refine text embeddings to generate image-adaptive prompts. This closed-loop process progressively aligns visual observations with semantic priors to enhance authenticity assessment. By design, HAMLET-FFD freezes all pretrained parameters, serving as an external plugin that preserves CLIP's original capabilities. Extensive experiments demonstrate its superior generalization to unseen manipulations across multiple benchmarks, and visual analyses reveal a division of labor among embeddings, with distinct representations specializing in fine-grained artifact recognition.
<div id='section'>Paperid: <span id='pid'>1920, <a href='https://arxiv.org/pdf/2507.17924.pdf' target='_blank'>https://arxiv.org/pdf/2507.17924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongrong Yang, Markus Schlaepfer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17924">UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate population flow prediction is essential for urban planning, transportation management, and public health. Yet existing methods face key limitations: traditional models rely on static spatial assumptions, deep learning models struggle with cross-city generalization, and Large Language Models (LLMs) incur high computational costs while failing to capture spatial structure. Moreover, many approaches sacrifice resolution by clustering Points of Interest (POIs) or restricting coverage to subregions, limiting their utility for city-wide analytics. We introduce UrbanPulse, a scalable deep learning framework that delivers ultra-fine-grained, city-wide OD flow predictions by treating each POI as an individual node. It combines a temporal graph convolutional encoder with a transformer-based decoder to model multi-scale spatiotemporal dependencies. To ensure robust generalization across urban contexts, UrbanPulse employs a three-stage transfer learning strategy: pretraining on large-scale urban graphs, cold-start adaptation, and reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS records from three metropolitan areas in California, UrbanPulse achieves state-of-the-art accuracy and scalability. Through efficient transfer learning, UrbanPulse takes a key step toward making high-resolution, AI-powered urban forecasting deployable in practice across diverse cities.
<div id='section'>Paperid: <span id='pid'>1921, <a href='https://arxiv.org/pdf/2507.17326.pdf' target='_blank'>https://arxiv.org/pdf/2507.17326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milena Davudova, Ziyuan Cai, Valentina Giunchiglia, Dragos C. Gruia, Giulia Sanguedolce, Adam Hampshire, Fatemeh Geranmayeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17326">Application of Whisper in Clinical Practice: the Post-Stroke Speech Assessment during a Naming Task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detailed assessment of language impairment following stroke remains a cognitively complex and clinician-intensive task, limiting timely and scalable diagnosis. Automatic Speech Recognition (ASR) foundation models offer a promising pathway to augment human evaluation through intelligent systems, but their effectiveness in the context of speech and language impairment remains uncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASR foundation model, can be applied to transcribe and analyze speech from patients with stroke during a commonly used picture-naming task. We assess both verbatim transcription accuracy and the model's ability to support downstream prediction of language function, which has major implications for outcomes after stroke. Our results show that the baseline Whisper model performs poorly on single-word speech utterances. Nevertheless, fine-tuning Whisper significantly improves transcription accuracy (reducing Word Error Rate by 87.72% in healthy speech and 71.22% in speech from patients). Further, learned representations from the model enable accurate prediction of speech quality (average F1 Macro of 0.74 for healthy, 0.75 for patients). However, evaluations on an unseen (TORGO) dataset reveal limited generalizability, highlighting the inability of Whisper to perform zero-shot transcription of single-word utterances on out-of-domain clinical speech and emphasizing the need to adapt models to specific clinical populations. While challenges remain in cross-domain generalization, these findings highlight the potential of foundation models, when appropriately fine-tuned, to advance automated speech and language assessment and rehabilitation for stroke-related impairments.
<div id='section'>Paperid: <span id='pid'>1922, <a href='https://arxiv.org/pdf/2507.16571.pdf' target='_blank'>https://arxiv.org/pdf/2507.16571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>G. de RomÃ©mont, F. Renac, F. Chinesta, J. Nunez, D. Gueyffier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16571">Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume Computations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel data-driven approach for enhancing gradient reconstruction in unstructured finite volume methods for hyperbolic conservation laws, specifically for the 2D Euler equations. Our approach extends previous structured-grid methodologies to unstructured meshes through a modified DeepONet architecture that incorporates local geometry in the neural network. The architecture employs local mesh topology to ensure rotation invariance, while also ensuring first-order constraint on the learned operator. The training methodology incorporates physics-informed regularization through entropy penalization, total variation diminishing penalization, and parameter regularization to ensure physically consistent solutions, particularly in shock-dominated regions. The model is trained on high-fidelity datasets solutions derived from sine waves and randomized piecewise constant initial conditions with periodic boundary conditions, enabling robust generalization to complex flow configurations or geometries. Validation test cases from the literature, including challenging geometry configuration, demonstrates substantial improvements in accuracy compared to traditional second-order finite volume schemes. The method achieves gains of 20-60% in solution accuracy while enhancing computational efficiency. A convergence study has been conveyed and reveal improved mesh convergence rates compared to the conventional solver. The proposed algorithm is faster and more accurate than the traditional second-order finite volume solver, enabling high-fidelity simulations on coarser grids while preserving the stability and conservation properties essential for hyperbolic conservation laws. This work is a part of a new generation of solvers that are built by combining Machine-Learning (ML) tools with traditional numerical schemes, all while ensuring physical constraint on the results.
<div id='section'>Paperid: <span id='pid'>1923, <a href='https://arxiv.org/pdf/2507.16406.pdf' target='_blank'>https://arxiv.org/pdf/2507.16406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanveer Younis, Zhanglin Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16406">Sparse-View 3D Reconstruction: Recent Advances and Open Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse-view 3D reconstruction is essential for applications in which dense image acquisition is impractical, such as robotics, augmented/virtual reality (AR/VR), and autonomous systems. In these settings, minimal image overlap prevents reliable correspondence matching, causing traditional methods, such as structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey reviews the latest advances in neural implicit models (e.g., NeRF and its regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian Splatting), and hybrid frameworks that leverage priors from diffusion and vision foundation models (VFMs).We analyze how geometric regularization, explicit shape modeling, and generative inference are used to mitigate artifacts such as floaters and pose ambiguities in sparse-view settings. Comparative results on standard benchmarks reveal key trade-offs between the reconstruction accuracy, efficiency, and generalization. Unlike previous reviews, our survey provides a unified perspective on geometry-based, neural implicit, and generative (diffusion-based) methods. We highlight the persistent challenges in domain generalization and pose-free reconstruction and outline future directions for developing 3D-native generative priors and achieving real-time, unconstrained sparse-view reconstruction.
<div id='section'>Paperid: <span id='pid'>1924, <a href='https://arxiv.org/pdf/2507.14592.pdf' target='_blank'>https://arxiv.org/pdf/2507.14592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Liu, Jia Bi, Xiaomin Wang, Xin Yang, Ling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14592">A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance, logistics, agriculture, disaster management, and military operations. Accurate detection and classification of UAV flight states, such as hovering, cruising, ascending, or transitioning, which are essential for safe and effective operations. However, conventional time series classification (TSC) methods often lack robustness and generalization for dynamic UAV environments, while state of the art(SOTA) models like Transformers and LSTM based architectures typically require large datasets and entail high computational costs, especially with high-dimensional data streams. This paper proposes a novel framework that integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET) to address these challenges in UAV flight state classification. The Transformer encoder captures long-range temporal dependencies and complex telemetry dynamics, while the GAN module augments limited datasets with realistic synthetic samples. MIL is incorporated to focus attention on the most discriminative input segments, reducing noise and computational overhead. Experimental results show that the proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and 98.6% on the DroneRF dataset that outperforming other SOTA approaches. The framework also demonstrates strong computational efficiency and robust generalization across diverse UAV platforms and flight states, highlighting its potential for real-time deployment in resource constrained environments.
<div id='section'>Paperid: <span id='pid'>1925, <a href='https://arxiv.org/pdf/2507.06111.pdf' target='_blank'>https://arxiv.org/pdf/2507.06111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad H. Danesh, Maxime Wabartha, Stanley Wu, Joelle Pineau, Hsiu-Chin Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06111">Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying reinforcement learning (RL) policies in real-world involves significant challenges, including distribution shifts, safety concerns, and the impracticality of direct interactions during policy refinement. Existing methods, such as domain randomization (DR) and off-dynamics RL, enhance policy robustness by direct interaction with the target domain, an inherently unsafe practice. We propose Uncertainty-Aware RL (UARL), a novel framework that prioritizes safety during training by addressing Out-Of-Distribution (OOD) detection and policy adaptation without requiring direct interactions in target domain. UARL employs an ensemble of critics to quantify policy uncertainty and incorporates progressive environmental randomization to prepare the policy for diverse real-world conditions. By iteratively refining over high-uncertainty regions of the state space in simulated environments, UARL enhances robust generalization to the target domain without explicitly training on it. We evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its effectiveness in reliable OOD detection, improved performance, and enhanced sample efficiency compared to baselines.
<div id='section'>Paperid: <span id='pid'>1926, <a href='https://arxiv.org/pdf/2507.04302.pdf' target='_blank'>https://arxiv.org/pdf/2507.04302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuyu Zhang, Ning Chen, Yongshan Liu, Qinghua Zhang, Xu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04302">Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Domain Generalization (SDG) aims to develop models capable of generalizing to unseen target domains using only one source domain, a task complicated by substantial domain shifts and limited data diversity. Existing SDG approaches primarily rely on data augmentation techniques, which struggle to effectively adapt training dynamics to accommodate large domain shifts. To address this, we propose LEAwareSGD, a novel Lyapunov Exponent (LE)-guided optimization approach inspired by dynamical systems theory. By leveraging LE measurements to modulate the learning rate, LEAwareSGD encourages model training near the edge of chaos, a critical state that optimally balances stability and adaptability. This dynamic adjustment allows the model to explore a wider parameter space and capture more generalizable features, ultimately enhancing the model's generalization capability. Extensive experiments on PACS, OfficeHome, and DomainNet demonstrate that LEAwareSGD yields substantial generalization gains, achieving up to 9.47\% improvement on PACS in low-data regimes. These results underscore the effectiveness of training near the edge of chaos for enhancing model generalization capability in SDG tasks.
<div id='section'>Paperid: <span id='pid'>1927, <a href='https://arxiv.org/pdf/2507.03146.pdf' target='_blank'>https://arxiv.org/pdf/2507.03146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ron Tsibulsky, Daniel Nevo, Uri Shalit
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03146">Set Valued Predictions For Robust Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the impressive advancements in modern machine learning, achieving robustness in Domain Generalization (DG) tasks remains a significant challenge. In DG, models are expected to perform well on samples from unseen test distributions (also called domains), by learning from multiple related training distributions. Most existing approaches to this problem rely on single-valued predictions, which inherently limit their robustness. We argue that set-valued predictors could be leveraged to enhance robustness across unseen domains, while also taking into account that these sets should be as small as possible. We introduce a theoretical framework defining successful set prediction in the DG setting, focusing on meeting a predefined performance criterion across as many domains as possible, and provide theoretical insights into the conditions under which such domain generalization is achievable. We further propose a practical optimization method compatible with modern learning architectures, that balances robust performance on unseen domains with small prediction set sizes. We evaluate our approach on several real-world datasets from the WILDS benchmark, demonstrating its potential as a promising direction for robust domain generalization.
<div id='section'>Paperid: <span id='pid'>1928, <a href='https://arxiv.org/pdf/2506.22454.pdf' target='_blank'>https://arxiv.org/pdf/2506.22454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Luiza S. Tavares, Artur Pedro M. Neto, Francinaldo L. Gomes, Paul Rodrigo dos Reis, Arthur G. da Silva, Antonio P. Junior, Bruno D. Gomes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22454">Microelectrode Signal Dynamics as Biomarkers of Subthalamic Nucleus Entry on Deep Brain Stimulation: A Nonlinear Feature Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate intraoperative localization of the subthalamic nucleus (STN) is essential for the efficacy of Deep Brain Stimulation (DBS) in patients with Parkinson's disease. While microelectrode recordings (MERs) provide rich electrophysiological information during DBS electrode implantation, current localization practices often rely on subjective interpretation of signal features. In this study, we propose a quantitative framework that leverages nonlinear dynamics and entropy-based metrics to classify neural activity recorded inside versus outside the STN. MER data from three patients were preprocessed using a robust artifact correction pipeline, segmented, and labelled based on surgical annotations. A comprehensive set of recurrence quantification analysis, nonlinear, and entropy features were extracted from each segment. Multiple supervised classifiers were trained on every combination of feature domains using stratified 10-fold cross-validation, followed by statistical comparison using paired Wilcoxon signed-rank tests with Holm-Bonferroni correction. The combination of entropy and nonlinear features yielded the highest discriminative power, and the Extra Trees classifier emerged as the best model with a cross-validated F1-score of 0.902+/-0.027 and ROC AUC of 0.887+/-0.055. Final evaluation on a 20% hold-out test set confirmed robust generalization (F1= 0.922, ROC AUC = 0.941). These results highlight the potential of nonlinear and entropy signal descriptors in supporting real-time, data-driven decision-making during DBS surgeries
<div id='section'>Paperid: <span id='pid'>1929, <a href='https://arxiv.org/pdf/2506.20575.pdf' target='_blank'>https://arxiv.org/pdf/2506.20575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Itay Niv, Neta Rabin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20575">Exploring Graph-Transformer Out-of-Distribution Generalization Abilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning on graphs has shown remarkable success across numerous applications, including social networks, bio-physics, traffic networks, and recommendation systems. Regardless of their successes, current methods frequently depend on the assumption that training and testing data share the same distribution, a condition rarely met in real-world scenarios. While graph-transformer (GT) backbones have recently outperformed traditional message-passing neural networks (MPNNs) in multiple in-distribution (ID) benchmarks, their effectiveness under distribution shifts remains largely unexplored. In this work, we address the challenge of out-of-distribution (OOD) generalization for graph neural networks, with a special focus on the impact of backbone architecture. We systematically evaluate GT and hybrid backbones in OOD settings and compare them to MPNNs. To do so, we adapt several leading domain generalization (DG) algorithms to work with GTs and assess their performance on a benchmark designed to test a variety of distribution shifts. Our results reveal that GT and hybrid GT-MPNN backbones demonstrate stronger generalization ability compared to MPNNs, even without specialized DG algorithms (on four out of six benchmarks). Additionally, we propose a novel post-training analysis approach that compares the clustering structure of the entire ID and OOD test datasets, specifically examining domain alignment and class separation. Highlighting its model-agnostic design, the method yielded valuable insights into both GT and MPNN backbones and appears well suited for broader DG applications beyond graph learning, offering a deeper perspective on generalization abilities that goes beyond standard accuracy metrics. Together, our findings highlight the promise of graph-transformers for robust, real-world graph learning and set a new direction for future research in OOD generalization.
<div id='section'>Paperid: <span id='pid'>1930, <a href='https://arxiv.org/pdf/2506.17896.pdf' target='_blank'>https://arxiv.org/pdf/2506.17896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junho Park, Andrew Sangwoo Ye, Taein Kwon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17896">EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric vision is essential for both human and machine visual understanding, particularly in capturing the detailed hand-object interactions needed for manipulation tasks. Translating third-person views into first-person views significantly benefits augmented reality (AR), virtual reality (VR) and robotics applications. However, current exocentric-to-egocentric translation methods are limited by their dependence on 2D cues, synchronized multi-view settings, and unrealistic assumptions such as necessity of initial egocentric frame and relative camera poses during inference. To overcome these challenges, we introduce EgoWorld, a novel two-stage framework that reconstructs an egocentric view from rich exocentric observations, including projected point clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a point cloud from estimated exocentric depth maps, reprojects it into the egocentric perspective, and then applies diffusion-based inpainting to produce dense, semantically coherent egocentric images. Evaluated on the H2O and TACO datasets, EgoWorld achieves state-of-the-art performance and demonstrates robust generalization to new objects, actions, scenes, and subjects. Moreover, EgoWorld shows promising results even on unlabeled real-world examples.
<div id='section'>Paperid: <span id='pid'>1931, <a href='https://arxiv.org/pdf/2506.14835.pdf' target='_blank'>https://arxiv.org/pdf/2506.14835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiet Dang Vu, Trung Thai Tran, Duc Dung Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14835">MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precisely localizing 3D objects from a single image constitutes a central challenge in monocular 3D detection. While DETR-like architectures offer a powerful paradigm, their direct application in this domain encounters inherent limitations, preventing optimal performance. Our work addresses these challenges by introducing MonoVQD, a novel framework designed to fundamentally advance DETR-based monocular 3D detection. We propose three main contributions. First, we propose the Mask Separated Self-Attention mechanism that enables the integration of the denoising process into a DETR architecture. This improves the stability of Hungarian matching to achieve a consistent optimization objective. Second, we present the Variational Query Denoising technique to address the gradient vanishing problem of conventional denoising methods, which severely restricts the efficiency of the denoising process. This explicitly introduces stochastic properties to mitigate this fundamental limitation and unlock substantial performance gains. Finally, we introduce a sophisticated self-distillation strategy, leveraging insights from later decoder layers to synergistically improve query quality in earlier layers, thereby amplifying the iterative refinement process. Rigorous experimentation demonstrates that MonoVQD achieves superior performance on the challenging KITTI monocular benchmark. Highlighting its broad applicability, MonoVQD's core components seamlessly integrate into other architectures, delivering significant performance gains even in multi-view 3D detection scenarios on the nuScenes dataset and underscoring its robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1932, <a href='https://arxiv.org/pdf/2506.11615.pdf' target='_blank'>https://arxiv.org/pdf/2506.11615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deliang Jin, Gang Chen, Shuo Feng, Yufeng Ling, Haoran Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11615">Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and Neuron Pruning in Noisy Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have achieved remarkable success across diverse domains, but their performance can be severely degraded by noisy or corrupted training data. Conventional noise mitigation methods often rely on explicit assumptions about noise distributions or require extensive retraining, which can be impractical for large-scale models. Inspired by the principles of machine unlearning, we propose a novel framework that integrates attribution-guided data partitioning, discriminative neuron pruning, and targeted fine-tuning to mitigate the impact of noisy samples. Our approach employs gradient-based attribution to probabilistically distinguish high-quality examples from potentially corrupted ones without imposing restrictive assumptions on the noise. It then applies regression-based sensitivity analysis to identify and prune neurons that are most vulnerable to noise. Finally, the resulting network is fine-tuned on the high-quality data subset to efficiently recover and enhance its generalization performance. This integrated unlearning-inspired framework provides several advantages over conventional noise-robust learning approaches. Notably, it combines data-level unlearning with model-level adaptation, thereby avoiding the need for full model retraining or explicit noise modeling. We evaluate our method on representative tasks (e.g., CIFAR-10 image classification and speech recognition) under various noise levels and observe substantial gains in both accuracy and efficiency. For example, our framework achieves approximately a 10% absolute accuracy improvement over standard retraining on CIFAR-10 with injected label noise, while reducing retraining time by up to 47% in some settings. These results demonstrate the effectiveness and scalability of the proposed approach for achieving robust generalization in noisy environments.
<div id='section'>Paperid: <span id='pid'>1933, <a href='https://arxiv.org/pdf/2506.07060.pdf' target='_blank'>https://arxiv.org/pdf/2506.07060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Cohen, Xavier Hinaut, Lilyana Petrova, Alexandre Pitti, Syd Reynal, Ichiro Tsuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07060">Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural intelligence (NI) consistently achieves more with less. Infants learn language, develop abstract concepts, and acquire sensorimotor skills from sparse data, all within tight neural and energy limits. In contrast, today's AI relies on virtually unlimited computational power, energy, and data to reach high performance. This paper argues that constraints in NI are paradoxically catalysts for efficiency, adaptability, and creativity. We first show how limited neural bandwidth promotes concise codes that still capture complex patterns. Spiking neurons, hierarchical structures, and symbolic-like representations emerge naturally from bandwidth constraints, enabling robust generalization. Next, we discuss chaotic itinerancy, illustrating how the brain transits among transient attractors to flexibly retrieve memories and manage uncertainty. We then highlight reservoir computing, where random projections facilitate rapid generalization from small datasets. Drawing on developmental perspectives, we emphasize how intrinsic motivation, along with responsive social environments, drives infant language learning and discovery of meaning. Such active, embodied processes are largely absent in current AI. Finally, we suggest that adopting 'less is more' principles -- energy constraints, parsimonious architectures, and real-world interaction -- can foster the emergence of more efficient, interpretable, and biologically grounded artificial systems.
<div id='section'>Paperid: <span id='pid'>1934, <a href='https://arxiv.org/pdf/2505.24592.pdf' target='_blank'>https://arxiv.org/pdf/2505.24592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weebum Yoo, Sung Whan Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24592">A Flat Minima Perspective on Understanding Augmentations and Model Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. Data augmentation is one of the prevalent and effective ways to enhance robustness. Despite the great success of augmentations in different fields, a general theoretical understanding of their efficacy in improving model robustness is lacking. We offer a unified theoretical framework to clarify how augmentations can enhance model robustness through the lens of loss surface flatness and PAC generalization bound. Our work diverges from prior studies in that our analysis i) broadly encompasses much of the existing augmentation methods, and ii) is not limited to specific types of distribution shifts like adversarial attacks. We confirm our theories through simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets, as well as domain generalization benchmarks including PACS and OfficeHome.
<div id='section'>Paperid: <span id='pid'>1935, <a href='https://arxiv.org/pdf/2505.19971.pdf' target='_blank'>https://arxiv.org/pdf/2505.19971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kilian Sennrich, Sina Ahmadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19971">Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge graphs offer an excellent solution for representing the lexical-semantic structures of lexicographic data. However, working with the SPARQL query language represents a considerable hurdle for many non-expert users who could benefit from the advantages of this technology. This paper addresses the challenge of creating natural language interfaces for lexicographic data retrieval on knowledge graphs such as Wikidata. We develop a multidimensional taxonomy capturing the complexity of Wikidata's lexicographic data ontology module through four dimensions and create a template-based dataset with over 1.2 million mappings from natural language utterances to SPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and GPT-3.5-Turbo reveal significant differences in model capabilities. While all models perform well on familiar patterns, only GPT-3.5-Turbo demonstrates meaningful generalization capabilities, suggesting that model size and diverse pre-training are crucial for adaptability in this domain. However, significant challenges remain in achieving robust generalization, handling diverse linguistic data, and developing scalable solutions that can accommodate the full complexity of lexicographic knowledge representation.
<div id='section'>Paperid: <span id='pid'>1936, <a href='https://arxiv.org/pdf/2505.09319.pdf' target='_blank'>https://arxiv.org/pdf/2505.09319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaustabha Ray, Nelson Mimura Gonzalez, Bruno Wassermann, Rachel Tzoref-Brill, Dean H. Lorenz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09319">Statistical Modeling and Uncertainty Estimation of LLM Inference Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM) inference systems present significant challenges in statistical performance characterization due to dynamic workload variations, diverse hardware architectures, and complex interactions between model size, batch processing, and throughput requirements. Accurate statistical characterization enables better workload scheduling, adaptive resource provisioning, and cost-aware inference optimization, making it crucial for improving efficiency in large-scale AI deployments. Traditional analytical models provide explainability but cannot cover the vast diversity of real-world workloads, making it impossible to benchmark every scenario in advance. Machine learning (ML) approaches effectively predict performance for non-benchmarked cases but struggle when extrapolating beyond their observed training space. To address these limitations for LLM inference systems, we propose an Analytical with Learning Augmentation (ALA) framework that bridges analytical modeling with \ml for robust statistical prediction and uncertainty estimation in LLM inference workloads. Our method employs an analytical throughput model with parameters estimated for benchmarked workloads, then extends to unobserved configurations using \ml predictions. We enhance this with simulated annealing to exploit subsets of the workload data point combinations and develop an error predictor. Finally, we quantify uncertainty based on vector space similarity between new and observed workloads to ensure robust generalization. Through extensive experimentation on diverse LLM inference workloads, we demonstrate that our framework achieves low median errors while maintaining adaptability to new inference scenarios.
<div id='section'>Paperid: <span id='pid'>1937, <a href='https://arxiv.org/pdf/2505.09082.pdf' target='_blank'>https://arxiv.org/pdf/2505.09082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sophie Zhang, Zhiming Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09082">CEC-Zero: Chinese Error Correction Solution Based on LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.
<div id='section'>Paperid: <span id='pid'>1938, <a href='https://arxiv.org/pdf/2505.05895.pdf' target='_blank'>https://arxiv.org/pdf/2505.05895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Raphael Ernhofer, Daniil Prokhorov, Jannica Langner, Dominik Bollmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05895">Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern automotive infotainment systems necessitate intelligent and adaptive solutions to manage frequent User Interface (UI) updates and diverse design variations. This work introduces a vision-language framework to facilitate the understanding of and interaction with automotive UIs, enabling seamless adaptation across different UI designs. To support research in this field, AutomotiveUI-Bench-4K, an open-source dataset comprising 998 images with 4,208 annotations, is also released. Additionally, a data pipeline for generating training data is presented. A Molmo-7B-based model is fine-tuned using Low-Rank Adaptation (LoRa), incorporating generated reasoning along with visual grounding and evaluation capabilities. The fine-tuned Evaluative Large Action Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and dataset are available on Hugging Face). The approach demonstrates strong cross-domain generalization, including a +5.6% improvement on ScreenSpot over the baseline model. An average accuracy of 80.8% is achieved on ScreenSpot, closely matching or surpassing specialized models for desktop, mobile, and web, despite being trained primarily on the automotive domain. This research investigates how data collection and subsequent fine-tuning can lead to AI-driven advancements in automotive UI understanding and interaction. The applied method is cost-efficient, and fine-tuned models can be deployed on consumer-grade GPUs.
<div id='section'>Paperid: <span id='pid'>1939, <a href='https://arxiv.org/pdf/2505.03575.pdf' target='_blank'>https://arxiv.org/pdf/2505.03575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Kainz, Johannes K. Krondorfer, Malte Jaschik, Maria Jernej, Harald Ganster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03575">Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recycling textile fibers is critical to reducing the environmental impact of the textile industry. Hyperspectral near-infrared (NIR) imaging combined with advanced deep learning algorithms offers a promising solution for efficient fiber classification and sorting. In this study, we investigate supervised and unsupervised deep learning models and test their generalization capabilities on different textile structures. We show that optimized convolutional neural networks (CNNs) and autoencoder networks achieve robust generalization under varying conditions. These results highlight the potential of hyperspectral imaging and deep learning to advance sustainable textile recycling through accurate and robust classification.
<div id='section'>Paperid: <span id='pid'>1940, <a href='https://arxiv.org/pdf/2505.01558.pdf' target='_blank'>https://arxiv.org/pdf/2505.01558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anan Yaghmour, Melba M. Crawford, Saurabh Prasad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01558">A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. Advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. Domain adaptation offers a promising solution to improve model generalization. This paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. We further provide new mathematical insights into MAE-based generative learning for domain-invariant feature learning. Experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.
<div id='section'>Paperid: <span id='pid'>1941, <a href='https://arxiv.org/pdf/2504.19362.pdf' target='_blank'>https://arxiv.org/pdf/2504.19362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunxuan Wang, Ray Yin, Yumei Tan, Hao Chen, Haiying Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19362">Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one of the primary causes of vision loss among retinal vascular diseases. Deep learning methods have been extensively applied in the grading of diabetic retinopathy (DR). However, their performance declines significantly when applied to data outside the training distribution due to domain shifts. Domain generalization (DG) has emerged as a solution to this challenge. However, most existing DG methods overlook lesion-specific features, resulting in insufficient accuracy. In this paper, we propose a novel approach that enhances existing DG methods by incorporating structural priors, inspired by the observation that DR grading is heavily dependent on vessel and lesion structures. We introduce Low-rank Adaptive Structural Priors (LoASP), a plug-and-play framework designed for seamless integration with existing DG models. LoASP improves generalization by learning adaptive structural representations that are finely tuned to the complexities of DR diagnosis. Extensive experiments on eight diverse datasets validate its effectiveness in both single-source and multi-source domain scenarios. Furthermore, visualizations reveal that the learned structural priors intuitively align with the intricate architecture of the vessels and lesions, providing compelling insights into their interpretability and diagnostic relevance.
<div id='section'>Paperid: <span id='pid'>1942, <a href='https://arxiv.org/pdf/2504.15743.pdf' target='_blank'>https://arxiv.org/pdf/2504.15743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seung Gyu Jeong, Sung Woo Nam, Seong Kwan Jung, Seong-Eun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15743">iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Respiratory auscultation is crucial for early detection of pediatric pneumonia, a condition that can quickly worsen without timely intervention. In areas with limited physician access, effective auscultation is challenging. We present a smartphone-based system that leverages built-in microphones and advanced deep learning algorithms to detect abnormal respiratory sounds indicative of pneumonia risk. Our end-to-end deep learning framework employs domain generalization to integrate a large electronic stethoscope dataset with a smaller smartphone-derived dataset, enabling robust feature learning for accurate respiratory assessments without expensive equipment. The accompanying mobile application guides caregivers in collecting high-quality lung sound samples and provides immediate feedback on potential pneumonia risks. User studies show strong classification performance and high acceptance, demonstrating the system's ability to facilitate proactive interventions and reduce preventable childhood pneumonia deaths. By seamlessly integrating into ubiquitous smartphones, this approach offers a promising avenue for more equitable and comprehensive remote pediatric care.
<div id='section'>Paperid: <span id='pid'>1943, <a href='https://arxiv.org/pdf/2504.14237.pdf' target='_blank'>https://arxiv.org/pdf/2504.14237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dekang Zhang, Dan Niu, Zhou Jin, Yichao Dong, Jingweijia Tan, Changyin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14237">A Novel Frequency-Spatial Domain Aware Network for Fast Thermal Prediction in 2.5D ICs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the post-Moore era, 2.5D chiplet-based ICs present significant challenges in thermal management due to increased power density and thermal hotspots. Neural network-based thermal prediction models can perform real-time predictions for many unseen new designs. However, existing CNN-based and GCN-based methods cannot effectively capture the global thermal features, especially for high-frequency components, hindering prediction accuracy enhancement. In this paper, we propose a novel frequency-spatial dual domain aware prediction network (FSA-Heat) for fast and high-accuracy thermal prediction in 2.5D ICs. It integrates high-to-low frequency and spatial domain encoder (FSTE) module with frequency domain cross-scale interaction module (FCIFormer) to achieve high-to-low frequency and global-to-local thermal dissipation feature extraction. Additionally, a frequency-spatial hybrid loss (FSL) is designed to effectively attenuate high-frequency thermal gradient noise and spatial misalignments. The experimental results show that the performance enhancements offered by our proposed method are substantial, outperforming the newly-proposed 2.5D method, GCN+PNA, by considerable margins (over 99% RMSE reduction, 4.23X inference time speedup). Moreover, extensive experiments demonstrate that FSA-Heat also exhibits robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1944, <a href='https://arxiv.org/pdf/2504.11477.pdf' target='_blank'>https://arxiv.org/pdf/2504.11477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunkai Zhang, Shiyin Wei, Yong Huang, Yawu Su, Shanshan Lu, Hui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11477">SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing computer vision(CV)-based structural damage identification models demonstrate notable accuracy in categorizing and localizing damage. However, these models present several critical limitations that hinder their practical application in civil engineering(CE). Primarily, their ability to recognize damage types remains constrained, preventing comprehensive analysis of the highly varied and complex conditions encountered in real-world CE structures. Second, these models lack linguistic capabilities, rendering them unable to articulate structural damage characteristics through natural language descriptions. With the continuous advancement of artificial intelligence(AI), large multi-modal models(LMMs) have emerged as a transformative solution, enabling the unified encoding and alignment of textual and visual data. These models can autonomously generate detailed descriptive narratives of structural damage while demonstrating robust generalization across diverse scenarios and tasks. This study introduces SDIGLM, an innovative LMM for structural damage identification, developed based on the open-source VisualGLM-6B architecture. To address the challenge of adapting LMMs to the intricate and varied operating conditions in CE, this work integrates a U-Net-based semantic segmentation module to generate defect segmentation maps as visual Chain of Thought(CoT). Additionally, a multi-round dialogue fine-tuning dataset is constructed to enhance logical reasoning, complemented by a language CoT formed through prompt engineering. By leveraging this multi-modal CoT, SDIGLM surpasses general-purpose LMMs in structural damage identification, achieving an accuracy of 95.24% across various infrastructure types. Moreover, the model effectively describes damage characteristics such as hole size, crack direction, and corrosion severity.
<div id='section'>Paperid: <span id='pid'>1945, <a href='https://arxiv.org/pdf/2504.10316.pdf' target='_blank'>https://arxiv.org/pdf/2504.10316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiqi Wu, Jianbo Mei, Yingjie Huang, Yining Xu, Jingjiao You, Yilong Liu, Li Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10316">ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content Generation with Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, significant advancements have been made in text-driven 3D content generation. However, several challenges remain. In practical applications, users often provide extremely simple text inputs while expecting high-quality 3D content. Generating optimal results from such minimal text is a difficult task due to the strong dependency of text-to-3D models on the quality of input prompts. Moreover, the generation process exhibits high variability, making it difficult to control. Consequently, multiple iterations are typically required to produce content that meets user expectations, reducing generation efficiency. To address this issue, we propose GPT-4V for self-optimization, which significantly enhances the efficiency of generating satisfactory content in a single attempt. Furthermore, the controllability of text-to-3D generation methods has not been fully explored. Our approach enables users to not only provide textual descriptions but also specify additional conditions, such as style, edges, scribbles, poses, or combinations of multiple conditions, allowing for more precise control over the generated 3D content. Additionally, during training, we effectively integrate multi-view information, including multi-view depth, masks, features, and images, to address the common Janus problem in 3D content generation. Extensive experiments demonstrate that our method achieves robust generalization, facilitating the efficient and controllable generation of high-quality 3D content.
<div id='section'>Paperid: <span id='pid'>1946, <a href='https://arxiv.org/pdf/2504.07736.pdf' target='_blank'>https://arxiv.org/pdf/2504.07736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Palak Patel, Luke McGuire, Abani Patra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07736">A Novel Deep Learning Approach for Emulating Computationally Expensive Postfire Debris Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional physics-based models of geophysical flows, such as debris flows and landslides that pose significant risks to human lives and infrastructure are computationally expensive, limiting their utility for large-scale parameter sweeps, uncertainty quantification, inversions or real-time applications. This study presents an efficient alternative, a deep learning-based surrogate model built using a modified U-Net architecture to predict the dynamics of runoff-generated debris flows across diverse terrain based on data from physics based simulations. The study area is divided into smaller patches for localized predictions using a patch-predict-stitch methodology (complemented by limited global data to accelerate training). The patches are then combined to reconstruct spatially continuous flow maps, ensuring scalability for large domains. To enable fast training using limited expensive simulations, the deep learning model was trained on data from an ensemble of physics based simulations using parameters generated via Latin Hypercube Sampling and validated on unseen parameter sets and terrain, achieving maximum pointwise errors below 10% and robust generalization. Uncertainty quantification using Monte Carlo methods are enabled using the validated surrogate, which can facilitate probabilistic hazard assessments. This study highlights the potential of deep learning surrogates as powerful tools for geophysical flow analysis, enabling computationally efficient and reliable probabilistic hazard map predictions.
<div id='section'>Paperid: <span id='pid'>1947, <a href='https://arxiv.org/pdf/2504.06781.pdf' target='_blank'>https://arxiv.org/pdf/2504.06781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reiji Saito, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06781">Domain Generalization through Attenuation of Domain-Specific Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a new evaluation metric called Domain Independence (DI) and Attenuation of Domain-Specific Information (ADSI) which is specifically designed for domain-generalized semantic segmentation in automotive images. DI measures the presence of domain-specific information: a lower DI value indicates strong domain dependence, while a higher DI value suggests greater domain independence. This makes it roughly where domain-specific information exists and up to which frequency range it is present. As a result, it becomes possible to effectively suppress only the regions in the image that contain domain-specific information, enabling feature extraction independent of the domain. ADSI uses a Butterworth filter to remove the low-frequency components of images that contain inherent domain-specific information such as sensor characteristics and lighting conditions. However, since low-frequency components also contain important information such as color, we should not remove them completely. Thus, a scalar value (ranging from 0 to 1) is multiplied by the low-frequency components to retain essential information. This helps the model learn more domain-independent features. In experiments, GTA5 (synthetic dataset) was used as training images, and a real-world dataset was used for evaluation, and the proposed method outperformed conventional approaches. Similarly, in experiments that the Cityscapes (real-world dataset) was used for training and various environment datasets such as rain and nighttime were used for evaluation, the proposed method demonstrated its robustness under nighttime conditions.
<div id='section'>Paperid: <span id='pid'>1948, <a href='https://arxiv.org/pdf/2503.23882.pdf' target='_blank'>https://arxiv.org/pdf/2503.23882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Halil Ä°brahim ÃztÃ¼rk, Muhammet Esat KalfaoÄlu, Ozsel Kilinc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23882">GLane3D : Detecting Lanes with Graph of 3D Keypoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and efficient lane detection in 3D space is essential for autonomous driving systems, where robust generalization is the foremost requirement for 3D lane detection algorithms. Considering the extensive variation in lane structures worldwide, achieving high generalization capacity is particularly challenging, as algorithms must accurately identify a wide variety of lane patterns worldwide. Traditional top-down approaches rely heavily on learning lane characteristics from training datasets, often struggling with lanes exhibiting previously unseen attributes. To address this generalization limitation, we propose a method that detects keypoints of lanes and subsequently predicts sequential connections between them to construct complete 3D lanes. Each key point is essential for maintaining lane continuity, and we predict multiple proposals per keypoint by allowing adjacent grids to predict the same keypoint using an offset mechanism. PointNMS is employed to eliminate overlapping proposal keypoints, reducing redundancy in the estimated BEV graph and minimizing computational overhead from connection estimations. Our model surpasses previous state-of-the-art methods on both the Apollo and OpenLane datasets, demonstrating superior F1 scores and a strong generalization capacity when models trained on OpenLane are evaluated on the Apollo dataset, compared to prior approaches.
<div id='section'>Paperid: <span id='pid'>1949, <a href='https://arxiv.org/pdf/2503.23605.pdf' target='_blank'>https://arxiv.org/pdf/2503.23605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kasra Jalaldoust, Alexis Bellot, Elias Bareinboim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23605">Partial Transportability for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A fundamental task in AI is providing performance guarantees for predictions made in unseen domains. In practice, there can be substantial uncertainty about the distribution of new data, and corresponding variability in the performance of existing predictors. Building on the theory of partial identification and transportability, this paper introduces new results for bounding the value of a functional of the target distribution, such as the generalization error of a classifier, given data from source domains and assumptions about the data generating mechanisms, encoded in causal diagrams. Our contribution is to provide the first general estimation technique for transportability problems, adapting existing parameterization schemes such Neural Causal Models to encode the structural constraints necessary for cross-population inference. We demonstrate the expressiveness and consistency of this procedure and further propose a gradient-based optimization scheme for making scalable inferences in practice. Our results are corroborated with experiments.
<div id='section'>Paperid: <span id='pid'>1950, <a href='https://arxiv.org/pdf/2503.23060.pdf' target='_blank'>https://arxiv.org/pdf/2503.23060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Jacob, Yanlei Diao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23060">Unsupervised Anomaly Detection in Multivariate Time Series across Heterogeneous Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread adoption of digital services, along with the scale and complexity at which they operate, has made incidents in IT operations increasingly more likely, diverse, and impactful. This has led to the rapid development of a central aspect of "Artificial Intelligence for IT Operations" (AIOps), focusing on detecting anomalies in vast amounts of multivariate time series data generated by service entities. In this paper, we begin by introducing a unifying framework for benchmarking unsupervised anomaly detection (AD) methods, and highlight the problem of shifts in normal behaviors that can occur in practical AIOps scenarios. To tackle anomaly detection under domain shift, we then cast the problem in the framework of domain generalization and propose a novel approach, Domain-Invariant VAE for Anomaly Detection (DIVAD), to learn domain-invariant representations for unsupervised anomaly detection. Our evaluation results using the Exathlon benchmark show that the two main DIVAD variants significantly outperform the best unsupervised AD method in maximum performance, with 20% and 15% improvements in maximum peak F1-scores, respectively. Evaluation using the Application Server Dataset further demonstrates the broader applicability of our domain generalization methods.
<div id='section'>Paperid: <span id='pid'>1951, <a href='https://arxiv.org/pdf/2503.22856.pdf' target='_blank'>https://arxiv.org/pdf/2503.22856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Bai, Anna Kruspe, Xiaoxiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22856">Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tweets provides valuable semantic context for earth observation tasks and serves as a complementary modality to remote sensing imagery. In building function classification (BFC), tweets are often collected using geographic heuristics and labeled via external databases, an inherently weakly supervised process that introduces both label noise and sentence level feature noise (e.g., irrelevant or uninformative tweets). While label noise has been widely studied, the impact of sentence level feature noise remains underexplored, largely due to the lack of clean benchmark datasets for controlled analysis. In this work, we propose a method for generating a synthetic oracle dataset using LLM, designed to contain only tweets that are both correctly labeled and semantically relevant to their associated buildings. This oracle dataset enables systematic investigation of noise impacts that are otherwise difficult to isolate in real-world data. To assess its utility, we compare model performance using Naive Bayes and mBERT classifiers under three configurations: real vs. synthetic training data, and cross-domain generalization. Results show that noise in real tweets significantly degrades the contextual learning capacity of mBERT, reducing its performance to that of a simple keyword-based model. In contrast, the clean synthetic dataset allows mBERT to learn effectively, outperforming Naive Bayes Bayes by a large margin. These findings highlight that addressing feature noise is more critical than model complexity in this task. Our synthetic dataset offers a novel experimental environment for future noise injection studies and is publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>1952, <a href='https://arxiv.org/pdf/2503.12215.pdf' target='_blank'>https://arxiv.org/pdf/2503.12215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amulya Reddy Maligireddy, Manohar Reddy Uppula, Nidhi Rastogi, Yaswanth Reddy Parla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12215">Gun Detection Using Combined Human Pose and Weapon Appearance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing frequency of firearm-related incidents has necessitated advancements in security and surveillance systems, particularly in firearm detection within public spaces. Traditional gun detection methods rely on manual inspections and continuous human monitoring of CCTV footage, which are labor-intensive and prone to high false positive and negative rates. To address these limitations, we propose a novel approach that integrates human pose estimation with weapon appearance recognition using deep learning techniques. Unlike prior studies that focus on either body pose estimation or firearm detection in isolation, our method jointly analyzes posture and weapon presence to enhance detection accuracy in real-world, dynamic environments. To train our model, we curated a diverse dataset comprising images from open-source repositories such as IMFDB and Monash Guns, supplemented with AI-generated and manually collected images from web sources. This dataset ensures robust generalization and realistic performance evaluation under various surveillance conditions. Our research aims to improve the precision and reliability of firearm detection systems, contributing to enhanced public safety and threat mitigation in high-risk areas.
<div id='section'>Paperid: <span id='pid'>1953, <a href='https://arxiv.org/pdf/2503.09050.pdf' target='_blank'>https://arxiv.org/pdf/2503.09050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alvin Kimbowa, Arjun Parmar, Maziar Badii, David Liu, Matthew Harkey, Ilker Hacihaliloglu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09050">Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage Segmentation on Out-of-Distribution 2D Ultrasound Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated knee cartilage segmentation using point-of-care ultrasound devices and deep-learning networks has the potential to enhance the management of knee osteoarthritis. However, segmentation algorithms often struggle with domain shifts caused by variations in ultrasound devices and acquisition parameters, limiting their generalizability. In this paper, we propose Mono2D, a monogenic layer that extracts multi-scale, contrast- and intensity-invariant local phase features using trainable bandpass quadrature filters. This layer mitigates domain shifts, improving generalization to out-of-distribution domains. Mono2D is integrated before the first layer of a segmentation network, and its parameters jointly trained alongside the network's parameters. We evaluated Mono2D on a multi-domain 2D ultrasound knee cartilage dataset for single-source domain generalization (SSDG). Our results demonstrate that Mono2D outperforms other SSDG methods in terms of Dice score and mean average surface distance. To further assess its generalizability, we evaluate Mono2D on a multi-site prostate MRI dataset, where it continues to outperform other SSDG methods, highlighting its potential to improve domain generalization in medical imaging. Nevertheless, further evaluation on diverse datasets is still necessary to assess its clinical utility.
<div id='section'>Paperid: <span id='pid'>1954, <a href='https://arxiv.org/pdf/2503.02311.pdf' target='_blank'>https://arxiv.org/pdf/2503.02311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kensuke Tatematsu, Akifumi Wachi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02311">Target Return Optimizer for Multi-Game Decision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving autonomous agents with robust generalization capabilities across diverse games and tasks remains one of the ultimate goals in AI research. Recent advancements in transformer-based offline reinforcement learning, exemplified by the MultiGame Decision Transformer [Lee et al., 2022], have shown remarkable performance across various games or tasks. However, these approaches depend heavily on human expertise, presenting substantial challenges for practical deployment, particularly in scenarios with limited prior game-specific knowledge. In this paper, we propose an algorithm called Multi-Game Target Return Optimizer (MTRO) to autonomously determine game-specific target returns within the Multi-Game Decision Transformer framework using solely offline datasets. MTRO addresses the existing limitations by automating the target return configuration process, leveraging environmental reward information extracted from offline datasets. Notably, MTRO does not require additional training, enabling seamless integration into existing Multi-Game Decision Transformer architectures. Our experimental evaluations on Atari games demonstrate that MTRO enhances the performance of RL policies across a wide array of games, underscoring its potential to advance the field of autonomous agent development.
<div id='section'>Paperid: <span id='pid'>1955, <a href='https://arxiv.org/pdf/2502.19665.pdf' target='_blank'>https://arxiv.org/pdf/2502.19665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanchao Wang, Zhao-Rong Lai, Tianqi Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19665">Out-of-distribution Generalization for Total Variation based Invariant Risk Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invariant risk minimization is an important general machine learning framework that has recently been interpreted as a total variation model (IRM-TV). However, how to improve out-of-distribution (OOD) generalization in the IRM-TV setting remains unsolved. In this paper, we extend IRM-TV to a Lagrangian multiplier model named OOD-TV-IRM. We find that the autonomous TV penalty hyperparameter is exactly the Lagrangian multiplier. Thus OOD-TV-IRM is essentially a primal-dual optimization model, where the primal optimization minimizes the entire invariant risk and the dual optimization strengthens the TV penalty. The objective is to reach a semi-Nash equilibrium where the balance between the training loss and OOD generalization is maintained. We also develop a convergent primal-dual algorithm that facilitates an adversarial learning scheme. Experimental results show that OOD-TV-IRM outperforms IRM-TV in most situations.
<div id='section'>Paperid: <span id='pid'>1956, <a href='https://arxiv.org/pdf/2502.18975.pdf' target='_blank'>https://arxiv.org/pdf/2502.18975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Surner, Abdelmajid Khelil, Ludwig Bothmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18975">Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution generalization of machine learning models remains challenging since the models are inherently bound to the training data distribution. This especially manifests, when the learned models rely on spurious correlations. Most of the existing approaches apply data manipulation, representation learning, or learning strategies to achieve generalizable models. Unfortunately, these approaches usually require multiple training domains, group labels, specialized augmentation, or pre-processing to reach generalizable models. We propose a novel approach that addresses these limitations by providing a technique to guide the neural network through the training phase. We first establish input pairs, representing the spurious attribute and describing the invariance, a characteristic that should not affect the outcome of the model. Based on these pairs, we form a corrective gradient complementing the traditional gradient descent approach. We further make this correction mechanism adaptive based on a predefined invariance condition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasets demonstrate the effectiveness of our approach and the robustness to group shifts.
<div id='section'>Paperid: <span id='pid'>1957, <a href='https://arxiv.org/pdf/2502.18188.pdf' target='_blank'>https://arxiv.org/pdf/2502.18188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanzi Chen, Jiying Zhang, Yang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18188">Graph Augmentation for Cross Graph Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-graph node classification, utilizing the abundant labeled nodes from one graph to help classify unlabeled nodes in another graph, can be viewed as a domain generalization problem of graph neural networks (GNNs) due to the structure shift commonly appearing among various graphs. Nevertheless, current endeavors for cross-graph node classification mainly focus on model training. Data augmentation approaches, a simple and easy-to-implement domain generalization technique, remain under-explored. In this paper, we develop a new graph structure augmentation for the crossgraph domain generalization problem. Specifically, low-weight edgedropping is applied to remove potential noise edges that may hinder the generalization ability of GNNs, stimulating the GNNs to capture the essential invariant information underlying different structures. Meanwhile, clustering-based edge-adding is proposed to generate invariant structures based on the node features from the same distribution. Consequently, with these augmentation techniques, the GNNs can maintain the domain invariant structure information that can improve the generalization ability. The experiments on out-ofdistribution citation network datasets verify our method achieves state-of-the-art performance among conventional augmentations.
<div id='section'>Paperid: <span id='pid'>1958, <a href='https://arxiv.org/pdf/2502.10408.pdf' target='_blank'>https://arxiv.org/pdf/2502.10408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Doyoun Kim, Suin Kim, Yojan Jo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10408">Knowledge Tracing in Programming Education Integrating Students' Questions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge tracing (KT) in programming education presents unique challenges due to the complexity of coding tasks and the diverse methods students use to solve problems. Although students' questions often contain valuable signals about their understanding and misconceptions, traditional KT models often neglect to incorporate these questions as inputs to address these challenges. This paper introduces SQKT (Students' Question-based Knowledge Tracing), a knowledge tracing model that leverages students' questions and automatically extracted skill information to enhance the accuracy of predicting students' performance on subsequent problems in programming education. Our method creates semantically rich embeddings that capture not only the surface-level content of the questions but also the student's mastery level and conceptual understanding. Experimental results demonstrate SQKT's superior performance in predicting student completion across various Python programming courses of differing difficulty levels. In in-domain experiments, SQKT achieved a 33.1\% absolute improvement in AUC compared to baseline models. The model also exhibited robust generalization capabilities in cross-domain settings, effectively addressing data scarcity issues in advanced programming courses. SQKT can be used to tailor educational content to individual learning needs and design adaptive learning systems in computer science education.
<div id='section'>Paperid: <span id='pid'>1959, <a href='https://arxiv.org/pdf/2502.10277.pdf' target='_blank'>https://arxiv.org/pdf/2502.10277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin-Chih Chelsea Wang, Tsao-Lun Chen, Shankeeth Vinayahalingam, Tai-Hsien Wu, Chu Wei Chang, Hsuan Hao Chang, Hung-Jen Wei, Mu-Hsiung Chen, Ching-Chang Ko, David Anssari Moin, Bram van Ginneken, Tong Xi, Hsiao-Cheng Tsai, Min-Huey Chen, Tzu-Ming Harry Hsu, Hye Chou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10277">Artificial Intelligence to Assess Dental Findings from Panoramic Radiographs -- A Multinational Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dental panoramic radiographs (DPRs) are widely used in clinical practice for comprehensive oral assessment but present challenges due to overlapping structures and time constraints in interpretation.
  This study aimed to establish a solid baseline for the AI-automated assessment of findings in DPRs by developing, evaluating an AI system, and comparing its performance with that of human readers across multinational data sets.
  We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and Taiwan), focusing on 8 types of dental findings. The AI system combined object detection and semantic segmentation techniques for per-tooth finding identification. Performance metrics included sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC). AI generalizability was tested across data sets, and performance was compared with human dental practitioners.
  The AI system demonstrated comparable or superior performance to human readers, particularly +67.9% (95% CI: 54.0%-81.9%; p < .001) sensitivity for identifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008) sensitivity for identifying missing teeth. The AI achieved a macro-averaged AUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with the reference were comparable to inter-human agreements in 7 of 8 findings except for caries (p = .024). The AI system demonstrated robust generalization across diverse imaging and demographic settings and processed images 79 times faster (95% CI: 75-82) than human readers.
  The AI system effectively assessed findings in DPRs, achieving performance on par with or better than human experts while significantly reducing interpretation time. These results highlight the potential for integrating AI into clinical workflows to improve diagnostic efficiency and accuracy, and patient management.
<div id='section'>Paperid: <span id='pid'>1960, <a href='https://arxiv.org/pdf/2502.03835.pdf' target='_blank'>https://arxiv.org/pdf/2502.03835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenwei He, Hongsu Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03835">Single-Domain Generalized Object Detection by Balancing Domain Diversity and Invariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-domain generalization for object detection (S-DGOD) seeks to transfer learned representations from a single source domain to unseen target domains. While recent approaches have primarily focused on achieving feature invariance, they ignore that domain diversity also presents significant challenges for the task. First, such invariance-driven strategies often lead to the loss of domain-specific information, resulting in incomplete feature representations. Second, cross-domain feature alignment forces the model to overlook domain-specific discrepancies, thereby increasing the complexity of the training process. To address these limitations, this paper proposes the Diversity Invariant Detection Model (DIDM), which achieves a harmonious integration of domain-specific diversity and domain invariance. Our key idea is to learn the invariant representations by keeping the inherent domain-specific features. Specifically, we introduce the Diversity Learning Module (DLM). This module limits the invariant semantics while explicitly enhancing domain-specific feature representation through a proposed feature diversity loss. Furthermore, to ensure cross-domain invariance without sacrificing diversity, we incorporate the Weighted Aligning Module (WAM) to enable feature alignment while maintaining the discriminative domain-specific information. Extensive experiments on multiple diverse datasets demonstrate the effectiveness of the proposed model, achieving superior performance compared to existing methods.
<div id='section'>Paperid: <span id='pid'>1961, <a href='https://arxiv.org/pdf/2501.18957.pdf' target='_blank'>https://arxiv.org/pdf/2501.18957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alfred Bexley, Lukas Radcliffe, Giles Weatherstone, Joseph Sakau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18957">Intrinsic Tensor Field Propagation in Large Language Models: A Novel Approach to Contextual Information Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context propagation remains a central challenge in language model architectures, particularly in tasks requiring the retention of long-range dependencies. Conventional attention mechanisms, while effective in many applications, exhibit limitations in maintaining coherent contextual representations over extended sequences due to their reliance on discrete token interactions. A novel approach is introduced through the formulation of Intrinsic Tensor Field Propagation (ITFP), which models contextual relationships as continuous tensor fields distributed across token embeddings. The propagation dynamics are governed through differential equations that enable a structured flow of contextual information, augmenting the standard attention mechanism to enhance coherence and recall. A series of experiments conducted on an open-source transformer-based model demonstrate that ITFP provides measurable improvements in contextual retention, dependency resolution, and inference stability across various linguistic structures. Comparisons with baseline models reveal a reduction in syntactic inconsistencies and factual errors, while ablation studies indicate that the choice of propagation depth and integration strength significantly impacts model performance. Additional evaluations assessing domain generalization suggest that ITFP effectively adapts across different text genres, reinforcing its applicability beyond conventional language modeling tasks. Although computational trade-offs are introduced through the inclusion of tensor field computations, empirical findings suggest that the benefits in accuracy and coherence outweigh the increased processing demands.
<div id='section'>Paperid: <span id='pid'>1962, <a href='https://arxiv.org/pdf/2501.16848.pdf' target='_blank'>https://arxiv.org/pdf/2501.16848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ron van Bree, Diego Marcos, Ioannis Athanasiadis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16848">Hybrid Phenology Modeling for Predicting Temperature Effects on Tree Dormancy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biophysical models offer valuable insights into climate-phenology relationships in both natural and agricultural settings. However, there are substantial structural discrepancies across models which require site-specific recalibration, often yielding inconsistent predictions under similar climate scenarios. Machine learning methods offer data-driven solutions, but often lack interpretability and alignment with existing knowledge. We present a phenology model describing dormancy in fruit trees, integrating conventional biophysical models with a neural network to address their structural disparities. We evaluate our hybrid model in an extensive case study predicting cherry tree phenology in Japan, South Korea and Switzerland. Our approach consistently outperforms both traditional biophysical and machine learning models in predicting blooming dates across years. Additionally, the neural network's adaptability facilitates parameter learning for specific tree varieties, enabling robust generalization to new sites without site-specific recalibration. This hybrid model leverages both biophysical constraints and data-driven flexibility, offering a promising avenue for accurate and interpretable phenology modeling.
<div id='section'>Paperid: <span id='pid'>1963, <a href='https://arxiv.org/pdf/2501.14119.pdf' target='_blank'>https://arxiv.org/pdf/2501.14119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Derek Yotheringhay, Alistair Kirkland, Humphrey Kirkbride, Josiah Whitesteeple
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14119">Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformative innovations in model architectures have introduced hierarchical embedding augmentation as a means to redefine the representation of tokens through multi-level semantic structures, offering enhanced adaptability to complex linguistic inputs. Autonomous structural memory manipulation further advances this paradigm through dynamic memory reallocation mechanisms that prioritize critical contextual features while suppressing less relevant information, enabling scalable and efficient performance across diverse tasks. Experimental results reveal substantial improvements in computational efficiency, with marked reductions in processing overhead for longer input sequences, achieved through memory reorganization strategies that adapt to evolving contextual requirements. Hierarchical embeddings not only improved contextual alignment but also facilitated task generalization by capturing relationships at varying semantic granularities, ensuring coherence across layers without introducing significant computational redundancies. Comparative analysis against baseline models demonstrated unique advantages in accuracy, efficiency, and interpretability, particularly in tasks requiring complex contextual understanding or domain-specific adaptability. The ability to dynamically adjust token representations and memory configurations contributed to the model's robustness under varied and unpredictable input conditions. Applications benefiting from these advancements include multi-domain generalization, interactive systems, and scenarios involving real-time decision-making, where traditional static memory architectures often face limitations. The proposed methodology combines advanced embedding and memory management strategies into a cohesive framework that addresses scalability challenges while preserving task-specific relevance.
<div id='section'>Paperid: <span id='pid'>1964, <a href='https://arxiv.org/pdf/2501.11896.pdf' target='_blank'>https://arxiv.org/pdf/2501.11896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhong-Hua Sun, Ru-Yuan Zhang, Zonglei Zhen, Da-Hui Wang, Yong-Jie Li, Xiaohong Wan, Hongzhi You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11896">Systematic Abductive Reasoning via Diverse Relation Representations in Vector-symbolic Architecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In abstract visual reasoning, monolithic deep learning models suffer from limited interpretability and generalization, while existing neuro-symbolic approaches fall short in capturing the diversity and systematicity of attributes and relation representations. To address these challenges, we propose a Systematic Abductive Reasoning model with diverse relation representations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve Raven's Progressive Matrices (RPM). To derive attribute representations with symbolic reasoning potential, we introduce not only various types of atomic vectors that represent numeric, periodic and logical semantics, but also the structured high-dimentional representation (SHDR) for the overall Grid component. For systematic reasoning, we propose novel numerical and logical relation functions and perform rule abduction and execution in a unified framework that integrates these relation representations. Experimental results demonstrate that Rel-SAR achieves significant improvement on RPM tasks and exhibits robust out-of-distribution generalization. Rel-SAR leverages the synergy between HD attribute representations and symbolic reasoning to achieve systematic abductive reasoning with both interpretable and computable semantics.
<div id='section'>Paperid: <span id='pid'>1965, <a href='https://arxiv.org/pdf/2501.05496.pdf' target='_blank'>https://arxiv.org/pdf/2501.05496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbing Zhou, Xiangmou Qu, Chenlong You, Jiyang Zhou, Jingyue Tang, Xin Zheng, Chunmao Cai, Yingbo Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05496">FedSA: A Unified Representation Learning via Semantic Anchors for Prototype-based Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prototype-based federated learning has emerged as a promising approach that shares lightweight prototypes to transfer knowledge among clients with data heterogeneity in a model-agnostic manner. However, existing methods often collect prototypes directly from local models, which inevitably introduce inconsistencies into representation learning due to the biased data distributions and differing model architectures among clients. In this paper, we identify that both statistical and model heterogeneity create a vicious cycle of representation inconsistency, classifier divergence, and skewed prototype alignment, which negatively impacts the performance of clients. To break the vicious cycle, we propose a novel framework named Federated Learning via Semantic Anchors (FedSA) to decouple the generation of prototypes from local representation learning. We introduce a novel perspective that uses simple yet effective semantic anchors serving as prototypes to guide local models in learning consistent representations. By incorporating semantic anchors, we further propose anchor-based regularization with margin-enhanced contrastive learning and anchor-based classifier calibration to correct feature extractors and calibrate classifiers across clients, achieving intra-class compactness and inter-class separability of prototypes while ensuring consistent decision boundaries. We then update the semantic anchors with these consistent and discriminative prototypes, which iteratively encourage clients to collaboratively learn a unified data representation with robust generalization. Extensive experiments under both statistical and model heterogeneity settings show that FedSA significantly outperforms existing prototype-based FL methods on various classification tasks.
<div id='section'>Paperid: <span id='pid'>1966, <a href='https://arxiv.org/pdf/2501.03221.pdf' target='_blank'>https://arxiv.org/pdf/2501.03221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haosheng Zhang, Hao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03221">RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet Transform Projection-based Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of 3D object classification, a fundamental challenge lies in addressing the scarcity of labeled data, which limits the applicability of traditional data-intensive learning paradigms. This challenge is particularly pronounced in few-shot learning scenarios, where the objective is to achieve robust generalization from minimal annotated samples. To overcome these limitations, it is crucial to identify and leverage the most salient and discriminative features of 3D objects, thereby enhancing learning efficiency and reducing dependency on large-scale labeled datasets. This work introduces RW-Net, a novel framework designed to address the challenges above by integrating Rate-Distortion Explanation (RDE) and wavelet transform into a state-of-the-art projection-based 3D object classification architecture. The proposed method capitalizes on RDE to extract critical features by identifying and preserving the most informative data components while reducing redundancy. This process ensures the retention of essential information for effective decision-making, optimizing the model's ability to learn from limited data. Complementing RDE, incorporating the wavelet transform further enhances the framework's capability to generalize in low-data regimes. By emphasizing low-frequency components of the input data, the wavelet transform captures fundamental geometric and structural attributes of 3D objects. These attributes are instrumental in mitigating overfitting and improving the robustness of the learned representations across diverse tasks and domains. To validate the effectiveness of our RW-Net, we conduct extensive experiments on three datasets: ModelNet40, ModelNet40-C, and ScanObjectNN for few-shot 3D object classification. The results demonstrate that our approach achieves state-of-the-art performance and exhibits superior generalization and robustness in few-shot learning scenarios.
<div id='section'>Paperid: <span id='pid'>1967, <a href='https://arxiv.org/pdf/2412.12349.pdf' target='_blank'>https://arxiv.org/pdf/2412.12349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madiyar Alimov, Temirlan Meiramkhanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12349">Domain Generalization in Autonomous Driving: Evaluating YOLOv8s, RT-DETR, and YOLO-NAS with the ROAD-Almaty Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the domain generalization capabilities of three state-of-the-art object detection models - YOLOv8s, RT-DETR, and YOLO-NAS - within the unique driving environment of Kazakhstan. Utilizing the newly constructed ROAD-Almaty dataset, which encompasses diverse weather, lighting, and traffic conditions, we evaluated the models' performance without any retraining. Quantitative analysis revealed that RT-DETR achieved an average F1-score of 0.672 at IoU=0.5, outperforming YOLOv8s (0.458) and YOLO-NAS (0.526) by approximately 46% and 27%, respectively. Additionally, all models exhibited significant performance declines at higher IoU thresholds (e.g., a drop of approximately 20% when increasing IoU from 0.5 to 0.75) and under challenging environmental conditions, such as heavy snowfall and low-light scenarios. These findings underscore the necessity for geographically diverse training datasets and the implementation of specialized domain adaptation techniques to enhance the reliability of autonomous vehicle detection systems globally. This research contributes to the understanding of domain generalization challenges in autonomous driving, particularly in underrepresented regions.
<div id='section'>Paperid: <span id='pid'>1968, <a href='https://arxiv.org/pdf/2412.11171.pdf' target='_blank'>https://arxiv.org/pdf/2412.11171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songgaojun Deng, Maarten de Rijke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11171">Learning Latent Spaces for Domain Generalization in Time Series Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series forecasting is vital in many real-world applications, yet developing models that generalize well on unseen relevant domains -- such as forecasting web traffic data on new platforms/websites or estimating e-commerce demand in new regions -- remains underexplored. Existing forecasting models often struggle with domain shifts in time series data, as the temporal patterns involve complex components like trends, seasonality, etc. While some prior work addresses this by matching feature distributions across domains or disentangling domain-shared features using label information, they fail to reveal insights into the latent temporal dependencies, which are critical for identifying common patterns across domains and achieving generalization.
  We propose a framework for domain generalization in time series forecasting by mining the latent factors that govern temporal dependencies across domains. Our approach uses a decomposition-based architecture with a new Conditional $Î²$-Variational Autoencoder (VAE), wherein time series data is first decomposed into trend-cyclical and seasonal components, each modeled independently through separate $Î²$-VAE modules. The $Î²$-VAE aims to capture disentangled latent factors that control temporal dependencies across domains. We enhance the learning of domain-specific information with a decoder-conditional design and introduce domain regularization to improve the separation of domain-shared and domain-specific latent factors. Our proposed method is flexible and can be applied to various time series forecasting models, enabling effective domain generalization with simplicity and efficiency. We validate its effectiveness on five real-world time series datasets, covering web traffic, e-commerce, finance and power consumption, demonstrating improved generalization performance over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1969, <a href='https://arxiv.org/pdf/2412.08479.pdf' target='_blank'>https://arxiv.org/pdf/2412.08479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sumaiya Zoha, Jeong-Gun Lee, Young-Woong Ko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08479">CAT: Class Aware Adaptive Thresholding for Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) seeks to transfer knowledge from multiple source domains to unseen target domains, even in the presence of domain shifts. Achieving effective generalization typically requires a large and diverse set of labeled source data to learn robust representations that can generalize to new, unseen domains. However, obtaining such high-quality labeled data is often costly and labor-intensive, limiting the practical applicability of DG. To address this, we investigate a more practical and challenging problem: semi-supervised domain generalization (SSDG) under a label-efficient paradigm. In this paper, we propose a novel method, CAT, which leverages semi-supervised learning with limited labeled data to achieve competitive generalization performance under domain shifts. Our method addresses key limitations of previous approaches, such as reliance on fixed thresholds and sensitivity to noisy pseudo-labels. CAT combines adaptive thresholding with noisy label refinement techniques, creating a straightforward yet highly effective solution for SSDG tasks. Specifically, our approach uses flexible thresholding to generate high-quality pseudo-labels with higher class diversity while refining noisy pseudo-labels to improve their reliability. Extensive experiments across multiple benchmark datasets demonstrate the superior performance of our method, highlighting its effectiveness in achieving robust generalization under domain shift.
<div id='section'>Paperid: <span id='pid'>1970, <a href='https://arxiv.org/pdf/2412.07226.pdf' target='_blank'>https://arxiv.org/pdf/2412.07226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingfan Wang, Guoliang Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07226">Attention Head Purification: A New Perspective to Harness CLIP for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to learn a model from multiple source domains to achieve satisfactory performance on unseen target domains. Recent works introduce CLIP to DG tasks due to its superior image-text alignment and zeros-shot performance. Previous methods either utilize full fine-tuning or prompt-learning paradigms to harness CLIP for DG tasks. Those works focus on avoiding catastrophic forgetting of the original knowledge encoded in CLIP but ignore that the knowledge encoded in CLIP in nature may contain domain-specific cues that constrain its domain generalization performance. In this paper, we propose a new perspective to harness CLIP for DG, i.e., attention head purification. We observe that different attention heads may encode different properties of an image and selecting heads appropriately may yield remarkable performance improvement across domains. Based on such observations, we purify the attention heads of CLIP from two levels, including task-level purification and domain-level purification. For task-level purification, we design head-aware LoRA to make each head more adapted to the task we considered. For domain-level purification, we perform head selection via a simple gating strategy. We utilize MMD loss to encourage masked head features to be more domain-invariant to emphasize more generalizable properties/heads. During training, we jointly perform task-level purification and domain-level purification. We conduct experiments on various representative DG benchmarks. Though simple, extensive experiments demonstrate that our method performs favorably against previous state-of-the-arts.
<div id='section'>Paperid: <span id='pid'>1971, <a href='https://arxiv.org/pdf/2412.07214.pdf' target='_blank'>https://arxiv.org/pdf/2412.07214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun-Peng Zhu, Boyan Niu, Peng Cai, Zheming Ni, Jianwei Wan, Kai Xu, Jiajun Huang, Shengbo Ma, Bing Wang, Xuan Zhou, Guanglei Bao, Donghui Zhang, Liu Tang, Qi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07214">Towards Automated Cross-domain Exploratory Data Analysis through Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploratory data analysis (EDA), coupled with SQL, is essential for data analysts involved in data exploration and analysis. However, data analysts often encounter two primary challenges: (1) the need to craft SQL queries skillfully, and (2) the requirement to generate suitable visualization types that enhance the interpretation of query results. Due to its significance, substantial research efforts have been made to explore different approaches to address these challenges, including leveraging large language models (LLMs). However, existing methods fail to meet real-world data exploration requirements primarily due to (1) complex database schema; (2) unclear user intent; (3) limited cross-domain generalization capability; and (4) insufficient end-to-end text-to-visualization capability.
  This paper presents TiInsight, an automated SQL-based cross-domain exploratory data analysis system. First, we propose hierarchical data context (i.e., HDC), which leverages LLMs to summarize the contexts related to the database schema, which is crucial for open-world EDA systems to generalize across data domains. Second, the EDA system is divided into four components (i.e., stages): HDC generation, question clarification and decomposition, text-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart). Finally, we implemented an end-to-end EDA system with a user-friendly GUI interface in the production environment at PingCAP. We have also open-sourced all APIs of TiInsight to facilitate research within the EDA community. Through extensive evaluations by a real-world user study, we demonstrate that TiInsight offers remarkable performance compared to human experts. Specifically, TiSQL achieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It also demonstrates state-of-the-art performance on the Bird dataset.
<div id='section'>Paperid: <span id='pid'>1972, <a href='https://arxiv.org/pdf/2412.07127.pdf' target='_blank'>https://arxiv.org/pdf/2412.07127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Li, Song Wang, Chen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07127">Deep Learning-Enhanced Preconditioning for Efficient Conjugate Gradient Solvers in Large-Scale PDE Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preconditioning techniques are crucial for enhancing the efficiency of solving large-scale linear equation systems that arise from partial differential equation (PDE) discretization. These techniques, such as Incomplete Cholesky factorization (IC) and data-driven neural network methods, accelerate the convergence of iterative solvers like Conjugate Gradient (CG) by approximating the original matrices. This paper introduces a novel approach that integrates Graph Neural Network (GNN) with traditional IC, addressing the shortcomings of direct generation methods based on GNN and achieving significant improvements in computational efficiency and scalability. Experimental results demonstrate an average reduction in iteration counts by 24.8% compared to IC and a two-order-of-magnitude increase in training scale compared to previous methods. A three-dimensional static structural analysis utilizing finite element methods was validated on training sparse matrices of up to 5 million dimensions and inference scales of up to 10 million. Furthermore, the approach demon-strates robust generalization capabilities across scales, facilitating the effective acceleration of CG solvers for large-scale linear equations using small-scale data on modest hardware. The method's robustness and scalability make it a practical solution for computational science.
<div id='section'>Paperid: <span id='pid'>1973, <a href='https://arxiv.org/pdf/2412.05169.pdf' target='_blank'>https://arxiv.org/pdf/2412.05169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Schapiro, Han Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05169">Towards Understanding the Role of Sharpness-Aware Minimization Algorithms for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, sharpness-aware minimization (SAM) has emerged as a promising method to improve generalization by minimizing sharpness, which is known to correlate well with generalization ability. Since the original proposal of SAM, many variants of SAM have been proposed to improve its accuracy and efficiency, but comparisons have mainly been restricted to the i.i.d. setting. In this paper we study SAM for out-of-distribution (OOD) generalization. First, we perform a comprehensive comparison of eight SAM variants on zero-shot OOD generalization, finding that the original SAM outperforms the Adam baseline by $4.76\%$ and the strongest SAM variants outperform the Adam baseline by $8.01\%$ on average. We then provide an OOD generalization bound in terms of sharpness for this setting. Next, we extend our study of SAM to the related setting of gradual domain adaptation (GDA), another form of OOD generalization where intermediate domains are constructed between the source and target domains, and iterative self-training is done on intermediate domains, to improve the overall target domain error. In this setting, our experimental results demonstrate that the original SAM outperforms the baseline of Adam on each of the experimental datasets by $0.82\%$ on average and the strongest SAM variants outperform Adam by $1.52\%$ on average. We then provide a generalization bound for SAM in the GDA setting. Asymptotically, this generalization bound is no better than the one for self-training in the literature of GDA. This highlights a further disconnection between the theoretical justification for SAM versus its empirical performance, with recent work finding that low sharpness alone does not account for all of SAM's generalization benefits. For future work, we provide several potential avenues for obtaining a tighter analysis for SAM in the OOD setting.
<div id='section'>Paperid: <span id='pid'>1974, <a href='https://arxiv.org/pdf/2411.19451.pdf' target='_blank'>https://arxiv.org/pdf/2411.19451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhao, Chang Xu, Bailu Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19451">Learning Visual Abstract Reasoning through Dual-Stream Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual abstract reasoning tasks present challenges for deep neural networks, exposing limitations in their capabilities. In this work, we present a neural network model that addresses the challenges posed by Raven's Progressive Matrices (RPM). Inspired by the two-stream hypothesis of visual processing, we introduce the Dual-stream Reasoning Network (DRNet), which utilizes two parallel branches to capture image features. On top of the two streams, a reasoning module first learns to merge the high-level features of the same image. Then, it employs a rule extractor to handle combinations involving the eight context images and each candidate image, extracting discrete abstract rules and utilizing an multilayer perceptron (MLP) to make predictions. Empirical results demonstrate that the proposed DRNet achieves state-of-the-art average performance across multiple RPM benchmarks. Furthermore, DRNet demonstrates robust generalization capabilities, even extending to various out-of-distribution scenarios. The dual streams within DRNet serve distinct functions by addressing local or spatial information. They are then integrated into the reasoning module, leveraging abstract rules to facilitate the execution of visual reasoning tasks. These findings indicate that the dual-stream architecture could play a crucial role in visual abstract reasoning.
<div id='section'>Paperid: <span id='pid'>1975, <a href='https://arxiv.org/pdf/2411.19434.pdf' target='_blank'>https://arxiv.org/pdf/2411.19434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Safaa Abdullahi Moallim Mohamud, Ho-Young Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19434">Actions and Objects Pathways for Domain Adaptation in Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce the Actions and Objects Pathways (AOPath) for out-of-domain generalization in video question answering tasks. AOPath leverages features from a large pretrained model to enhance generalizability without the need for explicit training on the unseen domains. Inspired by human brain, AOPath dissociates the pretrained features into action and object features, and subsequently processes them through separate reasoning pathways. It utilizes a novel module which converts out-of-domain features into domain-agnostic features without introducing any trainable weights. We validate the proposed approach on the TVQA dataset, which is partitioned into multiple subsets based on genre to facilitate the assessment of generalizability. The proposed approach demonstrates 5% and 4% superior performance over conventional classifiers on out-of-domain and in-domain datasets, respectively. It also outperforms prior methods that involve training millions of parameters, whereas the proposed approach trains very few parameters.
<div id='section'>Paperid: <span id='pid'>1976, <a href='https://arxiv.org/pdf/2411.16877.pdf' target='_blank'>https://arxiv.org/pdf/2411.16877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zequn Chen, Jiezhi Yang, Heng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16877">PreF3R: Pose-Free Feed-Forward 3D Gaussian Splatting from Variable-length Image Sequence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PreF3R, Pose-Free Feed-forward 3D Reconstruction from an image sequence of variable length. Unlike previous approaches, PreF3R removes the need for camera calibration and reconstructs the 3D Gaussian field within a canonical coordinate frame directly from a sequence of unposed images, enabling efficient novel-view rendering. We leverage DUSt3R's ability for pair-wise 3D structure reconstruction, and extend it to sequential multi-view input via a spatial memory network, eliminating the need for optimization-based global alignment. Additionally, PreF3R incorporates a dense Gaussian parameter prediction head, which enables subsequent novel-view synthesis with differentiable rasterization. This allows supervising our model with the combination of photometric loss and pointmap regression loss, enhancing both photorealism and structural accuracy. Given a sequence of ordered images, PreF3R incrementally reconstructs the 3D Gaussian field at 20 FPS, therefore enabling real-time novel-view rendering. Empirical experiments demonstrate that PreF3R is an effective solution for the challenging task of pose-free feed-forward novel-view synthesis, while also exhibiting robust generalization to unseen scenes.
<div id='section'>Paperid: <span id='pid'>1977, <a href='https://arxiv.org/pdf/2411.10886.pdf' target='_blank'>https://arxiv.org/pdf/2411.10886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ansh Shah, K Madhava Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10886">MetricGold: Leveraging Text-To-Image Latent Diffusion Models for Metric Depth Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering metric depth from a single image remains a fundamental challenge in computer vision, requiring both scene understanding and accurate scaling. While deep learning has advanced monocular depth estimation, current models often struggle with unfamiliar scenes and layouts, particularly in zero-shot scenarios and when predicting scale-ergodic metric depth. We present MetricGold, a novel approach that harnesses generative diffusion model's rich priors to improve metric depth estimation. Building upon recent advances in MariGold, DDVM and Depth Anything V2 respectively, our method combines latent diffusion, log-scaled metric depth representation, and synthetic data training. MetricGold achieves efficient training on a single RTX 3090 within two days using photo-realistic synthetic data from HyperSIM, VirtualKitti, and TartanAir. Our experiments demonstrate robust generalization across diverse datasets, producing sharper and higher quality metric depth estimates compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>1978, <a href='https://arxiv.org/pdf/2411.10819.pdf' target='_blank'>https://arxiv.org/pdf/2411.10819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Yan, Zhong Chen, Cai Xu, Xinglei Shen, Jay Shiao, John Einck, Ronald C Chen, Hao Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10819">An Oversampling-enhanced Multi-class Imbalanced Classification Framework for Patient Health Status Prediction Using Patient-reported Outcomes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patient-reported outcomes (PROs) directly collected from cancer patients being treated with radiation therapy play a vital role in assisting clinicians in counseling patients regarding likely toxicities. Precise prediction and evaluation of symptoms or health status associated with PROs are fundamental to enhancing decision-making and planning for the required services and support as patients transition into survivorship. However, the raw PRO data collected from hospitals exhibits some intrinsic challenges such as incomplete item reports and imbalance patient toxicities. To the end, in this study, we explore various machine learning techniques to predict patient outcomes related to health status such as pain levels and sleep discomfort using PRO datasets from a cancer photon/proton therapy center. Specifically, we deploy six advanced machine learning classifiers -- Random Forest (RF), XGBoost, Gradient Boosting (GB), Support Vector Machine (SVM), Multi-Layer Perceptron with Bagging (MLP-Bagging), and Logistic Regression (LR) -- to tackle a multi-class imbalance classification problem across three prevalent cancer types: head and neck, prostate, and breast cancers. To address the class imbalance issue, we employ an oversampling strategy, adjusting the training set sample sizes through interpolations of in-class neighboring samples, thereby augmenting minority classes without deviating from the original skewed class distribution. Our experimental findings across multiple PRO datasets indicate that the RF and XGB methods achieve robust generalization performance, evidenced by weighted AUC and detailed confusion matrices, in categorizing outcomes as mild, intermediate, and severe post-radiation therapy. These results underscore the models' effectiveness and potential utility in clinical settings.
<div id='section'>Paperid: <span id='pid'>1979, <a href='https://arxiv.org/pdf/2411.06798.pdf' target='_blank'>https://arxiv.org/pdf/2411.06798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David R. Nelson, Ashish Kumar Jaiswal, Noha Ismail, Alexandra Mystikou, Kourosh Salehi-Ashtiani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06798">LA4SR: illuminating the dark proteome with generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI language models (LMs) show promise for biological sequence analysis. We re-engineered open-source LMs (GPT-2, BLOOM, DistilRoBERTa, ELECTRA, and Mamba, ranging from 70M to 12B parameters) for microbial sequence classification. The models achieved F1 scores up to 95 and operated 16,580x faster and at 2.9x the recall of BLASTP. They effectively classified the algal dark proteome - uncharacterized proteins comprising about 65% of total proteins - validated on new data including a new, complete Hi-C/Pacbio Chlamydomonas genome. Larger (>1B) LA4SR models reached high accuracy (F1 > 86) when trained on less than 2% of available data, rapidly achieving strong generalization capacity. High accuracy was achieved when training data had intact or scrambled terminal information, demonstrating robust generalization to incomplete sequences. Finally, we provide custom AI explainability software tools for attributing amino acid patterns to AI generative processes and interpret their outputs in evolutionary and biophysical contexts.
<div id='section'>Paperid: <span id='pid'>1980, <a href='https://arxiv.org/pdf/2411.06214.pdf' target='_blank'>https://arxiv.org/pdf/2411.06214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuguang Li, Zhonglin Zuo, Zheng Dong, Yang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06214">Early Prediction of Natural Gas Pipeline Leaks Using the MKTCN Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural gas pipeline leaks pose severe risks, leading to substantial economic losses and potential hazards to human safety. In this study, we develop an accurate model for the early prediction of pipeline leaks. To the best of our knowledge, unlike previous anomaly detection, this is the first application to use internal pipeline data for early prediction of leaks. The modeling process addresses two main challenges: long-term dependencies and sample imbalance. First, we introduce a dilated convolution-based prediction model to capture long-term dependencies, as dilated convolution expands the model's receptive field without added computational cost. Second, to mitigate sample imbalance, we propose the MKTCN model, which incorporates the Kolmogorov-Arnold Network as the fully connected layer in a dilated convolution model, enhancing network generalization. Finally, we validate the MKTCN model through extensive experiments on two real-world datasets. Results demonstrate that MKTCN outperforms in generalization and classification, particularly under severe data imbalance, and effectively predicts leaks up to 5000 seconds in advance. Overall, the MKTCN model represents a significant advancement in early pipeline leak prediction, providing robust generalization and improved modeling of the long-term dependencies inherent in multi-dimensional time-series data.
<div id='section'>Paperid: <span id='pid'>1981, <a href='https://arxiv.org/pdf/2411.04777.pdf' target='_blank'>https://arxiv.org/pdf/2411.04777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elija Deineko, Carina Kehrt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04777">Learn to Solve Vehicle Routing Problems ASAP: A Neural Optimization Approach for Time-Constrained Vehicle Routing Problems with Finite Vehicle Fleet</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding a feasible and prompt solution to the Vehicle Routing Problem (VRP) is a prerequisite for efficient freight transportation, seamless logistics, and sustainable mobility. Traditional optimization methods reach their limits when confronted with the real-world complexity of VRPs, which involve numerous constraints and objectives. Recently, the ability of generative Artificial Intelligence (AI) to solve combinatorial tasks, known as Neural Combinatorial Optimization (NCO), demonstrated promising results, offering new perspectives. In this study, we propose an NCO approach to solve a time-constrained capacitated VRP with a finite vehicle fleet size. The approach is based on an encoder-decoder architecture, formulated in line with the Policy Optimization with Multiple Optima (POMO) protocol and trained via a Proximal Policy Optimization (PPO) algorithm. We successfully trained the policy with multiple objectives (minimizing the total distance while maximizing vehicle utilization) and evaluated it on medium and large instances, benchmarking it against state-of-the-art heuristics. The method is able to find adequate and cost-efficient solutions, showing both flexibility and robust generalization. Finally, we provide a critical analysis of the solution generated by NCO and discuss the challenges and opportunities of this new branch of intelligent learning algorithms emerging in optimization science, focusing on freight transportation.
<div id='section'>Paperid: <span id='pid'>1982, <a href='https://arxiv.org/pdf/2410.20102.pdf' target='_blank'>https://arxiv.org/pdf/2410.20102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuto Shibata, Yasunori Kudo, Yohei Sugawara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20102">Anatomical 3D Style Transfer Enabling Efficient Federated Learning with Extremely Low Communication Costs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose a novel federated learning (FL) approach that utilizes 3D style transfer for the multi-organ segmentation task. The multi-organ dataset, obtained by integrating multiple datasets, has high scalability and can improve generalization performance as the data volume increases. However, the heterogeneity of data owing to different clients with diverse imaging conditions and target organs can lead to severe overfitting of local models. To align models that overfit to different local datasets, existing methods require frequent communication with the central server, resulting in higher communication costs and risk of privacy leakage. To achieve an efficient and safe FL, we propose an Anatomical 3D Frequency Domain Generalization (A3DFDG) method for FL. A3DFDG utilizes structural information of human organs and clusters the 3D styles based on the location of organs. By mixing styles based on these clusters, it preserves the anatomical information and leads models to learn intra-organ diversity, while aligning the optimization of each local model. Experiments indicate that our method can maintain its accuracy even in cases where the communication cost is highly limited (=1.25% of the original cost) while achieving a significant difference compared to baselines, with a higher global dice similarity coefficient score of 4.3%. Despite its simplicity and minimal computational overhead, these results demonstrate that our method has high practicality in real-world scenarios where low communication costs and a simple pipeline are required. The code used in this project will be publicly available.
<div id='section'>Paperid: <span id='pid'>1983, <a href='https://arxiv.org/pdf/2410.01213.pdf' target='_blank'>https://arxiv.org/pdf/2410.01213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arda Genc, Justin Marlowe, Anika Jalil, Libor Kovarik, Phillip Christopher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01213">A versatile machine learning workflow for high-throughput analysis of supported metal catalyst particles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and efficient characterization of nanoparticles (NPs), particularly regarding particle size distribution, is essential for advancing our understanding of their structure-property relationships and facilitating their design for various applications. In this study, we introduce a novel two-stage artificial intelligence (AI)-driven workflow for NP analysis that leverages prompt engineering techniques from state-of-the-art single-stage object detection and large-scale vision transformer (ViT) architectures. This methodology was applied to transmission electron microscopy (TEM) and scanning TEM (STEM) images of heterogeneous catalysts, enabling high-resolution, high-throughput analysis of particle size distributions for supported metal catalysts. The model's performance in detecting and segmenting NPs was validated across diverse heterogeneous catalyst systems, including various metals (Cu, Ru, Pt, and PtCo), supports (silica ($\text{SiO}_2$), $Î³$-alumina ($Î³$-$\text{Al}_2\text{O}_3$), and carbon black), and particle diameter size distributions with means and standard deviations of 2.9 $\pm$ 1.1 nm, 1.6 $\pm$ 0.2 nm, 9.7 $\pm$ 4.6 nm, and 4 $\pm$ 1.0 nm. Additionally, the proposed machine learning (ML) approach successfully detects and segments overlapping NPs anchored on non-uniform catalytic support materials, providing critical insights into their spatial arrangements and interactions. Our AI-assisted NP analysis workflow demonstrates robust generalization across diverse datasets and can be readily applied to similar NP segmentation tasks without requiring costly model retraining.
<div id='section'>Paperid: <span id='pid'>1984, <a href='https://arxiv.org/pdf/2409.14060.pdf' target='_blank'>https://arxiv.org/pdf/2409.14060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjun Kim, Ohtae Jang, Haekang Song, Heesub Shin, Jaewoo Ok, Minyoung Back, Jaehyuk Youn, Sungho Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14060">Soft Segmented Randomization: Enhancing Domain Generalization in SAR-ATR for Synthetic-to-Measured</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic aperture radar technology is crucial for high-resolution imaging under various conditions; however, the acquisition of real-world synthetic aperture radar data for deep learning-based automatic target recognition remains challenging due to high costs and data availability issues. To overcome these challenges, synthetic data generated through simulations have been employed, although discrepancies between synthetic and real data can degrade model performance. In this study, we introduce a novel framework, soft segmented randomization, designed to reduce domain discrepancy and improve the generalize ability of synthetic aperture radar automatic target recognition models. The soft segmented randomization framework applies a Gaussian mixture model to segment target and clutter regions softly, introducing randomized variations that align the synthetic data's statistical properties more closely with those of real-world data. Experimental results demonstrate that the proposed soft segmented randomization framework significantly enhances model performance on measured synthetic aperture radar data, making it a promising approach for robust automatic target recognition in scenarios with limited or no access to measured data.
<div id='section'>Paperid: <span id='pid'>1985, <a href='https://arxiv.org/pdf/2409.10048.pdf' target='_blank'>https://arxiv.org/pdf/2409.10048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wessel Ledder, Yuzhen Qin, Kiki van der Heijden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10048">Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although deep reinforcement learning (DRL) approaches in audio signal processing have seen substantial progress in recent years, audio-driven DRL for tasks such as navigation, gaze control and head-orientation control in the context of human-robot interaction have received little attention. Here, we propose an audio-driven DRL framework in which we utilise deep Q-learning to develop an autonomous agent that orients towards a talker in the acoustic environment based on stereo speech recordings. Our results show that the agent learned to perform the task at a near perfect level when trained on speech segments in anechoic environments (that is, without reverberation). The presence of reverberation in naturalistic acoustic environments affected the agent's performance, although the agent still substantially outperformed a baseline, randomly acting agent. Finally, we quantified the degree of generalization of the proposed DRL approach across naturalistic acoustic environments. Our experiments revealed that policies learned by agents trained on medium or high reverb environments generalized to low reverb environments, but policies learned by agents trained on anechoic or low reverb environments did not generalize to medium or high reverb environments. Taken together, this study demonstrates the potential of audio-driven DRL for tasks such as head-orientation control and highlights the need for training strategies that enable robust generalization across environments for real-world audio-driven DRL applications.
<div id='section'>Paperid: <span id='pid'>1986, <a href='https://arxiv.org/pdf/2409.09052.pdf' target='_blank'>https://arxiv.org/pdf/2409.09052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youzhu Jin, Yichen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09052">OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have achieved significant success in the general field of image processing. Their emerging task generalization and freeform conversational capabilities can greatly facilitate medical diagnostic assistance, helping patients better understand their conditions and enhancing doctor-patient trust. Computed Tomography (CT) is a non-invasive imaging technique used to capture the internal mechanisms of a patient's condition and is widely utilized. However, in past research, the complex textural features of this imaging data have made accurate interpretation by algorithms challenging, impeding the performance of general LLMs in diagnostic assistance. To address this, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is trained on 120,000 CT images and diagnostic reports and includes a Retrieval-Augmented Generation (RAG) module capable of effectively mitigating model hallucinations. This module is informed by extensive medical literature, textbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT images but also stores, understands, and reasons over medical knowledge and language. In extensive experiments, OrthoDoc outperforms commercial models led by GPT-4, demonstrating superior diagnostic capabilities and accuracy. Specifically, OrthoDoc significantly surpasses existing models in the diagnosis of common orthopedic conditions such as fractures, arthritis, and tumors. Additionally, OrthoDoc exhibits robust generalization and stability when handling rare and complex cases.
<div id='section'>Paperid: <span id='pid'>1987, <a href='https://arxiv.org/pdf/2409.05885.pdf' target='_blank'>https://arxiv.org/pdf/2409.05885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Wu, Teng Wang, Jiaqi Nan, Lijun Yang, Jingxuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05885">A Dual-Path neural network model to construct the flame nonlinear thermoacoustic response in the time domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional numerical simulation methods require substantial computational resources to accurately determine the complete nonlinear thermoacoustic response of flames to various perturbation frequencies and amplitudes. In this paper, we have developed deep learning algorithms that can construct a comprehensive flame nonlinear response from limited numerical simulation data. To achieve this, we propose using a frequency-sweeping data type as the training dataset, which incorporates a rich array of learnable information within a constrained dataset. To enhance the precision in learning flame nonlinear response patterns from the training data, we introduce a Dual-Path neural network. This network consists of a Chronological Feature Path and a Temporal Detail Feature Path. The Dual-Path network is specifically designed to focus intensively on the temporal characteristics of velocity perturbation sequences, yielding more accurate flame response patterns and enhanced generalization capabilities. Validations confirm that our approach can accurately model flame nonlinear responses, even under conditions of significant nonlinearity, and exhibits robust generalization capabilities across various test scenarios.
<div id='section'>Paperid: <span id='pid'>1988, <a href='https://arxiv.org/pdf/2409.01930.pdf' target='_blank'>https://arxiv.org/pdf/2409.01930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajesh Upadhayayaya, Manish Raj Osti, Zachary Smith, Chritopher Kottmyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01930">Efficient LLM Context Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) demonstrate proficiency across diverse tasks but often require targeted adaptations for specific applications. Various methods have been proposed to facilitate this adaptation, including fewshot fine-tuning, in-context learning, and context distillation. This paper specifically investigates context distillation a method that extends the utility of task-specific examples by internalizing them, thus augmenting the example set accessible for model inference. We conduct a comparative analysis of context distillation with in-context learning (ICL) and few-shot fine-tuning (FT), aiming to ascertain the efficacy of context distillation in adapting models using minimal in-context examples. Employing matched datasets from Mobach, our experiments leverage OPT models of various sizes. The results indicate that context distillation effectively adapts models, with student models attaining comparable in-domain and out-of-domain accuracies to in-context learning. Although context distillation surpasses ICL in out-of-domain generalization, it does not achieve the performance levels of FT. However, the reduced dataset size and computational demands position context distillation as a viable alternative, especially for smaller datasets. Overall, this study presents context distillation as an efficient and potent method for customizing LLMs to specific tasks.
<div id='section'>Paperid: <span id='pid'>1989, <a href='https://arxiv.org/pdf/2408.16964.pdf' target='_blank'>https://arxiv.org/pdf/2408.16964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Younghan Kim, Kangryun Moon, Yongjun Park, Yonggyu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16964">Causal Representation-Based Domain Generalization on Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of extensive datasets containing gaze information for each subject has significantly enhanced gaze estimation accuracy. However, the discrepancy between domains severely affects a model's performance explicitly trained for a particular domain. In this paper, we propose the Causal Representation-Based Domain Generalization on Gaze Estimation (CauGE) framework designed based on the general principle of causal mechanisms, which is consistent with the domain difference. We employ an adversarial training manner and an additional penalizing term to extract domain-invariant features. After extracting features, we position the attention layer to make features sufficient for inferring the actual gaze. By leveraging these modules, CauGE ensures that the neural networks learn from representations that meet the causal mechanisms' general principles. By this, CauGE generalizes across domains by extracting domain-invariant features, and spurious correlations cannot influence the model. Our method achieves state-of-the-art performance in the domain generalization on gaze estimation benchmark.
<div id='section'>Paperid: <span id='pid'>1990, <a href='https://arxiv.org/pdf/2408.15569.pdf' target='_blank'>https://arxiv.org/pdf/2408.15569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Yuan, Frederic Maire, Feras Dayoub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15569">Temporal Attention for Cross-View Sequential Image Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel approach to enhancing cross-view localization, focusing on the fine-grained, sequential localization of street-view images within a single known satellite image patch, a significant departure from traditional one-to-one image retrieval methods. By expanding to sequential image fine-grained localization, our model, equipped with a novel Temporal Attention Module (TAM), leverages contextual information to significantly improve sequential image localization accuracy. Our method shows substantial reductions in both mean and median localization errors on the Cross-View Image Sequence (CVIS) dataset, outperforming current state-of-the-art single-image localization techniques. Additionally, by adapting the KITTI-CVL dataset into sequential image sets, we not only offer a more realistic dataset for future research but also demonstrate our model's robust generalization capabilities across varying times and areas, evidenced by a 75.3% reduction in mean distance error in cross-view sequential image localization.
<div id='section'>Paperid: <span id='pid'>1991, <a href='https://arxiv.org/pdf/2408.12834.pdf' target='_blank'>https://arxiv.org/pdf/2408.12834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yafeng Zhang, Zilan Yu, Yuang Huang, Jing Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12834">CLLMFS: A Contrastive Learning enhanced Large Language Model Framework for Few-Shot Named Entity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot Named Entity Recognition (NER), the task of identifying named entities with only a limited amount of labeled data, has gained increasing significance in natural language processing. While existing methodologies have shown some effectiveness, such as enriching label semantics through various prompting modes or employing metric learning techniques, their performance exhibits limited robustness across diverse domains due to the lack of rich knowledge in their pre-trained models. To address this issue, we propose CLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework for Few-Shot Named Entity Recognition, achieving promising results with limited training data. Considering the impact of LLM's internal representations on downstream tasks, CLLMFS integrates Low-Rank Adaptation (LoRA) and contrastive learning mechanisms specifically tailored for few-shot NER. By enhancing the model's internal representations, CLLMFS effectively improves both entity boundary awareness ability and entity recognition accuracy. Our method has achieved state-of-the-art performance improvements on F1-score ranging from 2.58\% to 97.74\% over existing best-performing methods across several recognized benchmarks. Furthermore, through cross-domain NER experiments conducted on multiple datasets, we have further validated the robust generalization capability of our method. Our code will be released in the near future.
<div id='section'>Paperid: <span id='pid'>1992, <a href='https://arxiv.org/pdf/2408.03078.pdf' target='_blank'>https://arxiv.org/pdf/2408.03078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>G. Manni, C. Lauretti, F. Prata, R. Papalia, L. Zollo, P. Soda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03078">BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Endoscopic surgery relies on two-dimensional views, posing challenges for surgeons in depth perception and instrument manipulation. While Monocular Visual Simultaneous Localization and Mapping (MVSLAM) has emerged as a promising solution, its implementation in endoscopic procedures faces significant challenges due to hardware limitations, such as the use of a monocular camera and the absence of odometry sensors. This study presents BodySLAM, a robust deep learning-based MVSLAM approach that addresses these challenges through three key components: CycleVO, a novel unsupervised monocular pose estimation module; the integration of the state-of-the-art Zoe architecture for monocular depth estimation; and a 3D reconstruction module creating a coherent surgical map. The approach is rigorously evaluated using three publicly available datasets (Hamlyn, EndoSLAM, and SCARED) spanning laparoscopy, gastroscopy, and colonoscopy scenarios, and benchmarked against four state-of-the-art methods. Results demonstrate that CycleVO exhibited competitive performance with the lowest inference time among pose estimation methods, while maintaining robust generalization capabilities, whereas Zoe significantly outperformed existing algorithms for depth estimation in endoscopy. BodySLAM's strong performance across diverse endoscopic scenarios demonstrates its potential as a viable MVSLAM solution for endoscopic applications.
<div id='section'>Paperid: <span id='pid'>1993, <a href='https://arxiv.org/pdf/2407.13808.pdf' target='_blank'>https://arxiv.org/pdf/2407.13808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gun Lee, Subin An, Sungyong Baik, Soochahn Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13808">CoAPT: Context Attribute words for Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel prompt tuning method called CoAPT(Context Attribute words in Prompt Tuning) for few/zero-shot image classification. The core motivation is that attributes are descriptive words with rich information about a given concept. Thus, we aim to enrich text queries of existing prompt tuning methods, improving alignment between text and image embeddings in CLIP embedding space. To do so, CoAPT integrates attribute words as additional prompts within learnable prompt tuning and can be easily incorporated into various existing prompt tuning methods. To facilitate the incorporation of attributes into text embeddings and the alignment with image embeddings, soft prompts are trained together with an additional meta-network that generates input-image-wise feature biases from the concatenated feature encodings of the image-text combined queries. Our experiments demonstrate that CoAPT leads to considerable improvements for existing baseline methods on several few/zero-shot image classification tasks, including base-to-novel generalization, cross-dataset transfer, and domain generalization. Our findings highlight the importance of combining hard and soft prompts and pave the way for future research on the interplay between text and image latent spaces in pre-trained models.
<div id='section'>Paperid: <span id='pid'>1994, <a href='https://arxiv.org/pdf/2407.13417.pdf' target='_blank'>https://arxiv.org/pdf/2407.13417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyong Chen, Yaxiu Zhang, Yan Zhang, Xin Zhang, Xingwei Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13417">GDDS: A Single Domain Generalized Defect Detection Frame of Open World Scenario using Gather and Distribute Domain-shift Suppression Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and intelligent surface defect detection of photovoltaic modules is crucial for improving the quality of photovoltaic modules and ensuring the reliable operation of large-scale infrastructure. However, the scenario characteristics of data distribution deviation make the construction of defect detection models for open world scenarios such as photovoltaic manufacturing and power plant inspections a challenge. Therefore, we propose the Gather and Distribute Domain shift Suppression Network (GDDS). It adopts a single domain generalized method that is completely independent of the test samples to address the problem of distribution shift. Using a one-stage network as the baseline network breaks through the limitations of traditional domain generalization methods that typically use two-stage networks. It not only balances detection accuracy and speed but also simplifies the model deployment and application process. The GDDS includes two modules: DeepSpine Module and Gather and Distribute Module. Specifically, the DeepSpine Module applies a wider range of contextual information and suppresses background style shift by acquiring and concatenating multi-scale features. The Gather and Distribute Module collects and distributes global information to achieve cross layer interactive learning of multi-scale channel features and suppress defect instance shift. Furthermore, the GDDS utilizes normalized Wasserstein distance for similarity measurement, reducing measurement errors caused by bounding box position deviations. We conducted a comprehensive evaluation of GDDS on the EL endogenous shift dataset and Photovoltaic inspection infrared image dataset. The experimental results showed that GDDS can adapt to defect detection in open world scenarios faster and better than other state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1995, <a href='https://arxiv.org/pdf/2407.07225.pdf' target='_blank'>https://arxiv.org/pdf/2407.07225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suriya Prakash Jambunathan, Ashwath Shankarnarayan, Parijat Dube
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07225">ConvNLP: Image-based AI Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The potentials of Generative-AI technologies like Large Language models (LLMs) to revolutionize education are undermined by ethical considerations around their misuse which worsens the problem of academic dishonesty. LLMs like GPT-4 and Llama 2 are becoming increasingly powerful in generating sophisticated content and answering questions, from writing academic essays to solving complex math problems. Students are relying on these LLMs to complete their assignments and thus compromising academic integrity. Solutions to detect LLM-generated text are compute-intensive and often lack generalization. This paper presents a novel approach for detecting LLM-generated AI-text using a visual representation of word embedding. We have formulated a novel Convolutional Neural Network called ZigZag ResNet, as well as a scheduler for improving generalization, named ZigZag Scheduler. Through extensive evaluation using datasets of text generated by six different state-of-the-art LLMs, our model demonstrates strong intra-domain and inter-domain generalization capabilities. Our best model detects AI-generated text with an impressive average detection rate (over inter- and intra-domain test data) of 88.35%. Through an exhaustive ablation study, our ZigZag ResNet and ZigZag Scheduler provide a performance improvement of nearly 4% over the vanilla ResNet. The end-to-end inference latency of our model is below 2.5ms per sentence. Our solution offers a lightweight, computationally efficient, and faster alternative to existing tools for AI-generated text detection, with better generalization performance. It can help academic institutions in their fight against the misuse of LLMs in academic settings. Through this work, we aim to contribute to safeguarding the principles of academic integrity and ensuring the trustworthiness of student work in the era of advanced LLMs.
<div id='section'>Paperid: <span id='pid'>1996, <a href='https://arxiv.org/pdf/2407.02910.pdf' target='_blank'>https://arxiv.org/pdf/2407.02910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas BÃ¼hler, Jonas Fehrenbach, Lucas Steinmann, Christian Nauck, Marios Koulakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02910">Domain-independent detection of known anomalies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One persistent obstacle in industrial quality inspection is the detection of anomalies. In real-world use cases, two problems must be addressed: anomalous data is sparse and the same types of anomalies need to be detected on previously unseen objects. Current anomaly detection approaches can be trained with sparse nominal data, whereas domain generalization approaches enable detecting objects in previously unseen domains. Utilizing those two observations, we introduce the hybrid task of domain generalization on sparse classes. To introduce an accompanying dataset for this task, we present a modification of the well-established MVTec AD dataset by generating three new datasets. In addition to applying existing methods for benchmark, we design two embedding-based approaches, Spatial Embedding MLP (SEMLP) and Labeled PatchCore. Overall, SEMLP achieves the best performance with an average image-level AUROC of 87.2 % vs. 80.4 % by MIRO. The new and openly available datasets allow for further research to improve industrial anomaly detection.
<div id='section'>Paperid: <span id='pid'>1997, <a href='https://arxiv.org/pdf/2406.16872.pdf' target='_blank'>https://arxiv.org/pdf/2406.16872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianguo Pan, Zhengxin Hu, Lingdun Zhang, Xia Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16872">Multi-channel Time Series Decomposition Network For Generalizable Sensor-Based Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sensor-based human activity recognition is important in daily scenarios such as smart healthcare and homes due to its non-intrusive privacy and low cost advantages, but the problem of out-of-domain generalization caused by differences in focusing individuals and operating environments can lead to significant accuracy degradation on cross-person behavior recognition due to the inconsistent distributions of training and test data. To address the above problems, this paper proposes a new method, Multi-channel Time Series Decomposition Network (MTSDNet). Firstly, MTSDNet decomposes the original signal into a combination of multiple polynomials and trigonometric functions by the trainable parameterized temporal decomposition to learn the low-rank representation of the original signal for improving the extraterritorial generalization ability of the model. Then, the different components obtained by the decomposition are classified layer by layer and the layer attention is used to aggregate components to obtain the final classification result. Extensive evaluation on DSADS, OPPORTUNITY, PAMAP2, UCIHAR and UniMib public datasets shows the advantages in predicting accuracy and stability of our method compared with other competing strategies, including the state-of-the-art ones. And the visualization is conducted to reveal MTSDNet's interpretability and layer-by-layer characteristics.
<div id='section'>Paperid: <span id='pid'>1998, <a href='https://arxiv.org/pdf/2406.10296.pdf' target='_blank'>https://arxiv.org/pdf/2406.10296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heeseok Jung, Jaesang Yoo, Yohaan Yoon, Yeonju Jang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10296">CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge tracing (KT), wherein students' problem-solving histories are used to estimate their current levels of knowledge, has attracted significant interest from researchers. However, most existing KT models were developed with an ID-based paradigm, which exhibits limitations in cold-start performance. These limitations can be mitigated by leveraging the vast quantities of external knowledge possessed by generative large language models (LLMs). In this study, we propose cold-start mitigation in knowledge tracing by aligning a generative language model as a students' knowledge tracer (CLST) as a framework that utilizes a generative LLM as a knowledge tracer. Upon collecting data from math, social studies, and science subjects, we framed the KT task as a natural language processing task, wherein problem-solving data are expressed in natural language, and fine-tuned the generative LLM using the formatted KT dataset. Subsequently, we evaluated the performance of the CLST in situations of data scarcity using various baseline models for comparison. The results indicate that the CLST significantly enhanced performance with a dataset of fewer than 100 students in terms of prediction, reliability, and cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>1999, <a href='https://arxiv.org/pdf/2406.09884.pdf' target='_blank'>https://arxiv.org/pdf/2406.09884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanqing Zhao, Yuta Nakashima, Haiyuan Chen, Noboru Babaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09884">Enhancing Fake News Detection in Social Media via Label Propagation on Cross-modal Tweet Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fake news detection in social media has become increasingly important due to the rapid proliferation of personal media channels and the consequential dissemination of misleading information. Existing methods, which primarily rely on multimodal features and graph-based techniques, have shown promising performance in detecting fake news. However, they still face a limitation, i.e., sparsity in graph connections, which hinders capturing possible interactions among tweets. This challenge has motivated us to explore a novel method that densifies the graph's connectivity to capture denser interaction better. Our method constructs a cross-modal tweet graph using CLIP, which encodes images and text into a unified space, allowing us to extract potential connections based on similarities in text and images. We then design a Feature Contextualization Network with Label Propagation (FCN-LP) to model the interaction among tweets as well as positive or negative correlations between predicted labels of connected tweets. The propagated labels from the graph are weighted and aggregated for the final detection. To enhance the model's generalization ability to unseen events, we introduce a domain generalization loss that ensures consistent features between tweets on seen and unseen events. We use three publicly available fake news datasets, Twitter, PHEME, and Weibo, for evaluation. Our method consistently improves the performance over the state-of-the-art methods on all benchmark datasets and effectively demonstrates its aptitude for generalizing fake news detection in social media.
<div id='section'>Paperid: <span id='pid'>2000, <a href='https://arxiv.org/pdf/2406.07332.pdf' target='_blank'>https://arxiv.org/pdf/2406.07332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Challapalli Phanindra Revanth, Sumohana S. Channappayya, C Krishna Mohan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07332">Minimizing Energy Costs in Deep Learning Model Training: The Gaussian Sampling Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computing the loss gradient via backpropagation consumes considerable energy during deep learning (DL) model training. In this paper, we propose a novel approach to efficiently compute DL models' gradients to mitigate the substantial energy overhead associated with backpropagation. Exploiting the over-parameterized nature of DL models and the smoothness of their loss landscapes, we propose a method called {\em GradSamp} for sampling gradient updates from a Gaussian distribution. Specifically, we update model parameters at a given epoch (chosen periodically or randomly) by perturbing the parameters (element-wise) from the previous epoch with Gaussian ``noise''. The parameters of the Gaussian distribution are estimated using the error between the model parameter values from the two previous epochs. {\em GradSamp} not only streamlines gradient computation but also enables skipping entire epochs, thereby enhancing overall efficiency. We rigorously validate our hypothesis across a diverse set of standard and non-standard CNN and transformer-based models, spanning various computer vision tasks such as image classification, object detection, and image segmentation. Additionally, we explore its efficacy in out-of-distribution scenarios such as Domain Adaptation (DA), Domain Generalization (DG), and decentralized settings like Federated Learning (FL). Our experimental results affirm the effectiveness of {\em GradSamp} in achieving notable energy savings without compromising performance, underscoring its versatility and potential impact in practical DL applications.
<div id='section'>Paperid: <span id='pid'>2001, <a href='https://arxiv.org/pdf/2406.06354.pdf' target='_blank'>https://arxiv.org/pdf/2406.06354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denys Pushkin, RaphaÃ«l Berthier, Emmanuel Abbe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06354">On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the out-of-domain generalization of random feature (RF) models and Transformers. We first prove that in the `generalization on the unseen (GOTU)' setting, where training data is fully seen in some part of the domain but testing is made on another part, and for RF models in the small feature regime, the convergence takes place to interpolators of minimal degree as in the Boolean case (Abbe et al., 2023). We then consider the sparse target regime and explain how this regime relates to the small feature regime, but with a different regularization term that can alter the picture in the non-Boolean case. We show two different outcomes for the sparse regime with q-ary data tokens: (1) if the data is embedded with roots of unities, then a min-degree interpolator is learned like in the Boolean case for RF models, (2) if the data is not embedded as such, e.g., simply as integers, then RF models and Transformers may not learn minimal degree interpolators. This shows that the Boolean setting and its roots of unities generalization are special cases where the minimal degree interpolator offers a rare characterization of how learning takes place. For more general integer and real-valued settings, a more nuanced picture remains to be fully characterized.
<div id='section'>Paperid: <span id='pid'>2002, <a href='https://arxiv.org/pdf/2406.04158.pdf' target='_blank'>https://arxiv.org/pdf/2406.04158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Da Li, Guoqiang Zhao, Chen Yao, Kaiqiang Zhu, Houjun Sun, Jiacheng Bao, Maokun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04158">Deep Learning-based Cross-modal Reconstruction of Vehicle Target from Sparse 3D SAR Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three-dimensional synthetic aperture radar (3D SAR) is an advanced active microwave imaging technology widely utilized in remote sensing area. To achieve high-resolution 3D imaging,3D SAR requires observations from multiple aspects and altitude baselines surrounding the target. However, constrained flight trajectories often lead to sparse observations, which degrade imaging quality, particularly for anisotropic man-made small targets, such as vehicles and aircraft. In the past, compressive sensing (CS) was the mainstream approach for sparse 3D SAR image reconstruction. More recently, deep learning (DL) has emerged as a powerful alternative, markedly boosting reconstruction quality and efficiency. However, existing DL-based methods typically rely solely on high-quality 3D SAR images as supervisory signals to train deep neural networks (DNNs). This unimodal learning paradigm prevents the integration of complementary information from other data modalities, which limits reconstruction performance and reduces target discriminability due to the inherent constraints of electromagnetic scattering. In this paper, we introduce cross-modal learning and propose a Cross-Modal 3D-SAR Reconstruction Network (CMAR-Net) for enhancing sparse 3D SAR images of vehicle targets by fusing optical information. Leveraging cross-modal supervision from 2D optical images and error propagation guaranteed by differentiable rendering, CMAR-Net achieves efficient training and reconstructs sparse 3D SAR images, which are derived from highly sparse-aspect observations, into visually structured 3D vehicle images. Trained exclusively on simulated data, CMAR-Net exhibits robust generalization to real-world data, outperforming state-of-the-art CS and DL methods in structural accuracy within a large-scale parking lot experiment involving numerous civilian vehicles, thereby demonstrating its strong practical applicability.
<div id='section'>Paperid: <span id='pid'>2003, <a href='https://arxiv.org/pdf/2405.20451.pdf' target='_blank'>https://arxiv.org/pdf/2405.20451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyi Li, Yunbei Xu, Ruohan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20451">Statistical Properties of Robust Satisficing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Robust Satisficing (RS) model is an emerging approach to robust optimization, offering streamlined procedures and robust generalization across various applications. However, the statistical theory of RS remains unexplored in the literature. This paper fills in the gap by comprehensively analyzing the theoretical properties of the RS model. Notably, the RS structure offers a more straightforward path to deriving statistical guarantees compared to the seminal Distributionally Robust Optimization (DRO), resulting in a richer set of results. In particular, we establish two-sided confidence intervals for the optimal loss without the need to solve a minimax optimization problem explicitly. We further provide finite-sample generalization error bounds for the RS optimizer. Importantly, our results extend to scenarios involving distribution shifts, where discrepancies exist between the sampling and target distributions. Our numerical experiments show that the RS model consistently outperforms the baseline empirical risk minimization in small-sample regimes and under distribution shifts. Furthermore, compared to the DRO model, the RS model exhibits lower sensitivity to hyperparameter tuning, highlighting its practicability for robustness considerations.
<div id='section'>Paperid: <span id='pid'>2004, <a href='https://arxiv.org/pdf/2405.13181.pdf' target='_blank'>https://arxiv.org/pdf/2405.13181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Prasad Varadarajan Srinivasan, Prasanth Gumpena, Madhusudhana Yattapu, Vishal H. Brahmbhatt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13181">Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of large language models (LLMs), arXiv:2305.16938 showed that few-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and Pattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize similarly on Out-Of-Domain (OOD) datasets, but vary in terms of task adaptation. However, they both pose challenges, especially in term of memory requirements. In this paper, we further try to push the understanding of different fine-tuning strategies for LLM and aim to bring a myriad of these on the same pedestal for an elaborate comparison with full-model fine-tuning on two diverse datasets. To that end, we conducted a series of experiments, beginning with state-of-the-art methods like vanilla fine-tuning and Pattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets, COLA and MNLI. We then investigate adaptive fine-tuning and the efficiency of LoRA adapters in a few-shot setting. Finally, we also compare an alternative approach that has gained recent popularity -- context distillation -- with the vanilla FT and PBFT with and without few-shot setup.
  Our findings suggest that these alternative strategies that we explored can exhibit out-of-domain generalization comparable to that of vanilla FT and PBFT. PBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the need for effective prompts. Further, our adaptive-fine tuning and LoRA experiments perform comparable or slightly worse than the standard fine-tunings as anticipated, since standard fine-tunings involve tuning the entire model. Finally, our context distillation experiments out-perform the standard fine-tuning methods. These findings underscore that eventually the choice of an appropriate fine-tuning method depends on the available resources (memory, compute, data) and task adaptability.
<div id='section'>Paperid: <span id='pid'>2005, <a href='https://arxiv.org/pdf/2405.11212.pdf' target='_blank'>https://arxiv.org/pdf/2405.11212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claudiu Creanga, Liviu Petrisor Dinu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11212">Automated Text Identification Using CNN and Training Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We used Data Maps to model and characterize the AuTexTification dataset. This provides insights about the behaviour of individual samples during training across epochs (training dynamics). We characterized the samples across 3 dimensions: confidence, variability and correctness. This shows the presence of 3 regions: easy-to-learn, ambiguous and hard-to-learn examples. We used a classic CNN architecture and found out that training the model only on a subset of ambiguous examples improves the model's out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>2006, <a href='https://arxiv.org/pdf/2405.05336.pdf' target='_blank'>https://arxiv.org/pdf/2405.05336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alvaro Gomariz, Yusuke Kikuchi, Yun Yvonna Li, Thomas Albrecht, Andreas Maunz, Daniela Ferrara, Huanxiang Lu, Orcun Goksel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05336">Joint semi-supervised and contrastive learning enables domain generalization and multi-domain segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their effectiveness, current deep learning models face challenges with images coming from different domains with varying appearance and content. We introduce SegCLR, a versatile framework designed to segment images across different domains, employing supervised and contrastive learning simultaneously to effectively learn from both labeled and unlabeled data. We demonstrate the superior performance of SegCLR through a comprehensive evaluation involving three diverse clinical datasets of 3D retinal Optical Coherence Tomography (OCT) images, for the slice-wise segmentation of fluids with various network configurations and verification across 10 different network initializations. In an unsupervised domain adaptation context, SegCLR achieves results on par with a supervised upper-bound model trained on the intended target domain. Notably, we discover that the segmentation performance of SegCLR framework is marginally impacted by the abundance of unlabeled data from the target domain, thereby we also propose an effective domain generalization extension of SegCLR, known also as zero-shot domain adaptation, which eliminates the need for any target domain information. This shows that our proposed addition of contrastive loss in standard supervised training for segmentation leads to superior models, inherently more generalizable to both in- and out-of-domain test data. We additionally propose a pragmatic solution for SegCLR deployment in realistic scenarios with multiple domains containing labeled data. Accordingly, our framework pushes the boundaries of deep-learning based segmentation in multi-domain applications, regardless of data availability - labeled, unlabeled, or nonexistent.
<div id='section'>Paperid: <span id='pid'>2007, <a href='https://arxiv.org/pdf/2405.01439.pdf' target='_blank'>https://arxiv.org/pdf/2405.01439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Zhao, Pinyan Tang, Sihui Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01439">Improving Domain Generalization on Gaze Estimation via Branch-out Auxiliary Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite remarkable advancements, mainstream gaze estimation techniques, particularly appearance-based methods, often suffer from performance degradation in uncontrolled environments due to variations in illumination and individual facial attributes. Existing domain adaptation strategies, limited by their need for target domain samples, may fall short in real-world applications. This letter introduces Branch-out Auxiliary Regularization (BAR), an innovative method designed to boost gaze estimation's generalization capabilities without requiring direct access to target domain data. Specifically, BAR integrates two auxiliary consistency regularization branches: one that uses augmented samples to counteract environmental variations, and another that aligns gaze directions with positive source domain samples to encourage the learning of consistent gaze features. These auxiliary pathways strengthen the core network and are integrated in a smooth, plug-and-play manner, facilitating easy adaptation to various other models. Comprehensive experimental evaluations on four cross-dataset tasks demonstrate the superiority of our approach.
<div id='section'>Paperid: <span id='pid'>2008, <a href='https://arxiv.org/pdf/2405.01389.pdf' target='_blank'>https://arxiv.org/pdf/2405.01389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao-Rong Lai, Weiwen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01389">Invariant Risk Minimization Is A Total Variation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invariant risk minimization (IRM) is an arising approach to generalize invariant features to different environments in machine learning. While most related works focus on new IRM settings or new application scenarios, the mathematical essence of IRM remains to be properly explained. We verify that IRM is essentially a total variation based on $L^2$ norm (TV-$\ell_2$) of the learning risk with respect to the classifier variable. Moreover, we propose a novel IRM framework based on the TV-$\ell_1$ model. It not only expands the classes of functions that can be used as the learning risk and the feature extractor, but also has robust performance in denoising and invariant feature preservation based on the coarea formula. We also illustrate some requirements for IRM-TV-$\ell_1$ to achieve out-of-distribution generalization. Experimental results show that the proposed framework achieves competitive performance in several benchmark machine learning scenarios.
<div id='section'>Paperid: <span id='pid'>2009, <a href='https://arxiv.org/pdf/2404.14856.pdf' target='_blank'>https://arxiv.org/pdf/2404.14856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuhang Li, Ning Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14856">Cross-Domain Causal Preference Learning for Out-of-Distribution Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recommender systems use users' historical interactions to learn their preferences and deliver personalized recommendations from a vast array of candidate items. Current recommender systems primarily rely on the assumption that the training and testing datasets have identical distributions, which may not hold true in reality. In fact, the distribution shift between training and testing datasets often occurs as a result of the evolution of user attributes, which degrades the performance of the conventional recommender systems because they fail in Out-of-Distribution (OOD) generalization, particularly in situations of data sparsity. This study delves deeply into the challenge of OOD generalization and proposes a novel model called Cross-Domain Causal Preference Learning for Out-of-Distribution Recommendation (CDCOR), which involves employing a domain adversarial network to uncover users' domain-shared preferences and utilizing a causal structure learner to capture causal invariance to deal with the OOD problem. Through extensive experiments on two real-world datasets, we validate the remarkable performance of our model in handling diverse scenarios of data sparsity and out-of-distribution environments. Furthermore, our approach surpasses the benchmark models, showcasing outstanding capabilities in out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>2010, <a href='https://arxiv.org/pdf/2404.02084.pdf' target='_blank'>https://arxiv.org/pdf/2404.02084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyuan Zhong, Hu Ke, Ming Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02084">Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fundus image segmentation on unseen domains is challenging, especially for the over-parameterized deep models trained on the small medical datasets. To address this challenge, we propose a method named Adaptive Feature-fusion Neural Network (AFNN) for glaucoma segmentation on unseen domains, which mainly consists of three modules: domain adaptor, feature-fusion network, and self-supervised multi-task learning. Specifically, the domain adaptor helps the pretrained-model fast adapt from other image domains to the medical fundus image domain. Feature-fusion network and self-supervised multi-task learning for the encoder and decoder are introduced to improve the domain generalization ability. In addition, we also design the weighted-dice-loss to improve model performance on complex optic-cup segmentation tasks. Our proposed method achieves a competitive performance over existing fundus segmentation methods on four public glaucoma datasets.
<div id='section'>Paperid: <span id='pid'>2011, <a href='https://arxiv.org/pdf/2404.01608.pdf' target='_blank'>https://arxiv.org/pdf/2404.01608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Li, Linjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01608">FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning methods often assume that the test data have the same distribution as the training data. However, this assumption may not hold due to multiple levels of heterogeneity in applications, raising issues in algorithmic fairness and domain generalization. In this work, we address the problem of fair and generalizable machine learning by invariant principles. We propose a training environment-based oracle, FAIRM, which has desirable fairness and domain generalization properties under a diversity-type condition. We then provide an empirical FAIRM with finite-sample theoretical guarantees under weak distributional assumptions. We then develop efficient algorithms to realize FAIRM in linear models and demonstrate the nonasymptotic performance with minimax optimality. We evaluate our method in numerical experiments with synthetic data and MNIST data and show that it outperforms its counterparts.
<div id='section'>Paperid: <span id='pid'>2012, <a href='https://arxiv.org/pdf/2403.11448.pdf' target='_blank'>https://arxiv.org/pdf/2403.11448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linyu Tang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11448">Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous studies have demonstrated the susceptibility of deep neural networks (DNNs) to subtle adversarial perturbations, prompting the development of many advanced adversarial defense methods aimed at mitigating adversarial attacks. Current defense strategies usually train DNNs for a specific adversarial attack method and can achieve good robustness in defense against this type of adversarial attack. Nevertheless, when subjected to evaluations involving unfamiliar attack modalities, empirical evidence reveals a pronounced deterioration in the robustness of DNNs. Meanwhile, there is a trade-off between the classification accuracy of clean examples and adversarial examples. Most defense methods often sacrifice the accuracy of clean examples in order to improve the adversarial robustness of DNNs. To alleviate these problems and enhance the overall robust generalization of DNNs, we propose the Test-Time Pixel-Level Adversarial Purification (TPAP) method. This approach is based on the robust overfitting characteristic of DNNs to the fast gradient sign method (FGSM) on training and test datasets. It utilizes FGSM for adversarial purification, to process images for purifying unknown adversarial perturbations from pixels at testing time in a "counter changes with changelessness" manner, thereby enhancing the defense capability of DNNs against various unknown adversarial attacks. Extensive experimental results show that our method can effectively improve both overall robust generalization of DNNs, notably over previous methods.
<div id='section'>Paperid: <span id='pid'>2013, <a href='https://arxiv.org/pdf/2403.11292.pdf' target='_blank'>https://arxiv.org/pdf/2403.11292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Asma Sattar, Georgios Deligiorgis, Marco Trincavelli, Davide Bacciu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11292">Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic multi-relational graphs are an expressive relational representation for data enclosing entities and relations of different types, and where relationships are allowed to vary in time. Addressing predictive tasks over such data requires the ability to find structure embeddings that capture the diversity of the relationships involved, as well as their dynamic evolution. In this work, we establish a novel class of challenging tasks for dynamic multi-relational graphs involving out-of-domain link prediction, where the relationship being predicted is not available in the input graph. We then introduce a novel Graph Neural Network model, named GOOD, designed specifically to tackle the out-of-domain generalization problem. GOOD introduces a novel design concept for multi-relation embedding aggregation, based on the idea that good representations are such when it is possible to disentangle the mixing proportions of the different relational embeddings that have produced it. We also propose five benchmarks based on two retail domains, where we show that GOOD can effectively generalize predictions out of known relationship types and achieve state-of-the-art results. Most importantly, we provide insights into problems where out-of-domain prediction might be preferred to an in-domain formulation, that is, where the relationship to be predicted has very few positive examples.
<div id='section'>Paperid: <span id='pid'>2014, <a href='https://arxiv.org/pdf/2403.06174.pdf' target='_blank'>https://arxiv.org/pdf/2403.06174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianting Chen, Ling Ding, Yunxiao Yang, Zaiyuan Di, Yang Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06174">Domain Adversarial Active Learning for Domain Generalization Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization models aim to learn cross-domain knowledge from source domain data, to improve performance on unknown target domains. Recent research has demonstrated that diverse and rich source domain samples can enhance domain generalization capability. This paper argues that the impact of each sample on the model's generalization ability varies. Despite its small scale, a high-quality dataset can still attain a certain level of generalization ability. Motivated by this, we propose a domain-adversarial active learning (DAAL) algorithm for classification tasks in domain generalization. First, we analyze that the objective of tasks is to maximize the inter-class distance within the same domain and minimize the intra-class distance across different domains. To achieve this objective, we design a domain adversarial selection method that prioritizes challenging samples. Second, we posit that even in a converged model, there are subsets of features that lack discriminatory power within each domain. We attempt to identify these feature subsets and optimize them by a constraint loss. We validate and analyze our DAAL algorithm on multiple domain generalization datasets, comparing it with various domain generalization algorithms and active learning algorithms. Our results demonstrate that the DAAL algorithm can achieve strong generalization ability with fewer data resources, thereby reducing data annotation costs in domain generalization tasks.
<div id='section'>Paperid: <span id='pid'>2015, <a href='https://arxiv.org/pdf/2402.18209.pdf' target='_blank'>https://arxiv.org/pdf/2402.18209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kenneth Enevoldsen, Emil Trenckner Jessen, Rebekah Baglini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18209">DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Named entity recognition is one of the cornerstones of Danish NLP, essential for language technology applications within both industry and research. However, Danish NER is inhibited by a lack of available datasets. As a consequence, no current models are capable of fine-grained named entity recognition, nor have they been evaluated for potential generalizability issues across datasets and domains. To alleviate these limitations, this paper introduces: 1) DANSK: a named entity dataset providing for high-granularity tagging as well as within-domain evaluation of models across a diverse set of domains; 2) DaCy 2.6.0 that includes three generalizable models with fine-grained annotation; and 3) an evaluation of current state-of-the-art models' ability to generalize across domains. The evaluation of existing and new models revealed notable performance discrepancies across domains, which should be addressed within the field. Shortcomings of the annotation quality of the dataset and its impact on model training and evaluation are also discussed. Despite these limitations, we advocate for the use of the new dataset DANSK alongside further work on the generalizability within Danish NER.
<div id='section'>Paperid: <span id='pid'>2016, <a href='https://arxiv.org/pdf/2402.17863.pdf' target='_blank'>https://arxiv.org/pdf/2402.17863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Young Kyung Kim, J. MatÃ­as Di Martino, Guillermo Sapiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17863">Vision Transformers with Natural Language Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tokens or patches within Vision Transformers (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP). Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information. We introduce a novel transformer model, Semantic Vision Transformers (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies. sViT effectively harnesses semantic information, creating an inductive bias reminiscent of convolutional neural networks while capturing global dependencies and contextual information within images that are characteristic of transformers. Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance. Furthermore, sViT demonstrates significant superiority in out-of-distribution generalization and robustness to natural distribution shifts, attributed to its scale invariance semantic characteristic. Notably, the use of semantic tokens significantly enhances the model's interpretability. Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities. Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust vision transformers.
<div id='section'>Paperid: <span id='pid'>2017, <a href='https://arxiv.org/pdf/2402.04038.pdf' target='_blank'>https://arxiv.org/pdf/2402.04038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan Sun, Junhong Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04038">PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural networks (GNNs) have gained popularity for various graph-related tasks. However, similar to deep neural networks, GNNs are also vulnerable to adversarial attacks. Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks. In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular GNNs, graph convolutional network (GCN) and message passing graph neural network, using the PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models. Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree. As corollaries, we derive better PAC-Bayesian robust generalization bounds for GCN in the standard setting, which improve the bounds in (Liao et al., 2020) by avoiding exponential dependence on the maximum node degree.
<div id='section'>Paperid: <span id='pid'>2018, <a href='https://arxiv.org/pdf/2402.00046.pdf' target='_blank'>https://arxiv.org/pdf/2402.00046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sofiene Lassoued, Andreas Schwung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00046">Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Resource utilization and production process optimization are crucial for companies in today's competitive industrial landscape. Addressing the complexities of job shop scheduling problems (JSSP) is essential to improving productivity, reducing costs, and ensuring timely delivery. We propose PetriRL, a novel framework integrating Petri nets and deep reinforcement learning (DRL) for JSSP optimization. PetriRL capitalizes on the inherent strengths of Petri nets in modelling discrete event systems while leveraging the advantages of a graph structure. The Petri net governs automated components of the process, ensuring adherence to JSSP constraints. This allows for synergistic collaboration with optimization algorithms such as DRL, particularly in critical decision-making. Unlike traditional methods, PetriRL eliminates the need to preprocess JSSP instances into disjunctive graphs and enhances the explainability of process status through its graphical structure based on places and transitions. Additionally, the inherent graph structure of Petri nets enables the dynamic additions of job operations during the inference phase without requiring agent retraining, thus enhancing flexibility. Experimental results demonstrate PetriRL's robust generalization across various instance sizes and its competitive performance on public test benchmarks and randomly generated instances. Results are compared to a wide range of optimization solutions such as heuristics, metaheuristics, and learning-based algorithms. Finally, the added values of the framework's key elements, such as event-based control and action masking, are studied in the ablation study.
<div id='section'>Paperid: <span id='pid'>2019, <a href='https://arxiv.org/pdf/2401.10272.pdf' target='_blank'>https://arxiv.org/pdf/2401.10272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikang Wei, Yahong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10272">Multi-Source Collaborative Gradient Discrepancy Minimization for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Domain Generalization aims to learn a domain-invariant model from multiple decentralized source domains for deployment on unseen target domain. Due to privacy concerns, the data from different source domains are kept isolated, which poses challenges in bridging the domain gap. To address this issue, we propose a Multi-source Collaborative Gradient Discrepancy Minimization (MCGDM) method for federated domain generalization. Specifically, we propose intra-domain gradient matching between the original images and augmented images to avoid overfitting the domain-specific information within isolated domains. Additionally, we propose inter-domain gradient matching with the collaboration of other domains, which can further reduce the domain shift across decentralized domains. Combining intra-domain and inter-domain gradient matching, our method enables the learned model to generalize well on unseen domains. Furthermore, our method can be extended to the federated domain adaptation task by fine-tuning the target model on the pseudo-labeled target domain. The extensive experiments on federated domain generalization and adaptation indicate that our method outperforms the state-of-the-art methods significantly.
<div id='section'>Paperid: <span id='pid'>2020, <a href='https://arxiv.org/pdf/2311.18007.pdf' target='_blank'>https://arxiv.org/pdf/2311.18007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yash Gondhalekar, Sultan Hassan, Naomi Saphra, Sambatra Andrianomena
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18007">Towards out-of-distribution generalization in large-scale astronomical surveys: robust networks learn similar representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization of machine learning (ML) models to out-of-distribution (OOD) examples remains a key challenge in extracting information from upcoming astronomical surveys. Interpretability approaches are a natural way to gain insights into the OOD generalization problem. We use Centered Kernel Alignment (CKA), a similarity measure metric of neural network representations, to examine the relationship between representation similarity and performance of pre-trained Convolutional Neural Networks (CNNs) on the CAMELS Multifield Dataset. We find that when models are robust to a distribution shift, they produce substantially different representations across their layers on OOD data. However, when they fail to generalize, these representations change less from layer to layer on OOD data. We discuss the potential application of similarity representation in guiding model design, training strategy, and mitigating the OOD problem by incorporating CKA as an inductive bias during training.
<div id='section'>Paperid: <span id='pid'>2021, <a href='https://arxiv.org/pdf/2311.16589.pdf' target='_blank'>https://arxiv.org/pdf/2311.16589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daeun Lee, Minhyeok Heo, Jiwon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16589">HD Maps are Lane Detection Generalizers: A Novel Generative Framework for Single-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lane detection is a vital task for vehicles to navigate and localize their position on the road. To ensure reliable driving, lane detection models must have robust generalization performance in various road environments. However, despite the advanced performance in the trained domain, their generalization performance still falls short of expectations due to the domain discrepancy. To bridge this gap, we propose a novel generative framework using HD Maps for Single-Source Domain Generalization (SSDG) in lane detection. We first generate numerous front-view images from lane markings of HD Maps. Next, we strategically select a core subset among the generated images using (i) lane structure and (ii) road surrounding criteria to maximize their diversity. In the end, utilizing this core set, we train lane detection models to boost their generalization performance. We validate that our generative framework from HD Maps outperforms the Domain Adaptation model MLDA with +3.01%p accuracy improvement, even though we do not access the target domain images.
<div id='section'>Paperid: <span id='pid'>2022, <a href='https://arxiv.org/pdf/2311.16200.pdf' target='_blank'>https://arxiv.org/pdf/2311.16200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianhao Chen, Jietao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16200">Streaming Lossless Volumetric Compression of Medical Images Using Gated Recurrent Convolutional Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based lossless compression methods offer substantial advantages in compressing medical volumetric images. Nevertheless, many learning-based algorithms encounter a trade-off between practicality and compression performance. This paper introduces a hardware-friendly streaming lossless volumetric compression framework, utilizing merely one-thousandth of the model weights compared to other learning-based compression frameworks. We propose a gated recurrent convolutional neural network that combines diverse convolutional structures and fusion gate mechanisms to capture the inter-slice dependencies in volumetric images. Based on such contextual information, we can predict the pixel-by-pixel distribution for entropy coding. Guided by hardware/software co-design principles, we implement the proposed framework on Field Programmable Gate Array to achieve enhanced real-time performance. Extensive experimental results indicate that our method outperforms traditional lossless volumetric compressors and state-of-the-art learning-based lossless compression methods across various medical image benchmarks. Additionally, our method exhibits robust generalization ability and competitive compression speed
<div id='section'>Paperid: <span id='pid'>2023, <a href='https://arxiv.org/pdf/2311.01022.pdf' target='_blank'>https://arxiv.org/pdf/2311.01022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kottakota Asish, P. Sarath Teja, R. Kishan Chander, D. Deva Hema
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01022">NeuroWrite: Predictive Handwritten Digit Classification using Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of deep neural networks has revolutionized the field of machine learning, enabling remarkable advancements in various domains. In this article, we introduce NeuroWrite, a unique method for predicting the categorization of handwritten digits using deep neural networks. Our model exhibits outstanding accuracy in identifying and categorising handwritten digits by utilising the strength of convolutional neural networks (CNNs) and recurrent neural networks (RNNs).In this article, we give a thorough examination of the data preparation methods, network design, and training methods used in NeuroWrite. By implementing state-of-the-art techniques, we showcase how NeuroWrite can achieve high classification accuracy and robust generalization on handwritten digit datasets, such as MNIST. Furthermore, we explore the model's potential for real-world applications, including digit recognition in digitized documents, signature verification, and automated postal code recognition. NeuroWrite is a useful tool for computer vision and pattern recognition because of its performance and adaptability.The architecture, training procedure, and evaluation metrics of NeuroWrite are covered in detail in this study, illustrating how it can improve a number of applications that call for handwritten digit classification. The outcomes show that NeuroWrite is a promising method for raising the bar for deep neural network-based handwritten digit recognition.
<div id='section'>Paperid: <span id='pid'>2024, <a href='https://arxiv.org/pdf/2310.18702.pdf' target='_blank'>https://arxiv.org/pdf/2310.18702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phillip Pope, David Jacobs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18702">Towards Combinatorial Generalization for Catalysts: A Kohn-Sham Charge-Density Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Kohn-Sham equations underlie many important applications such as the discovery of new catalysts. Recent machine learning work on catalyst modeling has focused on prediction of the energy, but has so far not yet demonstrated significant out-of-distribution generalization. Here we investigate another approach based on the pointwise learning of the Kohn-Sham charge-density. On a new dataset of bulk catalysts with charge densities, we show density models can generalize to new structures with combinations of elements not seen at train time, a form of combinatorial generalization. We show that over 80% of binary and ternary test cases achieve faster convergence than standard baselines in Density Functional Theory, amounting to an average reduction of 13% in the number of iterations required to reach convergence, which may be of independent interest. Our results suggest that density learning is a viable alternative, trading greater inference costs for a step towards combinatorial generalization, a key property for applications.
<div id='section'>Paperid: <span id='pid'>2025, <a href='https://arxiv.org/pdf/2310.15913.pdf' target='_blank'>https://arxiv.org/pdf/2310.15913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qilei Li, Shaogang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15913">Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep learning has significantly improved ReID model accuracy under the independent and identical distribution (IID) assumption, it has also become clear that such models degrade notably when applied to an unseen novel domain due to unpredictable/unknown domain shift. Contemporary domain generalization (DG) ReID models struggle in learning domain-invariant representation solely through training on an instance classification objective. We consider that a deep learning model is heavily influenced and therefore biased towards domain-specific characteristics, e.g., background clutter, scale and viewpoint variations, limiting the generalizability of the learned model, and hypothesize that the pedestrians are domain invariant owning they share the same structural characteristics. To enable the ReID model to be less domain-specific from these pure pedestrians, we introduce a method that guides model learning of the primary ReID instance classification objective by a concurrent auxiliary learning objective on weakly labeled pedestrian saliency detection. To solve the problem of conflicting optimization criteria in the model parameter space between the two learning objectives, we introduce a Primary-Auxiliary Objectives Association (PAOA) mechanism to calibrate the loss gradients of the auxiliary task towards the primary learning task gradients. Benefiting from the harmonious multitask learning design, our model can be extended with the recent test-time diagram to form the PAOA+, which performs on-the-fly optimization against the auxiliary objective in order to maximize the model's generative capacity in the test target domain. Experiments demonstrate the superiority of the proposed PAOA model.
<div id='section'>Paperid: <span id='pid'>2026, <a href='https://arxiv.org/pdf/2310.07253.pdf' target='_blank'>https://arxiv.org/pdf/2310.07253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuoying Wei, Xinlong Wen, Lida Zhu, Songquan Li, Rongbo Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07253">ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obtaining accurate and valid information for drug molecules is a crucial and challenging task. However, chemical knowledge and information have been accumulated over the past 100 years from various regions, laboratories, and experimental purposes. Little has been explored in terms of the out-of-distribution (OOD) problem with noise and inconsistency, which may lead to weak robustness and unsatisfied performance. This study proposes a novel benchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. ADMEOOD obtained 27 ADME (Absorption, Distribution, Metabolism, Excretion) drug properties from Chembl and relevant literature. Additionally, it includes two kinds of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD). Noise Shift responds to the noise level by categorizing the environment into different confidence levels. On the other hand, CCD describes the data which has inconsistent label among the original data. Finally, it tested on a variety of domain generalization models, and the experimental results demonstrate the effectiveness of the proposed partition method in ADMEOOD: ADMEOOD demonstrates a significant difference performance between in-distribution and out-of-distribution data. Moreover, ERM (Empirical Risk Minimization) and other models exhibit distinct trends in performance across different domains and measurement types.
<div id='section'>Paperid: <span id='pid'>2027, <a href='https://arxiv.org/pdf/2310.01037.pdf' target='_blank'>https://arxiv.org/pdf/2310.01037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Li, Xu Yang, Anye Cao, Changbin Wang, Yaoqi Liu, Yapeng Liu, Qiang Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01037">SeisT: A foundational deep learning model for earthquake monitoring tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Seismograms, the fundamental seismic records, have revolutionized earthquake research and monitoring. Recent advancements in deep learning have further enhanced seismic signal processing, leading to even more precise and effective earthquake monitoring capabilities. This paper introduces a foundational deep learning model, the Seismogram Transformer (SeisT), designed for a variety of earthquake monitoring tasks. SeisT combines multiple modules tailored to different tasks and exhibits impressive out-of-distribution generalization performance, outperforming or matching state-of-the-art models in tasks like earthquake detection, seismic phase picking, first-motion polarity classification, magnitude estimation, back-azimuth estimation, and epicentral distance estimation. The performance scores on the tasks are 0.96, 0.96, 0.68, 0.95, 0.86, 0.55, and 0.81, respectively. The most significant improvements, in comparison to existing models, are observed in phase-P picking, phase-S picking, and magnitude estimation, with gains of 1.7%, 9.5%, and 8.0%, respectively. Our study, through rigorous experiments and evaluations, suggests that SeisT has the potential to contribute to the advancement of seismic signal processing and earthquake research.
<div id='section'>Paperid: <span id='pid'>2028, <a href='https://arxiv.org/pdf/2309.11446.pdf' target='_blank'>https://arxiv.org/pdf/2309.11446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valeriy Berezovskiy, Nikita Morozov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11446">Weight Averaging Improves Knowledge Distillation under Domain Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation (KD) is a powerful model compression technique broadly used in practical deep learning applications. It is focused on training a small student network to mimic a larger teacher network. While it is widely known that KD can offer an improvement to student generalization in i.i.d setting, its performance under domain shift, i.e. the performance of student networks on data from domains unseen during training, has received little attention in the literature. In this paper we make a step towards bridging the research fields of knowledge distillation and domain generalization. We show that weight averaging techniques proposed in domain generalization literature, such as SWAD and SMA, also improve the performance of knowledge distillation under domain shift. In addition, we propose a simplistic weight averaging strategy that does not require evaluation on validation data during training and show that it performs on par with SWAD and SMA when applied to KD. We name our final distillation approach Weight-Averaged Knowledge Distillation (WAKD).
<div id='section'>Paperid: <span id='pid'>2029, <a href='https://arxiv.org/pdf/2309.11301.pdf' target='_blank'>https://arxiv.org/pdf/2309.11301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharon Chokuwa, Muhammad H. Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11301">Generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization for Diabetic Retinopathy (DR) classification allows a model to adeptly classify retinal images from previously unseen domains with various imaging conditions and patient demographics, thereby enhancing its applicability in a wide range of clinical environments. In this study, we explore the inherent capacity of variational autoencoders to disentangle the latent space of fundus images, with an aim to obtain a more robust and adaptable domain-invariant representation that effectively tackles the domain shift encountered in DR datasets. Despite the simplicity of our approach, we explore the efficacy of this classical method and demonstrate its ability to outperform contemporary state-of-the-art approaches for this task using publicly available datasets. Our findings challenge the prevailing assumption that highly sophisticated methods for DR classification are inherently superior for domain generalization. This highlights the importance of considering simple methods and adapting them to the challenging task of generalizing medical images, rather than solely relying on advanced techniques.
<div id='section'>Paperid: <span id='pid'>2030, <a href='https://arxiv.org/pdf/2308.16089.pdf' target='_blank'>https://arxiv.org/pdf/2308.16089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ujjal Kr Dutta, Aldo Lipani, Chuan Wang, Yukun Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16089">Application of Zone Method based Physics-Informed Neural Networks in Reheating Furnaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation Industries (FIs) constitute glass, metals, cement, ceramics, bulk chemicals, paper, steel, etc. and provide crucial, foundational materials for a diverse set of economically relevant industries: automobiles, machinery, construction, household appliances, chemicals, etc. Reheating furnaces within the manufacturing chain of FIs are energy-intensive. Accurate and real-time prediction of underlying temperatures in reheating furnaces has the potential to reduce the overall heating time, thereby controlling the energy consumption for achieving the Net-Zero goals in FIs. In this paper, we cast this prediction as a regression task and explore neural networks due to their inherent capability of being effective and efficient, given adequate data. However, due to the infeasibility of achieving good-quality real data in scenarios like reheating furnaces, classical Hottel's zone method based computational model has been used to generate data for model training. To further enhance the Out-Of-Distribution generalization capability of the trained model, we propose a Physics-Informed Neural Network (PINN) by incorporating prior physical knowledge using a set of novel Energy-Balance regularizers.
<div id='section'>Paperid: <span id='pid'>2031, <a href='https://arxiv.org/pdf/2308.09931.pdf' target='_blank'>https://arxiv.org/pdf/2308.09931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Geng Liu, Yuxi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09931">TDG: Text-guided Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) attempts to generalize a model trained on single or multiple source domains to the unseen target domain. Benefiting from the success of Visual-and-Language Pre-trained models in recent years, we argue that it is crucial for domain generalization by introducing extra text information. In this paper, we develop a novel Text-guided Domain Generalization (TDG) paradigm for domain generalization, which includes three following aspects. Specifically, we first devise an automatic words generation method to extend the description of current domains with novel domain-relevant words. Then, we embed the generated domain information into the text feature space, by the proposed prompt learning-based text feature generation method, which shares a common representation space with the image feature. Finally, we utilize both input image features and generated text features to train a specially designed classifier that generalizes well on unseen target domains, while the image encoder is also updated under the supervision of gradients back propagated from the classifier. Our experimental results show that the techniques incorporated by TDG contribute to the performance in an easy implementation manner. Experimental results on several domain generalization benchmarks show that our proposed framework achieves superior performance by effectively leveraging generated text information in domain generalization.
<div id='section'>Paperid: <span id='pid'>2032, <a href='https://arxiv.org/pdf/2308.02951.pdf' target='_blank'>https://arxiv.org/pdf/2308.02951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueling Li, Sebastian Martschat, Simone Paolo Ponzetto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02951">Multi-Source (Pre-)Training for Cross-Domain Measurement, Unit and Context Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a cross-domain approach for automated measurement and context extraction based on pre-trained language models. We construct a multi-source, multi-domain corpus and train an end-to-end extraction pipeline. We then apply multi-source task-adaptive pre-training and fine-tuning to benchmark the cross-domain generalization capability of our model. Further, we conceptualize and apply a task-specific error analysis and derive insights for future work. Our results suggest that multi-source training leads to the best overall results, while single-source training yields the best results for the respective individual domain. While our setup is successful at extracting quantity values and units, more research is needed to improve the extraction of contextual entities. We make the cross-domain corpus used in this work available online.
<div id='section'>Paperid: <span id='pid'>2033, <a href='https://arxiv.org/pdf/2307.05624.pdf' target='_blank'>https://arxiv.org/pdf/2307.05624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengyi Li, Qifan Xue, Yezhuo Zhang, Xuanpeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05624">CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trajectory prediction is critical for autonomous driving vehicles. Most existing methods tend to model the correlation between history trajectory (input) and future trajectory (output). Since correlation is just a superficial description of reality, these methods rely heavily on the i.i.d. assumption and evince a heightened susceptibility to out-of-distribution data. To address this problem, we propose an Out-of- Distribution Causal Graph (OOD-CG), which explicitly defines the underlying causal structure of the data with three entangled latent features: 1) domain-invariant causal feature (IC), 2) domain-variant causal feature (VC), and 3) domain-variant non-causal feature (VN ). While these features are confounded by confounder (C) and domain selector (D). To leverage causal features for prediction, we propose a Causal Inspired Learning Framework (CILF), which includes three steps: 1) extracting domain-invariant causal feature by means of an invariance loss, 2) extracting domain variant feature by domain contrastive learning, and 3) separating domain-variant causal and non-causal feature by encouraging causal sufficiency. We evaluate the performance of CILF in different vehicle trajectory prediction models on the mainstream datasets NGSIM and INTERACTION. Experiments show promising improvements in CILF on domain generalization.
<div id='section'>Paperid: <span id='pid'>2034, <a href='https://arxiv.org/pdf/2306.14122.pdf' target='_blank'>https://arxiv.org/pdf/2306.14122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Chen, Yujian Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.14122">Chain-of-Thought Prompt Distillation for Multimodal Named Entity Recognition and Multimodal Relation Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) necessitate the fundamental reasoning capacity for intricate linguistic and multimodal comprehension. In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps. Specifically, we commence by exemplifying the elicitation of such reasoning ability from LLMs through CoT prompts covering multi-grain (noun, sentence, multimodality) and data-augmentation (style, entity, image) dimensions. Subsequently, we present a novel conditional prompt distillation method to assimilate the commonsense reasoning ability from LLMs, thereby enhancing the utility of the student model in addressing text-only inputs without the requisite addition of image and CoT knowledge. Extensive experiments reveal that our approach attains state-of-the-art accuracy and manifests a plethora of advantages concerning interpretability, data efficiency, and cross-domain generalization on MNER and MRE datasets.
<div id='section'>Paperid: <span id='pid'>2035, <a href='https://arxiv.org/pdf/2306.12078.pdf' target='_blank'>https://arxiv.org/pdf/2306.12078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A. Asensio Ramos, S. Esteban Pozuelo, C. Kuckein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12078">Accelerating Multiframe Blind Deconvolution via Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ground-based solar image restoration is a computationally expensive procedure that involves nonlinear optimization techniques. The presence of atmospheric turbulence produces perturbations in individual images that make it necessary to apply blind deconvolution techniques. These techniques rely on the observation of many short exposure frames that are used to simultaneously infer the instantaneous state of the atmosphere and the unperturbed object. We have recently explored the use of machine learning to accelerate this process, with promising results. We build upon this previous work to propose several interesting improvements that lead to better models. As well, we propose a new method to accelerate the restoration based on algorithm unrolling. In this method, the image restoration problem is solved with a gradient descent method that is unrolled and accelerated aided by a few small neural networks. The role of the neural networks is to correct the estimation of the solution at each iterative step. The model is trained to perform the optimization in a small fixed number of steps with a curated dataset. Our findings demonstrate that both methods significantly reduce the restoration time compared to the standard optimization procedure. Furthermore, we showcase that these models can be trained in an unsupervised manner using observed images from three different instruments. Remarkably, they also exhibit robust generalization capabilities when applied to new datasets. To foster further research and collaboration, we openly provide the trained models, along with the corresponding training and evaluation code, as well as the training dataset, to the scientific community.
<div id='section'>Paperid: <span id='pid'>2036, <a href='https://arxiv.org/pdf/2306.11928.pdf' target='_blank'>https://arxiv.org/pdf/2306.11928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vivien Cabannes, Carles Domingo-Enrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11928">Open Problem: Learning with Variational Objectives on Measures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The theory of statistical learning has focused on variational objectives expressed on functions. In this note, we discuss motivations to write similar objectives on measures, in particular to discuss out-of-distribution generalization and weakly-supervised learning. It raises a natural question: can one cast usual statistical learning results to objectives expressed on measures? Does the resulting construction lead to new algorithms of practical interest?
<div id='section'>Paperid: <span id='pid'>2037, <a href='https://arxiv.org/pdf/2306.01449.pdf' target='_blank'>https://arxiv.org/pdf/2306.01449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chia-Chun Chung, Pei-Chun Chang, Yong-Sheng Chen, HaoYuan He, Chinson Yeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01449">SASMU: boost the performance of generalized recognition model using synthetic face dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, deploying a robust face recognition product becomes easy with the development of face recognition techniques for decades. Not only profile image verification but also the state-of-the-art method can handle the in-the-wild image almost perfectly. However, the concern of privacy issues raise rapidly since mainstream research results are powered by tons of web-crawled data, which faces the privacy invasion issue. The community tries to escape this predicament completely by training the face recognition model with synthetic data but faces severe domain gap issues, which still need to access real images and identity labels to fine-tune the model. In this paper, we propose SASMU, a simple, novel, and effective method for face recognition using a synthetic dataset. Our proposed method consists of spatial data augmentation (SA) and spectrum mixup (SMU). We first analyze the existing synthetic datasets for developing a face recognition system. Then, we reveal that heavy data augmentation is helpful for boosting performance when using synthetic data. By analyzing the previous frequency mixup studies, we proposed a novel method for domain generalization. Extensive experimental results have demonstrated the effectiveness of SASMU, achieving state-of-the-art performance on several common benchmarks, such as LFW, AgeDB-30, CA-LFW, CFP-FP, and CP-LFW.
<div id='section'>Paperid: <span id='pid'>2038, <a href='https://arxiv.org/pdf/2305.17305.pdf' target='_blank'>https://arxiv.org/pdf/2305.17305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elahe Rahimian, Golara Javadi, Frederick Tung, Gabriel Oliveira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17305">DynaShare: Task and Instance Conditioned Parameter Sharing for Multi-Task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task networks rely on effective parameter sharing to achieve robust generalization across tasks. In this paper, we present a novel parameter sharing method for multi-task learning that conditions parameter sharing on both the task and the intermediate feature representations at inference time. In contrast to traditional parameter sharing approaches, which fix or learn a deterministic sharing pattern during training and apply the same pattern to all examples during inference, we propose to dynamically decide which parts of the network to activate based on both the task and the input instance. Our approach learns a hierarchical gating policy consisting of a task-specific policy for coarse layer selection and gating units for individual input instances, which work together to determine the execution path at inference time. Experiments on the NYU v2, Cityscapes and MIMIC-III datasets demonstrate the potential of the proposed approach and its applicability across problem domains.
<div id='section'>Paperid: <span id='pid'>2039, <a href='https://arxiv.org/pdf/2305.16820.pdf' target='_blank'>https://arxiv.org/pdf/2305.16820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Ajit Nair, Sukomal Pal, Pradeepika Verma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16820">Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number of target domain documents. The similarity of the generated summaries to their corresponding documents is used for calculating weights required to average source prefixes. In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging allows for the computationally efficient addition of new source domains. When evaluated on four diverse summarization domains, DAPA shows comparable or better performance against the baselines, demonstrating the effectiveness of its prefix averaging scheme.
<div id='section'>Paperid: <span id='pid'>2040, <a href='https://arxiv.org/pdf/2305.14890.pdf' target='_blank'>https://arxiv.org/pdf/2305.14890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arne F. Nix, Max F. Burg, Fabian H. Sinz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14890">HARD: Hard Augmentations for Robust Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation (KD) is a simple and successful method to transfer knowledge from a teacher to a student model solely based on functional activity. However, current KD has a few shortcomings: it has recently been shown that this method is unsuitable to transfer simple inductive biases like shift equivariance, struggles to transfer out of domain generalization, and optimization time is magnitudes longer compared to default non-KD model training. To improve these aspects of KD, we propose Hard Augmentations for Robust Distillation (HARD), a generally applicable data augmentation framework, that generates synthetic data points for which the teacher and the student disagree. We show in a simple toy example that our augmentation framework solves the problem of transferring simple equivariances with KD. We then apply our framework in real-world tasks for a variety of augmentation models, ranging from simple spatial transformations to unconstrained image manipulations with a pretrained variational autoencoder. We find that our learned augmentations significantly improve KD performance on in-domain and out-of-domain evaluation. Moreover, our method outperforms even state-of-the-art data augmentations and since the augmented training inputs can be visualized, they offer a qualitative insight into the properties that are transferred from the teacher to the student. Thus HARD represents a generally applicable, dynamically optimized data augmentation technique tailored to improve the generalization and convergence speed of models trained with KD.
<div id='section'>Paperid: <span id='pid'>2041, <a href='https://arxiv.org/pdf/2305.13046.pdf' target='_blank'>https://arxiv.org/pdf/2305.13046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sang-Yeong Jo, Sung Whan Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13046">POEM: Polarization of Embeddings for Domain-Invariant Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Handling out-of-distribution samples is a long-lasting challenge for deep visual models. In particular, domain generalization (DG) is one of the most relevant tasks that aims to train a model with a generalization capability on novel domains. Most existing DG approaches share the same philosophy to minimize the discrepancy between domains by finding the domain-invariant representations. On the contrary, our proposed method called POEM acquires a strong DG capability by learning domain-invariant and domain-specific representations and polarizing them. Specifically, POEM cotrains category-classifying and domain-classifying embeddings while regularizing them to be orthogonal via minimizing the cosine-similarity between their features, i.e., the polarization of embeddings. The clear separation of embeddings suppresses domain-specific features in the domain-invariant embeddings. The concept of POEM shows a unique direction to enhance the domain robustness of representations that brings considerable and consistent performance gains when combined with existing DG methods. Extensive simulation results in popular DG benchmarks with the PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet datasets show that POEM indeed facilitates the category-classifying embedding to be more domain-invariant.
<div id='section'>Paperid: <span id='pid'>2042, <a href='https://arxiv.org/pdf/2305.08302.pdf' target='_blank'>https://arxiv.org/pdf/2305.08302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aboli Marathe, Sanjana Prabhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08302">t-RAIN: Robust generalization under weather-aliasing label shift attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the classical supervised learning settings, classifiers are fit with the assumption of balanced label distributions and produce remarkable results on the same. In the real world, however, these assumptions often bend and in turn adversely impact model performance. Identifying bad learners in skewed target distributions is even more challenging. Thus achieving model robustness under these "label shift" settings is an important task in autonomous perception. In this paper, we analyze the impact of label shift on the task of multi-weather classification for autonomous vehicles. We use this information as a prior to better assess pedestrian detection in adverse weather. We model the classification performance as an indicator of robustness under 4 label shift scenarios and study the behavior of multiple classes of models. We propose t-RAIN a similarity mapping technique for synthetic data augmentation using large scale generative models and evaluate the performance on DAWN dataset. This mapping boosts model test accuracy by 2.1, 4.4, 1.9, 2.7 % in no-shift, fog, snow, dust shifts respectively. We present state-of-the-art pedestrian detection results on real and synthetic weather domains with best performing 82.69 AP (snow) and 62.31 AP (fog) respectively.
<div id='section'>Paperid: <span id='pid'>2043, <a href='https://arxiv.org/pdf/2304.10805.pdf' target='_blank'>https://arxiv.org/pdf/2304.10805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>YongTaek Lim, Yewon Kim, Suho Kang, Dokyung Yoon, KyungWoo Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10805">RPLKG: Robust Prompt Learning with Knowledge Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale pre-trained models surpass in transferability and robust generalization across diverse datasets. The emergence of multimodal pre-trained models like CLIP has significantly boosted performance in various experiments. However, generalizing to new datasets or domains remains challenging, especially with limited labeled data. Also, existing methods often lack interpretability and impose high computational costs. To address this, we propose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the knowledge graph to curate diverse, interpretable prompt sets automatically. Our method autonomously selects the optimal interpretable prompt based on dataset characteristics, achieving performance improvements over zero-shot learning and competitive performance compared to various prompt learning methods. Also, RPLKG efficiently reuses cached prompt embeddings from a single model pass and optimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast training. Moreover, RPLKG advances few-shot learning effectiveness while enhancing interpretability and efficiency in model adaptation. Our
<div id='section'>Paperid: <span id='pid'>2044, <a href='https://arxiv.org/pdf/2304.09050.pdf' target='_blank'>https://arxiv.org/pdf/2304.09050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephen M. Gordon, Jonathan R. McDaniel, Kevin W. King, Vernon J. Lawhern, Jonathan Touryan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09050">Decoding Neural Activity to Assess Individual Latent State in Ecologically Valid Contexts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There exist very few ways to isolate cognitive processes, historically defined via highly controlled laboratory studies, in more ecologically valid contexts. Specifically, it remains unclear as to what extent patterns of neural activity observed under such constraints actually manifest outside the laboratory in a manner that can be used to make an accurate inference about the latent state, associated cognitive process, or proximal behavior of the individual. Improving our understanding of when and how specific patterns of neural activity manifest in ecologically valid scenarios would provide validation for laboratory-based approaches that study similar neural phenomena in isolation and meaningful insight into the latent states that occur during complex tasks. We argue that domain generalization methods from the brain-computer interface community have the potential to address this challenge. We previously used such an approach to decode phasic neural responses associated with visual target discrimination. Here, we extend that work to more tonic phenomena such as internal latent states. We use data from two highly controlled laboratory paradigms to train two separate domain-generalized models. We apply the trained models to an ecologically valid paradigm in which participants performed multiple, concurrent driving-related tasks. Using the pretrained models, we derive estimates of the underlying latent state and associated patterns of neural activity. Importantly, as the patterns of neural activity change along the axis defined by the original training data, we find changes in behavior and task performance consistent with the observations from the original, laboratory paradigms. We argue that these results lend ecological validity to those experimental designs and provide a methodology for understanding the relationship between observed neural activity and behavior during complex tasks.
<div id='section'>Paperid: <span id='pid'>2045, <a href='https://arxiv.org/pdf/2304.06309.pdf' target='_blank'>https://arxiv.org/pdf/2304.06309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyun Zhang, Lanqing Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06309">Out-of-distribution Few-shot Learning For Edge Devices without Model Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot learning (FSL) via customization of a deep learning network with limited data has emerged as a promising technique to achieve personalized user experiences on edge devices. However, existing FSL methods primarily assume independent and identically distributed (IID) data and utilize either computational backpropagation updates for each task or a common model with task-specific prototypes. Unfortunately, the former solution is infeasible for edge devices that lack on-device backpropagation capabilities, while the latter often struggles with limited generalization ability, especially for out-of-distribution (OOD) data. This paper proposes a lightweight, plug-and-play FSL module called Task-aware Normalization (TANO) that enables efficient and task-aware adaptation of a deep neural network without backpropagation. TANO covers the properties of multiple user groups by coordinating the updates of several groups of the normalization statistics during meta-training and automatically identifies the appropriate normalization group for a downstream few-shot task. Consequently, TANO provides stable but task-specific estimations of the normalization statistics to close the distribution gaps and achieve efficient model adaptation. Results on both intra-domain and out-of-domain generalization experiments demonstrate that TANO outperforms recent methods in terms of accuracy, inference speed, and model size. Moreover, TANO achieves promising results on widely-used FSL benchmarks and data from real applications.
<div id='section'>Paperid: <span id='pid'>2046, <a href='https://arxiv.org/pdf/2304.05675.pdf' target='_blank'>https://arxiv.org/pdf/2304.05675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengchao Xu, Xinmei Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05675">Semantic-Aware Mixup for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have shown exciting performance in various tasks, yet suffer generalization failures when meeting unknown target domains. One of the most promising approaches to achieve domain generalization (DG) is generating unseen data, e.g., mixup, to cover the unknown target data. However, existing works overlook the challenges induced by the simultaneous appearance of changes in both the semantic and distribution space. Accordingly, such a challenge makes source distributions hard to fit for DNNs. To mitigate the hard-fitting issue, we propose to perform a semantic-aware mixup (SAM) for domain generalization, where whether to perform mixup depends on the semantic and domain information. The feasibility of SAM shares the same spirits with the Fourier-based mixup. Namely, the Fourier phase spectrum is expected to contain semantics information (relating to labels), while the Fourier amplitude retains other information (relating to style information). Built upon the insight, SAM applies different mixup strategies to the Fourier phase spectrum and amplitude information. For instance, SAM merely performs mixup on the amplitude spectrum when both the semantic and domain information changes. Consequently, the overwhelmingly large change can be avoided. We validate the effectiveness of SAM using image classification tasks on several DG benchmarks.
<div id='section'>Paperid: <span id='pid'>2047, <a href='https://arxiv.org/pdf/2303.02801.pdf' target='_blank'>https://arxiv.org/pdf/2303.02801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roberto Santana, Ivan Hidalgo-Cenalmor, Unai Garciarena, Alexander Mendiburu, Jose Antonio Lozano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02801">Neuroevolutionary algorithms driven by neuron coverage metrics for semi-supervised classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In some machine learning applications the availability of labeled instances for supervised classification is limited while unlabeled instances are abundant. Semi-supervised learning algorithms deal with these scenarios and attempt to exploit the information contained in the unlabeled examples. In this paper, we address the question of how to evolve neural networks for semi-supervised problems. We introduce neuroevolutionary approaches that exploit unlabeled instances by using neuron coverage metrics computed on the neural network architecture encoded by each candidate solution. Neuron coverage metrics resemble code coverage metrics used to test software, but are oriented to quantify how the different neural network components are covered by test instances. In our neuroevolutionary approach, we define fitness functions that combine classification accuracy computed on labeled examples and neuron coverage metrics evaluated using unlabeled examples. We assess the impact of these functions on semi-supervised problems with a varying amount of labeled instances. Our results show that the use of neuron coverage metrics helps neuroevolution to become less sensitive to the scarcity of labeled data, and can lead in some cases to a more robust generalization of the learned classifiers.
<div id='section'>Paperid: <span id='pid'>2048, <a href='https://arxiv.org/pdf/2301.07927.pdf' target='_blank'>https://arxiv.org/pdf/2301.07927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuzhen Rao, Jun Huang, Zengming Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07927">Exploiting Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cross-domain few-shot learning, the core issue is that the model trained on source domains struggles to generalize to the target domain, especially when the domain shift is large. Motivated by the observation that the domain shift between training tasks and target tasks usually can reflect in their style variation, we propose Task Augmented Meta-Learning (TAML) to conduct style transfer-based task augmentation to improve the domain generalization ability. Firstly, Multi-task Interpolation (MTI) is introduced to fuse features from multiple tasks with different styles, which makes more diverse styles available. Furthermore, a novel task-augmentation strategy called Multi-Task Style Transfer (MTST) is proposed to perform style transfer on existing tasks to learn discriminative style-independent features. We also introduce a Feature Modulation module (FM) to add random styles and improve generalization of the model. The proposed TAML increases the diversity of styles of training tasks, and contributes to training a model with better domain generalization ability. The effectiveness is demonstrated via theoretical analysis and thorough experiments on two popular cross-domain few-shot benchmarks.
<div id='section'>Paperid: <span id='pid'>2049, <a href='https://arxiv.org/pdf/2211.15724.pdf' target='_blank'>https://arxiv.org/pdf/2211.15724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoav Wald, Gal Yona, Uri Shalit, Yair Carmon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15724">Malign Overfitting: Interpolation Can Provably Preclude Invariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learned classifiers should often possess certain invariance properties meant to encourage fairness, robustness, or out-of-distribution generalization. However, multiple recent works empirically demonstrate that common invariance-inducing regularizers are ineffective in the over-parameterized regime, in which classifiers perfectly fit (i.e. interpolate) the training data. This suggests that the phenomenon of "benign overfitting", in which models generalize well despite interpolating, might not favorably extend to settings in which robustness or fairness are desirable.
  In this work we provide a theoretical justification for these observations. We prove that -- even in the simplest of settings -- any interpolating learning rule (with arbitrarily small margin) will not satisfy these invariance properties. We then propose and analyze an algorithm that -- in the same setting -- successfully learns a non-interpolating classifier that is provably invariant. We validate our theoretical observations on simulated data and the Waterbirds dataset.
<div id='section'>Paperid: <span id='pid'>2050, <a href='https://arxiv.org/pdf/2211.13662.pdf' target='_blank'>https://arxiv.org/pdf/2211.13662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Schlagenhauf, Tim Scheurenbrand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13662">Cross-domain Transfer of defect features in technical domains based on partial target data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A common challenge in real world classification scenarios with sequentially appending target domain data is insufficient training datasets during the training phase. Therefore, conventional deep learning and transfer learning classifiers are not applicable especially when individual classes are not represented or are severely underrepresented at the outset. In many technical domains, however, it is only the defect or worn reject classes that are insufficiently represented, while the non-defect class is often available from the beginning. The proposed classification approach addresses such conditions and is based on a CNN encoder. Following a contrastive learning approach, it is trained with a modified triplet loss function using two datasets: Besides the non-defective target domain class 1st dataset, a state-of-the-art labeled source domain dataset that contains highly related classes e.g., a related manufacturing error or wear defect but originates from a highly different domain e.g., different product, material, or appearance = 2nd dataset is utilized. The approach learns the classification features from the source domain dataset while at the same time learning the differences between the source and the target domain in a single training step, aiming to transfer the relevant features to the target domain. The classifier becomes sensitive to the classification features and by architecture robust against the highly domain-specific context. The approach is benchmarked in a technical and a non-technical domain and shows convincing classification results. In particular, it is shown that the domain generalization capabilities and classification results are improved by the proposed architecture, allowing for larger domain shifts between source and target domains.
<div id='section'>Paperid: <span id='pid'>2051, <a href='https://arxiv.org/pdf/2211.12287.pdf' target='_blank'>https://arxiv.org/pdf/2211.12287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daulet Kurmantayev, Dohyun Kwun, Hyoil Kim, Sung Whan Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12287">RiSi: Spectro-temporal RAN-agnostic Modulation Identification for OFDMA Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RAN-agnostic communications can identify intrinsic features of the unknown signal without any prior knowledge, with which incompatible RANs in the same unlicensed band could achieve better coexistence performance than today's LBT-based coexistence. Blind modulation identification is its key building block, which blindly identifies the modulation type of an incompatible signal without any prior knowledge. Recent blind modulation identification schemes are built upon deep neural networks, which are limited to single-carrier signal recognition thus not pragmatic for identifying spectro-temporal OFDMA signals whose modulation varies with time and frequency. Therefore, this paper proposes RiSi, a semantic segmentation neural network designed to work on OFDMA's spectrograms, that employs flattened convolutions to better identify the grid-like pattern of OFDMA's resource blocks. We trained RiSi with a realistic OFDMA dataset including various channel impairments, and achieved the modulation identification accuracy of 86% on average over four modulation types of BPSK, QPSK, 16-QAM, 64-QAM. Then, we enhanced the generalization performance of RiSi by applying domain generalization methods while treating varying FFT size or varying CP length as different domains, showing that thus-generalized RiSi can perform reasonably well with unseen data.
<div id='section'>Paperid: <span id='pid'>2052, <a href='https://arxiv.org/pdf/2210.05152.pdf' target='_blank'>https://arxiv.org/pdf/2210.05152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Zhang, Rui Zheng, Luosang Gadeng, Pei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05152">TriangleNet: Edge Prior Augmented Network for Semantic Segmentation through Cross-Task Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the task of semantic segmentation in computer vision, aiming to achieve precise pixel-wise classification. We investigate the joint training of models for semantic edge detection and semantic segmentation, which has shown promise. However, implicit cross-task consistency learning in multi-task networks is limited. To address this, we propose a novel "decoupled cross-task consistency loss" that explicitly enhances cross-task consistency. Our semantic segmentation network, TriangleNet, achieves a substantial 2.88\% improvement over the Baseline in mean Intersection over Union (mIoU) on the Cityscapes test set. Notably, TriangleNet operates at 77.4\% mIoU/46.2 FPS on Cityscapes, showcasing real-time inference capabilities at full resolution. With multi-scale inference, performance is further enhanced to 77.8\%. Furthermore, TriangleNet consistently outperforms the Baseline on the FloodNet dataset, demonstrating its robust generalization capabilities. The proposed method underscores the significance of multi-task learning and explicit cross-task consistency enhancement for advancing semantic segmentation and highlights the potential of multitasking in real-time semantic segmentation.
<div id='section'>Paperid: <span id='pid'>2053, <a href='https://arxiv.org/pdf/2208.10024.pdf' target='_blank'>https://arxiv.org/pdf/2208.10024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gilhyun Nam, Gyeongjae Choi, Kyungmin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.10024">GCISG: Guided Causal Invariant Learning for Improved Syn-to-real Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a deep learning model with artificially generated data can be an alternative when training data are scarce, yet it suffers from poor generalization performance due to a large domain gap. In this paper, we characterize the domain gap by using a causal framework for data generation. We assume that the real and synthetic data have common content variables but different style variables. Thus, a model trained on synthetic dataset might have poor generalization as the model learns the nuisance style variables. To that end, we propose causal invariance learning which encourages the model to learn a style-invariant representation that enhances the syn-to-real generalization. Furthermore, we propose a simple yet effective feature distillation method that prevents catastrophic forgetting of semantic knowledge of the real domain. In sum, we refer to our method as Guided Causal Invariant Syn-to-real Generalization that effectively improves the performance of syn-to-real generalization. We empirically verify the validity of proposed methods, and especially, our method achieves state-of-the-art on visual syn-to-real domain generalization tasks such as image classification and semantic segmentation.
<div id='section'>Paperid: <span id='pid'>2054, <a href='https://arxiv.org/pdf/2208.01996.pdf' target='_blank'>https://arxiv.org/pdf/2208.01996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhang, Ying-Cong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.01996">Adaptive Domain Generalization via Online Disagreement Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samples into a domain-invariant space, and the multiple classifiers capture the distinct decision boundaries that each of them relates to a specific source domain. During testing, distribution differences between target and source domains could be effectively measured by leveraging prediction disagreement among source classifiers. By fine-tuning source models to minimize the disagreement at test time, target domain features are well aligned to the invariant feature space. We verify AdaODM on two popular DG methods, namely ERM and CORAL, and four DG benchmarks, namely VLCS, PACS, OfficeHome, and TerraIncognita. The results show AdaODM stably improves the generalization capacity on unseen domains and achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>2055, <a href='https://arxiv.org/pdf/2203.17067.pdf' target='_blank'>https://arxiv.org/pdf/2203.17067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Dai, Yingqiao Lin, Fan Li, Xiyao Li, Donglin Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.17067">CADG: A Model Based on Cross Attention for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Domain Generalization (DG) tasks, models are trained by using only training data from the source domains to achieve generalization on an unseen target domain, this will suffer from the distribution shift problem. So it's important to learn a classifier to focus on the common representation which can be used to classify on multi-domains, so that this classifier can achieve a high performance on an unseen target domain as well. With the success of cross attention in various cross-modal tasks, we find that cross attention is a powerful mechanism to align the features come from different distributions. So we design a model named CADG (cross attention for domain generalization), wherein cross attention plays a important role, to address distribution shift problem. Such design makes the classifier can be adopted on multi-domains, so the classifier will generalize well on an unseen domain. Experiments show that our proposed method achieves state-of-the-art performance on a variety of domain generalization benchmarks compared with other single model and can even achieve a better performance than some ensemble-based methods.
<div id='section'>Paperid: <span id='pid'>2056, <a href='https://arxiv.org/pdf/2201.12143.pdf' target='_blank'>https://arxiv.org/pdf/2201.12143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amit Dhurandhar, Karthikeyan Ramamurthy, Kartik Ahuja, Vijay Arya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.12143">Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed, few provide a simple way to produce high fidelity explanations that are also stable and intuitive. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle -- originally proposed for (global) out-of-distribution generalization -- to provide such high fidelity explanations that are also stable and unidirectional across nearby examples. Our method is based on a game theoretic formulation where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a behavior which can be highly desirable for recourse. Empirically, we show on tabular, image and text data that the quality of our explanations with neighborhoods formed using random perturbations are much better than LIME and in some cases even comparable to other methods that use realistic neighbors sampled from the data manifold. This is desirable given that learning a manifold to either create realistic neighbors or to project explanations is typically expensive or may even be impossible. Moreover, our algorithm is simple and efficient to train, and can ascertain stable input features for local decisions of a black-box without access to side information such as a (partial) causal graph as has been seen in some recent works.
<div id='section'>Paperid: <span id='pid'>2057, <a href='https://arxiv.org/pdf/2201.05149.pdf' target='_blank'>https://arxiv.org/pdf/2201.05149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamed Hassani, Adel Javanmard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.05149">The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful deep learning models often involve training neural network architectures that contain more parameters than the number of training samples. Such overparametrized models have been extensively studied in recent years, and the virtues of overparametrization have been established from both the statistical perspective, via the double-descent phenomenon, and the computational perspective via the structural properties of the optimization landscape.
  Despite the remarkable success of deep learning architectures in the overparametrized regime, it is also well known that these models are highly vulnerable to small adversarial perturbations in their inputs. Even when adversarially trained, their performance on perturbed inputs (robust generalization) is considerably worse than their best attainable performance on benign inputs (standard generalization). It is thus imperative to understand how overparametrization fundamentally affects robustness.
  In this paper, we will provide a precise characterization of the role of overparametrization on robustness by focusing on random features regression models (two-layer neural networks with random first layer weights). We consider a regime where the sample size, the input dimension and the number of parameters grow in proportion to each other, and derive an asymptotically exact formula for the robust generalization error when the model is adversarially trained. Our developed theory reveals the nontrivial effect of overparametrization on robustness and indicates that for adversarially trained random features models, high overparametrization can hurt robust generalization.
<div id='section'>Paperid: <span id='pid'>2058, <a href='https://arxiv.org/pdf/2108.11684.pdf' target='_blank'>https://arxiv.org/pdf/2108.11684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stathi Fotiadis, Mario Lino, Shunlong Hu, Stef Garasto, Chris D Cantwell, Anil Anthony Bharath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.11684">Disentangled Generative Models for Robust Prediction of System Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks have become increasingly of interest in dynamical system prediction, but out-of-distribution generalization and long-term stability still remains challenging. In this work, we treat the domain parameters of dynamical systems as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement and causal factorization, we aim to separate the domain parameters from the dynamics in the latent space of generative models. In our experiments we model dynamics both in phase space and in video sequences and conduct rigorous OOD evaluations. Results indicate that disentangled VAEs adapt better to domain parameters spaces that were not present in the training data. At the same time, disentanglement can improve the long-term and out-of-distribution predictions of state-of-the-art models in video sequences.
<div id='section'>Paperid: <span id='pid'>2059, <a href='https://arxiv.org/pdf/2107.05729.pdf' target='_blank'>https://arxiv.org/pdf/2107.05729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Fei, Xaq Pitkow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.05729">Generalization of graph network inferences in higher-order graphical models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probabilistic graphical models provide a powerful tool to describe complex statistical structure, with many real-world applications in science and engineering from controlling robotic arms to understanding neuronal computations. A major challenge for these graphical models is that inferences such as marginalization are intractable for general graphs. These inferences are often approximated by a distributed message-passing algorithm such as Belief Propagation, which does not always perform well on graphs with cycles, nor can it always be easily specified for complex continuous probability distributions. Such difficulties arise frequently in expressive graphical models that include intractable higher-order interactions. In this paper we define the Recurrent Factor Graph Neural Network (RF-GNN) to achieve fast approximate inference on graphical models that involve many-variable interactions. Experimental results on several families of graphical models demonstrate the out-of-distribution generalization capability of our method to different sized graphs, and indicate the domain in which our method outperforms Belief Propagation (BP). Moreover, we test the RF-GNN on a real-world Low-Density Parity-Check dataset as a benchmark along with other baseline models including BP variants and other GNN methods. Overall we find that RF-GNNs outperform other methods under high noise levels.
<div id='section'>Paperid: <span id='pid'>2060, <a href='https://arxiv.org/pdf/2012.02920.pdf' target='_blank'>https://arxiv.org/pdf/2012.02920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gowoon Cheon, Lusann Yang, Kevin McCloskey, Evan J. Reed, Ekin D. Cubuk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2012.02920">Dataset of Random Relaxations for Crystal Structure Search of Li-Si System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crystal structure search is a long-standing challenge in materials design. We present a dataset of more than 100,000 structural relaxations of potential battery anode materials from randomized structures using density functional theory calculations. We illustrate the usage of the dataset by training graph neural networks to predict structural relaxations from randomly generated structures. Our models directly predict stresses in addition to forces, which allows them to accurately simulate relaxations of both ionic positions and lattice vectors. We show that models trained on the molecular dynamics simulations fail to simulate relaxations from random structures, while training on our data leads to up to two orders of magnitude decrease in error for the same task. Our model is able to find an experimentally verified structure of a stoichiometry held out from training. We find that randomly perturbing atomic positions during training improves both the accuracy and out of domain generalization of the models.
<div id='section'>Paperid: <span id='pid'>2061, <a href='https://arxiv.org/pdf/2510.07350.pdf' target='_blank'>https://arxiv.org/pdf/2510.07350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Chakravarty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07350">Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate change is increasingly disrupting agricultural systems, making accurate crop yield forecasting essential for food security. While deep learning models have shown promise in yield prediction using satellite and weather data, their ability to generalize across geographic regions and years - critical for real-world deployment - remains largely untested. We benchmark two state-of-the-art models, GNN-RNN and MMST-ViT, under realistic out-of-distribution (OOD) conditions using the large-scale CropNet dataset spanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out cross-validation across seven USDA Farm Resource Regions and year-ahead prediction scenarios, we identify substantial variability in cross-region transferability. GNN-RNN demonstrates superior generalization with positive correlations under geographic shifts, while MMST-ViT performs well in-domain but degrades sharply under OOD conditions. Regions like Heartland and Northern Great Plains show stable transfer dynamics (RMSE less than 10 bu/acre for soybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE greater than 20 bu/acre) across both models and crops, revealing structural dissimilarities likely driven by semi-arid climate, irrigation patterns, and incomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves 135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more viable for sustainable deployment. Our findings underscore that spatial-temporal alignment - not merely model complexity or data scale - is key to robust generalization, and highlight the need for transparent OOD evaluation protocols to ensure equitable and reliable climate-aware agricultural forecasting.
<div id='section'>Paperid: <span id='pid'>2062, <a href='https://arxiv.org/pdf/2510.02114.pdf' target='_blank'>https://arxiv.org/pdf/2510.02114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ding-Ruei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02114">FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federeated Learning (FL) offers a privacy-preserving solution for Semantic Segmentation (SS) tasks to adapt to new domains, but faces significant challenges from these domain shifts, particularly when client data is unlabeled. However, most existing FL methods unrealistically assume access to labeled data on remote clients or fail to leverage the power of modern Vision Foundation Models (VFMs). Here, we propose a novel and challenging task, FFREEDG, in which a model is pretrained on a server's labeled source dataset and subsequently trained across clients using only their unlabeled data, without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a framework that leverages the knowledge of a VFM by integrating vision and language modalities. Our approach employs a Vision-Language decoder guided by CLIP-based text embeddings to improve semantic disambiguation and uses a weak-to-strong consistency learning strategy for robust local training on pseudo-labels. Our experiments on synthetic-to-real and clear-to-adverse-weather benchmarks demonstrate that our framework effectively tackles this new task, achieving competitive performance against established domain generalization and adaptation methods and setting a strong baseline for future research.
<div id='section'>Paperid: <span id='pid'>2063, <a href='https://arxiv.org/pdf/2509.25241.pdf' target='_blank'>https://arxiv.org/pdf/2509.25241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25241">Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in training paradigms for Large Language Models (LLMs) have unlocked their remarkable capabilities in natural language processing and cross-domain generalization. While LLMs excel in tasks like programming and mathematical problem-solving, their zero-shot performance in specialized domains requiring expert knowledge, such as cybersecurity, is often suboptimal. This limitation arises because foundational LLMs are designed for general-purpose applications, constraining their ability to encapsulate domain-specific expertise within their parameter space. To address this, we explore fine-tuning strategies to embed cybersecurity knowledge into LLMs, enhancing their performance in cybersecurity question-answering (Q\&A) tasks while prioritizing computational efficiency. Specifically, we investigate Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized Low-Rank Adaptation (QLoRA) using a cybersecurity Q\&A dataset. Our results demonstrate that these fine-tuning approaches significantly outperform the foundational model in cybersecurity Q\&A tasks. Moreover, LoRA and QLoRA achieve comparable performance to SFT with substantially lower computational costs, offering an efficient pathway for adapting LLMs to specialized domains. Our work highlights the potential of low-rank fine-tuning strategies to bridge the gap between general-purpose LLMs and domain-specific applications.
<div id='section'>Paperid: <span id='pid'>2064, <a href='https://arxiv.org/pdf/2509.19100.pdf' target='_blank'>https://arxiv.org/pdf/2509.19100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Robey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19100">Algorithms for Adversarially Robust Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given the widespread use of deep learning models in safety-critical applications, ensuring that the decisions of such models are robust against adversarial exploitation is of fundamental importance. In this thesis, we discuss recent progress toward designing algorithms that exhibit desirable robustness properties. First, we discuss the problem of adversarial examples in computer vision, for which we introduce new technical results, training paradigms, and certification algorithms. Next, we consider the problem of domain generalization, wherein the task is to train neural networks to generalize from a family of training distributions to unseen test distributions. We present new algorithms that achieve state-of-the-art generalization in medical imaging, molecular identification, and image classification. Finally, we study the setting of jailbreaking large language models (LLMs), wherein an adversarial user attempts to design prompts that elicit objectionable content from an LLM. We propose new attacks and defenses, which represent the frontier of progress toward designing robust language-based agents.
<div id='section'>Paperid: <span id='pid'>2065, <a href='https://arxiv.org/pdf/2509.12081.pdf' target='_blank'>https://arxiv.org/pdf/2509.12081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12081">Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes deception as a mechanism for out-of-distribution (OOD) generalization: by learning data representations that make training data appear independent and identically distributed (iid) to an observer, we can identify stable features that eliminate spurious correlations and generalize to unseen domains. We refer to this principle as deceptive risk minimization (DRM) and instantiate it with a practical differentiable objective that simultaneously learns features that eliminate distribution shifts from the perspective of a detector based on conformal martingales while minimizing a task-specific loss. In contrast to domain adaptation or prior invariant representation learning methods, DRM does not require access to test data or a partitioning of training data into a finite number of data-generating domains. We demonstrate the efficacy of DRM on numerical experiments with concept shift and a simulated imitation learning setting with covariate shift in environments that a robot is deployed in.
<div id='section'>Paperid: <span id='pid'>2066, <a href='https://arxiv.org/pdf/2509.03017.pdf' target='_blank'>https://arxiv.org/pdf/2509.03017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryandhimas E. Zezario
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03017">Non-Intrusive Intelligibility Prediction for Hearing Aids: Recent Advances, Trends, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides an overview of recent progress in non-intrusive speech intelligibility prediction for hearing aids (HA). We summarize developments in robust acoustic feature extraction, hearing loss modeling, and the use of emerging architectures for long-sequence processing. Listener-specific adaptation strategies and domain generalization approaches that aim to improve robustness in unseen acoustic environments are also discussed. Remaining challenges, such as the need for large-scale, diverse datasets and reliable cross-profile generalization, are acknowledged. Our goal is to offer a perspective on current trends, ongoing challenges, and possible future directions toward practical and reliable HA-oriented intelligibility prediction systems.
<div id='section'>Paperid: <span id='pid'>2067, <a href='https://arxiv.org/pdf/2509.00476.pdf' target='_blank'>https://arxiv.org/pdf/2509.00476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Khalid Ali Mohamed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00476">Cross-Domain Malware Detection via Probability-Level Fusion of Lightweight Gradient Boosting Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The escalating sophistication of malware necessitates robust detection mechanisms that generalize across diverse data sources. Traditional single-dataset models struggle with cross-domain generalization and often incur high computational costs. This paper presents a novel, lightweight framework for malware detection that employs probability-level fusion across three distinct datasets: EMBER (static features), API Call Sequences (behavioral features), and CIC Obfuscated Memory (memory patterns). Our method trains individual LightGBM classifiers on each dataset, selects top predictive features to ensure efficiency, and fuses their prediction probabilities using optimized weights determined via grid search. Extensive experiments demonstrate that our fusion approach achieves a macro F1-score of 0.823 on a cross-domain validation set, significantly outperforming individual models and providing superior generalization. The framework maintains low computational overhead, making it suitable for real-time deployment, and all code and data are provided for full reproducibility.
<div id='section'>Paperid: <span id='pid'>2068, <a href='https://arxiv.org/pdf/2508.08298.pdf' target='_blank'>https://arxiv.org/pdf/2508.08298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathan Breslow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08298">Channel-Wise MLPs Improve the Generalization of Recurrent Convolutional Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the impact of channel-wise mixing via multi-layer perceptrons (MLPs) on the generalization capabilities of recurrent convolutional networks. Specifically, we compare two architectures: DARC (Depth Aware Recurrent Convolution), which employs a simple recurrent convolutional structure, and DAMP (Depth Aware Multi-layer Perceptron), which extends DARC with a gated MLP for channel mixing. Using the Re-ARC benchmark, we find that DAMP significantly outperforms DARC in both in-distribution and out-of-distribution generalization under exact-match grading criteria. These results suggest that explicit channel mixing through MLPs enables recurrent convolutional networks to learn more robust and generalizable computational patterns. Our findings have implications for neural program synthesis and highlight the potential of DAMP as a target architecture for hypernetwork approaches.
<div id='section'>Paperid: <span id='pid'>2069, <a href='https://arxiv.org/pdf/2508.01948.pdf' target='_blank'>https://arxiv.org/pdf/2508.01948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01948">Navigating High Dimensional Concept Space with Metalearning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapidly learning abstract concepts from limited examples is a hallmark of human intelligence. This work investigates whether gradient-based meta-learning can equip neural networks with inductive biases for efficient few-shot acquisition of discrete concepts. I compare meta-learning methods against a supervised learning baseline on Boolean concepts (logical statements) generated by a probabilistic context-free grammar (PCFG). By systematically varying concept dimensionality (number of features) and recursive compositionality (depth of grammar recursion), I delineate between complexity regimes in which meta-learning robustly improves few-shot concept learning and regimes in which it does not. Meta-learners are much better able to handle compositional complexity than featural complexity. I highlight some reasons for this with a representational analysis of the weights of meta-learners and a loss landscape analysis demonstrating how featural complexity increases the roughness of loss trajectories, allowing curvature-aware optimization to be more effective than first-order methods. I find improvements in out-of-distribution generalization on complex concepts by increasing the number of adaptation steps in meta-SGD, where adaptation acts as a way of encouraging exploration of rougher loss basins. Overall, this work highlights the intricacies of learning compositional versus featural complexity in high dimensional concept spaces and provides a road to understanding the role of 2nd order methods and extended gradient adaptation in few-shot concept learning.
<div id='section'>Paperid: <span id='pid'>2070, <a href='https://arxiv.org/pdf/2507.15877.pdf' target='_blank'>https://arxiv.org/pdf/2507.15877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Ouellette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15877">Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We run a controlled compositional generalization experiment in the ARC-AGI domain: an open-world problem domain in which the ability to generalize out-of-distribution is, by design, an essential characteristic for success. We compare neural program synthesis and test-time fine-tuning approaches on this experiment. We find that execution-guided neural program synthesis outperforms all reference algorithms in its ability to compose novel solutions. Our empirical findings also suggest that the success of TTFT on ARC-AGI lies mainly in eliciting in-distribution knowledge that the LLM otherwise fails to rely on directly.
<div id='section'>Paperid: <span id='pid'>2071, <a href='https://arxiv.org/pdf/2506.06024.pdf' target='_blank'>https://arxiv.org/pdf/2506.06024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deborah Pereg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06024">On Inverse Problems, Parameter Estimation, and Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Signal restoration and inverse problems are key elements in most real-world data science applications. In the past decades, with the emergence of machine learning methods, inversion of measurements has become a popular step in almost all physical applications, which is normally executed prior to downstream tasks that often involve parameter estimation. In this work, we analyze the general problem of parameter estimation in an inverse problem setting. First, we address the domain-shift problem by re-formulating it in direct relation with the discrete parameter estimation analysis. We analyze a significant vulnerability in current attempts to enforce domain generalization, which we dubbed the Double Meaning Theorem. Our theoretical findings are experimentally illustrated for domain shift examples in image deblurring and speckle suppression in medical imaging. We then proceed to a theoretical analysis of parameter estimation given observed measurements before and after data processing involving an inversion of the observations. We compare this setting for invertible and non-invertible (degradation) processes. We distinguish between continuous and discrete parameter estimation, corresponding with regression and classification problems, respectively. Our theoretical findings align with the well-known information-theoretic data processing inequality, and to a certain degree question the common misconception that data-processing for inversion, based on modern generative models that may often produce outstanding perceptual quality, will necessarily improve the following parameter estimation objective. It is our hope that this paper will provide practitioners with deeper insights that may be leveraged in the future for the development of more efficient and informed strategic system planning, critical in safety-sensitive applications.
<div id='section'>Paperid: <span id='pid'>2072, <a href='https://arxiv.org/pdf/2505.10152.pdf' target='_blank'>https://arxiv.org/pdf/2505.10152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10152">Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization aims to learn a generalizable model from multiple decentralized source domains for deploying on the unseen target domain. The style augmentation methods have achieved great progress on domain generalization. However, the existing style augmentation methods either explore the data styles within isolated source domain or interpolate the style information across existing source domains under the data decentralization scenario, which leads to limited style space. To address this issue, we propose a Multi-source Collaborative Style Augmentation and Domain-invariant learning method (MCSAD) for federated domain generalization. Specifically, we propose a multi-source collaborative style augmentation module to generate data in the broader style space. Furthermore, we conduct domain-invariant learning between the original data and augmented data by cross-domain feature alignment within the same class and classes relation ensemble distillation between different classes to learn a domain-invariant model. By alternatively conducting collaborative style augmentation and domain-invariant learning, the model can generalize well on unseen target domain. Extensive experiments on multiple domain generalization datasets indicate that our method significantly outperforms the state-of-the-art federated domain generalization methods.
<div id='section'>Paperid: <span id='pid'>2073, <a href='https://arxiv.org/pdf/2504.12652.pdf' target='_blank'>https://arxiv.org/pdf/2504.12652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Sanaullah Chowdhury Lameya Sabrin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12652">AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and Scalable Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces AdaptoVision, a novel convolutional neural network (CNN) architecture designed to efficiently balance computational complexity and classification accuracy. By leveraging enhanced residual units, depth-wise separable convolutions, and hierarchical skip connections, AdaptoVision significantly reduces parameter count and computational requirements while preserving competitive performance across various benchmark and medical image datasets. Extensive experimentation demonstrates that AdaptoVision achieves state-of-the-art on BreakHis dataset and comparable accuracy levels, notably 95.3\% on CIFAR-10 and 85.77\% on CIFAR-100, without relying on any pretrained weights. The model's streamlined architecture and strategic simplifications promote effective feature extraction and robust generalization, making it particularly suitable for deployment in real-time and resource-constrained environments.
<div id='section'>Paperid: <span id='pid'>2074, <a href='https://arxiv.org/pdf/2504.02898.pdf' target='_blank'>https://arxiv.org/pdf/2504.02898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lele Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02898">A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.
<div id='section'>Paperid: <span id='pid'>2075, <a href='https://arxiv.org/pdf/2504.02898.pdf' target='_blank'>https://arxiv.org/pdf/2504.02898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lele Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02898">A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.
<div id='section'>Paperid: <span id='pid'>2076, <a href='https://arxiv.org/pdf/2503.19929.pdf' target='_blank'>https://arxiv.org/pdf/2503.19929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinhao Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19929">Robust Object Detection of Underwater Robot based on Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection aims to obtain the location and the category of specific objects in a given image, which includes two tasks: classification and location. In recent years, researchers tend to apply object detection to underwater robots equipped with vision systems to complete tasks including seafood fishing, fish farming, biodiversity monitoring and so on. However, the diversity and complexity of underwater environments bring new challenges to object detection. First, aquatic organisms tend to live together, which leads to severe occlusion. Second, theaquatic organisms are good at hiding themselves, which have a similar color to the background. Third, the various water quality and changeable and extreme lighting conditions lead to the distorted, low contrast, blue or green images obtained by the underwater camera, resulting in domain shift. And the deep model is generally vulnerable to facing domain shift. Fourth, the movement of the underwater robot leads to the blur of the captured image and makes the water muddy, which results in low visibility of the water. This paper investigates the problems brought by the underwater environment mentioned above, and aims to design a high-performance and robust underwater object detector.
<div id='section'>Paperid: <span id='pid'>2077, <a href='https://arxiv.org/pdf/2503.19564.pdf' target='_blank'>https://arxiv.org/pdf/2503.19564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sree Bhargavi Balija
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19564">FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As artificial intelligence systems increasingly operate in Real-world environments, the integration of multi-modal data sources such as vision, language, and audio presents both unprecedented opportunities and critical challenges for achieving trustworthy intelligence. In this paper, we propose a novel framework that unifies federated learning with explainable multi-modal reasoning to ensure trustworthiness in decentralized, dynamic settings. Our approach, called FedMM-X (Federated Multi-Modal Explainable Intelligence), leverages cross-modal consistency checks, client-level interpretability mechanisms, and dynamic trust calibration to address challenges posed by data heterogeneity, modality imbalance, and out-of-distribution generalization. Through rigorous evaluation across federated multi-modal benchmarks involving vision-language tasks, we demonstrate improved performance in both accuracy and interpretability while reducing vulnerabilities to adversarial and spurious correlations. Further, we introduce a novel trust score aggregation method to quantify global model reliability under dynamic client participation. Our findings pave the way toward developing robust, interpretable, and socially responsible AI systems in Real-world environments.
<div id='section'>Paperid: <span id='pid'>2078, <a href='https://arxiv.org/pdf/2503.06436.pdf' target='_blank'>https://arxiv.org/pdf/2503.06436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06436">Physics-Informed Residual Neural Ordinary Differential Equations for Enhanced Tropical Cyclone Intensity Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tropical cyclone (TC) intensity prediction is crucial for mitigating storm hazards, yet its complex dynamics pose challenges to traditional methods. Here, we introduce a Physics-Informed Residual Neural Ordinary Differential Equation (PIR-NODE) model to precisely forecast TC intensity evolution. This model leverages the powerful non-linear fitting capabilities of deep learning, integrates residual connections to enhance model depth and training stability, and explicitly models the continuous temporal evolution of TC intensity using Neural ODEs. Experimental results in the SHIPS dataset demonstrate that the PIR-NODE model achieves a significant improvement in 24-hour intensity prediction accuracy compared to traditional statistical models and benchmark deep learning methods, with a 25. 2\% reduction in the root mean square error (RMSE) and a 19.5\% increase in R-square (R2) relative to a baseline of neural network. Crucially, the residual structure effectively preserves initial state information, and the model exhibits robust generalization capabilities. This study details the PIR-NODE model architecture, physics-informed integration strategies, and comprehensive experimental validation, revealing the substantial potential of deep learning techniques in predicting complex geophysical systems and laying the foundation for future refined TC forecasting research.
<div id='section'>Paperid: <span id='pid'>2079, <a href='https://arxiv.org/pdf/2501.11841.pdf' target='_blank'>https://arxiv.org/pdf/2501.11841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiuling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11841">Survey on Monocular Metric Depth Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular Depth Estimation (MDE) enables spatial understanding, 3D reconstruction, and autonomous navigation, yet deep learning approaches often predict only relative depth without a consistent metric scale. This limitation reduces reliability in applications such as visual SLAM, precise 3D modeling, and view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this challenge by producing depth maps with absolute scale, ensuring geometric consistency and enabling deployment without additional calibration. This survey reviews the evolution of MMDE, from geometry-based methods to state-of-the-art deep models, with emphasis on the datasets that drive progress. Key benchmarks, including KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of modality, scene type, and application domain. Methodological advances are analyzed, covering domain generalization, boundary preservation, and the integration of synthetic and real data. Techniques such as unsupervised and semi-supervised learning, patch-based inference, architectural innovations, and generative modeling are evaluated for their strengths and limitations. By synthesizing current progress, highlighting the importance of high-quality datasets, and identifying open challenges, this survey provides a structured reference for advancing MMDE and supporting its adoption in real-world computer vision systems.
<div id='section'>Paperid: <span id='pid'>2080, <a href='https://arxiv.org/pdf/2501.08361.pdf' target='_blank'>https://arxiv.org/pdf/2501.08361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijian Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08361">Weight Averaging for Out-of-Distribution Generalization and Few-Shot Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empirical risk minimization (ERM) is not robust to changes in the distribution of data. When the distribution of test data is different from that of training data, the problem is known as out-of-distribution generalization. Recently, two techniques have been developed for addressing out-of-distribution generalization in computer vision: weight averaging (WA) and sharpness-aware minimization (SAM). WA involves training multiple models with different hyperparameters and then averaging the weights of these models, which can significantly improve out-of-distribution generalization performance. SAM optimizes a neural network to find minima in flat regions, which have been proven to perform well under distribution shifts. While these techniques have made great progress, there is still room for improvement and further exploration. In this thesis, we propose increasing the model diversity in WA explicitly by introducing gradient similarity as a loss regularizer to further improve out-of-distribution generalization performance. We also propose combining WA and SAM to solve the problem of few-shot domain adaptation. Our extensive experiments on digits datasets (MNIST, SVHN, USPS, MNIST-M) and other domain adaptation datasets (VLCS, PACS) show that combining WA and SAM leads to improved out-of-distribution generalization performance and significantly increases few-shot domain adaptation accuracy.
<div id='section'>Paperid: <span id='pid'>2081, <a href='https://arxiv.org/pdf/2412.14211.pdf' target='_blank'>https://arxiv.org/pdf/2412.14211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aroj Subedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14211">Improving Generalization Performance of YOLOv8 for Camera Trap Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camera traps have become integral tools in wildlife conservation, providing non-intrusive means to monitor and study wildlife in their natural habitats. The utilization of object detection algorithms to automate species identification from Camera Trap images is of huge importance for research and conservation purposes. However, the generalization issue, where the trained model is unable to apply its learnings to a never-before-seen dataset, is prevalent. This thesis explores the enhancements made to the YOLOv8 object detection algorithm to address the problem of generalization. The study delves into the limitations of the baseline YOLOv8 model, emphasizing its struggles with generalization in real-world environments. To overcome these limitations, enhancements are proposed, including the incorporation of a Global Attention Mechanism (GAM) module, modified multi-scale feature fusion, and Wise Intersection over Union (WIoUv3) as a bounding box regression loss function. A thorough evaluation and ablation experiments reveal the improved model's ability to suppress the background noise, focus on object properties, and exhibit robust generalization in novel environments. The proposed enhancements not only address the challenges inherent in camera trap datasets but also pave the way for broader applicability in real-world conservation scenarios, ultimately aiding in the effective management of wildlife populations and habitats.
<div id='section'>Paperid: <span id='pid'>2082, <a href='https://arxiv.org/pdf/2412.09439.pdf' target='_blank'>https://arxiv.org/pdf/2412.09439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh-Dat Truong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09439">Towards Robust and Fair Vision Learning in Open-World Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The dissertation presents four key contributions toward fairness and robustness in vision learning. First, to address the problem of large-scale data requirements, the dissertation presents a novel Fairness Domain Adaptation approach derived from two major novel research findings of Bijective Maximum Likelihood and Fairness Adaptation Learning. Second, to enable the capability of open-world modeling of vision learning, this dissertation presents a novel Open-world Fairness Continual Learning Framework. The success of this research direction is the result of two research lines, i.e., Fairness Continual Learning and Open-world Continual Learning. Third, since visual data are often captured from multiple camera views, robust vision learning methods should be capable of modeling invariant features across views. To achieve this desired goal, the research in this thesis will present a novel Geometry-based Cross-view Adaptation framework to learn robust feature representations across views. Finally, with the recent increase in large-scale videos and multimodal data, understanding the feature representations and improving the robustness of large-scale visual foundation models is critical. Therefore, this thesis will present novel Transformer-based approaches to improve the robust feature representations against multimodal and temporal data. Then, a novel Domain Generalization Approach will be presented to improve the robustness of visual foundation models. The research's theoretical analysis and experimental results have shown the effectiveness of the proposed approaches, demonstrating their superior performance compared to prior studies. The contributions in this dissertation have advanced the fairness and robustness of machine vision learning.
<div id='section'>Paperid: <span id='pid'>2083, <a href='https://arxiv.org/pdf/2411.07556.pdf' target='_blank'>https://arxiv.org/pdf/2411.07556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07556">Multi-task Feature Enhancement Network for No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the scarcity of labeled samples in Image Quality Assessment (IQA) datasets, numerous recent studies have proposed multi-task based strategies, which explore feature information from other tasks or domains to boost the IQA task. Nevertheless, multi-task strategies based No-Reference Image Quality Assessment (NR-IQA) methods encounter several challenges. First, existing methods have not explicitly exploited texture details, which significantly influence the image quality. Second, multi-task methods conventionally integrate features through simple operations such as addition or concatenation, thereby diminishing the network's capacity to accurately represent distorted features. To tackle these challenges, we introduce a novel multi-task NR-IQA framework. Our framework consists of three key components: a high-frequency extraction network, a quality estimation network, and a distortion-aware network. The high-frequency extraction network is designed to guide the model's focus towards high-frequency information, which is highly related to the texture details. Meanwhile, the distortion-aware network extracts distortion-related features to distinguish different distortion types. To effectively integrate features from different tasks, a feature fusion module is developed based on an attention mechanism. Empirical results from five standard IQA databases confirm that our method not only achieves high performance but also exhibits robust generalization ability.
<div id='section'>Paperid: <span id='pid'>2084, <a href='https://arxiv.org/pdf/2411.04892.pdf' target='_blank'>https://arxiv.org/pdf/2411.04892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04892">In the Era of Prompt Learning with Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale foundation models like CLIP have shown strong zero-shot generalization but struggle with domain shifts, limiting their adaptability. In our work, we introduce \textsc{StyLIP}, a novel domain-agnostic prompt learning strategy for Domain Generalization (DG). StyLIP disentangles visual style and content in CLIP`s vision encoder by using style projectors to learn domain-specific prompt tokens and combining them with content features. Trained contrastively, this approach enables seamless adaptation across domains, outperforming state-of-the-art methods on multiple DG benchmarks. Additionally, we propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s frozen vision backbone to learn domain-invariant prompts through image style and content features. By aligning domains in embedding space with entropy minimization, AD-CLIP effectively handles domain shifts, even when only target domain samples are available. Lastly, we outline future work on class discovery using prompt learning for semantic segmentation in remote sensing, focusing on identifying novel or rare classes in unstructured environments. This paves the way for more adaptive and generalizable models in complex, real-world scenarios.
<div id='section'>Paperid: <span id='pid'>2085, <a href='https://arxiv.org/pdf/2410.21313.pdf' target='_blank'>https://arxiv.org/pdf/2410.21313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyue Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21313">Towards Robust Out-of-Distribution Generalization: Data Augmentation and Neural Architecture Search Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has been demonstrated with tremendous success in recent years. Despite so, its performance in practice often degenerates drastically when encountering out-of-distribution (OoD) data, i.e. training and test data are sampled from different distributions. In this thesis, we study ways toward robust OoD generalization for deep learning, i.e., its performance is not susceptible to distribution shift in the test data.
  We first propose a novel and effective approach to disentangle the spurious correlation between features that are not essential for recognition. It employs decomposed feature representation by orthogonalizing the two gradients of losses for category and context branches. Furthermore, we perform gradient-based augmentation on context-related features (e.g., styles, backgrounds, or scenes of target objects) to improve the robustness of learned representations. Results show that our approach generalizes well for different distribution shifts.
  We then study the problem of strengthening neural architecture search in OoD scenarios. We propose to optimize the architecture parameters that minimize the validation loss on synthetic OoD data, under the condition that corresponding network parameters minimize the training loss. Moreover, to obtain a proper validation set, we learn a conditional generator by maximizing their losses computed by different neural architectures. Results show that our approach effectively discovers robust architectures that perform well for OoD generalization.
<div id='section'>Paperid: <span id='pid'>2086, <a href='https://arxiv.org/pdf/2410.07124.pdf' target='_blank'>https://arxiv.org/pdf/2410.07124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrian Galdran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07124">Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This short abstract describes a solution to the COSAS 2024 competition on Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation from histopathological image patches. The main challenge in the task of segmenting this type of cancer is a noticeable domain shift encountered when changing acquisition devices (microscopes) and also when tissue comes from different organs. The two tasks proposed in COSAS were to train on a dataset of images from three different organs, and then predict segmentations on data from unseen organs (dataset T1), and to train on a dataset of images acquired on three different scanners and then segment images acquired with another unseen microscope. We attempted to bridge the domain shift gap by experimenting with three different strategies: standard training for each dataset, pretraining on dataset T1 and then fine-tuning on dataset T2 (and vice-versa, a strategy we call \textit{Cross-Task Pretraining}), and training on the combination of dataset A and B. Our experiments showed that Cross-Task Pre-training is a more promising approach to domain generalization.
<div id='section'>Paperid: <span id='pid'>2087, <a href='https://arxiv.org/pdf/2409.09858.pdf' target='_blank'>https://arxiv.org/pdf/2409.09858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09858">A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph machine learning (GML) has been successfully applied across a wide range of tasks. Nonetheless, GML faces significant challenges in generalizing over out-of-distribution (OOD) data, which raises concerns about its wider applicability. Recent advancements have underscored the crucial role of causality-driven approaches in overcoming these generalization challenges. Distinct from traditional GML methods that primarily rely on statistical dependencies, causality-focused strategies delve into the underlying causal mechanisms of data generation and model prediction, thus significantly improving the generalization of GML across different environments. This paper offers a thorough review of recent progress in causality-involved GML generalization. We elucidate the fundamental concepts of employing causality to enhance graph model generalization and categorize the various approaches, providing detailed descriptions of their methodologies and the connections among them. Furthermore, we explore the incorporation of causality in other related important areas of trustworthy GML, such as explanation, fairness, and robustness. Concluding with a discussion on potential future research directions, this review seeks to articulate the continuing development and future potential of causality in enhancing the trustworthiness of graph machine learning.
<div id='section'>Paperid: <span id='pid'>2088, <a href='https://arxiv.org/pdf/2408.04114.pdf' target='_blank'>https://arxiv.org/pdf/2408.04114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raunak Agarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04114">Zero-shot Factual Consistency Evaluation Across Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the challenge of factual consistency in text generation systems. We unify the tasks of Natural Language Inference, Summarization Evaluation, Factuality Verification and Factual Consistency Evaluation to train models capable of evaluating the factual consistency of source-target pairs across diverse domains. We rigorously evaluate these against eight baselines on a comprehensive benchmark suite comprising 22 datasets that span various tasks, domains, and document lengths. Results demonstrate that our method achieves state-of-the-art performance on this heterogeneous benchmark while addressing efficiency concerns and attaining cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>2089, <a href='https://arxiv.org/pdf/2406.17828.pdf' target='_blank'>https://arxiv.org/pdf/2406.17828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ergun BiÃ§ici
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17828">Extreme Learning Machines for Fast Training of Click-Through Rate Prediction Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extreme Learning Machines (ELM) provide a fast alternative to traditional gradient-based learning in neural networks, offering rapid training and robust generalization capabilities. Its theoretical basis shows its universal approximation capability. We explore the application of ELMs for the task of Click-Through Rate (CTR) prediction, which is largely unexplored by ELMs due to the high dimensionality of the problem. We introduce an ELM-based model enhanced with embedding layers to improve the performance on CTR tasks, which is a novel addition to the field. Experimental results on benchmark datasets, including Avazu and Criteo, demonstrate that our proposed ELM with embeddings achieves competitive F1 results while significantly reducing training time compared to state-of-the-art models such as Masknet. Our findings show that ELMs can be useful for CTR prediction, especially when fast training is needed.
<div id='section'>Paperid: <span id='pid'>2090, <a href='https://arxiv.org/pdf/2405.08793.pdf' target='_blank'>https://arxiv.org/pdf/2405.08793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08793">A Brief Introduction to Causal Inference in Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This is a lecture note produced for DS-GA 3001.003 "Special Topics in DS - Causal Inference in Machine Learning" at the Center for Data Science, New York University in Spring, 2024. This course was created to target master's and PhD level students with basic background in machine learning but who were not exposed to causal inference or causal reasoning in general previously. In particular, this course focuses on introducing such students to expand their view and knowledge of machine learning to incorporate causal reasoning, as this aspect is at the core of so-called out-of-distribution generalization (or lack thereof.)
<div id='section'>Paperid: <span id='pid'>2091, <a href='https://arxiv.org/pdf/2404.07520.pdf' target='_blank'>https://arxiv.org/pdf/2404.07520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anant Khandelwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07520">PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The potential for zero-shot generalization in vision-language (V-L) models such as CLIP has spurred their widespread adoption in addressing numerous downstream tasks. Previous methods have employed test-time prompt tuning to adapt the model to unseen domains, but they overlooked the issue of imbalanced class distributions. In this study, we explicitly address this problem by employing class-aware prototype alignment weighted by mean class probabilities obtained for the test sample and filtered augmented views. Additionally, we ensure that the class probabilities are as accurate as possible by performing prototype discrimination using contrastive learning. The combination of alignment and discriminative loss serves as a geometric regularizer, preventing the prompt representation from collapsing onto a single class and effectively bridging the distribution gap between the source and test domains. Our method, named PromptSync, synchronizes the prompts for each test sample on both the text and vision branches of the V-L model. In empirical evaluations on the domain generalization benchmark, our method outperforms previous best methods by 2.33% in overall performance, by 1% in base-to-novel generalization, and by 2.84% in cross-dataset transfer tasks.
<div id='section'>Paperid: <span id='pid'>2092, <a href='https://arxiv.org/pdf/2312.01792.pdf' target='_blank'>https://arxiv.org/pdf/2312.01792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergey Kolesnikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01792">Wild-Tab: A Benchmark For Out-Of-Distribution Generalization In Tabular Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-Distribution (OOD) generalization, a cornerstone for building robust machine learning models capable of handling data diverging from the training set's distribution, is an ongoing challenge in deep learning. While significant progress has been observed in computer vision and natural language processing, its exploration in tabular data, ubiquitous in many industrial applications, remains nascent. To bridge this gap, we present Wild-Tab, a large-scale benchmark tailored for OOD generalization in tabular regression tasks. The benchmark incorporates 3 industrial datasets sourced from fields like weather prediction and power consumption estimation, providing a challenging testbed for evaluating OOD performance under real-world conditions. Our extensive experiments, evaluating 10 distinct OOD generalization methods on Wild-Tab, reveal nuanced insights. We observe that many of these methods often struggle to maintain high-performance levels on unseen data, with OOD performance showing a marked drop compared to in-distribution performance. At the same time, Empirical Risk Minimization (ERM), despite its simplicity, delivers robust performance across all evaluations, rivaling the results of state-of-the-art methods. Looking forward, we hope that the release of Wild-Tab will facilitate further research on OOD generalization and aid in the deployment of machine learning models in various real-world contexts where handling distribution shifts is a crucial requirement.
<div id='section'>Paperid: <span id='pid'>2093, <a href='https://arxiv.org/pdf/2310.00436.pdf' target='_blank'>https://arxiv.org/pdf/2310.00436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haining Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00436">Enhancing Representation Generalization in Authorship Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Authorship identification ascertains the authorship of texts whose origins remain undisclosed. That authorship identification techniques work as reliably as they do has been attributed to the fact that authorial style is properly captured and represented. Although modern authorship identification methods have evolved significantly over the years and have proven effective in distinguishing authorial styles, the generalization of stylistic features across domains has not been systematically reviewed. The presented work addresses the challenge of enhancing the generalization of stylistic representations in authorship identification, particularly when there are discrepancies between training and testing samples. A comprehensive review of empirical studies was conducted, focusing on various stylistic features and their effectiveness in representing an author's style. The influencing factors such as topic, genre, and register on writing style were also explored, along with strategies to mitigate their impact. While some stylistic features, like character n-grams and function words, have proven to be robust and discriminative, others, such as content words, can introduce biases and hinder cross-domain generalization. Representations learned using deep learning models, especially those incorporating character n-grams and syntactic information, show promise in enhancing representation generalization. The findings underscore the importance of selecting appropriate stylistic features for authorship identification, especially in cross-domain scenarios. The recognition of the strengths and weaknesses of various linguistic features paves the way for more accurate authorship identification in diverse contexts.
<div id='section'>Paperid: <span id='pid'>2094, <a href='https://arxiv.org/pdf/2309.15522.pdf' target='_blank'>https://arxiv.org/pdf/2309.15522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Rostami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15522">Robust Internal Representations for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper which is part of the New Faculty Highlights Invited Speaker Program of AAAI'23, serves as a comprehensive survey of my research in transfer learning by utilizing embedding spaces. The work reviewed in this paper specifically revolves around the inherent challenges associated with continual learning and limited availability of labeled data. By providing an overview of my past and ongoing contributions, this paper aims to present a holistic understanding of my research, paving the way for future explorations and advancements in the field. My research delves into the various settings of transfer learning, including, few-shot learning, zero-shot learning, continual learning, domain adaptation, and distributed learning. I hope this survey provides a forward-looking perspective for researchers who would like to focus on similar research directions.
<div id='section'>Paperid: <span id='pid'>2095, <a href='https://arxiv.org/pdf/2305.05100.pdf' target='_blank'>https://arxiv.org/pdf/2305.05100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Walker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05100">Adaptive Domain Generalization for Digital Pathology Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In AI-based histopathology, domain shifts are common and well-studied. However, this research focuses on stain and scanner variations, which do not show the full picture -- shifts may be combinations of other shifts, or "invisible" shifts that are not obvious but still damage performance of machine learning models. Furthermore, it is important for models to generalize to these shifts without expensive or scarce annotations, especially in the histopathology space and if wanting to deploy models on a larger scale. Thus, there is a need for "reactive" domain generalization techniques: ones that adapt to domain shifts at test-time rather than requiring predictions of or examples of the shifts at training time. We conduct a literature review and introduce techniques that react to domain shifts rather than requiring a prediction of them in advance. We investigate test time training, a technique for domain generalization that adapts model parameters at test-time through optimization of a secondary self-supervised task.
<div id='section'>Paperid: <span id='pid'>2096, <a href='https://arxiv.org/pdf/2304.13001.pdf' target='_blank'>https://arxiv.org/pdf/2304.13001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Dittadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13001">On the Generalization of Learned Structured Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite tremendous progress over the past decade, deep learning methods generally fall short of human-level systematic generalization. It has been argued that explicitly capturing the underlying structure of data should allow connectionist systems to generalize in a more predictable and systematic manner. Indeed, evidence in humans suggests that interpreting the world in terms of symbol-like compositional entities may be crucial for intelligent behavior and high-level reasoning. Another common limitation of deep learning systems is that they require large amounts of training data, which can be expensive to obtain. In representation learning, large datasets are leveraged to learn generic data representations that may be useful for efficient learning of arbitrary downstream tasks.
  This thesis is about structured representation learning. We study methods that learn, with little or no supervision, representations of unstructured data that capture its hidden structure. In the first part of the thesis, we focus on representations that disentangle the explanatory factors of variation of the data. We scale up disentangled representation learning to a novel robotic dataset, and perform a systematic large-scale study on the role of pretrained representations for out-of-distribution generalization in downstream robotic tasks. The second part of this thesis focuses on object-centric representations, which capture the compositional structure of the input in terms of symbol-like entities, such as objects in visual scenes. Object-centric learning methods learn to form meaningful entities from unstructured input, enabling symbolic information processing on a connectionist substrate. In this study, we train a selection of methods on several common datasets, and investigate their usefulness for downstream tasks and their ability to generalize out of distribution.
<div id='section'>Paperid: <span id='pid'>2097, <a href='https://arxiv.org/pdf/2304.03440.pdf' target='_blank'>https://arxiv.org/pdf/2304.03440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takuro Kutsuna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03440">Supervised Contrastive Learning with Heterogeneous Similarity for Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distribution shifts are problems where the distribution of data changes between training and testing, which can significantly degrade the performance of a model deployed in the real world. Recent studies suggest that one reason for the degradation is a type of overfitting, and that proper regularization can mitigate the degradation, especially when using highly representative models such as neural networks. In this paper, we propose a new regularization using the supervised contrastive learning to prevent such overfitting and to train models that do not degrade their performance under the distribution shifts. We extend the cosine similarity in contrastive loss to a more general similarity measure and propose to use different parameters in the measure when comparing a sample to a positive or negative example, which is analytically shown to act as a kind of margin in contrastive loss. Experiments on benchmark datasets that emulate distribution shifts, including subpopulation shift and domain generalization, demonstrate the advantage of the proposed method over existing regularization methods.
<div id='section'>Paperid: <span id='pid'>2098, <a href='https://arxiv.org/pdf/2303.06841.pdf' target='_blank'>https://arxiv.org/pdf/2303.06841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06841">Learning Transductions and Alignments with RNN Seq2seq Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper studies the capabilities of Recurrent-Neural-Network sequence to sequence (RNN seq2seq) models in learning four transduction tasks: identity, reversal, total reduplication, and quadratic copying. These transductions are traditionally well studied under finite state transducers and attributed with increasing complexity. We find that RNN seq2seq models are only able to approximate a mapping that fits the training or in-distribution data, instead of learning the underlying functions. Although attention makes learning more efficient and robust, it does not overcome the out-of-distribution generalization limitation. We establish a novel complexity hierarchy for learning the four tasks for attention-less RNN seq2seq models, which may be understood in terms of the complexity hierarchy of formal languages, instead of string transductions. RNN variants also play a role in the results. In particular, we show that Simple RNN seq2seq models cannot count the input length.
<div id='section'>Paperid: <span id='pid'>2099, <a href='https://arxiv.org/pdf/2302.08674.pdf' target='_blank'>https://arxiv.org/pdf/2302.08674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08674">EnfoMax: Domain Entropy and Mutual Information Maximization for Domain Generalized Face Anti-spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The face anti-spoofing (FAS) method performs well under intra-domain setups. However, its cross-domain performance is unsatisfactory. As a result, the domain generalization (DG) method has gained more attention in FAS. Existing methods treat FAS as a simple binary classification task and propose a heuristic training objective to learn domain-invariant features. However, there is no theoretical explanation of what a domain-invariant feature is. Additionally, the lack of theoretical support makes domain generalization techniques such as adversarial training lack training stability. To address these issues, this paper proposes the EnfoMax framework, which uses information theory to analyze cross-domain FAS tasks. This framework provides theoretical guarantees and optimization objectives for domain-generalized FAS tasks. EnfoMax maximizes the domain entropy and mutual information of live samples in source domains without using adversarial learning. Experimental results demonstrate that our approach performs well on extensive public datasets and outperforms state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>2100, <a href='https://arxiv.org/pdf/2301.12032.pdf' target='_blank'>https://arxiv.org/pdf/2301.12032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Borji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12032">BinaryVQA: A Versatile Test Set to Evaluate the Out-of-Distribution Generalization of VQA Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new test set for visual question answering (VQA) called BinaryVQA to push the limits of VQA models. Our dataset includes 7,800 questions across 1,024 images and covers a wide variety of objects, topics, and concepts. For easy model evaluation, we only consider binary questions. Questions and answers are formulated and verified carefully and manually. Around 63% of the questions have positive answers. The median number of questions per image and question length are 7 and 5, respectively. The state of the art OFA model achieves 75% accuracy on BinaryVQA dataset, which is significantly lower than its performance on the VQA v2 test-dev dataset (94.7%). We also analyze the model behavior along several dimensions including: a) performance over different categories such as text, counting and gaze direction, b) model interpretability, c) the effect of question length on accuracy, d) bias of models towards positive answers and introduction of a new score called the ShuffleAcc, and e) sensitivity to spelling and grammar errors. Our investigation demonstrates the difficulty of our dataset and shows that it can challenge VQA models for next few years. Data and code are publicly available at: DATA and CODE.
<div id='section'>Paperid: <span id='pid'>2101, <a href='https://arxiv.org/pdf/2301.08842.pdf' target='_blank'>https://arxiv.org/pdf/2301.08842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Klas Leino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08842">Limitations of Piecewise Linearity for Efficient Robustness Certification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Certified defenses against small-norm adversarial examples have received growing attention in recent years; though certified accuracies of state-of-the-art methods remain far below their non-robust counterparts, despite the fact that benchmark datasets have been shown to be well-separated at far larger radii than the literature generally attempts to certify. In this work, we offer insights that identify potential factors in this performance gap. Specifically, our analysis reveals that piecewise linearity imposes fundamental limitations on the tightness of leading certification techniques. These limitations are felt in practical terms as a greater need for capacity in models hoped to be certified efficiently. Moreover, this is in addition to the capacity necessary to learn a robust boundary, studied in prior work. However, we argue that addressing the limitations of piecewise linearity through scaling up model capacity may give rise to potential difficulties -- particularly regarding robust generalization -- therefore, we conclude by suggesting that developing smooth activation functions may be the way forward for advancing the performance of certified neural networks.
<div id='section'>Paperid: <span id='pid'>2102, <a href='https://arxiv.org/pdf/2209.01610.pdf' target='_blank'>https://arxiv.org/pdf/2209.01610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chris Rohlfs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01610">Generalization in Neural Networks: A Broad Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Strategies for (1) sample generalization from training to test data are discussed, with suggestive evidence presented that, at least for the ImageNet dataset, popular classification models show substantial overfitting. An empirical example and perspectives from statistics highlight how models' (2) distribution generalization can benefit from consideration of causal relationships and counterfactual scenarios. Transfer learning approaches and results for (3) domain generalization are summarized, as is the wealth of domain generalization benchmark datasets available. Recent breakthroughs surveyed in (4) task generalization include few-shot meta-learning approaches and the emergence of transformer-based foundation models such as those used for language processing. Studies performing (5) modality generalization are reviewed, including those that integrate image and text data and that apply a biologically-inspired network across olfactory, visual, and auditory modalities. Higher-level (6) scope generalization results are surveyed, including graph-based approaches to represent symbolic knowledge in networks and attribution strategies for improving networks' explainability. Additionally, concepts from neuroscience are discussed on the modular architecture of brains and the steps by which dopamine-driven conditioning leads to abstract thinking.
<div id='section'>Paperid: <span id='pid'>2103, <a href='https://arxiv.org/pdf/2206.02786.pdf' target='_blank'>https://arxiv.org/pdf/2206.02786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krikamol Muandet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.02786">(Im)possibility of Collective Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern applications of AI involve training and deploying machine learning models across heterogeneous and potentially massive environments. Emerging diversity of data not only brings about new possibilities to advance AI systems, but also restricts the extent to which information can be shared across environments due to pressing concerns such as privacy, security, and equity. Based on a novel characterization of learning algorithms as choice correspondences on a hypothesis space, this work provides a minimum requirement in terms of intuitive and reasonable axioms under which the only rational learning algorithm in heterogeneous environments is an empirical risk minimization (ERM) that unilaterally learns from a single environment without information sharing across environments. Our (im)possibility result underscores the fundamental trade-off that any algorithms will face in order to achieve Collective Intelligence (CI), i.e., the ability to learn across heterogeneous environments. Ultimately, collective learning in heterogeneous environments are inherently hard because, in critical areas of machine learning such as out-of-distribution generalization, federated/collaborative learning, algorithmic fairness, and multi-modal learning, it can be infeasible to make meaningful comparisons of model predictive performance across environments.
<div id='section'>Paperid: <span id='pid'>2104, <a href='https://arxiv.org/pdf/2002.05379.pdf' target='_blank'>https://arxiv.org/pdf/2002.05379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2002.05379">The Conditional Entropy Bottleneck</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.
