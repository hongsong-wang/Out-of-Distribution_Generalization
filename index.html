<!DOCTYPE html>
<html>
<head>
<title>arXiv Papers of Out-of-Distribution Generalization</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 10px 20px 10px 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body><div id='title' style='font-size:1.3em; font-weight:bold;'>arXiv Papers of Out-of-Distribution Generalization</div><br>
<div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2601.21999.pdf' target='_blank'>https://arxiv.org/pdf/2601.21999.pdf</a></span>   <span><a href='https://github.com/Alrash/NDCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Cao, Jiexi Liu, Songcan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21999">Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imbalanced Domain Generalization (IDG) focuses on mitigating both domain and label shifts, both of which fundamentally shape the model's decision boundaries, particularly under heterogeneous long-tailed distributions across domains. Despite its practical significance, it remains underexplored, primarily due to the technical complexity of handling their entanglement and the paucity of theoretical foundations. In this paper, we begin by theoretically establishing the generalization bound for IDG, highlighting the role of posterior discrepancy and decision margin. This bound motivates us to focus on directly steering decision boundaries, marking a clear departure from existing methods. Subsequently, we technically propose a novel Negative-Dominant Contrastive Learning (NDCL) for IDG to enhance discriminability while enforce posterior consistency across domains. Specifically, inter-class decision-boundary separation is enhanced by placing greater emphasis on negatives as the primary signal in our contrastive learning, naturally amplifying gradient signals for minority classes to avoid the decision boundary being biased toward majority classes. Meanwhile, intra-class compactness is encouraged through a re-weighted cross-entropy strategy, and posterior consistency across domains is enforced through a prediction-central alignment strategy. Finally, rigorous yet challenging experiments on benchmarks validate the effectiveness of our NDCL. The code is available at https://github.com/Alrash/NDCL.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2601.21716.pdf' target='_blank'>https://arxiv.org/pdf/2601.21716.pdf</a></span>   <span><a href='https://grisoon.github.io/DreamActor-M2/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingshuang Luo, Shuang Liang, Zhengkun Rong, Yuxuan Luo, Tianshu Hu, Ruibing Hou, Hong Chang, Yong Li, Yuan Zhang, Mingyuan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21716">DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2601.20675.pdf' target='_blank'>https://arxiv.org/pdf/2601.20675.pdf</a></span>   <span><a href='https://github.com/ipankhi/BiMoRS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pankhi Kashyap, Mainak Singha, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20675">bi-modal textual prompt learning for vision-language models in remote sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2601.17586.pdf' target='_blank'>https://arxiv.org/pdf/2601.17586.pdf</a></span>   <span><a href='https://github.com/sdoerrich97/stylizing-vit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Doerrich, Francesco Di Salvo, Jonas Alle, Christian Ledig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17586">Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2601.17486.pdf' target='_blank'>https://arxiv.org/pdf/2601.17486.pdf</a></span>   <span><a href='https://ZhangZhiyuanZhang.github.io/equiform-website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Zhang, Yu She
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17486">EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual imitation learning with 3D point clouds has advanced robotic manipulation by providing geometry-aware, appearance-invariant observations. However, point cloud-based policies remain highly sensitive to sensor noise, pose perturbations, and occlusion-induced artifacts, which distort geometric structure and break the equivariance assumptions required for robust generalization. Existing equivariant approaches primarily encode symmetry constraints into neural architectures, but do not explicitly correct noise-induced geometric deviations or enforce equivariant consistency in learned representations. We introduce EquiForm, a noise-robust SE(3)-equivariant policy learning framework for point cloud-based manipulation. EquiForm formalizes how noise-induced geometric distortions lead to equivariance deviations in observation-to-action mappings, and introduces a geometric denoising module to restore consistent 3D structure under noisy or incomplete observations. In addition, we propose a contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations. Built upon these components, EquiForm forms a flexible policy learning pipeline that integrates noise-robust geometric reasoning with modern generative models. We evaluate EquiForm on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Compared to state-of-the-art point cloud imitation learning methods, EquiForm achieves an average improvement of 17.2% in simulation and 28.1% in real-world experiments, demonstrating strong noise robustness and spatial generalization.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2601.17468.pdf' target='_blank'>https://arxiv.org/pdf/2601.17468.pdf</a></span>   <span><a href='https://wuw2135.github.io/ReflexSplit-ProjectPage/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wuw2135/ReflexSplit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chia-Ming Lee, Yu-Fan Lin, Jing-Hui Jung, Yu-Jou Hsiao, Chih-Chung Hsu, Yu-Lun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17468">ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2601.15849.pdf' target='_blank'>https://arxiv.org/pdf/2601.15849.pdf</a></span>   <span><a href='https://github.com/yumeow0122/CGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tsung-Hsiang Chou, Chen-Jui Yu, Shui-Hsiang Hsu, Yao-Chung Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15849">CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2601.15624.pdf' target='_blank'>https://arxiv.org/pdf/2601.15624.pdf</a></span>   <span><a href='https://github.com/deon1219/rlsbi' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Jiang, Dingheng Zeng, Yanhong Liu, Haiyang Yi, Shijie Yu, Minghe Weng, Haifeng Shen, Ying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15624">Explainable Deepfake Detection with RL Enhanced Self-Blended Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2601.15615.pdf' target='_blank'>https://arxiv.org/pdf/2601.15615.pdf</a></span>   <span><a href='https://github.com/RyanLi-X/RSM-CoDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiwei Wu, Yueyang Li, Yuhu Shi, Weiming Zeng, Lang Qin, Yang Yang, Ke Zhou, Zhiguo Zhang, Wai Ting Siok, Nizhuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15615">Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2601.15316.pdf' target='_blank'>https://arxiv.org/pdf/2601.15316.pdf</a></span>   <span><a href='https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Ai, Yilong Tan, Yuntao Shou, Tao Meng, Haowen Chen, Zhixiong He, Keqin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15316">The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2601.09812.pdf' target='_blank'>https://arxiv.org/pdf/2601.09812.pdf</a></span>   <span><a href='https://github.com/CarloSgaravatti/LCF3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlo Sgaravatti, Riccardo Pieroni, Matteo Corno, Sergio M. Savaresi, Luca Magri, Giacomo Boracchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09812">LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2601.09031.pdf' target='_blank'>https://arxiv.org/pdf/2601.09031.pdf</a></span>   <span><a href='https://github.com/xtli12/RGMP-S.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuetao Li, Wenke Huang, Mang Ye, Jifeng Xuan, Bo Du, Sheng Liu, Miao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09031">Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely hindering the applicability and generalizability of existing frameworks. This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. To ground high-level reasoning in physical reality, we leverage lightweight 2D geometric inductive biases to enable precise 3D scene understanding within the vision-language model. Specifically, we construct a Long-horizon Geometric Prior Skill Selector that effectively aligns the semantic instructions with spatial constraints, ultimately achieving robust generalization in unseen environments. For the data efficiency issue in robotic action generation, we introduce a Recursive Adaptive Spiking Network. We parameterize robot-object interactions via recursive spiking for spatiotemporal consistency, fully distilling long-horizon dynamic features while mitigating the overfitting issue in sparse demonstration scenarios. Extensive experiments are conducted across the Maniskill simulation benchmark and three heterogeneous real-world robotic systems, encompassing a custom-developed humanoid, a desktop manipulator, and a commercial robotic platform. Empirical results substantiate the superiority of our method over state-of-the-art baselines and validate the efficacy of the proposed modules in diverse generalization scenarios. To facilitate reproducibility, the source code and video demonstrations are publicly available at https://github.com/xtli12/RGMP-S.git.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2601.07309.pdf' target='_blank'>https://arxiv.org/pdf/2601.07309.pdf</a></span>   <span><a href='https://arkazhuo.github.io/ARM-homepage/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07309">ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2601.06525.pdf' target='_blank'>https://arxiv.org/pdf/2601.06525.pdf</a></span>   <span><a href='https://vegdog007.github.io/GLOWDeblur_Website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanting Gao, Shuo Cao, Xiaohui Li, Yuandong Pu, Yihao Liu, Kai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06525">Toward Generalizable Deblurring: Leveraging Massive Blur Priors with Linear Attention for Real-World Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image deblurring has advanced rapidly with deep learning, yet most methods exhibit poor generalization beyond their training datasets, with performance dropping significantly in real-world scenarios. Our analysis shows this limitation stems from two factors: datasets face an inherent trade-off between realism and coverage of diverse blur patterns, and algorithmic designs remain restrictive, as pixel-wise losses drive models toward local detail recovery while overlooking structural and semantic consistency, whereas diffusion-based approaches, though perceptually strong, still fail to generalize when trained on narrow datasets with simplistic strategies. Through systematic investigation, we identify blur pattern diversity as the decisive factor for robust generalization and propose Blur Pattern Pretraining (BPP), which acquires blur priors from simulation datasets and transfers them through joint fine-tuning on real data. We further introduce Motion and Semantic Guidance (MoSeG) to strengthen blur priors under severe degradation, and integrate it into GLOWDeblur, a Generalizable reaL-wOrld lightWeight Deblur model that combines convolution-based pre-reconstruction & domain alignment module with a lightweight diffusion backbone. Extensive experiments on six widely-used benchmarks and two real-world datasets validate our approach, confirming the importance of blur priors for robust generalization and demonstrating that the lightweight design of GLOWDeblur ensures practicality in real-world applications. The project page is available at https://vegdog007.github.io/GLOWDeblur_Website/.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2601.05103.pdf' target='_blank'>https://arxiv.org/pdf/2601.05103.pdf</a></span>   <span><a href='https://github.com/zhiyintan/SOFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changxu Duan, Zhiyin Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05103">Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2601.04992.pdf' target='_blank'>https://arxiv.org/pdf/2601.04992.pdf</a></span>   <span><a href='https://github.com/Eureka-Maggie/GLOW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyun Tian, Minghua Ma, Bingbing Xu, Nuoyan Lyu, Wei Li, Heng Dong, Zheng Chu, Yuanzhuo Wang, Huawei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04992">Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2601.04582.pdf' target='_blank'>https://arxiv.org/pdf/2601.04582.pdf</a></span>   <span><a href='https://github.com/vis-nlp/RL-Text2Vis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mizanur Rahman, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04582">Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2601.02994.pdf' target='_blank'>https://arxiv.org/pdf/2601.02994.pdf</a></span>   <span><a href='https://joon-stack.github.io/VILA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjoon Jeong, Junha Chun, Taesup Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02994">Learning to Act Robustly with View-Invariant Latent Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based robotic policies often struggle with even minor viewpoint changes, underscoring the need for view-invariant visual representations. This challenge becomes more pronounced in real-world settings, where viewpoint variability is unavoidable and can significantly disrupt policy performance. Existing methods typically learn invariance from multi-view observations at the scene level, but such approaches rely on visual appearance and fail to incorporate the physical dynamics essential for robust generalization. We propose View-Invariant Latent Action (VILA), which models a latent action capturing transition patterns across trajectories to learn view-invariant representations grounded in physical dynamics. VILA aligns these latent actions across viewpoints using an action-guided objective based on ground-truth action sequences. Experiments in both simulation and the real world show that VILA-based policies generalize effectively to unseen viewpoints and transfer well to new tasks, establishing VILA as a strong pretraining framework that improves robustness and downstream learning performance.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2601.01485.pdf' target='_blank'>https://arxiv.org/pdf/2601.01485.pdf</a></span>   <span><a href='https://github.com/zobia111/Extended-Mixstyle' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zobia Batool, Diala Lteif, Vijaya B. Kolachalama, Huseyin Ozkan, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01485">Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2512.24679.pdf' target='_blank'>https://arxiv.org/pdf/2512.24679.pdf</a></span>   <span><a href='https://github.com/xiapc1996/MMDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Xia, Yixiang Huang, Chengjin Qin, Chengliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24679">Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2512.24321.pdf' target='_blank'>https://arxiv.org/pdf/2512.24321.pdf</a></span>   <span><a href='https://jnnan.github.io/uniact/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Jiang, Zimo He, Wanhe Yu, Lexi Pang, Yunhao Li, Hongjie Li, Jieming Cui, Yuhan Li, Yizhou Wang, Yixin Zhu, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24321">UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A long-standing objective in humanoid robotics is the realization of versatile agents capable of following diverse multimodal instructions with human-level flexibility. Despite advances in humanoid control, bridging high-level multimodal perception with whole-body execution remains a significant bottleneck. Existing methods often struggle to translate heterogeneous instructions -- such as language, music, and trajectories -- into stable, real-time actions. Here we show that UniAct, a two-stage framework integrating a fine-tuned MLLM with a causal streaming pipeline, enables humanoid robots to execute multimodal instructions with sub-500 ms latency. By unifying inputs through a shared discrete codebook via FSQ, UniAct ensures cross-modal alignment while constraining motions to a physically grounded manifold. This approach yields a 19% improvement in the success rate of zero-shot tracking of imperfect reference motions. We validate UniAct on UniMoCap, our 20-hour humanoid motion benchmark, demonstrating robust generalization across diverse real-world scenarios. Our results mark a critical step toward responsive, general-purpose humanoid assistants capable of seamless interaction through unified perception and control.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2512.24014.pdf' target='_blank'>https://arxiv.org/pdf/2512.24014.pdf</a></span>   <span><a href='https://github.com/AgenticFinLab/latent-planning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Chen, Di Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24014">iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2512.19134.pdf' target='_blank'>https://arxiv.org/pdf/2512.19134.pdf</a></span>   <span><a href='https://github.com/ZhishanQ/QuCo-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dehai Min, Kailin Zhang, Tongtong Wu, Lu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19134">QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2512.17864.pdf' target='_blank'>https://arxiv.org/pdf/2512.17864.pdf</a></span>   <span><a href='https://github.com/BS0111/PlantAttentionCBAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Balram Singh, Ram Prakash Sharma, Somnath Dey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17864">Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2512.15729.pdf' target='_blank'>https://arxiv.org/pdf/2512.15729.pdf</a></span>   <span><a href='https://github.com/pulp-bio/BioFoundation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Fasulo, Giusy Spacone, Thorir Mar Ingolfsson, Yawei Li, Luca Benini, Andrea Cossettini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15729">TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2512.11218.pdf' target='_blank'>https://arxiv.org/pdf/2512.11218.pdf</a></span>   <span><a href='https://xukechun.github.io/papers/BayesVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kechun Xu, Zhenjie Zhu, Anzhe Chen, Shuqi Zhao, Qing Huang, Yifei Yang, Haojian Lu, Rong Xiong, Masayoshi Tomizuka, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11218">Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2512.10807.pdf' target='_blank'>https://arxiv.org/pdf/2512.10807.pdf</a></span>   <span><a href='https://github.com/AIFrontierLab/HAROOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Lu, Yao Zhu, Jindong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10807">HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2512.08912.pdf' target='_blank'>https://arxiv.org/pdf/2512.08912.pdf</a></span>   <span><a href='https://simondemoreau.github.io/LiDAS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon de Moreau, Andrei Bursuc, Hafid El-Idrissi, Fabien Moutarde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08912">LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2512.08042.pdf' target='_blank'>https://arxiv.org/pdf/2512.08042.pdf</a></span>   <span><a href='https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chandler Timm C. Doloriel, Habib Ullah, Kristian Hovde Liland, Fadi Al Machot, Ngai-Man Cheung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08042">Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2512.06689.pdf' target='_blank'>https://arxiv.org/pdf/2512.06689.pdf</a></span>   <span><a href='https://github.com/jisoo-o/UniVoiceLite' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jisoo Park, Seonghak Lee, Guisik Kim, Taewoo Kim, Junseok Kwon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06689">Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2512.05693.pdf' target='_blank'>https://arxiv.org/pdf/2512.05693.pdf</a></span>   <span><a href='https://github.com/ZhiyingDu/HiMoE-VLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Du, Bei Liu, Yaobo Liang, Yichao Shen, Haidong Cao, Xiangyu Zheng, Zhiyuan Feng, Zuxuan Wu, Jiaolong Yang, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05693">HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2512.03508.pdf' target='_blank'>https://arxiv.org/pdf/2512.03508.pdf</a></span>   <span><a href='https://github.com/jone1222/DPMFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seogkyu Jeon, Kibeom Hong, Hyeran Byun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03508">Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2512.02697.pdf' target='_blank'>https://arxiv.org/pdf/2512.02697.pdf</a></span>   <span><a href='https://github.com/MiliLab/GeoBridge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Song, Jing Zhang, Di Wang, Zidie Zhou, Wenbin Liu, Haonan Guo, En Wang, Bo Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02697">GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2512.01970.pdf' target='_blank'>https://arxiv.org/pdf/2512.01970.pdf</a></span>   <span><a href='https://github.com/sitaocheng/from_atomic_to_composite' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sitao Cheng, Xunjian Yin, Ruiwen Zhou, Yuxuan Li, Xinyi Wang, Liangming Pan, William Yang Wang, Victor Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01970">From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2512.00716.pdf' target='_blank'>https://arxiv.org/pdf/2512.00716.pdf</a></span>   <span><a href='https://github.com/flzeng1/MPAIACL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanlong Zeng, Wensheng Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00716">Graph Data Augmentation with Contrastive Learning on Covariate Distribution Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Covariate distribution shift occurs when certain structural features present in the test set are absent from the training set. It is a common type of out-of-distribution (OOD) problem, frequently encountered in real-world graph data with complex structures. Existing research has revealed that most out-of-the-box graph neural networks (GNNs) fail to account for covariate shifts. Furthermore, we observe that existing methods aimed at addressing covariate shifts often fail to fully leverage the rich information contained within the latent space. Motivated by the potential of the latent space, we introduce a new method called MPAIACL for More Powerful Adversarial Invariant Augmentation using Contrastive Learning. MPAIACL leverages contrastive learning to unlock the full potential of vector representations by harnessing their intrinsic information. Through extensive experiments, MPAIACL demonstrates its robust generalization and effectiveness, as it performs well compared with other baselines across various public OOD datasets. The code is publicly available at https://github.com/flzeng1/MPAIACL.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2511.22948.pdf' target='_blank'>https://arxiv.org/pdf/2511.22948.pdf</a></span>   <span><a href='https://github.com/VisualScienceLab-KHU/FLEX-Seg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeyeong Kim, SeungJoon Lee, Jung Uk Kim, MyeongAh Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22948">Do We Need Perfect Data? Leveraging Noise for Domain Generalized Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization in semantic segmentation faces challenges from domain shifts, particularly under adverse conditions. While diffusion-based data generation methods show promise, they introduce inherent misalignment between generated images and semantic masks. This paper presents FLEX-Seg (FLexible Edge eXploitation for Segmentation), a framework that transforms this limitation into an opportunity for robust learning. FLEX-Seg comprises three key components: (1) Granular Adaptive Prototypes that captures boundary characteristics across multiple scales, (2) Uncertainty Boundary Emphasis that dynamically adjusts learning emphasis based on prediction entropy, and (3) Hardness-Aware Sampling that progressively focuses on challenging examples. By leveraging inherent misalignment rather than enforcing strict alignment, FLEX-Seg learns robust representations while capturing rich stylistic variations. Experiments across five real-world datasets demonstrate consistent improvements over state-of-the-art methods, achieving 2.44% and 2.63% mIoU gains on ACDC and Dark Zurich. Our findings validate that adaptive strategies for handling imperfect synthetic data lead to superior domain generalization. Code is available at https://github.com/VisualScienceLab-KHU/FLEX-Seg.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2511.22897.pdf' target='_blank'>https://arxiv.org/pdf/2511.22897.pdf</a></span>   <span><a href='https://vranlee.github.io/P2C/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiran Li, Yeqiang Liu, Yijie Wei, Mina Han, Xin Liu, Zhenbo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22897">From Points to Clouds: Learning Robust Semantic Distributions for Multi-modal Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Prompt Learning (MPL) has emerged as a pivotal technique for adapting large-scale Visual Language Models (VLMs). However, current MPL methods are fundamentally limited by their optimization of a single, static point representation. This paradigm is inherently brittle, leads to overfitting on base classes, and generalizes poorly to novel or ambiguous categories. We challenge this point paradigm, proposing that robust generalization requires learning a semantic cloud (i.e., a distribution over the embedding space). To achieve this, we introduce Points-to-Clouds (P2C), a novel framework inspired by diffusion models that reframes prompt learning as a dynamic denoising task. At the core of P2C is a dual denoising mechanism: a Dynamic Prompt Denoising (DPD) mechanism perturbs text prompts with sophisticated, annealed noise to learn a smoother semantic landscape, while an auxiliary V-L Mapper denoising loss re-tasks the mapper as a denoising autoencoder. This forces the mapper to reconstruct clean visual prompts from noisy text inputs, ensuring robust cross-modal alignment. Extensive experiments across 11 datasets demonstrate that P2C consistently outperforms strong baselines. On the base-to-novel generalization benchmark, our method achieves a Harmonic Mean of 79.7%, representing a relative improvement of 1.4% over the baseline. The code and models are available at https://vranlee.github.io/P2C/.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2511.22664.pdf' target='_blank'>https://arxiv.org/pdf/2511.22664.pdf</a></span>   <span><a href='https://visual-ai.github.io/vamp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Silin Cheng, Kai Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22664">VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Prompt Learning (VaMP) framework that enables sample-specific, uncertainty-aware prompt tuning in multi-modal representation learning. VaMP generates instance-conditioned prompts by sampling from a learned posterior distribution, allowing the model to personalize its behavior based on input content. To further enhance the integration of local and global semantics, we introduce a class-aware prior derived from the instance representation and class prototype. Building upon these, we formulate prompt tuning as variational inference over latent prompt representations and train the entire framework end-to-end through reparameterized sampling. Experiments on few-shot and domain generalization benchmarks show that VaMP achieves state-of-the-art performance, highlighting the benefits of modeling both uncertainty and task structure in our method. Project page: https://visual-ai.github.io/vamp
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2511.21395.pdf' target='_blank'>https://arxiv.org/pdf/2511.21395.pdf</a></span>   <span><a href='https://github.com/NOVAglow646/Monet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, Yisen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21395">Monet: Reasoning in Latent Visual Space Beyond Images and Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>"Thinking with images" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2511.18254.pdf' target='_blank'>https://arxiv.org/pdf/2511.18254.pdf</a></span>   <span><a href='https://lisiyi777.github.io/UniFlow/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyi Li, Qingwen Zhang, Ishan Khatri, Kyle Vedder, Deva Ramanan, Neehar Peri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18254">UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2511.13105.pdf' target='_blank'>https://arxiv.org/pdf/2511.13105.pdf</a></span>   <span><a href='https://github.com/VisualScienceLab-KHU/PlugTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjae Kim, SeungJoon Lee, MyeongAh Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13105">PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2511.12410.pdf' target='_blank'>https://arxiv.org/pdf/2511.12410.pdf</a></span>   <span><a href='https://github.com/xixiaouab/PROBE/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Xiao, Zhuxuanzi Wang, Mingqiao Mo, Chen Liu, Chenrui Ma, Yanshu Li, Smita Krishnaswamy, Xiao Wang, Tianyang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12410">Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2511.10707.pdf' target='_blank'>https://arxiv.org/pdf/2511.10707.pdf</a></span>   <span><a href='https://github.com/LiangThree/BREP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Liang, Pengfei Cao, Jian Zhao, Cong Huang, Jun Zhao, Kang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10707">Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2511.09737.pdf' target='_blank'>https://arxiv.org/pdf/2511.09737.pdf</a></span>   <span><a href='https://github.com/bramgrooten/sparc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bram Grooten, Patrick MacAlpine, Kaushik Subramanian, Peter Stone, Peter R. Wurman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09737">Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization to unseen environments is a significant challenge in the field of robotics and control. In this work, we focus on contextual reinforcement learning, where agents act within environments with varying contexts, such as self-driving cars or quadrupedal robots that need to operate in different terrains or weather conditions than they were trained for. We tackle the critical task of generalizing to out-of-distribution (OOD) settings, without access to explicit context information at test time. Recent work has addressed this problem by training a context encoder and a history adaptation module in separate stages. While promising, this two-phase approach is cumbersome to implement and train. We simplify the methodology and introduce SPARC: single-phase adaptation for robust control. We test SPARC on varying contexts within the high-fidelity racing simulator Gran Turismo 7 and wind-perturbed MuJoCo environments, and find that it achieves reliable and robust OOD generalization.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2511.08909.pdf' target='_blank'>https://arxiv.org/pdf/2511.08909.pdf</a></span>   <span><a href='https://github.com/nidongpinyinme/NESCap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zimao Lu, Hui Xu, Bing Liu, Ke Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08909">Negative Entity Suppression for Zero-Shot Captioning with Synthetic Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-only training provides an attractive approach to address data scarcity challenges in zero-shot image captioning (ZIC), avoiding the expense of collecting paired image-text annotations. However, although these approaches perform well within training domains, they suffer from poor cross-domain generalization, often producing hallucinated content when encountering novel visual environments. Retrieval-based methods attempt to mitigate this limitation by leveraging external knowledge, but they can paradoxically exacerbate hallucination when retrieved captions contain entities irrelevant to the inputs. We introduce the concept of negative entities--objects that appear in generated caption but are absent from the input--and propose Negative Entity Suppression (NES) to tackle this challenge. NES seamlessly integrates three stages: (1) it employs synthetic images to ensure consistent image-to-text retrieval across both training and inference; (2) it filters negative entities from retrieved content to enhance accuracy; and (3) it applies attention-level suppression using identified negative entities to further minimize the impact of hallucination-prone features. Evaluation across multiple benchmarks demonstrates that NES maintains competitive in-domain performance while improving cross-domain transfer and reducing hallucination rates, achieving new state-of-the-art results in ZIC. Our code is available at https://github.com/nidongpinyinme/NESCap.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2511.04668.pdf' target='_blank'>https://arxiv.org/pdf/2511.04668.pdf</a></span>   <span><a href='https://ellisbrown.github.io/sims-v' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04668">SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2511.03855.pdf' target='_blank'>https://arxiv.org/pdf/2511.03855.pdf</a></span>   <span><a href='https://github.com/Duongmai127/Noisy-ood' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duong Mai, Lawrence Hall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03855">Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learned (DL) models for image recognition have been shown to fail to generalize to data from different devices, populations, etc. COVID-19 detection from Chest X-rays (CXRs), in particular, has been shown to fail to generalize to out-of-distribution (OOD) data from new clinical sources not covered in the training set. This occurs because models learn to exploit shortcuts - source-specific artifacts that do not translate to new distributions - rather than reasonable biomarkers to maximize performance on in-distribution (ID) data. Rendering the models more robust to distribution shifts, our study investigates the use of fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during training. Our empirical results demonstrate that this technique can significantly reduce the performance gap between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results averaged over ten random seeds across key metrics such as AUC, F1, accuracy, recall and specificity. Our source code is publicly available at https://github.com/Duongmai127/Noisy-ood
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2511.03367.pdf' target='_blank'>https://arxiv.org/pdf/2511.03367.pdf</a></span>   <span><a href='https://github.com/Gahyeonkim09/AAPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gahyeon Kim, Sohee Kim, Seokju Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03367">Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: https://github.com/Gahyeonkim09/AAPL
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2511.02400.pdf' target='_blank'>https://arxiv.org/pdf/2511.02400.pdf</a></span>   <span><a href='https://github.com/Minds-R-Lab/MammoClean' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yalda Zafari, Hongyi Pan, Gorkem Durak, Ulas Bagci, Essam A. Rashed, Mohamed Mabrok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02400">MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: https://github.com/Minds-R-Lab/MammoClean.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2511.01767.pdf' target='_blank'>https://arxiv.org/pdf/2511.01767.pdf</a></span>   <span><a href='https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Yang, Xiao-Xiao Long, Zhiyang Dou, Cheng Lin, Yuan Liu, Qingsong Yan, Yuexin Ma, Haoqian Wang, Zhiqiang Wu, Wei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01767">Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce \textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2511.00480.pdf' target='_blank'>https://arxiv.org/pdf/2511.00480.pdf</a></span>   <span><a href='https://github.com/weihao-bo/FedMGP.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihao Bo, Yanpeng Sun, Yu Wang, Xinyu Zhang, Zechao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00480">FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce FedMGP, a new paradigm for personalized federated prompt learning in vision-language models. FedMGP equips each client with multiple groups of paired textual and visual prompts, enabling the model to capture diverse, fine-grained semantic and instance-level cues. A diversity loss is introduced to drive each prompt group to specialize in distinct and complementary semantic aspects, ensuring that the groups collectively cover a broader range of local characteristics. During communication, FedMGP employs a dynamic prompt aggregation strategy based on similarity-guided probabilistic sampling: each client computes the cosine similarity between its prompt groups and the global prompts from the previous round, then samples s groups via a softmax-weighted distribution. This soft selection mechanism preferentially aggregates semantically aligned knowledge while still enabling exploration of underrepresented patterns effectively balancing the preservation of common knowledge with client-specific features. Notably, FedMGP maintains parameter efficiency by redistributing a fixed prompt capacity across multiple groups, achieving state-of-the-art performance with the lowest communication parameters among all federated prompt learning methods. Theoretical analysis shows that our dynamic aggregation strategy promotes robust global representation learning by reinforcing shared semantics while suppressing client-specific noise. Extensive experiments demonstrate that FedMGP consistently outperforms prior approaches in both personalization and domain generalization across diverse federated vision-language benchmarks. The code will be released on https://github.com/weihao-bo/FedMGP.git.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2510.27210.pdf' target='_blank'>https://arxiv.org/pdf/2510.27210.pdf</a></span>   <span><a href='https://leon022.github.io/GUI-Rise' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Liu, Chongyu Wang, Rongjie Li, Yingchen Yu, Xuming He, Bai Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27210">GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multimodal Large Language Models (MLLMs) have advanced GUI navigation agents, current approaches face limitations in cross-domain generalization and effective history utilization. We present a reasoning-enhanced framework that systematically integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates coherent Chain-of-Thought analyses combining progress estimation and decision reasoning, which inform both immediate action predictions and compact history summaries for future steps. Based on this framework, we train a GUI agent, \textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). This framework employs specialized rewards, including a history-aware objective, directly linking summary quality to subsequent action performance. Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art results under identical training data conditions, with particularly strong performance in out-of-domain scenarios. These findings validate our framework's ability to maintain robust reasoning and generalization across diverse GUI navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2510.25801.pdf' target='_blank'>https://arxiv.org/pdf/2510.25801.pdf</a></span>   <span><a href='https://github.com/Kwen-Chen/SPECS-VL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Chen, Peng Shi, Haibo Qiu, Zhixiong Zeng, Siqi Yang, Wenji Mao, Lin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25801">Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of "MLLM-r1" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution "stuckness," improving exploration, stabilizing training, and raising the performance ceiling.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2510.22589.pdf' target='_blank'>https://arxiv.org/pdf/2510.22589.pdf</a></span>   <span><a href='https://github.com/boyiZheng99/PSScreen_V2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyi Zheng, Yalin Zheng, Hrvoje Bogunović, Qing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22589">PSScreen V2: Partially Supervised Multiple Retinal Disease Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose PSScreen V2, a partially supervised self-training framework for multiple retinal disease screening. Unlike previous methods that rely on fully labelled or single-domain datasets, PSScreen V2 is designed to learn from multiple partially labelled datasets with different distributions, addressing both label absence and domain shift challenges. To this end, PSScreen V2 adopts a three-branch architecture with one teacher and two student networks. The teacher branch generates pseudo labels from weakly augmented images to address missing labels, while the two student branches introduce novel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout), which enhances domain robustness by randomly discarding domain-related low-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which estimates uncertain domain variability via adversarially learned Gaussian perturbations of low-frequency statistics. Extensive experiments on multiple in-domain and out-of-domain fundus datasets demonstrate that PSScreen V2 achieves state-of-the-art performance and superior domain generalization ability. Furthermore, compatibility tests with diverse backbones, including the vision foundation model DINOv2, as well as evaluations on chest X-ray datasets, highlight the universality and adaptability of the proposed framework. The codes are available at https://github.com/boyiZheng99/PSScreen_V2.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2510.22197.pdf' target='_blank'>https://arxiv.org/pdf/2510.22197.pdf</a></span>   <span><a href='https://github.com/ncclab-sustech/mdJPT_nips2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingzhu Zhang, Jiani Zhong, Zongsheng Li, Xinke Shen, Quanying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22197">Multi-dataset Joint Pre-training of Emotional EEG Enables Generalizable Affective Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task-specific pre-training is essential when task representations diverge from generic pre-training features. Existing task-general pre-training EEG models struggle with complex tasks like emotion recognition due to mismatches between task-specific features and broad pre-training approaches. This work aims to develop a task-specific multi-dataset joint pre-training framework for cross-dataset emotion recognition, tackling problems of large inter-dataset distribution shifts, inconsistent emotion category definitions, and substantial inter-subject variability. We introduce a cross-dataset covariance alignment loss to align second-order statistical properties across datasets, enabling robust generalization without the need for extensive labels or per-subject calibration. To capture the long-term dependency and complex dynamics of EEG, we propose a hybrid encoder combining a Mamba-like linear attention channel encoder and a spatiotemporal dynamics model. Our method outperforms state-of-the-art large-scale EEG models by an average of 4.57% in AUROC for few-shot emotion recognition and 11.92% in accuracy for zero-shot generalization to a new dataset. Performance scales with the increase of datasets used in pre-training. Multi-dataset joint pre-training achieves a performance gain of 8.55% over single-dataset training. This work provides a scalable framework for task-specific pre-training and highlights its benefit in generalizable affective computing. Our code is available at https://github.com/ncclab-sustech/mdJPT_nips2025.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2510.21782.pdf' target='_blank'>https://arxiv.org/pdf/2510.21782.pdf</a></span>   <span><a href='https://github.com/UEmmanuel5/ProFSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Emmanuel U. Ugwu, Zhang Xinming
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21782">Promptable Fire Segmentation: Unleashing SAM2's Potential for Real-Time Mobile Deployment with Strategic Bounding Box Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fire segmentation remains a critical challenge in computer vision due to flames' irregular boundaries, translucent edges, and highly variable intensities. While the Segment Anything Models (SAM and SAM2) have demonstrated impressive cross-domain generalization capabilities, their effectiveness in fire segmentation -- particularly under mobile deployment constraints -- remains largely unexplored. This paper presents the first comprehensive evaluation of SAM2 variants for fire segmentation, focusing on bounding box prompting strategies to enhance deployment feasibility. We systematically evaluate four SAM2.1 variants (tiny, small, base_plus, large) alongside mobile-oriented variants (TinySAM, MobileSAM) across three fire datasets using multiple prompting strategies: automatic, single positive point (SP), single positive point + single negative point (SP+SN), multiple positive points (MP), bounding box (Box), and hybrid variants (Box+SP and Box+MP). Our experimental results demonstrate that bounding box prompts consistently outperform automatic and single point-based approaches, with Box+MP achieving the highest mean IoU (0.64) and Dice coefficient (0.75) on the Khan dataset. Lightweight variants such as TinySAM and MobileSAM further reduce memory and computational costs, making them more suitable for latency-tolerant edge scenarios. Overall, this work provides critical insights for deploying promptable segmentation models in fire monitoring systems and establishes benchmarks for future research in domain-specific SAM applications. Code is available at: https://github.com/UEmmanuel5/ProFSAM
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2510.16704.pdf' target='_blank'>https://arxiv.org/pdf/2510.16704.pdf</a></span>   <span><a href='https://github.com/weitianxin/DCCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxin Wei, Yifan Chen, Xinrui He, Wenxuan Bao, Jingrui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16704">Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distribution shifts between training and testing samples frequently occur in practice and impede model generalization performance. This crucial challenge thereby motivates studies on domain generalization (DG), which aim to predict the label on unseen target domain data by solely using data from source domains. It is intuitive to conceive the class-separated representations learned in contrastive learning (CL) are able to improve DG, while the reality is quite the opposite: users observe directly applying CL deteriorates the performance. We analyze the phenomenon with the insights from CL theory and discover lack of intra-class connectivity in the DG setting causes the deficiency. We thus propose a new paradigm, domain-connecting contrastive learning (DCCL), to enhance the conceptual connectivity across domains and obtain generalizable representations for DG. On the data side, more aggressive data augmentation and cross-domain positive samples are introduced to improve intra-class connectivity. On the model side, to better embed the unseen test domains, we propose model anchoring to exploit the intra-class connectivity in pre-trained representations and complement the anchoring with generative transformation loss. Extensive experiments on five standard DG benchmarks are performed. The results verify that DCCL outperforms state-of-the-art baselines even without domain supervision. The detailed model implementation and the code are provided through https://github.com/weitianxin/DCCL
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2510.16548.pdf' target='_blank'>https://arxiv.org/pdf/2510.16548.pdf</a></span>   <span><a href='https://ZzzitaoFang.github.io/projects/NeurIPT/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zitao Fang, Chenxuan Li, Hongting Zhou, Shuyang Yu, Guodong Du, Ashwaq Qasem, Yang Lu, Jing Li, Junsong Zhang, Sim Kuan Goh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16548">NeurIPT: Foundation Model for Neural Interfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography (EEG) has wide-ranging applications, from clinical diagnosis to brain-computer interfaces (BCIs). With the increasing volume and variety of EEG data, there has been growing interest in establishing foundation models (FMs) to scale up and generalize neural decoding. Despite showing early potential, applying FMs to EEG remains challenging due to substantial inter-subject, inter-task, and inter-condition variability, as well as diverse electrode configurations across recording setups. To tackle these open challenges, we propose NeurIPT, a foundation model developed for diverse EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP), masking based on signal amplitude rather than random intervals, to learn robust representations across varying signal intensities beyond local interpolation. Moreover, this temporal representation is enhanced by a Progressive Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks are progressively introduced at deeper layers, adapting effectively to the diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages the 3D physical coordinates of electrodes, enabling effective transfer of embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling (IILP) during fine-tuning to efficiently exploit regional brain features. Empirical evaluations across eight downstream BCI datasets, via fine-tuning, demonstrated NeurIPT consistently achieved state-of-the-art performance, highlighting its broad applicability and robust generalization. Our work pushes forward the state of FMs in EEG and offers insights into scalable and generalizable neural information processing systems.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2510.16382.pdf' target='_blank'>https://arxiv.org/pdf/2510.16382.pdf</a></span>   <span><a href='https://github.com/lambett/HSCM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ze Tao, Jian Zhang, Haowei Li, Xianshuai Li, Yifei Peng, Xiyao Liu, Senzhang Wang, Chao Liu, Sheng Ren, Shichao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16382">Humanoid-inspired Causal Representation Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a novel causal framework inspired by human intelligence, designed to overcome the limitations of conventional domain generalization models. Unlike approaches that rely on statistics to capture data-label dependencies and learn distortion-invariant representations, HSCM replicates the hierarchical processing and multi-level learning of human vision systems, focusing on modeling fine-grained causal mechanisms. By disentangling and reweighting key image attributes such as color, texture, and shape, HSCM enhances generalization across diverse domains, ensuring robust performance and interpretability. Leveraging the flexibility and adaptability of human intelligence, our approach enables more effective transfer and learning in dynamic, complex environments. Through both theoretical and empirical evaluations, we demonstrate that HSCM outperforms existing domain generalization models, providing a more principled method for capturing causal relationships and improving model robustness. The code is available at https://github.com/lambett/HSCM.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2510.12687.pdf' target='_blank'>https://arxiv.org/pdf/2510.12687.pdf</a></span>   <span><a href='https://github.com/KPeng9510/ERELIFM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/KPeng9510/ERELIFM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Peng, Di Wen, Kailun Yang, Jia Fu, Yufan Chen, Ruiping Liu, Jiamin Wu, Junwei Zheng, M. Saquib Sarfraz, Luc Van Gool, Danda Pani Paudel, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12687">EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Set Domain Generalization (OSDG) aims to enable deep learning models to recognize unseen categories in new domains, which is crucial for real-world applications. Label noise hinders open-set domain generalization by corrupting source-domain knowledge, making it harder to recognize known classes and reject unseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL) using hyperbolic prototype-guided meta-learning, they struggle to bridge domain gaps, especially with limited clean labeled data. In this paper, we propose Evidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first introduce an unsupervised two-stage evidential loss clustering method to promote label reliability awareness. Then, we propose a residual flow matching mechanism that models structured domain- and category-conditioned residuals, enabling diverse and uncertainty-aware transfer paths beyond interpolation-based augmentation. During this meta-learning process, the model is optimized such that the update direction on the clean set maximizes the loss decrease on the noisy set, using pseudo labels derived from the most confident predicted class for supervision. Experimental results show that EReLiFM outperforms existing methods on OSDG-NL, achieving state-of-the-art performance. The source code is available at https://github.com/KPeng9510/ERELIFM.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2510.12070.pdf' target='_blank'>https://arxiv.org/pdf/2510.12070.pdf</a></span>   <span><a href='https://github.com/ku-milab/Measure' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangmin Jo, Jee Seok Yoon, Wootaek Jeong, Kwanseok Oh, Heung-Il Suk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12070">MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based automatic sleep staging has significantly advanced in performance and plays a crucial role in the diagnosis of sleep disorders. However, those models often struggle to generalize on unseen subjects due to variability in physiological signals, resulting in degraded performance in out-of-distribution scenarios. To address this issue, domain generalization approaches have recently been studied to ensure generalized performance on unseen domains during training. Among those techniques, contrastive learning has proven its validity in learning domain-invariant features by aligning samples of the same class across different domains. Despite its potential, many existing methods are insufficient to extract adequately domain-invariant representations, as they do not explicitly address domain characteristics embedded within the unshared information across samples. In this paper, we posit that mitigating such domain-relevant attributes-referred to as excess domain-relevant information-is key to bridging the domain gap. However, the direct strategy to mitigate the domain-relevant attributes often overfits features at the high-level information, limiting their ability to leverage the diverse temporal and spectral information encoded in the multiple feature levels. To address these limitations, we propose a novel MEASURE (Multi-scalE minimAl SUfficient Representation lEarning) framework, which effectively reduces domain-relevant information while preserving essential temporal and spectral features for sleep stage classification. In our exhaustive experiments on publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS, our proposed method consistently outperformed state-of-the-art methods. Our code is available at : https://github.com/ku-milab/Measure
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2510.08575.pdf' target='_blank'>https://arxiv.org/pdf/2510.08575.pdf</a></span>   <span><a href='https://haofeixu.github.io/resplat/' target='_blank'>  GitHub</a></span> <span><a href='https://haofeixu.github.io/resplat/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haofei Xu, Daniel Barath, Andreas Geiger, Marc Pollefeys
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08575">ReSplat: Learning Recurrent Gaussian Splats</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \times$ subsampled space, producing $16 \times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \times 256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2510.08132.pdf' target='_blank'>https://arxiv.org/pdf/2510.08132.pdf</a></span>   <span><a href='https://kodaikawamura.github.io/Domain_Unlearning/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kodai Kawamura, Yuta Goto, Rintaro Yanagi, Hirokatsu Kataoka, Go Irie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08132">Approximate Domain Unlearning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on class unlearning, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize real cars while avoiding misrecognition of illustrated cars depicted in roadside advertisements as real cars, which could be hazardous. In this paper, we introduce Approximate Domain Unlearning (ADU), a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., illustration) while preserving accuracy for other domains (e.g., real). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments show that our approach outperforms baselines built upon VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code: https://kodaikawamura.github.io/Domain_Unlearning/.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2510.00237.pdf' target='_blank'>https://arxiv.org/pdf/2510.00237.pdf</a></span>   <span><a href='https://github.com/XiaofengLin7/debunking-sft-generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Lin, Hejian Sang, Zhipeng Wang, Xuezhou Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00237">Debunk the Myth of SFT Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A prevailing view holds that supervised fine-tuning (SFT) memorizes training data and fails to generalize, whereas reinforcement learning (RL) attains broader robustness. We revisit this claim through a systematic evaluation on two decision-making benchmarks, Sokoban and General Points, and arrive at a different conclusion. We show that much of SFT's perceived failure stems from frozen-prompt artifacts: when trained on fixed instruction templates, SFT models cling to training semantics rather than adapting to new ones. Introducing prompt diversity during training breaks this shortcut and yields strong generalization to unseen instruction variants without harming in-distribution performance. Beyond instruction shifts, we ask whether SFT can generalize to strictly harder tasks. Here, chain-of-thought (CoT) supervision provides an algorithmic scaffold that markedly improves transfer to more difficult regimes, such as larger Sokoban grids with additional boxes and arithmetic with out-of-distribution values or five-card compositions that increase combinatorial complexity. Finally, combining prompt diversity with CoT achieves the best of both worlds: robust generalization across both instruction-variant and difficulty-variant settings, matching or surpassing RL baselines on our benchmarks while retaining SFT's simplicity and stability. These findings challenge the narrative that SFT is inherently inferior to RL and support a data-centric perspective: with appropriately curated demonstrations, vanilla SFT can generalize as strongly as RL. Code reproducing the results in the paper can be found at: https://github.com/XiaofengLin7/debunking-sft-generalization.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2509.26062.pdf' target='_blank'>https://arxiv.org/pdf/2509.26062.pdf</a></span>   <span><a href='https://github.com/wyf23187/DyFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Zixiang Xu, Yue Huang, Xiangqi Wang, Zirui Song, Lang Gao, Chenxi Wang, Xiangru Tang, Yue Zhao, Arman Cohan, Xiangliang Zhang, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26062">DyFlow: Dynamic Workflow Framework for Agentic Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent systems based on large language models (LLMs) have shown great potential in complex reasoning tasks, but building efficient and generalizable workflows remains a major challenge. Most existing approaches rely on manually designed processes, which limits their adaptability across different tasks. While a few methods attempt automated workflow generation, they are often tied to specific datasets or query types and make limited use of intermediate feedback, reducing system robustness and reasoning depth. Moreover, their operations are typically predefined and inflexible. To address these limitations, we propose DyFlow, a dynamic workflow generation framework that adaptively constructs and adjusts reasoning procedures based on task requirements and real-time intermediate feedback, thereby enhancing cross-task generalization. DyFlow consists of two core components: a designer and an executor. The designer decomposes complex problems into a sequence of sub-goals defined by high-level objectives and dynamically plans the next steps based on intermediate outputs and feedback. These plans are then carried out by the executor, which executes each operation using dynamic operators with context-aware parameterization, enabling flexible and semantically grounded reasoning. We systematically evaluate DyFlow across diverse domains, including social reasoning, biomedical tasks, mathematical problem solving, and code generation. Results demonstrate that DyFlow significantly outperforms existing baselines, achieving substantial Pass@k improvements and exhibiting robust generalization across diverse domains. The code is publicly available at https://github.com/wyf23187/DyFlow.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2509.25172.pdf' target='_blank'>https://arxiv.org/pdf/2509.25172.pdf</a></span>   <span><a href='https://yuxinn-j.github.io/projects/PICO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Jiang, Yuchao Gu, Yiren Song, Ivor Tsang, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25172">Personalized Vision via Visual In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern vision models, trained on large-scale annotated datasets, excel at predefined tasks but struggle with personalized vision -- tasks defined at test time by users with customized objects or novel objectives. Existing personalization approaches rely on costly fine-tuning or synthetic data pipelines, which are inflexible and restricted to fixed task formats. Visual in-context learning (ICL) offers a promising alternative, yet prior methods confine to narrow, in-domain tasks and fail to generalize to open-ended personalization. We introduce Personalized In-Context Operator (PICO), a simple four-panel framework that repurposes diffusion transformers as visual in-context learners. Given a single annotated exemplar, PICO infers the underlying transformation and applies it to new inputs without retraining. To enable this, we construct VisRel, a compact yet diverse tuning dataset, showing that task diversity, rather than scale, drives robust generalization. We further propose an attention-guided seed scorer that improves reliability via efficient inference scaling. Extensive experiments demonstrate that PICO (i) surpasses fine-tuning and synthetic-data baselines, (ii) flexibly adapts to novel user-defined tasks, and (iii) generalizes across both recognition and generation.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2509.23616.pdf' target='_blank'>https://arxiv.org/pdf/2509.23616.pdf</a></span>   <span><a href='https://github.com/flzeng1/GraphIFE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanlong Zeng, Wensheng Gan, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23616">GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The class imbalance problem refers to the disproportionate distribution of samples across different classes within a dataset, where the minority classes are significantly underrepresented. This issue is also prevalent in graph-structured data. Most graph neural networks (GNNs) implicitly assume a balanced class distribution and therefore often fail to account for the challenges introduced by class imbalance, which can lead to biased learning and degraded performance on minority classes. We identify a quality inconsistency problem in synthesized nodes, which leads to suboptimal performance under graph imbalance conditions. To mitigate this issue, we propose GraphIFE (Graph Invariant Feature Extraction), a novel framework designed to mitigate quality inconsistency in synthesized nodes. Our approach incorporates two key concepts from graph invariant learning and introduces strategies to strengthen the embedding space representation, thereby enhancing the model's ability to identify invariant features. Extensive experiments demonstrate the framework's efficiency and robust generalization, as GraphIFE consistently outperforms various baselines across multiple datasets. The code is publicly available at https://github.com/flzeng1/GraphIFE.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2509.21320.pdf' target='_blank'>https://arxiv.org/pdf/2509.21320.pdf</a></span>   <span><a href='https://github.com/open-sciencelab/SciReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21320">SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2509.18165.pdf' target='_blank'>https://arxiv.org/pdf/2509.18165.pdf</a></span>   <span><a href='https://github.com/XiudingCai/SIM-pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiuding Cai, Yaoyao Zhu, Linjie Fu, Dong Miao, Yu Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18165">Self Identity Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Regularization is essential in deep learning to enhance generalization and mitigate overfitting. However, conventional techniques often rely on heuristics, making them less reliable or effective across diverse settings. We propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic regularization framework that leverages an inverse mapping mechanism to enhance representation learning. By reconstructing the input from its transformed output, SIM reduces information loss during forward propagation and facilitates smoother gradient flow. To address computational inefficiencies, We instantiate SIM as $ Ï\text{SIM} $ by incorporating patch-level feature sampling and projection-based method to reconstruct latent features, effectively lowering complexity. As a model-agnostic, task-agnostic regularizer, SIM can be seamlessly integrated as a plug-and-play module, making it applicable to different network architectures and tasks. We extensively evaluate $Ï\text{SIM}$ across three tasks: image classification, few-shot prompt learning, and domain generalization. Experimental results show consistent improvements over baseline methods, highlighting $Ï\text{SIM}$'s ability to enhance representation learning across various tasks. We also demonstrate that $Ï\text{SIM}$ is orthogonal to existing regularization methods, boosting their effectiveness. Moreover, our results confirm that $Ï\text{SIM}$ effectively preserves semantic information and enhances performance in dense-to-dense tasks, such as semantic segmentation and image translation, as well as in non-visual domains including audio classification and time series anomaly detection. The code is publicly available at https://github.com/XiudingCai/SIM-pytorch.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2509.17925.pdf' target='_blank'>https://arxiv.org/pdf/2509.17925.pdf</a></span>   <span><a href='https://github.com/baiyou1234/SmaRT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhan Wang, Yifei Chen, Shuo Jiang, Wenjing Yu, Mingxuan Liu, Beining Wu, Jinying Zong, Feiwei Qin, Changmiao Wang, Qiyuan Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17925">SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable brain tumor segmentation in MRI is indispensable for treatment planning and outcome monitoring, yet models trained on curated benchmarks often fail under domain shifts arising from scanner and protocol variability as well as population heterogeneity. Such gaps are especially severe in low-resource and pediatric cohorts, where conventional test-time or source-free adaptation strategies often suffer from instability and structural inconsistency. We propose SmaRT, a style-modulated robust test-time adaptation framework that enables source-free cross-domain generalization. SmaRT integrates style-aware augmentation to mitigate appearance discrepancies, a dual-branch momentum strategy for stable pseudo-label refinement, and structural priors enforcing consistency, integrity, and connectivity. This synergy ensures both adaptation stability and anatomical fidelity under extreme domain shifts. Extensive evaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT consistently outperforms state-of-the-art methods, with notable gains in Dice accuracy and boundary precision. Overall, SmaRT bridges the gap between algorithmic advances and equitable clinical applicability, supporting robust deployment of MRI-based neuro-oncology tools in diverse clinical environments. Our source code is available at https://github.com/baiyou1234/SmaRT.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2509.16063.pdf' target='_blank'>https://arxiv.org/pdf/2509.16063.pdf</a></span>   <span><a href='https://selen-suyue.github.io/DSPv2Net/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Su, Chubin Zhang, Sijin Chen, Liufan Tan, Yansong Tang, Jianan Wang, Xihui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16063">DSPv2: Improved Dense Policy for Effective and Generalizable Whole-body Mobile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning whole-body mobile manipulation via imitation is essential for generalizing robotic skills to diverse environments and complex tasks. However, this goal is hindered by significant challenges, particularly in effectively processing complex observation, achieving robust generalization, and generating coherent actions. To address these issues, we propose DSPv2, a novel policy architecture. DSPv2 introduces an effective encoding scheme that aligns 3D spatial features with multi-view 2D semantic features. This fusion enables the policy to achieve broad generalization while retaining the fine-grained perception necessary for precise control. Furthermore, we extend the Dense Policy paradigm to the whole-body mobile manipulation domain, demonstrating its effectiveness in generating coherent and precise actions for the whole-body robotic platform. Extensive experiments show that our method significantly outperforms existing approaches in both task performance and generalization ability. Project page is available at: https://selen-suyue.github.io/DSPv2Net/.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2509.15194.pdf' target='_blank'>https://arxiv.org/pdf/2509.15194.pdf</a></span>   <span><a href='https://github.com/YujunZhou/EVOL-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15194">Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing self-improvement approaches primarily rely on self-confirmation signals (e.g., confidence, entropy, or consistency) to generate rewards. This reliance drives models toward over-confident, majority-favored solutions, causing an entropy collapse that degrades pass@n and reasoning complexity. To address this, we propose EVOL-RL, a label-free framework that mirrors the evolutionary principle of balancing selection with variation. Concretely, EVOL-RL retains the majority-voted answer as an anchor for stability, but adds a novelty-aware reward that scores each sampled solution by how different its reasoning is from other concurrently generated responses. This majority-for-stability + novelty-for-exploration rule mirrors the variation-selection principle: selection prevents drift, while novelty prevents collapse. Evaluation results show that EVOL-RL consistently outperforms the majority-only baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from baseline's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents in-domain diversity collapse but also improves out-of-domain generalization (from math reasoning to broader tasks, e.g., GPQA, MMLU-Pro, and BBEH). The code is available at: https://github.com/YujunZhou/EVOL-RL.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2509.13172.pdf' target='_blank'>https://arxiv.org/pdf/2509.13172.pdf</a></span>   <span><a href='https://github.com/WHU-USI3DV/WHU-STree' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13172">WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Street trees are vital to urban livability, providing ecological and social benefits. Establishing a detailed, accurate, and dynamically updated street tree inventory has become essential for optimizing these multifunctional assets within space-constrained urban environments. Given that traditional field surveys are time-consuming and labor-intensive, automated surveys utilizing Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing MMS-acquired tree datasets are limited by small-scale scene, limited annotation, or single modality, restricting their utility for comprehensive analysis. To address these limitations, we introduce WHU-STree, a cross-city, richly annotated, and multi-modal urban street tree dataset. Collected across two distinct cities, WHU-STree integrates synchronized point clouds and high-resolution images, encompassing 21,007 annotated tree instances across 50 species and 2 morphological parameters. Leveraging the unique characteristics, WHU-STree concurrently supports over 10 tasks related to street tree inventory. We benchmark representative baselines for two key tasks--tree species classification and individual tree segmentation. Extensive experiments and in-depth analysis demonstrate the significant potential of multi-modal data fusion and underscore cross-domain applicability as a critical prerequisite for practical algorithm deployment. In particular, we identify key challenges and outline potential future works for fully exploiting WHU-STree, encompassing multi-modal fusion, multi-task collaboration, cross-domain generalization, spatial pattern learning, and Multi-modal Large Language Model for street tree asset management. The WHU-STree dataset is accessible at: https://github.com/WHU-USI3DV/WHU-STree.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2509.09674.pdf' target='_blank'>https://arxiv.org/pdf/2509.09674.pdf</a></span>   <span><a href='https://github.com/PRIME-RL/SimpleVLA-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09674">SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $Ï_0$ on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2509.09572.pdf' target='_blank'>https://arxiv.org/pdf/2509.09572.pdf</a></span>   <span><a href='https://github.com/dyzy41/PeftCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijun Dong, Yuxuan Hu, LiBo Wang, Geng Chen, Xiaoliang Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09572">PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2509.05592.pdf' target='_blank'>https://arxiv.org/pdf/2509.05592.pdf</a></span>   <span><a href='https://github.com/inclusionConf/MFFI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changtao Miao, Yi Zhang, Man Luo, Weiwei Feng, Kaiyuan Zheng, Qi Chu, Tao Gong, Jianshu Li, Yunfeng Diao, Wei Zhou, Joey Tianyi Zhou, Xiaoshuai Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05592">MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advances in Artificial Intelligence Generated Content (AIGC) have enabled increasingly sophisticated face forgeries, posing a significant threat to social security. However, current Deepfake detection methods are limited by constraints in existing datasets, which lack the diversity necessary in real-world scenarios. Specifically, these data sets fall short in four key areas: unknown of advanced forgery techniques, variability of facial scenes, richness of real data, and degradation of real-world propagation. To address these challenges, we propose the Multi-dimensional Face Forgery Image (\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation Operations. MFFI integrates $50$ different forgery methods and contains $1024K$ image samples. Benchmark evaluations show that MFFI outperforms existing public datasets in terms of scene complexity, cross-domain generalization capability, and detection difficulty gradients. These results validate the technical advance and practical utility of MFFI in simulating real-world conditions. The dataset and additional details are publicly available at {https://github.com/inclusionConf/MFFI}.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2509.00658.pdf' target='_blank'>https://arxiv.org/pdf/2509.00658.pdf</a></span>   <span><a href='https://meviuslab.github.io/Face4FairShifts/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Lin, Dong Li, Xintao Wu, Minglai Shao, Xujiang Zhao, Zhong Chen, Chen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00658">Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring fairness and robustness in machine learning models remains a challenge, particularly under domain shifts. We present Face4FairShifts, a large-scale facial image benchmark designed to systematically evaluate fairness-aware learning and domain generalization. The dataset includes 100,000 images across four visually distinct domains with 39 annotations within 14 attributes covering demographic and facial features. Through extensive experiments, we analyze model performance under distribution shifts and identify significant gaps. Our findings emphasize the limitations of existing related datasets and the need for more effective fairness-aware domain adaptation techniques. Face4FairShifts provides a comprehensive testbed for advancing equitable and reliable AI systems. The dataset is available online at https://meviuslab.github.io/Face4FairShifts/.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2509.00311.pdf' target='_blank'>https://arxiv.org/pdf/2509.00311.pdf</a></span>   <span><a href='https://github.com/hikmatkhan/MorphGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hikmat Khan, Syed Farhan Alam Zaidi, Pir Masoom Shah, Kiruthika Balakrishnan, Rabia Khan, Muhammad Waqas, Jia Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00311">MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization in computational histopathology is hindered by heterogeneity in whole slide images (WSIs), caused by variations in tissue preparation, staining, and imaging conditions across institutions. Unlike machine learning systems, pathologists rely on domain-invariant morphological cues such as nuclear atypia (enlargement, irregular contours, hyperchromasia, chromatin texture, spatial disorganization), structural atypia (abnormal architecture and gland formation), and overall morphological atypia that remain diagnostic across diverse settings. Motivated by this, we hypothesize that explicitly modeling biologically robust nuclear morphology and spatial organization will enable the learning of cancer representations that are resilient to domain shifts. We propose MorphGen (Morphology-Guided Generalization), a method that integrates histopathology images, augmentations, and nuclear segmentation masks within a supervised contrastive learning framework. By aligning latent representations of images and nuclear masks, MorphGen prioritizes diagnostic features such as nuclear and morphological atypia and spatial organization over staining artifacts and domain-specific features. To further enhance out-of-distribution robustness, we incorporate stochastic weight averaging (SWA), steering optimization toward flatter minima. Attention map analyses revealed that MorphGen primarily relies on nuclear morphology, cellular composition, and spatial cell organization within tumors or normal regions for final classification. Finally, we demonstrate resilience of the learned representations to image corruptions (such as staining artifacts) and adversarial attacks, showcasing not only OOD generalization but also addressing critical vulnerabilities in current deep learning systems for digital pathology. Code, datasets, and trained models are available at: https://github.com/hikmatkhan/MorphGen
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2508.20096.pdf' target='_blank'>https://arxiv.org/pdf/2508.20096.pdf</a></span>   <span><a href='https://github.com/OpenIXCLab/CODA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20096">CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2508.18440.pdf' target='_blank'>https://arxiv.org/pdf/2508.18440.pdf</a></span>   <span><a href='https://github.com/lars76/pitch-benchmark' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lars76/swift-f0,' target='_blank'>  GitHub</a></span> <span><a href='https://swift-f0.github.io/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lars Nieradzik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18440">SwiftF0: Fast and Accurate Monophonic Pitch Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and real-time monophonic pitch estimation in noisy conditions, particularly on resource-constrained devices, remains an open challenge in audio processing. We present \emph{SwiftF0}, a novel, lightweight neural model that sets a new state-of-the-art for monophonic pitch estimation. Through training on diverse speech, music, and synthetic datasets with extensive data augmentation, SwiftF0 achieves robust generalization across acoustic domains while maintaining computational efficiency. SwiftF0 achieves a 91.80\% harmonic mean (HM) at 10 dB SNR, outperforming baselines like CREPE by over 12 percentage points and degrading by only 2.3 points from clean audio. SwiftF0 requires only 95,842 parameters and runs approximately 42x faster than CREPE on CPU, making it ideal for efficient, real-time deployment. To address the critical lack of perfectly accurate ground truth pitch in speech corpora (which typically rely on algorithmic estimators or laryngograph signals), we introduce \emph{SpeechSynth}. This synthetic speech dataset, generated by a phoneme-level TTS model, provides exact, on-demand ground-truth pitch curves, enabling more robust model training and evaluation. Furthermore, we propose a unified metric, combining six complementary performance measures for comprehensive and reliable pitch evaluation, and release an open-source pitch benchmark suite. A live demo of SwiftF0 is available at https://swift-f0.github.io/, the source code at https://github.com/lars76/swift-f0, and the benchmark framework at https://github.com/lars76/pitch-benchmark.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2508.17054.pdf' target='_blank'>https://arxiv.org/pdf/2508.17054.pdf</a></span>   <span><a href='https://github.com/Kin-Zhang/DeltaFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingwen Zhang, Xiaomeng Zhu, Yushan Zhang, Yixi Cai, Olov Andersson, Patric Jensfelt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17054">DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($Î$Flow), a lightweight 3D framework that captures motion cues via a $Î$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2 and Waymo datasets show that $Î$Flow achieves state-of-the-art performance with up to 22% lower error and $2\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2508.15215.pdf' target='_blank'>https://arxiv.org/pdf/2508.15215.pdf</a></span>   <span><a href='https://github.com/Ben1001409/SleepDIFFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Wei Hao Chin, Yuin Torng Yew, Haocheng Wu, Lanxin Liang, Chow Khuen Chan, Norita Mohd Zain, Siti Balqis Samdin, Sim Kuan Goh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15215">Multi-Channel Differential Transformer for Cross-Domain Sleep Stage Classification with Heterogeneous EEG and EOG</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classification of sleep stages is essential for assessing sleep quality and diagnosing sleep disorders. However, manual inspection of EEG characteristics for each stage is time-consuming and prone to human error. Although machine learning and deep learning methods have been actively developed, they continue to face challenges arising from the non-stationarity and variability of electroencephalography (EEG) and electrooculography (EOG) signals across diverse clinical configurations, often resulting in poor generalization. In this work, we propose SleepDIFFormer, a multi-channel differential transformer framework for heterogeneous EEG-EOG representation learning. SleepDIFFormer is trained across multiple sleep staging datasets, each treated as a source domain, with the goal of generalizing to unseen target domains. Specifically, it employs a Multi-channel Differential Transformer Architecture (MDTA) designed to process raw EEG and EOG signals while incorporating cross-domain alignment. Our approach mitigates spatial and temporal attention noise and learns a domain-invariant EEG-EOG representation through feature distribution alignment across datasets, thereby enhancing generalization to new domains. Empirically, we evaluated SleepDIFFormer on five diverse sleep staging datasets under domain generalization settings and benchmarked it against existing approaches, achieving state-of-the-art performance. We further conducted a comprehensive ablation study and interpreted the differential attention weights, demonstrating their relevance to characteristic sleep EEG patterns. These findings advance the development of automated sleep stage classification and highlight its potential in quantifying sleep architecture and detecting abnormalities that disrupt restorative rest. Our source code and checkpoint are made publicly available at https://github.com/Ben1001409/SleepDIFFormer
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2508.12798.pdf' target='_blank'>https://arxiv.org/pdf/2508.12798.pdf</a></span>   <span><a href='https://chai-uk.github.io/ukairs25-causal-predictors/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Damian Machlanski, Stephanie Riley, Edward Moroshko, Kurt Butler, Panagiotis Dimitrakopoulos, Thomas Melistas, Akchunya Chanchal, Steven McDonagh, Ricardo Silva, Sotirios A. Tsaftaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12798">A Shift in Perspective on Causality in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promise that causal modelling can lead to robust AI generalization has been challenged in recent work on domain generalization (DG) benchmarks. We revisit the claims of the causality and DG literature, reconciling apparent contradictions and advocating for a more nuanced theory of the role of causality in generalization. We also provide an interactive demo at https://chai-uk.github.io/ukairs25-causal-predictors/.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2508.12137.pdf' target='_blank'>https://arxiv.org/pdf/2508.12137.pdf</a></span>   <span><a href='https://github.com/nikosips/infusing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos-Antonios Ypsilantis, Kaifeng Chen, AndrÃ© Araujo, OndÅej Chum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12137">Infusing fine-grained visual knowledge to Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale contrastive pre-training produces powerful Vision-and-Language Models (VLMs) capable of generating representations (embeddings) effective for a wide variety of visual and multimodal tasks. However, these pretrained embeddings remain suboptimal for fine-grained open-set visual retrieval, where state-of-the-art results require fine-tuning the vision encoder using annotated domain-specific samples. Naively performing such fine-tuning typically leads to catastrophic forgetting, severely diminishing the model's general-purpose visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve optimal balance between fine-grained domain adaptation and retention of the pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual learning literature, we systematically analyze standard regularization techniques aimed at knowledge retention and propose an efficient and effective combination strategy. Additionally, we address the commonly overlooked yet critical aspects of validation set design and hyperparameter tuning to ensure reproducibility and robust generalization across datasets and pretrained models. We extensively evaluate our method on both fine-grained and coarse-grained image-image and image-text retrieval benchmarks. Our approach consistently achieves strong results, notably retaining the visual-text alignment without utilizing any text data or the original text encoder during fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2508.11898.pdf' target='_blank'>https://arxiv.org/pdf/2508.11898.pdf</a></span>   <span><a href='https://github.com/1mather/omnid.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jilei Mao, Jiarui Guan, Yingjuan Tang, Qirui Hu, Zhihang Li, Junjie Yu, Yongjie Mao, Yunzhe Sun, Shuang Liu, Xiaozhu Ju
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11898">OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The visuomotor policy can easily overfit to its training datasets, such as fixed camera positions and backgrounds. This overfitting makes the policy perform well in the in-distribution scenarios but underperform in the out-of-distribution generalization. Additionally, the existing methods also have difficulty fusing multi-view information to generate an effective 3D representation. To tackle these issues, we propose Omni-Vision Diffusion Policy (OmniD), a multi-view fusion framework that synthesizes image observations into a unified bird's-eye view (BEV) representation. We introduce a deformable attention-based Omni-Feature Generator (OFG) to selectively abstract task-relevant features while suppressing view-specific noise and background distractions. OmniD achieves 11\%, 17\%, and 84\% average improvement over the best baseline model for in-distribution, out-of-distribution, and few-shot experiments, respectively. Training code and simulation benchmark are available: https://github.com/1mather/omnid.git
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2508.11695.pdf' target='_blank'>https://arxiv.org/pdf/2508.11695.pdf</a></span>   <span><a href='https://github.com/Anonymous-Name-139/RefAdgen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyun Chen, Weikai Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11695">RefAdGen: High-Fidelity Advertising Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Artificial Intelligence Generated Content (AIGC) techniques has unlocked opportunities in generating diverse and compelling advertising images based on referenced product images and textual scene descriptions. This capability substantially reduces human labor and production costs in traditional marketing workflows. However, existing AIGC techniques either demand extensive fine-tuning for each referenced image to achieve high fidelity, or they struggle to maintain fidelity across diverse products, making them impractical for e-commerce and marketing industries. To tackle this limitation, we first construct AdProd-100K, a large-scale advertising image generation dataset. A key innovation in its construction is our dual data augmentation strategy, which fosters robust, 3D-aware representations crucial for realistic and high-fidelity image synthesis. Leveraging this dataset, we propose RefAdGen, a generation framework that achieves high fidelity through a decoupled design. The framework enforces precise spatial control by injecting a product mask at the U-Net input, and employs an efficient Attention Fusion Module (AFM) to integrate product features. This design effectively resolves the fidelity-efficiency dilemma present in existing methods. Extensive experiments demonstrate that RefAdGen achieves state-of-the-art performance, showcasing robust generalization by maintaining high fidelity and remarkable visual results for both unseen products and challenging real-world, in-the-wild images. This offers a scalable and cost-effective alternative to traditional workflows. Code and datasets are publicly available at https://github.com/Anonymous-Name-139/RefAdgen.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2508.10729.pdf' target='_blank'>https://arxiv.org/pdf/2508.10729.pdf</a></span>   <span><a href='https://github.com/MyUniverse0726/EgoCross' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MyUniverse0726/EgoCross' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanjun Li, Yuqian Fu, Tianwen Qian, Qi'ao Xu, Silong Dai, Danda Pani Paudel, Luc Van Gool, Xiaoling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10729">EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Multimodal Large Language Models (MLLMs) have significantly pushed the frontier of egocentric video question answering (EgocentricQA). However, existing benchmarks and studies are mainly limited to common daily activities such as cooking and cleaning. In contrast, real-world deployment inevitably encounters domain shifts, where target domains differ substantially in both visual style and semantic content. To bridge this gap, we introduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate the cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four diverse and challenging domains, including surgery, industry, extreme sports, and animal perspective, representing realistic and high-impact application scenarios. It comprises approximately 1,000 QA pairs across 798 video clips, spanning four key QA tasks: prediction, recognition, localization, and counting. Each QA pair provides both OpenQA and CloseQA formats to support fine-grained evaluation. Extensive experiments show that most existing MLLMs, whether general-purpose or egocentric-specialized, struggle to generalize to domains beyond daily life, highlighting the limitations of current models. Furthermore, we conduct several pilot studies, \eg, fine-tuning and reinforcement learning, to explore potential improvements. We hope EgoCross and our accompanying analysis will serve as a foundation for advancing domain-adaptive, robust egocentric video understanding. Data and codes will be released at: \href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2508.10549.pdf' target='_blank'>https://arxiv.org/pdf/2508.10549.pdf</a></span>   <span><a href='https://github.com/boyiZheng99/PSScreen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyi Zheng, Qing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10549">PSScreen: Partially Supervised Multiple Retinal Disease Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging multiple partially labeled datasets to train a model for multiple retinal disease screening reduces the reliance on fully annotated datasets, but remains challenging due to significant domain shifts across training datasets from various medical sites, and the label absent issue for partial classes. To solve these challenges, we propose PSScreen, a novel Partially Supervised multiple retinal disease Screening model. Our PSScreen consists of two streams and one learns deterministic features and the other learns probabilistic features via uncertainty injection. Then, we leverage the textual guidance to decouple two types of features into disease-wise features and align them via feature distillation to boost the domain generalization ability. Meanwhile, we employ pseudo label consistency between two streams to address the label absent issue and introduce a self-distillation to transfer task-relevant semantics about known classes from the deterministic to the probabilistic stream to further enhance the detection performances. Experiments show that our PSScreen significantly enhances the detection performances on six retinal diseases and the normal state averagely and achieves state-of-the-art results on both in-domain and out-of-domain datasets. Codes are available at https://github.com/boyiZheng99/PSScreen.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2508.09926.pdf' target='_blank'>https://arxiv.org/pdf/2508.09926.pdf</a></span>   <span><a href='https://github.com/owkin/histoplus/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Adjadj, Pierre-Antoine Bannier, Guillaume Horent, Sebastien Mandela, Aurore Lyon, Kathryn Schutte, Ulysse Marteau, Valentin Gaury, Laura Dumont, Thomas Mathieu, MOSAIC consortium, Reda Belbahri, BenoÃ®t Schmauch, Eric Durand, Katharina Von Loga, Lucie Gillet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09926">Towards Comprehensive Cellular Characterisation of H&E slides</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell detection, segmentation and classification are essential for analyzing tumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing methods suffer from poor performance on understudied cell types (rare or not present in public datasets) and limited cross-domain generalization. To address these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei covering 13 cell types. In external validation across 4 independent cohorts, HistoPLUS outperforms current state-of-the-art models in detection quality by 5.2% and overall F1 classification score by 23.7%, while using 5x fewer parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types and brings significant improvements on 8 of 13 cell types. Moreover, we show that HistoPLUS robustly transfers to two oncology indications unseen during training. To support broader TME biomarker research, we release the model weights and inference code at https://github.com/owkin/histoplus/.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2508.09886.pdf' target='_blank'>https://arxiv.org/pdf/2508.09886.pdf</a></span>   <span><a href='https://universalcome.github.io/UniversalCOME/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyu Chen, Yawen Zeng, Yue Wang, Peng Wan, Guo-chen Ning, Hongen Liao, Daoqiang Zhang, Fang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09886">COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional single-dataset training often fails with new data distributions, especially in ultrasound (US) image analysis due to limited data, acoustic shadows, and speckle noise. Therefore, constructing a universal framework for multi-heterogeneous US datasets is imperative. However, a key challenge arises: how to effectively mitigate inter-dataset interference while preserving dataset-specific discriminative features for robust downstream task? Previous approaches utilize either a single source-specific decoder or a domain adaptation strategy, but these methods experienced a decline in performance when applied to other domains. Considering this, we propose a Universal Collaborative Mixture of Heterogeneous Source-Specific Experts (COME). Specifically, COME establishes dual structure-semantic shared experts that create a universal representation space and then collaborate with source-specific experts to extract discriminative features through providing complementary features. This design enables robust generalization by leveraging cross-datasets experience distributions and providing universal US priors for small-batch or unseen data scenarios. Extensive experiments under three evaluation modes (single-dataset, intra-organ, and inter-organ integration datasets) demonstrate COME's superiority, achieving significant mean AP improvements over state-of-the-art methods. Our project is available at: https://universalcome.github.io/UniversalCOME/.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2508.09547.pdf' target='_blank'>https://arxiv.org/pdf/2508.09547.pdf</a></span>   <span><a href='https://github.com/F1y1113/GoViG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengyi Wu, Yifei Dong, Zhi-Qi Cheng, Yilong Dai, Guangyu Chen, Hang Wang, Qi Dai, Alexander G. Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09547">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Goal-Conditioned Visual Navigation Instruction Generation (GoViG), a new task that aims to autonomously generate precise and contextually coherent navigation instructions solely from egocentric visual observations of initial and goal states. Unlike conventional approaches that rely on structured inputs such as semantic annotations or environmental maps, GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments. Our method addresses this task by decomposing it into two interconnected subtasks: (1) visual forecasting, which predicts intermediate visual states bridging the initial and goal views; and (2) instruction generation, which synthesizes linguistically coherent instructions grounded in both observed and anticipated visuals. These subtasks are integrated within an autoregressive multimodal large language model trained with tailored objectives to ensure spatial accuracy and linguistic clarity. Furthermore, we introduce two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, to mimic incremental human cognitive processes during navigation. To evaluate our method, we propose the R2R-Goal dataset, combining diverse synthetic and real-world trajectories. Empirical results demonstrate significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2508.08974.pdf' target='_blank'>https://arxiv.org/pdf/2508.08974.pdf</a></span>   <span><a href='https://github.com/Elman295/TCSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Elman Ghazaei, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08974">Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Earth's surface is constantly changing, and detecting these changes provides valuable insights that benefit various aspects of human society. While traditional change detection methods have been employed to detect changes from bi-temporal images, these approaches typically require expert knowledge for accurate interpretation. To enable broader and more flexible access to change information by non-expert users, the task of Change Detection Visual Question Answering (CDVQA) has been introduced. However, existing CDVQA methods have been developed under the assumption that training and testing datasets share similar distributions. This assumption does not hold in real-world applications, where domain shifts often occur. In this paper, the CDVQA task is revisited with a focus on addressing domain shift. To this end, a new multi-modal and multi-domain dataset, BrightVQA, is introduced to facilitate domain generalization research in CDVQA. Furthermore, a novel state space model, termed Text-Conditioned State Space Model (TCSSM), is proposed. The TCSSM framework is designed to leverage both bi-temporal imagery and geo-disaster-related textual information in an unified manner to extract domain-invariant features across domains. Input-dependent parameters existing in TCSSM are dynamically predicted by using both bi-temporal images and geo-disaster-related description, thereby facilitating the alignment between bi-temporal visual data and the associated textual descriptions. Extensive experiments are conducted to evaluate the proposed method against state-of-the-art models, and superior performance is consistently demonstrated. The code and dataset will be made publicly available upon acceptance at https://github.com/Elman295/TCSSM.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2508.05213.pdf' target='_blank'>https://arxiv.org/pdf/2508.05213.pdf</a></span>   <span><a href='https://github.com/ljm198134/TVGTANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianming Liu, Wenlong Qiu, Haitao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05213">Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-Shot Segmentation(FSS) aims to efficient segmentation of new objects with few labeled samples. However, its performance significantly degrades when domain discrepancies exist between training and deployment. Cross-Domain Few-Shot Segmentation(CD-FSS) is proposed to mitigate such performance degradation. Current CD-FSS methods primarily sought to develop segmentation models on a source domain capable of cross-domain generalization. However, driven by escalating concerns over data privacy and the imperative to minimize data transfer and training expenses, the development of source-free CD-FSS approaches has become essential. In this work, we propose a source-free CD-FSS method that leverages both textual and visual information to facilitate target domain task adaptation without requiring source domain data. Specifically, we first append Task-Specific Attention Adapters (TSAA) to the feature pyramid of a pretrained backbone, which adapt multi-level features extracted from the shared pre-trained backbone to the target task. Then, the parameters of the TSAA are trained through a Visual-Visual Embedding Alignment (VVEA) module and a Text-Visual Embedding Alignment (TVEA) module. The VVEA module utilizes global-local visual features to align image features across different views, while the TVEA module leverages textual priors from pre-aligned multi-modal features (e.g., from CLIP) to guide cross-modal adaptation. By combining the outputs of these modules through dense comparison operations and subsequent fusion via skip connections, our method produces refined prediction masks. Under both 1-shot and 5-shot settings, the proposed approach achieves average segmentation accuracy improvements of 2.18\% and 4.11\%, respectively, across four cross-domain datasets, significantly outperforming state-of-the-art CD-FSS methods. Code are available at https://github.com/ljm198134/TVGTANet.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2508.03982.pdf' target='_blank'>https://arxiv.org/pdf/2508.03982.pdf</a></span>   <span><a href='https://github.com/uponacceptance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinwei Zhang, Lianrui Zuo, Blake E. Dewey, Samuel W. Remedios, Yihao Liu, Savannah P. Hays, Dzung L. Pham, Ellen M. Mowry, Scott D. Newsome, Peter A. Calabresi, Aaron Carass, Jerry L. Prince
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03982">UNISELF: A Unified Network with Instance Normalization and Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated segmentation of multiple sclerosis (MS) lesions using multicontrast magnetic resonance (MR) images improves efficiency and reproducibility compared to manual delineation, with deep learning (DL) methods achieving state-of-the-art performance. However, these DL-based methods have yet to simultaneously optimize in-domain accuracy and out-of-domain generalization when trained on a single source with limited data, or their performance has been unsatisfactory. To fill this gap, we propose a method called UNISELF, which achieves high accuracy within a single training domain while demonstrating strong generalizability across multiple out-of-domain test datasets. UNISELF employs a novel test-time self-ensembled lesion fusion to improve segmentation accuracy, and leverages test-time instance normalization (TTIN) of latent features to address domain shifts and missing input contrasts. Trained on the ISBI 2015 longitudinal MS segmentation challenge training dataset, UNISELF ranks among the best-performing methods on the challenge test dataset. Additionally, UNISELF outperforms all benchmark methods trained on the same ISBI training data across diverse out-of-domain test datasets with domain shifts and missing contrasts, including the public MICCAI 2016 and UMCL datasets, as well as a private multisite dataset. These test datasets exhibit domain shifts and/or missing contrasts caused by variations in acquisition protocols, scanner types, and imaging artifacts arising from imperfect acquisition. Our code is available at https://github.com/uponacceptance.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2508.02314.pdf' target='_blank'>https://arxiv.org/pdf/2508.02314.pdf</a></span>   <span><a href='https://github.com/AI4Wireless/LAM4PHY_6G' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajia Guo, Yiming Cui, Shi Jin, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02314">Large AI Models for Wireless Physical Layer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large artificial intelligence models (LAMs) are transforming wireless physical layer technologies through their robust generalization, multitask processing, and multimodal capabilities. This article reviews recent advancements in LAM applications for physical layer communications, addressing limitations of conventional AI-based approaches. LAM applications are classified into two strategies: leveraging pre-trained LAMs and developing native LAMs designed specifically for physical layer tasks. The motivations and key frameworks of these approaches are comprehensively examined through multiple use cases. Both strategies significantly improve performance and adaptability across diverse wireless scenarios. Future research directions, including efficient architectures, interpretability, standardized datasets, and collaboration between large and small models, are proposed to advance LAM-based physical layer solutions for next-generation communication systems.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2508.01731.pdf' target='_blank'>https://arxiv.org/pdf/2508.01731.pdf</a></span>   <span><a href='https://github.com/YuxiangZhang-BIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Zhang, Wei Li, Mengmeng Zhang, Jiawei Han, Ran Tao, Shunlin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01731">SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Remote Sensing Foundation Models (RSFMs) have led to significant breakthroughs in the field. While many RSFMs have been pretrained with massive optical imagery, more multispectral/hyperspectral data remain lack of the corresponding foundation models. To leverage the advantages of spectral imagery in earth observation, we explore whether existing RSFMs can be effectively adapted to process diverse spectral modalities without requiring extensive spectral pretraining. In response to this challenge, we proposed SpectralX, an innovative parameter-efficient fine-tuning framework that adapt existing RSFMs as backbone while introducing a two-stage training approach to handle various spectral inputs, thereby significantly improving domain generalization performance. In the first stage, we employ a masked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) to extract attribute tokens from both spatial and spectral dimensions. Simultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA) that dynamically aggregates multi-attribute expert knowledge while performing layer-wise fine-tuning. With semantic segmentation as downstream task in the second stage, we insert an Attribute-refined Adapter (Are-adapter) into the first stage framework. By iteratively querying low-level semantic features with high-level representations, the model learns to focus on task-beneficial attributes, enabling customized adjustment of RSFMs. Following this two-phase adaptation process, SpectralX is capable of interpreting spectral imagery from new regions or seasons. The codes will be available from the website: https://github.com/YuxiangZhang-BIT.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2508.01667.pdf' target='_blank'>https://arxiv.org/pdf/2508.01667.pdf</a></span>   <span><a href='https://github.com/wloves/Rein' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixiang Wei, Xiaoxiao Ma, Ruishen Yan, Tao Tu, Huaian Chen, Jinjin Zheng, Yi Jin, Enhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01667">Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Foundation Models(VFMs) have achieved remarkable success in various computer vision tasks. However, their application to semantic segmentation is hindered by two significant challenges: (1) the disparity in data scale, as segmentation datasets are typically much smaller than those used for VFM pre-training, and (2) domain distribution shifts, where real-world segmentation scenarios are diverse and often underrepresented during pre-training. To overcome these limitations, we present Rein++, an efficient VFM-based segmentation framework that demonstrates superior generalization from limited data and enables effective adaptation to diverse unlabeled scenarios. Specifically, Rein++ comprises a domain generalization solution Rein-G and a domain adaptation solution Rein-A. Rein-G introduces a set of trainable, instance-aware tokens that effectively refine the VFM's features for the segmentation task. This parameter-efficient approach fine-tunes less than 1% of the backbone's parameters, enabling robust generalization. Building on the Rein-G, Rein-A performs unsupervised domain adaptation at both the instance and logit levels to mitigate domain shifts. In addition, it incorporates a semantic transfer module that leverages the class-agnostic capabilities of the segment anything model to enhance boundary details in the target domain. The integrated Rein++ pipeline first learns a generalizable model on a source domain (e.g., daytime scenes) and subsequently adapts it to diverse target domains (e.g., nighttime scenes) without any target labels. Comprehensive experiments demonstrate that Rein++ significantly outperforms state-of-the-art methods with efficient training, underscoring its roles an efficient, generalizable, and adaptive segmentation solution for VFMs, even for large models with billions of parameters. The code is available at https://github.com/wloves/Rein.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2507.21786.pdf' target='_blank'>https://arxiv.org/pdf/2507.21786.pdf</a></span>   <span><a href='https://github.com/Rain-Bus/MSGCoOp' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Rain-Bus/MSGCoOp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaolong Wang, Tongfeng Sun, Mingzheng Du, Yachao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21786">MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language pre-trained models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, and prompt learning has emerged as an efficient alternative to full fine-tuning. However, existing methods often struggle with generalization to novel classes, a phenomenon attributed to overfitting on seen classes and forgetting general knowledge. Furthermore, recent approaches that improve generalization often introduce complex architectures or heavy computational overhead. In this paper, we propose a Multiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance few-shot generalization while maintaining computational efficiency. Our approach leverages an ensemble of parallel learnable context vectors to capture diverse semantic aspects. To enrich these prompts, we introduce a semantic guidance mechanism that aligns them with comprehensive class descriptions automatically generated by a Large Language Model (LLM). Furthermore, a diversity regularization loss encourages the prompts to learn complementary and orthogonal features, preventing them from collapsing into redundant representations. Extensive experiments on 11 benchmark datasets show that MSGCoOp significantly improves performance on base-to-novel generalization, achieving an average harmonic mean improvement of 1.10\% over the strong KgCoOp baseline. Our method also demonstrates enhanced robustness in cross-domain generalization tasks. Our code is avaliable at: \href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2507.21745.pdf' target='_blank'>https://arxiv.org/pdf/2507.21745.pdf</a></span>   <span><a href='https://github.com/aybora/FewShotReasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aybora Koksal, A. Aydin Alatan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21745">Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language and vision-language models have enabled strong reasoning capabilities, yet they remain impractical for specialized domains like remote sensing, where annotated data is scarce and expensive. We present the first few-shot reinforcement learning with verifiable reward (RLVR) framework for satellite imagery that eliminates the need for caption supervision--relying solely on lightweight, rule-based binary or IoU-based rewards. Adapting the "1-shot RLVR" paradigm from language models to vision-language models, we employ policy-gradient optimization with as few as one curated example to align model outputs for satellite reasoning tasks. Comprehensive experiments across multiple remote sensing benchmarks--including classification, visual question answering, and grounding--show that even a single example yields substantial improvements over the base model. Scaling to 128 examples matches or exceeds models trained on thousands of annotated samples. While the extreme one-shot setting can induce mild, task-specific overfitting, our approach consistently demonstrates robust generalization and efficiency across diverse tasks. Further, we find that prompt design and loss weighting significantly influence training stability and final accuracy. Our method enables cost-effective and data-efficient development of domain-specialist vision-language reasoning models, offering a pragmatic recipe for data-scarce fields: start from a compact VLM, curate a handful of reward-checkable cases, and train via RLVR.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2507.19950.pdf' target='_blank'>https://arxiv.org/pdf/2507.19950.pdf</a></span>   <span><a href='https://github.com/zhengcy-lambo/RARE.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyu Zheng, Jin Huang, Honghua Chen, Mingqiang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19950">RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research leveraging large-scale pretrained diffusion models has demonstrated the potential of using diffusion features to establish semantic correspondences in images. Inspired by advancements in diffusion-based techniques, we propose a novel zero-shot method for refining point cloud registration algorithms. Our approach leverages correspondences derived from depth images to enhance point feature representations, eliminating the need for a dedicated training dataset. Specifically, we first project the point cloud into depth maps from multiple perspectives and extract implicit knowledge from a pretrained diffusion network as depth diffusion features. These features are then integrated with geometric features obtained from existing methods to establish more accurate correspondences between point clouds. By leveraging these refined correspondences, our approach achieves significantly improved registration accuracy. Extensive experiments demonstrate that our method not only enhances the performance of existing point cloud registration techniques but also exhibits robust generalization capabilities across diverse datasets. Codes are available at https://github.com/zhengcy-lambo/RARE.git.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2507.17312.pdf' target='_blank'>https://arxiv.org/pdf/2507.17312.pdf</a></span>   <span><a href='https://github.com/pq-chen/CasP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiqi Chen, Lei Yu, Yi Wan, Yingying Pei, Xinyi Liu, Yongxiang Yao, Yingying Zhang, Lixiang Ru, Liheng Zhong, Jingdong Chen, Ming Yang, Yongjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17312">CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-dense feature matching methods have shown strong performance in challenging scenarios. However, the existing pipeline relies on a global search across the entire feature map to establish coarse matches, limiting further improvements in accuracy and efficiency. Motivated by this limitation, we propose a novel pipeline, CasP, which leverages cascaded correspondence priors for guidance. Specifically, the matching stage is decomposed into two progressive phases, bridged by a region-based selective cross-attention mechanism designed to enhance feature discriminability. In the second phase, one-to-one matches are determined by restricting the search range to the one-to-many prior areas identified in the first phase. Additionally, this pipeline benefits from incorporating high-level features, which helps reduce the computational costs of low-level feature extraction. The acceleration gains of CasP increase with higher resolution, and our lite model achieves a speedup of $\sim2.2\times$ at a resolution of 1152 compared to the most efficient method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority in geometric estimation, particularly with impressive cross-domain generalization. These advantages highlight its potential for latency-sensitive and high-robustness applications, such as SLAM and UAV systems. Code is available at https://github.com/pq-chen/CasP.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2507.15037.pdf' target='_blank'>https://arxiv.org/pdf/2507.15037.pdf</a></span>   <span><a href='https://github.com/Jerome-Young/OmniVTON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaotong Yang, Yuhui Li, Shengfeng He, Xinzhe Li, Yangyang Xu, Junyu Dong, Yong Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15037">OmniVTON: Training-Free Universal Virtual Try-On</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based Virtual Try-On (VTON) techniques rely on either supervised in-shop approaches, which ensure high fidelity but struggle with cross-domain generalization, or unsupervised in-the-wild methods, which improve adaptability but remain constrained by data biases and limited universality. A unified, training-free solution that works across both scenarios remains an open challenge. We propose OmniVTON, the first training-free universal VTON framework that decouples garment and pose conditioning to achieve both texture fidelity and pose consistency across diverse settings. To preserve garment details, we introduce a garment prior generation mechanism that aligns clothing with the body, followed by continuous boundary stitching technique to achieve fine-grained texture retention. For precise pose alignment, we utilize DDIM inversion to capture structural cues while suppressing texture interference, ensuring accurate body alignment independent of the original image textures. By disentangling garment and pose constraints, OmniVTON eliminates the bias inherent in diffusion models when handling multiple conditions simultaneously. Experimental results demonstrate that OmniVTON achieves superior performance across diverse datasets, garment types, and application scenarios. Notably, it is the first framework capable of multi-human VTON, enabling realistic garment transfer across multiple individuals in a single scene. Code is available at https://github.com/Jerome-Young/OmniVTON
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2507.14935.pdf' target='_blank'>https://arxiv.org/pdf/2507.14935.pdf</a></span>   <span><a href='https://github.com/haihuangcode/CMG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Huang, Yan Xia, Shulei Wang, Hanting Wang, Minghui Fang, Shengpeng Ji, Sashuai Zhou, Tao Jin, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14935">Open-set Cross Modal Generalization via Multimodal Unified Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper extends Cross Modal Generalization (CMG) to open-set environments by proposing the more challenging Open-set Cross Modal Generalization (OSCMG) task. This task evaluates multimodal unified representations in open-set conditions, addressing the limitations of prior closed-set cross-modal evaluations. OSCMG requires not only cross-modal knowledge transfer but also robust generalization to unseen classes within new modalities, a scenario frequently encountered in real-world applications. Existing multimodal unified representation work lacks consideration for open-set environments. To tackle this, we propose MICU, comprising two key components: Fine-Coarse Masked multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI enhances multimodal alignment by applying contrastive learning at both holistic semantic and temporal levels, incorporating masking to enhance generalization. CUJP enhances feature diversity and model uncertainty by integrating modality-agnostic feature selection with self-supervised learning, thereby strengthening the model's ability to handle unknown categories in open-set tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the effectiveness of our approach. The code is available at https://github.com/haihuangcode/CMG.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2507.12851.pdf' target='_blank'>https://arxiv.org/pdf/2507.12851.pdf</a></span>   <span><a href='https://github.com/bitPrincy/SRE-DG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wang, Zhi Gao, Jin Chen, Qingjie Zhao, Xinxiao Wu, Jiebo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12851">Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn a model from source domains and apply it to unseen target domains with out-of-distribution data. Owing to CLIP's strong ability to encode semantic concepts, it has attracted increasing interest in domain generalization. However, CLIP often struggles to focus on task-relevant regions across domains, i.e., domain-invariant regions, resulting in suboptimal performance on unseen target domains. To address this challenge, we propose an attention-refocusing scheme, called Simulate, Refocus and Ensemble (SRE), which learns to reduce the domain shift by aligning the attention maps in CLIP via attention refocusing. SRE first simulates domain shifts by performing augmentation on the source data to generate simulated target domains. SRE then learns to reduce the domain shifts by refocusing the attention in CLIP between the source and simulated target domains. Finally, SRE utilizes ensemble learning to enhance the ability to capture domain-invariant attention maps between the source data and the simulated target data. Extensive experimental results on several datasets demonstrate that SRE generally achieves better results than state-of-the-art methods. The code is available at: https://github.com/bitPrincy/SRE-DG.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2507.12060.pdf' target='_blank'>https://arxiv.org/pdf/2507.12060.pdf</a></span>   <span><a href='https://kunkunlin1221.github.io/InstructFLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun-Hsiang Lin, Yu-Wen Tseng, Kang-Yang Huang, Jhih-Ciang Wu, Wen-Huang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12060">InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face anti-spoofing (FAS) aims to construct a robust system that can withstand diverse attacks. While recent efforts have concentrated mainly on cross-domain generalization, two significant challenges persist: limited semantic understanding of attack types and training redundancy across domains. We address the first by integrating vision-language models (VLMs) to enhance the perception of visual input. For the second challenge, we employ a meta-domain strategy to learn a unified model that generalizes well across multiple domains. Our proposed InstructFLIP is a novel instruction-tuned framework that leverages VLMs to enhance generalization via textual guidance trained solely on a single domain. At its core, InstructFLIP explicitly decouples instructions into content and style components, where content-based instructions focus on the essential semantics of spoofing, and style-based instructions consider variations related to the environment and camera characteristics. Extensive experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA models in accuracy and substantially reducing training redundancy across diverse domains in FAS. Project website is available at https://kunkunlin1221.github.io/InstructFLIP.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2507.09681.pdf' target='_blank'>https://arxiv.org/pdf/2507.09681.pdf</a></span>   <span><a href='https://osherr1996.github.io/prompt2dem_propage/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Osher Rafaeli, Tal Svoray, Ariel Nahlieli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09681">Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-resolution elevation estimations are essential to understand catchment and hillslope hydrology, study urban morphology and dynamics, and monitor the growth, decline, and mortality of terrestrial ecosystems. Various deep learning approaches (e.g., super-resolution techniques, monocular depth estimation) have been developed to create high-resolution Digital Elevation Models (DEMs). However, super-resolution techniques are limited by the upscaling factor, and monocular depth estimation lacks global elevation context, making its conversion to a seamless DEM restricted. The recently introduced technique of prompt-based monocular depth estimation has opened new opportunities to extract estimates of absolute elevation in a global context. We present here a framework for the estimation of high-resolution DEMs as a new paradigm for absolute global elevation mapping. It is exemplified using low-resolution Shuttle Radar Topography Mission (SRTM) elevation data as prompts and high-resolution RGB imagery from the National Agriculture Imagery Program (NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived DEMs and employs a versatile prompting strategy, enabling tasks such as DEM estimation, void filling, and updating. Our framework achieves a 100x resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of magnitude. Evaluations across three diverse U.S. landscapes show robust generalization, capturing urban structures and fine-scale terrain features with < 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological analysis confirms suitability for hazard and environmental studies. We demonstrate scalability by applying the framework to large regions in the U.S. and Israel. All code and pretrained models are publicly available at: https://osherr1996.github.io/prompt2dem_propage/.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2507.08340.pdf' target='_blank'>https://arxiv.org/pdf/2507.08340.pdf</a></span>   <span><a href='https://github.com/HopkinsKwong/MCCSDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08340">Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at https://github.com/HopkinsKwong/MCCSDG
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2507.06230.pdf' target='_blank'>https://arxiv.org/pdf/2507.06230.pdf</a></span>   <span><a href='https://visinf.github.io/scenedino' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tum-vision/scenedino' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar JevtiÄ, Christoph Reich, Felix Wimbauer, Oliver Hahn, Christian Rupprecht, Stefan Roth, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06230">Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in a feed-forward manner. Through a novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of a current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards a strong foundation for single image 3D scene understanding.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2507.06146.pdf' target='_blank'>https://arxiv.org/pdf/2507.06146.pdf</a></span>   <span><a href='https://github.com/00why00/PFCD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/00why00/PFCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang, Lei Zhang, Wei Wei, Chen Ding, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06146">Prompt-Free Conditional Diffusion for Multi-object Image Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models has underpinned much recent advances of dataset augmentation in various computer vision tasks. However, when involving generating multi-object images as real scenarios, most existing methods either rely entirely on text condition, resulting in a deviation between the generated objects and the original data, or rely too much on the original images, resulting in a lack of diversity in the generated images, which is of limited help to downstream tasks. To mitigate both problems with one stone, we propose a prompt-free conditional diffusion framework for multi-object image augmentation. Specifically, we introduce a local-global semantic fusion strategy to extract semantics from images to replace text, and inject knowledge into the diffusion model through LoRA to alleviate the category deviation between the original model and the target dataset. In addition, we design a reward model based counting loss to assist the traditional reconstruction loss for model training. By constraining the object counts of each category instead of pixel-by-pixel constraints, bridging the quantity deviation between the generated data and the original data while improving the diversity of the generated data. Experimental results demonstrate the superiority of the proposed method over several representative state-of-the-art baselines and showcase strong downstream task gain and out-of-domain generalization capabilities. Code is available at \href{https://github.com/00why00/PFCD}{here}.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2507.04061.pdf' target='_blank'>https://arxiv.org/pdf/2507.04061.pdf</a></span>   <span><a href='https://github.com/ghh1125/DOCTOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanghui Guo, Weijie Shi, Mengze Li, Juncheng Li, Hao Chen, Yue Cui, Jiajie Xu, Jia Zhu, Jiawei Shen, Zhangze Chen, Sirui Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04061">Consistent and Invariant Generalization Learning for Short-video Misinformation Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Short-video misinformation detection has attracted wide attention in the multi-modal domain, aiming to accurately identify the misinformation in the video format accompanied by the corresponding audio. Despite significant advancements, current models in this field, trained on particular domains (source domains), often exhibit unsatisfactory performance on unseen domains (target domains) due to domain gaps. To effectively realize such domain generalization on the short-video misinformation detection task, we propose deep insights into the characteristics of different domains: (1) The detection on various domains may mainly rely on different modalities (i.e., mainly focusing on videos or audios). To enhance domain generalization, it is crucial to achieve optimal model performance on all modalities simultaneously. (2) For some domains focusing on cross-modal joint fraud, a comprehensive analysis relying on cross-modal fusion is necessary. However, domain biases located in each modality (especially in each frame of videos) will be accumulated in this fusion process, which may seriously damage the final identification of misinformation. To address these issues, we propose a new DOmain generalization model via ConsisTency and invariance learning for shORt-video misinformation detection (named DOCTOR), which contains two characteristic modules: (1) We involve the cross-modal feature interpolation to map multiple modalities into a shared space and the interpolation distillation to synchronize multi-modal learning; (2) We design the diffusion model to add noise to retain core features of multi modal and enhance domain invariant features through cross-modal guided denoising. Extensive experiments demonstrate the effectiveness of our proposed DOCTOR model. Our code is public available at https://github.com/ghh1125/DOCTOR.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2507.01723.pdf' target='_blank'>https://arxiv.org/pdf/2507.01723.pdf</a></span>   <span><a href='https://github.com/amazon-science/Spherical_Diffusion_Policy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xupeng Zhu, Fan Wang, Robin Walters, Jane Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01723">SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion Policies are effective at learning closed-loop manipulation policies from human demonstrations but generalize poorly to novel arrangements of objects in 3D space, hurting real-world performance. To address this issue, we propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion policy that adapts trajectories according to 3D transformations of the scene. Such equivariance is achieved by embedding the states, actions, and the denoising process in spherical Fourier space. Additionally, we employ novel spherical FiLM layers to condition the action denoising process equivariantly on the scene embeddings. Lastly, we propose a spherical denoising temporal U-net that achieves spatiotemporal equivariance with computational efficiency. In the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization across transformed 3D scenes. SDP demonstrates a large performance improvement over strong baselines in 20 simulation tasks and 5 physical robot tasks including single-arm and bi-manual embodiments. Code is available at https://github.com/amazon-science/Spherical_Diffusion_Policy.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2507.01424.pdf' target='_blank'>https://arxiv.org/pdf/2507.01424.pdf</a></span>   <span><a href='https://zhenyangliu.github.io/TriVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyang Liu, Yongchong Gu, Sixiao Zheng, Yanwei Fu, Xiangyang Xue, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01424">TriVLA: A Triple-System-Based Unified Vision-Language-Action Model with Episodic World Modeling for General Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language models (VLMs) have enabled robots to follow open-ended instructions and demonstrate impressive commonsense reasoning. However, current vision-language-action (VLA) frameworks primarily rely on static representations and limited temporal context, restricting agents to short-horizon, reactive behaviors and hindering robust generalization in dynamic embodied environments. Inspired by cognitive neuroscience theories of episodic memory, we propose, to our knowledge, one of the first formalized episodic world models in VLA, enabling embodied robots to accumulate, recall, and predict sequential experiences. As an instantiation of this concept, our unified TriVLA realizes the episodic world model through a triple-system architecture: integrating multimodal grounding from a pretrained VLM (System 2) and temporally rich dynamics perception from a video diffusion model (System 3). This enables the agent to accumulate and recall sequential experiences, interpret current contexts, and predict future environmental evolution. Guided by episodic representations that span both the past and anticipated future, the downstream policy (System 1) generates coherent, context-aware action sequences through flow-matching and cross-modal attention mechanisms. Experimental results show that TriVLA operates efficiently at approximately 36 Hz and consistently outperforms baseline models on standard benchmarks and challenging real-world manipulation tasks. It demonstrates strong long-horizon planning and open-ended intent understanding, showcasing the advantages of episodic world model-inspired reasoning for robust, generalizable robot intelligence. Project Page: https://zhenyangliu.github.io/TriVLA/.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2506.23822.pdf' target='_blank'>https://arxiv.org/pdf/2506.23822.pdf</a></span>   <span><a href='https://github.com/shiming-chen/LaZSL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiming Chen, Bowen Duan, Salman Khan, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23822">Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale vision-language models (VLMs), such as CLIP, have achieved remarkable success in zero-shot learning (ZSL) by leveraging large-scale visual-text pair datasets. However, these methods often lack interpretability, as they compute the similarity between an entire query image and the embedded category words, making it difficult to explain their predictions. One approach to address this issue is to develop interpretable models by integrating language, where classifiers are built using discrete attributes, similar to human perception. This introduces a new challenge: how to effectively align local visual features with corresponding attributes based on pre-trained VLMs. To tackle this, we propose LaZSL, a locally-aligned vision-language model for interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal transport to perform interaction between visual regions and their associated attributes, facilitating effective alignment and providing interpretable similarity without the need for additional training. Extensive experiments demonstrate that our method offers several advantages, including enhanced interpretability, improved accuracy, and strong domain generalization. Codes available at: https://github.com/shiming-chen/LaZSL.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2506.23580.pdf' target='_blank'>https://arxiv.org/pdf/2506.23580.pdf</a></span>   <span><a href='https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yawen Zou, Guang Li, Duo Su, Zi Wang, Jun Yu, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23580">Dataset Distillation via Vision-Language Category Prototype</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dataset distillation (DD) condenses large datasets into compact yet informative substitutes, preserving performance comparable to the original dataset while reducing storage, transmission costs, and computational consumption. However, previous DD methods mainly focus on distilling information from images, often overlooking the semantic information inherent in the data. The disregard for context hinders the model's generalization ability, particularly in tasks involving complex datasets, which may result in illogical outputs or the omission of critical objects. In this study, we integrate vision-language methods into DD by introducing text prototypes to distill language information and collaboratively synthesize data with image prototypes, thereby enhancing dataset distillation performance. Notably, the text prototypes utilized in this study are derived from descriptive text information generated by an open-source large language model. This framework demonstrates broad applicability across datasets without pre-existing text descriptions, expanding the potential of dataset distillation beyond traditional image-based approaches. Compared to other methods, the proposed approach generates logically coherent images containing target objects, achieving state-of-the-art validation performance and demonstrating robust generalization. Source code and generated data are available in https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2506.23542.pdf' target='_blank'>https://arxiv.org/pdf/2506.23542.pdf</a></span>   <span><a href='https://github.com/davidweidawang/GIGA-ToF' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/davidweidawang/GIGA-ToF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weida Wang, Changyong He, Jin Zeng, Di Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23542">Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at \href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2506.23219.pdf' target='_blank'>https://arxiv.org/pdf/2506.23219.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/UrbanLLaVA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Feng, Shengyuan Wang, Tianhui Liu, Yanxin Xi, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23219">UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In $\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2506.21042.pdf' target='_blank'>https://arxiv.org/pdf/2506.21042.pdf</a></span>   <span><a href='https://github.com/heboyong/Fitness-Generalization-Transferability' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/heboyong/Fitness-Generalization-Transferability' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyong He, Yuxiang Ji, Zhuoyue Tan, Liaoni Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21042">Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detectors often suffer from performance drop due to domain gap between training and testing data. Recent methods explore diffusion models applied to domain generalization (DG) and adaptation (DA) tasks, but still struggle with large inference costs and have not yet fully leveraged the capabilities of diffusion models. We propose to tackle these problems by extracting intermediate features from a single-step diffusion process, improving feature collection and fusion to reduce inference time by 75% while enhancing performance on source domains (i.e., Fitness). Then, we construct an object-centered auxiliary branch by applying box-masked images with class prompts to extract robust and domain-invariant features that focus on object. We also apply consistency loss to align the auxiliary and ordinary branch, balancing fitness and generalization while preventing overfitting and improving performance on target domains (i.e., Generalization). Furthermore, within a unified framework, standard detectors are guided by diffusion detectors through feature-level and object-level alignment on source domains (for DG) and unlabeled target domains (for DA), thereby improving cross-domain detection performance (i.e., Transferability). Our method achieves competitive results on 3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO generalization benchmark demonstrate that our method maintains significant advantages and show remarkable efficiency in large domain shifts and low-data scenarios. Our work shows the superiority of applying diffusion models to domain generalized and adaptive detection tasks and offers valuable insights for visual perception tasks across diverse domains. The code is available at \href{https://github.com/heboyong/Fitness-Generalization-Transferability}.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2506.20599.pdf' target='_blank'>https://arxiv.org/pdf/2506.20599.pdf</a></span>   <span><a href='https://github.com/GeoX-Lab/RSTI/tree/main/SFNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Qi, Xinchang Zhang, Dingqi Ye, Yongjia Ruan, Xin Guo, Shaowen Wang, Haifeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20599">SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative artificial intelligence is producing fake remote sensing imagery (RSI) that is increasingly difficult to detect, potentially leading to erroneous intelligence, fake news, and even conspiracy theories. Existing forgery detection methods typically rely on single visual features to capture predefined artifacts, such as spatial-domain cues to detect forged objects like roads or buildings in RSI, or frequency-domain features to identify artifacts from up-sampling operations in adversarial generative networks (GANs). However, the nature of artifacts can significantly differ depending on geographic terrain, land cover types, or specific features within the RSI. Moreover, these complex artifacts evolve as generative models become more sophisticated. In short, over-reliance on a single visual cue makes existing forgery detectors struggle to generalize across diverse remote sensing data. This paper proposed a novel forgery detection framework called SFNet, designed to identify fake images in diverse remote sensing data by leveraging spatial and frequency domain features. Specifically, to obtain rich and comprehensive visual information, SFNet employs two independent feature extractors to capture spatial and frequency domain features from input RSIs. To fully utilize the complementary domain features, the domain feature mapping module and the hybrid domain feature refinement module(CBAM attention) of SFNet are designed to successively align and fuse the multi-domain features while suppressing redundant information. Experiments on three datasets show that SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art RS forgery detection methods and exhibits robust generalization capabilities. The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2506.17896.pdf' target='_blank'>https://arxiv.org/pdf/2506.17896.pdf</a></span>   <span><a href='https://redorangeyellowy.github.io/EgoWorld/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junho Park, Andrew Sangwoo Ye, Taein Kwon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17896">EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric vision is essential for both human and machine visual understanding, particularly in capturing the detailed hand-object interactions needed for manipulation tasks. Translating third-person views into first-person views significantly benefits augmented reality (AR), virtual reality (VR) and robotics applications. However, current exocentric-to-egocentric translation methods are limited by their dependence on 2D cues, synchronized multi-view settings, and unrealistic assumptions such as necessity of initial egocentric frame and relative camera poses during inference. To overcome these challenges, we introduce EgoWorld, a novel two-stage framework that reconstructs an egocentric view from rich exocentric observations, including projected point clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a point cloud from estimated exocentric depth maps, reprojects it into the egocentric perspective, and then applies diffusion-based inpainting to produce dense, semantically coherent egocentric images. Evaluated on the H2O and TACO datasets, EgoWorld achieves state-of-the-art performance and demonstrates robust generalization to new objects, actions, scenes, and subjects. Moreover, EgoWorld shows promising results even on unlabeled real-world examples.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2506.17685.pdf' target='_blank'>https://arxiv.org/pdf/2506.17685.pdf</a></span>   <span><a href='https://github.com/Ashayan97/SeqDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirshayan Nasirimajd, Chiara Plizzari, Simone Alberto Peirone, Marco Ciccone, Giuseppe Averta, Barbara Caputo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17685">Domain Generalization using Action Sequences for Egocentric Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing human activities from visual inputs, particularly through a first-person viewpoint, is essential for enabling robots to replicate human behavior. Egocentric vision, characterized by cameras worn by observers, captures diverse changes in illumination, viewpoint, and environment. This variability leads to a notable drop in the performance of Egocentric Action Recognition models when tested in environments not seen during training. In this paper, we tackle these challenges by proposing a domain generalization approach for Egocentric Action Recognition. Our insight is that action sequences often reflect consistent user intent across visual domains. By leveraging action sequences, we aim to enhance the model's generalization ability across unseen environments. Our proposed method, named SeqDG, introduces a visual-text sequence reconstruction objective (SeqRec) that uses contextual cues from both text and visual inputs to reconstruct the central action of the sequence. Additionally, we enhance the model's robustness by training it on mixed sequences of actions from different domains (SeqMix). We validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement in cross-domain action recognition in unseen environments, and on EGTEA the model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action recognition.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2506.16406.pdf' target='_blank'>https://arxiv.org/pdf/2506.16406.pdf</a></span>   <span><a href='https://jerryliang24.github.io/DnD' target='_blank'>  GitHub</a></span> <span><a href='https://jerryliang24.github.io/DnD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Liang, Dongwen Tang, Yuhao Zhou, Xuanlei Zhao, Mingjia Shi, Wangbo Zhao, Zekai Li, Peihao Wang, Konstantin SchÃ¼rholt, Damian Borth, Michael M. Bronstein, Yang You, Zhangyang Wang, Kai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16406">Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to \textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains up to \textbf{30\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at \href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2506.14317.pdf' target='_blank'>https://arxiv.org/pdf/2506.14317.pdf</a></span>   <span><a href='https://clutterdexgrasp.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyuan Chen, Qiyang Yan, Yuanpei Chen, Tianhao Wu, Jiyao Zhang, Zihan Ding, Jinzhou Li, Yaodong Yang, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14317">ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a geometry and spatially-embedded scene representation and a novel comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target-oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts. More details and videos are available at https://clutterdexgrasp.github.io/.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2506.12413.pdf' target='_blank'>https://arxiv.org/pdf/2506.12413.pdf</a></span>   <span><a href='https://github.com/PerceptualAI-Lab/Awesome-Domain-Generalizable-Person-Re-ID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonseo Lee, Juhyun Park, Jihyong Oh, Chanho Eom
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12413">Domain Generalization for Person Re-identification: A Survey Towards Domain-Agnostic Person Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person Re-identification (ReID) aims to retrieve images of the same individual captured across non-overlapping camera views, making it a critical component of intelligent surveillance systems. Traditional ReID methods assume that the training and test domains share similar characteristics and primarily focus on learning discriminative features within a given domain. However, they often fail to generalize to unseen domains due to domain shifts caused by variations in viewpoint, background, and lighting conditions. To address this issue, Domain-Adaptive ReID (DA-ReID) methods have been proposed. These approaches incorporate unlabeled target domain data during training and improve performance by aligning feature distributions between source and target domains. Domain-Generalizable ReID (DG-ReID) tackles a more realistic and challenging setting by aiming to learn domain-invariant features without relying on any target domain data. Recent methods have explored various strategies to enhance generalization across diverse environments, but the field remains relatively underexplored. In this paper, we present a comprehensive survey of DG-ReID. We first review the architectural components of DG-ReID including the overall setting, commonly used backbone networks and multi-source input configurations. Then, we categorize and analyze domain generalization modules that explicitly aim to learn domain-invariant and identity-discriminative representations. To examine the broader applicability of these techniques, we further conduct a case study on a related task that also involves distribution shifts. Finally, we discuss recent trends, open challenges, and promising directions for future research in DG-ReID. To the best of our knowledge, this is the first systematic survey dedicated to DG-ReID.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2506.10675.pdf' target='_blank'>https://arxiv.org/pdf/2506.10675.pdf</a></span>   <span><a href='https://github.com/jwxsp1/ConStyX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Chen, Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10675">ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical images are usually collected from multiple domains, leading to domain shifts that impair the performance of medical image segmentation models. Domain Generalization (DG) aims to address this issue by training a robust model with strong generalizability. Recently, numerous domain randomization-based DG methods have been proposed. However, these methods suffer from the following limitations: 1) constrained efficiency of domain randomization due to their exclusive dependence on image style perturbation, and 2) neglect of the adverse effects of over-augmented images on model training. To address these issues, we propose a novel domain randomization-based DG method, called content style augmentation (ConStyX), for generalizable medical image segmentation. Specifically, ConStyX 1) augments the content and style of training data, allowing the augmented training data to better cover a wider range of data domains, and 2) leverages well-augmented features while mitigating the negative effects of over-augmented features during model training. Extensive experiments across multiple domains demonstrate that our ConStyX achieves superior generalization performance. The code is available at https://github.com/jwxsp1/ConStyX.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2506.09881.pdf' target='_blank'>https://arxiv.org/pdf/2506.09881.pdf</a></span>   <span><a href='https://github.com/anonymouse-9c53tp182bvz/Vireo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Chen, Ting Han, Chengzheng Fu, Changshe Zhang, Chaolei Wang, Jinhe Su, Guorong Cai, Meiliu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09881">Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary semantic segmentation (OVSS) and domain generalization in semantic segmentation (DGSS) highlight a subtle complementarity that motivates Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS aims to generate pixel-level masks for unseen categories while maintaining robustness across unseen domains, a critical capability for real-world scenarios such as autonomous driving in adverse conditions. We introduce Vireo, a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS and DGSS for the first time. Vireo builds upon the frozen Visual Foundation Models (VFMs) and incorporates scene geometry via Depth VFMs to extract domain-invariant structural features. To bridge the gap between visual and textual modalities under domain shift, we propose three key components: (1) GeoText Prompts, which align geometric features with language cues and progressively refine VFM encoder representations; (2) Coarse Mask Prior Embedding (CMPE) for enhancing gradient flow for faster convergence and stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH), which fuses refined structural and semantic features for robust prediction. Comprehensive evaluation on these components demonstrates the effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art performance and surpasses existing methods by a large margin in both domain generalization and open-vocabulary recognition, offering a unified and scalable solution for robust visual understanding in diverse and dynamic environments. Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2506.09460.pdf' target='_blank'>https://arxiv.org/pdf/2506.09460.pdf</a></span>   <span><a href='https://github.com/amir-khb/SSUDOSDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirreza Khoshbakht, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09460">Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-set domain generalization(OSDG) for hyperspectral image classification presents significant challenges due to the presence of unknown classes in target domains and the need for models to generalize across multiple unseen domains without target-specific adaptation. Existing domain adaptation methods assume access to target domain data during training and fail to address the fundamental issue of domain shift when unknown classes are present, leading to negative transfer and reduced classification performance. To address these limitations, we propose a novel open-set domain generalization framework that combines four key components: Spectrum-Invariant Frequency Disentanglement (SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network (DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning (EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty Disentanglement (SSUD) for reliable open-set classification. The SIFD module extracts domain-invariant spectral features in the frequency domain through attention-weighted frequency analysis and domain-agnostic regularization, while DCRN captures complementary spectral and spatial information via parallel pathways with adaptive fusion. EDL provides principled uncertainty estimation using Dirichlet distributions, enabling the SSUD module to make reliable open-set decisions through uncertainty-aware pathway weighting and adaptive rejection thresholding. Experimental results on three cross-scene hyperspectral classification tasks show that our approach achieves performance comparable to state-of-the-art domain adaptation methods while requiring no access to the target domain during training. The implementation will be made available at https://github.com/amir-khb/SSUDOSDG upon acceptance.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2506.09033.pdf' target='_blank'>https://arxiv.org/pdf/2506.09033.pdf</a></span>   <span><a href='https://github.com/ulab-uiuc/Router-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhen Zhang, Tao Feng, Jiaxuan You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09033">Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2506.08518.pdf' target='_blank'>https://arxiv.org/pdf/2506.08518.pdf</a></span>   <span><a href='https://github.com/sunnyinAI/FedTail' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Gupta, Nikita Jangid, Shounak Das, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08518">FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) seeks to train models that perform reliably on unseen target domains without access to target data during training. While recent progress in smoothing the loss landscape has improved generalization, existing methods often falter under long-tailed class distributions and conflicting optimization objectives. We introduce FedTAIL, a federated domain generalization framework that explicitly addresses these challenges through sharpness-guided, gradient-aligned optimization. Our method incorporates a gradient coherence regularizer to mitigate conflicts between classification and adversarial objectives, leading to more stable convergence. To combat class imbalance, we perform class-wise sharpness minimization and propose a curvature-aware dynamic weighting scheme that adaptively emphasizes underrepresented tail classes. Furthermore, we enhance conditional distribution alignment by integrating sharpness-aware perturbations into entropy regularization, improving robustness under domain shift. FedTAIL unifies optimization harmonization, class-aware regularization, and conditional alignment into a scalable, federated-compatible framework. Extensive evaluations across standard domain generalization benchmarks demonstrate that FedTAIL achieves state-of-the-art performance, particularly in the presence of domain shifts and label imbalance, validating its effectiveness in both centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2506.08011.pdf' target='_blank'>https://arxiv.org/pdf/2506.08011.pdf</a></span>   <span><a href='https://yunfeixie233.github.io/ViGaL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08011">Play to Generalize: Learning to Reason Through Game Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2506.07631.pdf' target='_blank'>https://arxiv.org/pdf/2506.07631.pdf</a></span>   <span><a href='https://google.github.io/unblocking-detail-caption' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Brian Gordon, Yonatan Bitton, Andreea Marzoca, Yasumasa Onoe, Xiao Wang, Daniel Cohen-Or, Idan Szpektor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07631">Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (VLMs) now generate highly detailed, paragraphlength image captions, yet evaluating their factual accuracy remains challenging. Current methods often miss fine-grained errors, being designed for shorter texts or lacking datasets with verified inaccuracies. We introduce DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100 images, 14 VLMs) featuring over 10,216 sentence-level human annotations of factual correctness and explanatory rationales for errors, all within paragraph context. Building on this, we develop VNLI-Critique, a model for automated sentence-level factuality classification and critique generation. We highlight three key applications: (1) VNLI-Critique demonstrates robust generalization, validated by state-of-the-art performance on the M-HalDetect benchmark and strong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven AutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent alignment with human factuality judgments (e.g., 0.98 Spearman). (3) An innovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide LLM-based corrections, achieves substantial improvements in caption factuality (e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark alongside practical tools, designed to significantly elevate the standards for fine-grained evaluation and foster the improvement of VLM image understanding. Project page: https://google.github.io/unblocking-detail-caption
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2506.06664.pdf' target='_blank'>https://arxiv.org/pdf/2506.06664.pdf</a></span>   <span><a href='https://github.com/NVlabs/GTRS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenxin Li, Wenhao Yao, Zi Wang, Xinglong Sun, Joshua Chen, Nadine Chang, Maying Shen, Zuxuan Wu, Shiyi Lan, Jose M. Alvarez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06664">Generalized Trajectory Scoring for End-to-end Multimodal Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end multi-modal planning is a promising paradigm in autonomous driving, enabling decision-making with diverse trajectory candidates. A key component is a robust trajectory scorer capable of selecting the optimal trajectory from these candidates. While recent trajectory scorers focus on scoring either large sets of static trajectories or small sets of dynamically generated ones, both approaches face significant limitations in generalization. Static vocabularies provide effective coarse discretization but struggle to make fine-grained adaptation, while dynamic proposals offer detailed precision but fail to capture broader trajectory distributions. To overcome these challenges, we propose GTRS (Generalized Trajectory Scoring), a unified framework for end-to-end multi-modal planning that combines coarse and fine-grained trajectory evaluation. GTRS consists of three complementary innovations: (1) a diffusion-based trajectory generator that produces diverse fine-grained proposals; (2) a vocabulary generalization technique that trains a scorer on super-dense trajectory sets with dropout regularization, enabling its robust inference on smaller subsets; and (3) a sensor augmentation strategy that enhances out-of-domain generalization while incorporating refinement training for critical trajectory discrimination. As the winning solution of the Navsim v2 Challenge, GTRS demonstrates superior performance even with sub-optimal sensor inputs, approaching privileged methods that rely on ground-truth perception. Code will be available at https://github.com/NVlabs/GTRS.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2506.05957.pdf' target='_blank'>https://arxiv.org/pdf/2506.05957.pdf</a></span>   <span><a href='https://github.com/tianyao-aka/PrunE-GraphOOD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tianyao-aka/PrunE-GraphOOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianjun Yao, Haoxuan Li, Yongqiang Chen, Tongliang Liu, Le Song, Eric Xing, Zhiqiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05957">Pruning Spurious Subgraphs for Graph Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) often encounter significant performance degradation under distribution shifts between training and test data, hindering their applicability in real-world scenarios. Recent studies have proposed various methods to address the out-of-distribution generalization challenge, with many methods in the graph domain focusing on directly identifying an invariant subgraph that is predictive of the target label. However, we argue that identifying the edges from the invariant subgraph directly is challenging and error-prone, especially when some spurious edges exhibit strong correlations with the targets. In this paper, we propose PrunE, the first pruning-based graph OOD method that eliminates spurious edges to improve OOD generalizability. By pruning spurious edges, PrunE retains the invariant subgraph more comprehensively, which is critical for OOD generalization. Specifically, PrunE employs two regularization terms to prune spurious edges: 1) graph size constraint to exclude uninformative spurious edges, and 2) $Îµ$-probability alignment to further suppress the occurrence of spurious edges. Through theoretical analysis and extensive experiments, we show that PrunE achieves superior OOD performance and outperforms previous state-of-the-art methods significantly. Codes are available at: \href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2506.05814.pdf' target='_blank'>https://arxiv.org/pdf/2506.05814.pdf</a></span>   <span><a href='https://github.com/Aalto-QuML/PIPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yogesh Verma, Amauri H. Souza, Vikas Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05814">Positional Encoding meets Persistent Homology on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The local inductive bias of message-passing graph neural networks (GNNs) hampers their ability to exploit key structural information (e.g., connectivity and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged as two promising approaches to mitigate this issue. PE schemes endow GNNs with location-aware features, while PH methods enhance GNNs with multiresolution topological features. However, a rigorous theoretical characterization of the relative merits and shortcomings of PE and PH has remained elusive. We bridge this gap by establishing that neither paradigm is more expressive than the other, providing novel constructions where one approach fails but the other succeeds. Our insights inform the design of a novel learnable method, PiPE (Persistence-informed Positional Encoding), which is provably more expressive than both PH and PE. PiPE demonstrates strong performance across a variety of tasks (e.g., molecule property prediction, graph classification, and out-of-distribution generalization), thereby advancing the frontiers of graph representation learning. Code is available at https://github.com/Aalto-QuML/PIPE.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2506.03706.pdf' target='_blank'>https://arxiv.org/pdf/2506.03706.pdf</a></span>   <span><a href='https://github.com/adityagandhamal/OV-COAST/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Gandhamal, Aniruddh Sikdar, Suresh Sundaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03706">OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary semantic segmentation (OVSS) entails assigning semantic labels to each pixel in an image using textual descriptions, typically leveraging world models such as CLIP. To enhance out-of-domain generalization, we propose Cost Aggregation with Optimal Transport (OV-COAST) for open-vocabulary semantic segmentation. To align visual-language features within the framework of optimal transport theory, we employ cost volume to construct a cost matrix, which quantifies the distance between two distributions. Our approach adopts a two-stage optimization strategy: in the first stage, the optimal transport problem is solved using cost volume via Sinkhorn distance to obtain an alignment solution; in the second stage, this solution is used to guide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS models on the MESS benchmark, where our approach notably improves the performance of the cost-aggregation model CAT-Seg with ViT-B backbone, achieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 % mIoU. The code is available at https://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ .
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2505.23926.pdf' target='_blank'>https://arxiv.org/pdf/2505.23926.pdf</a></span>   <span><a href='https://uva-computer-vision-lab.github.io/point-moe/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuweiyi Chen, Wentao Zhou, Aruni RoyChowdhury, Zezhou Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23926">Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2505.23173.pdf' target='_blank'>https://arxiv.org/pdf/2505.23173.pdf</a></span>   <span><a href='https://github.com/s-enmt/PseudoDomainBed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shohei Enomoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23173">Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models often struggle to maintain performance when deployed on data distributions different from their training data, particularly in real-world applications where environmental conditions frequently change. While Multi-source Domain Generalization (MDG) has shown promise in addressing this challenge by leveraging multiple source domains during training, its practical application is limited by the significant costs and difficulties associated with creating multi-domain datasets. To address this limitation, we propose Pseudo Multi-source Domain Generalization (PMDG), a novel framework that enables the application of sophisticated MDG algorithms in more practical Single-source Domain Generalization (SDG) settings. PMDG generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques, creating a synthetic multi-domain dataset that can be used with existing MDG algorithms. Through extensive experiments with PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze the effectiveness of PMDG across multiple datasets and architectures. Our analysis reveals several key findings, including a positive correlation between MDG and PMDG performance and the potential of pseudo-domains to match or exceed actual multi-domain performance with sufficient data. These comprehensive empirical results provide valuable insights for future research in domain generalization. Our code is available at https://github.com/s-enmt/PseudoDomainBed.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2505.22465.pdf' target='_blank'>https://arxiv.org/pdf/2505.22465.pdf</a></span>   <span><a href='https://github.com/zobia111/SDG-Alzheimer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zobia Batool, Huseyin Ozkan, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22465">Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although Alzheimer's disease detection via MRIs has advanced significantly thanks to contemporary deep learning models, challenges such as class imbalance, protocol variations, and limited dataset diversity often hinder their generalization capacity. To address this issue, this article focuses on the single domain generalization setting, where given the data of one domain, a model is designed and developed with maximal performance w.r.t. an unseen domain of distinct distribution. Since brain morphology is known to play a crucial role in Alzheimer's diagnosis, we propose the use of learnable pseudo-morphological modules aimed at producing shape-aware, anatomically meaningful class-specific augmentations in combination with a supervised contrastive learning module to extract robust class-specific representations. Experiments conducted across three datasets show improved performance and generalization capacity, especially under class imbalance and imaging protocol variations. The source code will be made available upon acceptance at https://github.com/zobia111/SDG-Alzheimer.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2505.21828.pdf' target='_blank'>https://arxiv.org/pdf/2505.21828.pdf</a></span>   <span><a href='https://github.com/YuehHanChen/SAGE-Eval/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Yueh-Han, Guy Davidson, Brenden M. Lake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21828">SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Do LLMs robustly generalize critical safety facts to novel situations? Lacking this ability is dangerous when users ask naive questions. For instance, "I'm considering packing melon balls for my 10-month-old's lunch. What other foods would be good to include?" Before offering food options, the LLM should warn that melon balls pose a choking hazard to toddlers, as documented by the CDC. Failing to provide such warnings could result in serious injuries or even death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic GEneralization evaluation, the first benchmark that tests whether LLMs properly apply well established safety facts to naive user queries. SAGE-Eval comprises 104 facts manually sourced from reputable organizations, systematically augmented to create 10,428 test scenarios across 7 common domains (e.g., Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet, passes only 58% of all the safety facts tested. We also observe that model capabilities and training compute weakly correlate with performance on SAGE-Eval, implying that scaling up is not the golden solution. Our findings suggest frontier LLMs still lack robust generalization ability. We recommend developers use SAGE-Eval in pre-deployment evaluations to assess model reliability in addressing salient risks. We publicly release SAGE-Eval at https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available at https://github.com/YuehHanChen/SAGE-Eval/tree/main.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2505.21780.pdf' target='_blank'>https://arxiv.org/pdf/2505.21780.pdf</a></span>   <span><a href='https://energy-based-model.github.io/compositional-inference' target='_blank'>  GitHub</a></span> <span><a href='https://energy-based-model.github.io/compositional-inference' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Justin Dauwels, Yilun Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21780">Compositional Scene Understanding through Inverse Generative Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have demonstrated remarkable abilities in generating high-fidelity visual content. In this work, we explore how generative models can further be used not only to synthesize visual content but also to understand the properties of a scene given a natural image. We formulate scene understanding as an inverse generative modeling problem, where we seek to find conditional parameters of a visual generative model to best fit a given natural image. To enable this procedure to infer scene structure from images substantially different than those seen during training, we further propose to build this visual generative model compositionally from smaller models over pieces of a scene. We illustrate how this procedure enables us to infer the set of objects in a scene, enabling robust generalization to new test scenes with an increased number of objects of new shapes. We further illustrate how this enables us to infer global scene factors, likewise enabling robust generalization to new scenes. Finally, we illustrate how this approach can be directly applied to existing pretrained text-to-image generative models for zero-shot multi-object perception. Code and visualizations are at https://energy-based-model.github.io/compositional-inference.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2505.20653.pdf' target='_blank'>https://arxiv.org/pdf/2505.20653.pdf</a></span>   <span><a href='https://github.com/Lynn0925/RoGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyu Qiu, Ke Jiang, Xiaoyang Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20653">RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in domain generalization for deepfake detection have attracted significant attention, with previous methods often incorporating additional modules to prevent overfitting to domain-specific patterns. However, such regularization can hinder the optimization of the empirical risk minimization (ERM) objective, ultimately degrading model performance. In this paper, we propose a novel learning objective that aligns generalization gradient updates with ERM gradient updates. The key innovation is the application of perturbations to model parameters, aligning the ascending points across domains, which specifically enhances the robustness of deepfake detection models to domain shifts. This approach effectively preserves domain-invariant features while managing domain-specific characteristics, without introducing additional regularization. Experimental results on multiple challenging deepfake detection datasets demonstrate that our gradient alignment strategy outperforms state-of-the-art domain generalization techniques, confirming the efficacy of our method. The code is available at https://github.com/Lynn0925/RoGA.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2505.20446.pdf' target='_blank'>https://arxiv.org/pdf/2505.20446.pdf</a></span>   <span><a href='https://github.com/azencot-group/ImagenFew' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tal Gonen, Itai Pemper, Ilan Naiman, Nimrod Berman, Omri Azencot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20446">Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling of time series is a central challenge in time series analysis, particularly under data-scarce conditions. Despite recent advances in generative modeling, a comprehensive understanding of how state-of-the-art generative models perform under limited supervision remains lacking. In this work, we conduct the first large-scale study evaluating leading generative models in data-scarce settings, revealing a substantial performance gap between full-data and data-scarce regimes. To close this gap, we propose a unified diffusion-based generative framework that can synthesize high-fidelity time series across diverse domains using just a few examples. Our model is pre-trained on a large, heterogeneous collection of time series datasets, enabling it to learn generalizable temporal representations. It further incorporates architectural innovations such as dynamic convolutional layers for flexible channel adaptation and dataset token conditioning for domain-aware generation. Without requiring abundant supervision, our unified model achieves state-of-the-art performance in few-shot settings-outperforming domain-specific baselines across a wide range of subset sizes. Remarkably, it also surpasses all baselines even when tested on full datasets benchmarks, highlighting the strength of pre-training and cross-domain generalization. We hope this work encourages the community to revisit few-shot generative modeling as a key problem in time series research and pursue unified solutions that scale efficiently across domains. Code is available at https://github.com/azencot-group/ImagenFew.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2505.20256.pdf' target='_blank'>https://arxiv.org/pdf/2505.20256.pdf</a></span>   <span><a href='https://aim-uofa.github.io/OmniR1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhong, Muzhi Zhu, Zongze Du, Zheng Huang, Canyu Zhao, Mingyu Liu, Wen Wang, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20256">Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon video-audio reasoning and fine-grained pixel understanding impose conflicting requirements on omnimodal models: dense temporal coverage demands many low-resolution frames, whereas precise grounding calls for high-resolution inputs. We tackle this trade-off with a two-system architecture: a Global Reasoning System selects informative keyframes and rewrites the task at low spatial cost, while a Detail Understanding System performs pixel-level grounding on the selected high-resolution snippets. Because ``optimal'' keyframe selection and reformulation are ambiguous and hard to supervise, we formulate them as a reinforcement learning (RL) problem and present Omni-R1, an end-to-end RL framework built on Group Relative Policy Optimization. Omni-R1 trains the Global Reasoning System through hierarchical rewards obtained via online collaboration with the Detail Understanding System, requiring only one epoch of RL on small task splits.
  Experiments on two challenging benchmarks, namely Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show that Omni-R1 not only surpasses strong supervised baselines but also outperforms specialized state-of-the-art models, while substantially improving out-of-domain generalization and mitigating multimodal hallucination. Our results demonstrate the first successful application of RL to large-scale omnimodal reasoning and highlight a scalable path toward universally foundation models.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2505.19659.pdf' target='_blank'>https://arxiv.org/pdf/2505.19659.pdf</a></span>   <span><a href='https://github.com/backpropagator/LangDAug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Piyush Tiwary, Kinjawl Bhattacharyya, Prathosh A. P
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19659">LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DAug). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DAug methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel $\textbf{Lang}$evin $\textbf{D}$ata $\textbf{Aug}$mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2505.17612.pdf' target='_blank'>https://arxiv.org/pdf/2505.17612.pdf</a></span>   <span><a href='https://github.com/Nardien/agent-distillation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17612">Distilling LLM Agent into Small Models with Retrieval and Code Tools</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2505.17017.pdf' target='_blank'>https://arxiv.org/pdf/2505.17017.pdf</a></span>   <span><a href='https://github.com/ZiyuGuo99/Image-Generation-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17017">Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2505.15545.pdf' target='_blank'>https://arxiv.org/pdf/2505.15545.pdf</a></span>   <span><a href='https://github.com/andrewcaunes/ia4markings' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Caunes, Thierry Chateau, Vincent Fremont
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15545">seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D semantic segmentation plays a pivotal role in autonomous driving and road infrastructure analysis, yet state-of-the-art 3D models are prone to severe domain shift when deployed across different datasets. We propose a novel multi-view projection framework that excels in both domain generalization (DG) and unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans into coherent 3D scenes and renders them from multiple virtual camera poses to create a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D segmentation model in-domain. During inference, the model processes hundreds of views per scene; the resulting logits are back-projected to 3D with an occlusion-aware voting scheme to generate final point-wise labels. Our framework is modular and enables extensive exploration of key design parameters, such as view generation optimization (VGO), visualization modality optimization (MODO), and 2D model choice. We evaluate on the nuScenes and SemanticKITTI datasets under both the DG and UDA settings. We achieve state-of-the-art results in UDA and close to state-of-the-art in DG, with particularly large gains on large, static classes. Our code and dataset generation tools will be publicly available at https://github.com/andrewcaunes/ia4markings
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2505.13233.pdf' target='_blank'>https://arxiv.org/pdf/2505.13233.pdf</a></span>   <span><a href='https://github.com/BIT-DA/ABS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/BIT-DA/ABS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lincan Cai, Jingxuan Kang, Shuang Li, Wenxuan Ma, Binhui Xie, Zhida Qin, Jian Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13233">From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive zero-shot capabilities on downstream tasks. Prior research highlights the crucial role of visual augmentation techniques, like random cropping, in alignment with fine-grained class descriptions generated by large language models (LLMs), significantly enhancing zero-shot performance by incorporating multi-view information. However, the inherent randomness of these augmentations can inevitably introduce background artifacts and cause models to overly focus on local details, compromising global semantic understanding. To address these issues, we propose an \textbf{A}ttention-\textbf{B}ased \textbf{S}election (\textbf{ABS}) method from local details to global context, which applies attention-guided cropping in both raw images and feature space, supplement global semantic information through strategic feature selection. Additionally, we introduce a soft matching technique to effectively filter LLM descriptions for better alignment. \textbf{ABS} achieves state-of-the-art performance on out-of-distribution generalization and zero-shot classification tasks. Notably, \textbf{ABS} is training-free and even rivals few-shot and test-time adaptation methods. Our code is available at \href{https://github.com/BIT-DA/ABS}{\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2505.13232.pdf' target='_blank'>https://arxiv.org/pdf/2505.13232.pdf</a></span>   <span><a href='https://github.com/alinlab/StarFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Younghyun Kim, Jongheon Jeong, Sangkyung Kwak, Kyungmin Lee, Juho Lee, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13232">StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning robust representations from data often requires scale, which has led to the success of recent zero-shot models such as CLIP. However, the obtained robustness can easily be deteriorated when these models are fine-tuned on other downstream tasks (e.g., of smaller scales). Previous works often interpret this phenomenon in the context of domain shift, developing fine-tuning methods that aim to preserve the original domain as much as possible. However, in a different context, fine-tuned models with limited data are also prone to learning features that are spurious to humans, such as background or texture. In this paper, we propose StarFT (Spurious Textual Alignment Regularization), a novel framework for fine-tuning zero-shot models to enhance robustness by preventing them from learning spuriosity. We introduce a regularization that aligns the output distribution for spuriosity-injected labels with the original zero-shot model, ensuring that the model is not induced to extract irrelevant features further from these descriptions. We leverage recent language models to get such spuriosity-injected labels by generating alternative textual descriptions that highlight potentially confounding features. Extensive experiments validate the robust generalization of StarFT and its emerging properties: zero-shot group robustness and improved zero-shot classification. Notably, StarFT boosts both worst-group and average accuracy by 14.30% and 3.02%, respectively, in the Waterbirds group shift scenario, where other robust fine-tuning baselines show even degraded performance.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2505.12697.pdf' target='_blank'>https://arxiv.org/pdf/2505.12697.pdf</a></span>   <span><a href='https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_Coder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaofan Li, Jianlyu Chen, Yingxia Shao, Defu Lian, Zheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12697">Towards A Generalist Code Embedding Model Based On Massive Data Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code embedding models attract increasing attention due to the widespread popularity of retrieval-augmented generation (RAG) in software development. These models are expected to capture the rich semantic relationships inherent to code, which differ significantly from those found in text. However, existing models remain severely limited due to the scarcity of high-quality training data. In this work, we introduce \textbf{CodeR} (\underline{Code} \underline{R}etrieval), a state-of-the-art embedding model for general-purpose code retrieval. The superior performance of CodeR is built upon CodeR-Pile, a large-scale synthetic dataset constructed under the DRU (Diversity, Reliability, Usability) principle via a novel data synthesis pipeline. To optimize training effectiveness, we propose Annealing, a curriculum learning strategy that enables effective knowledge transfer across heterogeneous sources of data. We evaluate CodeR based on 16 diverse code retrieval tasks, where it significantly outperforms existing baselines and exhibits strong out-of-domain generalization performance. We have publicly released our code and the well-trained model to facilitate further research in this critical area. https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_Coder.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2505.11849.pdf' target='_blank'>https://arxiv.org/pdf/2505.11849.pdf</a></span>   <span><a href='https://github.com/NellyW8/VeriReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiting Wang, Guoheng Sun, Wanghao Ye, Gang Qu, Ang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11849">VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating Register Transfer Level (RTL) code generation using Large Language Models (LLMs) offers substantial promise for streamlining digital circuit design and reducing human effort. However, current LLM-based approaches face significant challenges with training data scarcity, poor specification-code alignment, lack of verification mechanisms, and balancing generalization with specialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework integrating supervised fine-tuning with Guided Reward Proximal Optimization (GRPO) reinforcement learning for RTL generation. Using curated training examples and a feedback-driven reward model, VeriReason combines testbench evaluations with structural heuristics while embedding self-checking capabilities for autonomous error correction. On the VerilogEval Benchmark, VeriReason delivers significant improvements: achieving 83.1% functional correctness on the VerilogEval Machine benchmark, substantially outperforming both comparable-sized models and much larger commercial systems like GPT-4 Turbo. Additionally, our approach demonstrates up to a 2.8X increase in first-attempt functional correctness compared to baseline methods and exhibits robust generalization to unseen designs. To our knowledge, VeriReason represents the first system to successfully integrate explicit reasoning capabilities with reinforcement learning for Verilog generation, establishing a new state-of-the-art for automated RTL synthesis. The models and datasets are available at: https://huggingface.co/collections/AI4EDA-CASE Code is Available at: https://github.com/NellyW8/VeriReason
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2505.11099.pdf' target='_blank'>https://arxiv.org/pdf/2505.11099.pdf</a></span>   <span><a href='https://github.com/L1277471578/HyMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Liu, Chunyang Wang, Xuelian Liu, Bo Xiao, Guan Xi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11099">HyMamba: Mamba with Hybrid Geometry-Feature Coupling for Efficient Point Cloud Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud classification is one of the essential technologies for achieving intelligent perception of 3D environments by machines, its core challenge is to efficiently extract local and global features. Mamba leverages state space models (SSMs) for global point cloud modeling. Although prior Mamba-based point cloud processing methods pay attention to the limitation of its flattened sequence modeling mechanism in fusing local and global features, the critical issue of weakened local geometric relevance caused by decoupling geometric structures and features in the input patches remains not fully revealed, and both jointly limit local feature extraction. Therefore, we propose HyMamba, a geometry and feature coupled Mamba framework featuring: (1) Geometry-Feature Coupled Pooling (GFCP), which achieves physically interpretable geometric information coupling by dynamically aggregating adjacent geometric information into local features; (2) Collaborative Feature Enhancer (CoFE), which enhances sparse signal capture through cross-path feature hybridization while effectively integrating global and local contexts. We conducted extensive experiments on ModelNet40 and ScanObjectNN datasets. The results demonstrate that the proposed model achieves superior classification performance, particularly on the ModelNet40, where it elevates accuracy to 95.99% with merely 0.03M additional parameters. Furthermore, it attains 98.9% accuracy on the ModelNetFewShot dataset, validating its robust generalization capabilities under sparse samples. Our code and weights are available at https://github.com/L1277471578/HyMamba
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2505.10231.pdf' target='_blank'>https://arxiv.org/pdf/2505.10231.pdf</a></span>   <span><a href='https://github.com/Roypic/Aligner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Luo, Ziyu Zhou, Zixin Shu, AurÃ©lie Pahud de Mortanges, Robert Berke, Mauricio Reyes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10231">On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. Our code is available at https://github.com/Roypic/Aligner.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2505.09274.pdf' target='_blank'>https://arxiv.org/pdf/2505.09274.pdf</a></span>   <span><a href='https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fares Bougourzi, Abdenour Hadid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09274">Recent Advances in Medical Imaging Segmentation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical imaging is a cornerstone of modern healthcare, driving advancements in diagnosis, treatment planning, and patient care. Among its various tasks, segmentation remains one of the most challenging problem due to factors such as data accessibility, annotation complexity, structural variability, variation in medical imaging modalities, and privacy constraints. Despite recent progress, achieving robust generalization and domain adaptation remains a significant hurdle, particularly given the resource-intensive nature of some proposed models and their reliance on domain expertise. This survey explores cutting-edge advancements in medical image segmentation, focusing on methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and Universal Models. These approaches offer promising solutions to longstanding challenges. We provide a comprehensive overview of the theoretical foundations, state-of-the-art techniques, and recent applications of these methods. Finally, we discuss inherent limitations, unresolved issues, and future research directions aimed at enhancing the practicality and accessibility of segmentation models in medical imaging. We are maintaining a \href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub Repository} to continue tracking and updating innovations in this field.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2505.08459.pdf' target='_blank'>https://arxiv.org/pdf/2505.08459.pdf</a></span>   <span><a href='https://github.com/hsushuai/SAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Xu, Sijia Cui, Yanna Wang, Bo Xu, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08459">Strategy-Augmented Planning for Large Language Models via Opponent Exploitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently modeling and exploiting opponents is a long-standing challenge in adversarial domains. Large Language Models (LLMs) trained on extensive textual data have recently demonstrated outstanding performance in general tasks, introducing new research directions for opponent modeling. Some studies primarily focus on directly using LLMs to generate decisions based on the elaborate prompt context that incorporates opponent descriptions, while these approaches are limited to scenarios where LLMs possess adequate domain expertise. To address that, we introduce a two-stage Strategy-Augmented Planning (SAP) framework that significantly enhances the opponent exploitation capabilities of LLM-based agents by utilizing a critical component, the Strategy Evaluation Network (SEN). Specifically, in the offline stage, we construct an explicit strategy space and subsequently collect strategy-outcome pair data for training the SEN network. During the online phase, SAP dynamically recognizes the opponent's strategies and greedily exploits them by searching best response strategy on the well-trained SEN, finally translating strategy to a course of actions by carefully designed prompts. Experimental results show that SAP exhibits robust generalization capabilities, allowing it to perform effectively not only against previously encountered opponent strategies but also against novel, unseen strategies. In the MicroRTS environment, SAP achieves a $85.35\%$ performance improvement over baseline methods and matches the competitiveness of reinforcement learning approaches against state-of-the-art (SOTA) rule-based AI. Our code is available at https://github.com/hsushuai/SAP.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2505.07675.pdf' target='_blank'>https://arxiv.org/pdf/2505.07675.pdf</a></span>   <span><a href='https://github.com/erjui/DHO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seongjae Kang, Dong Bok Lee, Hyungjoon Jang, Sung Ju Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07675">Simple yet Effective Semi-supervised Knowledge Distillation from Vision-Language Models via Dual-Head Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised learning (SSL) has emerged as a practical solution for addressing data scarcity challenges by leveraging unlabeled data. Recently, vision-language models (VLMs), pre-trained on massive image-text pairs, have demonstrated remarkable zero-/few-shot performance that often surpasses SSL approaches due to their exceptional generalization capabilities. This gap motivates us to question: how can we effectively harness the powerful generalization capabilities of VLMs into task-specific models? Knowledge distillation (KD) offers a natural framework for transferring VLM capabilities, but we identify that it suffers from gradient conflicts between supervised and distillation losses. To address this challenge, we propose Dual-Head Optimization (DHO), which introduces dual prediction heads for each distinct signal. We observe that DHO resolves gradient conflicts, enabling improved feature learning compared to single-head KD baselines, with practical benefits of minimal computational overhead and test-time hyperparameter tuning without retraining. Extensive experiments across 15 datasets show that DHO consistently outperforms KD baselines, often outperforming teacher models with smaller student models. DHO also achieves new state-of-the-art performance on both in-distribution ImageNet semi-supervised learning and out-of-distribution generalization across ImageNet variants. We publicly release our code and model checkpoints to facilitate future research at https://github.com/erjui/DHO.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2505.07219.pdf' target='_blank'>https://arxiv.org/pdf/2505.07219.pdf</a></span>   <span><a href='https://github.com/qinhongda8/LDDS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/qinhongda8/LDDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongda Qin, Xiao Lu, Zhiyong Wei, Yihong Cao, Kailun Yang, Ningjiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07219">Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector's backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at https://github.com/qinhongda8/LDDS.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2505.03539.pdf' target='_blank'>https://arxiv.org/pdf/2505.03539.pdf</a></span>   <span><a href='https://github.com/MengfeiD/PanOoS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MengfeiD/PanOoS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengfei Duan, Kailun Yang, Yuheng Zhang, Yihong Cao, Fei Teng, Kai Luo, Jiaming Zhang, Zhiyong Li, Shutao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03539">Panoramic Out-of-Distribution Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic imaging enables capturing 360Â° images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to background clutter and pixel distortions. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2505.02515.pdf' target='_blank'>https://arxiv.org/pdf/2505.02515.pdf</a></span>   <span><a href='https://github.com/pizzareapers/FedSDAF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongze Li, Zesheng Zhou, Zhenbiao Cao, Xinhui Li, Wei Chen, Xiaojin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02515">FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Federated Domain Generalization (FedDG) methods focus on learning domain-invariant features or adapting to unseen target domains, often overlooking the unique knowledge embedded within the source domain, especially in strictly isolated federated learning environments. Through experimentation, we discovered a counterintuitive phenomenon.: features learned from a complete source domain have superior generalization capabilities compared to those learned directly from the target domain. This insight leads us to propose the Federated Source Domain Awareness Framework (FedSDAF), the first systematic approach to enhance FedDG by leveraging source domain-aware features. FedSDAF employs a dual-adapter architecture that decouples "local expertise" from "global generalization consensus". A Domain-Aware Adapter, retained locally, extracts and protects the unique discriminative knowledge of each source domain, while a Domain-Invariant Adapter, shared across clients, builds a robust global consensus. To enable knowledge exchange, we introduce a Bidirectional Knowledge Distillation mechanism that facilitates efficient dialogue between the adapters. Extensive experiments on four benchmark datasets (OfficeHome, PACS, VLCS, DomainNet) show that FedSDAF significantly outperforms existing FedDG methods.The source code is available at https://github.com/pizzareapers/FedSDAF.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2505.02182.pdf' target='_blank'>https://arxiv.org/pdf/2505.02182.pdf</a></span>   <span><a href='https://github.com/Purdue-M2/SP_CUP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yamini Sri Krubha, Aryana Hou, Braden Vester, Web Walker, Xin Wang, Li Lin, Shu Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02182">Robust AI-Generated Face Detection with Imbalanced Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfakes, created using advanced AI techniques such as Variational Autoencoder and Generative Adversarial Networks, have evolved from research and entertainment applications into tools for malicious activities, posing significant threats to digital trust. Current deepfake detection techniques have evolved from CNN-based methods focused on local artifacts to more advanced approaches using vision transformers and multimodal models like CLIP, which capture global anomalies and improve cross-domain generalization. Despite recent progress, state-of-the-art deepfake detectors still face major challenges in handling distribution shifts from emerging generative models and addressing severe class imbalance between authentic and fake samples in deepfake datasets, which limits their robustness and detection accuracy. To address these challenges, we propose a framework that combines dynamic loss reweighting and ranking-based optimization, which achieves superior generalization and performance under imbalanced dataset conditions. The code is available at https://github.com/Purdue-M2/SP_CUP.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2504.21063.pdf' target='_blank'>https://arxiv.org/pdf/2504.21063.pdf</a></span>   <span><a href='https://github.com/GongShuai8210/TRIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Gong, Chaoran Cui, Xiaolin Dong, Xiushan Nie, Lei Zhu, Xiaojun Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21063">Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization (FedDG) aims to learn a globally generalizable model from decentralized clients with heterogeneous data while preserving privacy. Recent studies have introduced prompt learning to adapt vision-language models (VLMs) in FedDG by learning a single global prompt. However, such a one-prompt-fits-all learning paradigm typically leads to performance degradation on personalized samples. Although the mixture of experts (MoE) offers a promising solution for specialization, existing MoE-based methods suffer from coarse image-level expert assignment and high communication costs from parameterized routers. To address these limitations, we propose TRIP, a Token-level prompt mixture with parameter-free routing framework for FedDG, which treats multiple prompts as distinct experts. Unlike existing image-level routing designs, TRIP assigns different tokens within an image to specific experts. To ensure communication efficiency, TRIP incorporates a parameter-free routing mechanism based on token clustering and optimal transport. The instance-specific prompt is then synthesized by aggregating experts, weighted by the number of tokens assigned to each. Additionally, TRIP develops an unbiased learning strategy for prompt experts, leveraging the VLM's zero-shot generalization capability. Extensive experiments across four benchmarks demonstrate that TRIP achieves optimal generalization results, with communication of only 1K parameters per round. Our code is available at https://github.com/GongShuai8210/TRIP.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2504.20571.pdf' target='_blank'>https://arxiv.org/pdf/2504.20571.pdf</a></span>   <span><a href='https://github.com/ypwang61/One-Shot-RLVR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ypwang61/One-Shot-RLVR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20571">Reinforcement Learning for Reasoning in Large Language Models with One Training Example</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the mathematical reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Furthermore, RLVR with only two examples even slightly exceeds these results (MATH500: 74.8%, average: 36.6%). Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. We also show the critical role of promoting exploration (e.g., by incorporating entropy loss with an appropriate coefficient) in 1-shot RLVR training. We also further discuss related observations about format correction, label robustness and prompt modification. These findings can inspire future work on RLVR efficiency and encourage a re-examination of recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2504.19737.pdf' target='_blank'>https://arxiv.org/pdf/2504.19737.pdf</a></span>   <span><a href='https://github.com/Abhishek19009/CoDEx' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhishek Kuriyal, Elliot Vincent, Mathieu Aubry, Loic Landrieu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19737">CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global variations in terrain appearance raise a major challenge for satellite image analysis, leading to poor model performance when training on locations that differ from those encountered at test time. This remains true even with recent large global datasets. To address this challenge, we propose a novel domain-generalization framework for satellite images. Instead of trying to learn a single generalizable model, we train one expert model per training domain, while learning experts' similarity and encouraging similar experts to be consistent. A model selection module then identifies the most suitable experts for a given test sample and aggregates their predictions. Experiments on four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent gains over existing domain generalization and adaptation methods. Our code is publicly available at https://github.com/Abhishek19009/CoDEx.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2504.19574.pdf' target='_blank'>https://arxiv.org/pdf/2504.19574.pdf</a></span>   <span><a href='https://github.com/sminhwang/DG-DETR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seongmin Hwang, Daeyoung Han, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19574">DG-DETR: Toward Domain Generalized Detection Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end Transformer-based detectors (DETRs) have demonstrated strong detection performance. However, domain generalization (DG) research has primarily focused on convolutional neural network (CNN)-based detectors, while paying little attention to enhancing the robustness of DETRs. In this letter, we introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple, effective, and plug-and-play method that improves out-of-distribution (OOD) robustness for DETRs. Specifically, we propose a novel domain-agnostic query selection strategy that removes domain-induced biases from object queries via orthogonal projection onto the instance-specific style space. Additionally, we leverage a wavelet decomposition to disentangle features into domain-invariant and domain-specific components, enabling synthesis of diverse latent styles while preserving the semantic features of objects. Experimental results validate the effectiveness of DG-DETR. Our code is available at https://github.com/sminhwang/DG-DETR.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2504.17515.pdf' target='_blank'>https://arxiv.org/pdf/2504.17515.pdf</a></span>   <span><a href='https://github.com/orange-czh/Mamba-Sea' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/orange-czh/Mamba-Sea' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Cheng, Jintao Guo, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17515">Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To segment medical images with distribution shifts, domain generalization (DG) has emerged as a promising setting to train models on source domains that can generalize to unseen target domains. Existing DG methods are mainly based on CNN or ViT architectures. Recently, advanced state space models, represented by Mamba, have shown promising results in various supervised medical image segmentation. The success of Mamba is primarily owing to its ability to capture long-range dependencies while keeping linear complexity with input sequence length, making it a promising alternative to CNNs and ViTs. Inspired by the success, in the paper, we explore the potential of the Mamba architecture to address distribution shifts in DG for medical image segmentation. Specifically, we propose a novel Mamba-based framework, Mamba-Sea, incorporating global-to-local sequence augmentation to improve the model's generalizability under domain shift issues. Our Mamba-Sea introduces a global augmentation mechanism designed to simulate potential variations in appearance across different sites, aiming to suppress the model's learning of domain-specific information. At the local level, we propose a sequence-wise augmentation along input sequences, which perturbs the style of tokens within random continuous sub-sequences by modeling and resampling style statistics associated with domain shifts. To our best knowledge, Mamba-Sea is the first work to explore the generalization of Mamba for medical image segmentation, providing an advanced and promising Mamba-based architecture with strong robustness to domain shifts. Remarkably, our proposed method is the first to surpass a Dice coefficient of 90% on the Prostate dataset, which exceeds previous SOTA of 88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2504.16433.pdf' target='_blank'>https://arxiv.org/pdf/2504.16433.pdf</a></span>   <span><a href='https://github.com/HariseetharamG/FrogDogNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hariseetharam Gunduboina, Muhammad Haris Khan, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16433">FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, large-scale vision-language models (VLMs) like CLIP have gained attention for their zero-shot inference using instructional text prompts. While these models excel in general computer vision, their potential for domain generalization in remote sensing (RS) remains underexplored. Existing approaches enhance prompt learning by generating visual prompt tokens but rely on full-image features, introducing noise and background artifacts that vary within a class, causing misclassification. To address this, we propose FrogDogNet, a novel prompt learning framework integrating Fourier frequency filtering and self-attention to improve RS scene classification and domain generalization. FrogDogNet selectively retains invariant low-frequency components while eliminating noise and irrelevant backgrounds, ensuring robust feature representation across domains. The model first extracts significant features via projection and self-attention, then applies frequency-based filtering to preserve essential structural information for prompt learning. Extensive experiments on four RS datasets and three domain generalization tasks show that FrogDogNet consistently outperforms state-of-the-art prompt learning methods, demonstrating superior adaptability across domain shifts. Our findings highlight the effectiveness of frequency-based invariant feature retention in generalization, paving the way for broader applications. Our code is available at https://github.com/HariseetharamG/FrogDogNet
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2504.14280.pdf' target='_blank'>https://arxiv.org/pdf/2504.14280.pdf</a></span>   <span><a href='https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Li, Yongguang Li, Yali Fu, Jiahong Liu, Yixin Liu, Menglin Yang, Irwin King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14280">CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for enhancing model robustness across diverse environments. Contrastive Language-Image Pretraining (CLIP) plays a significant role in these tasks, offering powerful zero-shot capabilities that allow models to perform effectively in unseen domains. However, there remains a significant gap in the literature, as no comprehensive survey currently exists that systematically explores the applications of CLIP in DG and DA, highlighting the necessity for this review. This survey presents a comprehensive review of CLIP's applications in DG and DA. In DG, we categorize methods into optimizing prompt learning for task alignment and leveraging CLIP as a backbone for effective feature extraction, both enhancing model adaptability. For DA, we examine both source-available methods utilizing labeled source data and source-free approaches primarily based on target domain data, emphasizing knowledge transfer mechanisms and strategies for improved performance across diverse contexts. Key challenges, including overfitting, domain diversity, and computational efficiency, are addressed, alongside future research opportunities to advance robustness and efficiency in practical applications. By synthesizing existing literature and pinpointing critical gaps, this survey provides valuable insights for researchers and practitioners, proposing directions for effectively leveraging CLIP to enhance methodologies in domain generalization and adaptation. Ultimately, this work aims to foster innovation and collaboration in the quest for more resilient machine learning models that can perform reliably across diverse real-world scenarios. A more up-to-date version of the papers is maintained at: https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2504.14260.pdf' target='_blank'>https://arxiv.org/pdf/2504.14260.pdf</a></span>   <span><a href='https://github.com/TorchRWKV/flash-linear-attention' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Xiao, Li Zhiyuan, Lin Yueyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14260">Cross-attention for State-based model RWKV-7</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce CrossWKV, a novel cross-attention mechanism for the state-based RWKV-7 model, designed to enhance the expressive power of text-to-image generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV) architecture, CrossWKV integrates text and image modalities in a single pass, utilizing a generalized delta rule with vector-valued gating and low-rank adaptations (LoRA) to achieve superior cross-modal alignment. Unlike Transformer-based models, CrossWKV's non-diagonal, input-dependent transition matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$ complexity class, including all regular languages, as demonstrated by its ability to perform state-tracking tasks like $S_5$ permutation modeling. Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance while offering robust generalization across diverse prompts. The model's enhanced expressivity, combined with constant memory usage and linear scaling, positions it as a powerful solution for advanced cross-modal tasks, with potential applications in high-resolution generation and dynamic state manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2504.13111.pdf' target='_blank'>https://arxiv.org/pdf/2504.13111.pdf</a></span>   <span><a href='https://kumarmanas.github.io/SHIFT/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Manas, Christian Schlauch, Adrian Paschke, Christian Wirth, Nadja Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13111">Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their out-of-distribution generalization remains a significant challenge, particularly due to unbalanced data and a lack of enough data and diversity to ensure robustness and calibration. To address this, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework that uniquely combines well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and employs heteroscedastic spectral-normalized Gaussian processes to effectively disentangle epistemic and aleatoric uncertainties. We learn informative priors from training labels, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations over the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Project page: https://kumarmanas.github.io/SHIFT/.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2504.09696.pdf' target='_blank'>https://arxiv.org/pdf/2504.09696.pdf</a></span>   <span><a href='https://github.com/aeroplanepaper/GRPO-LEAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jixiao Zhang, Chunsheng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09696">GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Group Relative Policy Optimization (GRPO), which is widely adopted by R1-like reasoning models, has advanced mathematical reasoning. Nevertheless, GRPO faces challenges in reward sparsity, verbosity, and inadequate focus on problem difficulty. We propose GRPO-LEAD, enhancing GRPO with: (1) length-regularized rewards to encourage conciseness while maintaining accuracy; (2) explicit penalties for incorrect solutions to improve model precision; and (3) difficulty-aware advantage reweighting for robust generalization on challenging problems. Comprehensive evaluations demonstrate that GRPO-LEAD significantly improves reasoning accuracy, conciseness, and efficiency. Our approach achieves state-of-the-art performance for 14B-scale models, underscoring the synergy of our methods with appropriate model scale and high-quality data. Our source code, generated dataset, and models are available at https://github.com/aeroplanepaper/GRPO-LEAD.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2504.09448.pdf' target='_blank'>https://arxiv.org/pdf/2504.09448.pdf</a></span>   <span><a href='https://github.com/LinLLLL/BayesCAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Zhu, Xinbing Wang, Chenghu Zhou, Nanyang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09448">Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large pre-trained models showed promising results in few-shot learning. However, their generalization ability on two-dimensional Out-of-Distribution (OoD) data, i.e., correlation shift and diversity shift, has not been thoroughly investigated. Researches have shown that even with a significant amount of training data, few methods can achieve better performance than the standard empirical risk minimization method (ERM) in OoD generalization. This few-shot OoD generalization dilemma emerges as a challenging direction in deep neural network generalization research, where the performance suffers from overfitting on few-shot examples and OoD generalization errors. In this paper, leveraging a broader supervision source, we explore a novel Bayesian cross-modal image-text alignment learning method (Bayes-CAL) to address this issue. Specifically, the model is designed as only text representations are fine-tuned via a Bayesian modelling approach with gradient orthogonalization loss and invariant risk minimization (IRM) loss. The Bayesian approach is essentially introduced to avoid overfitting the base classes observed during training and improve generalization to broader unseen classes. The dedicated loss is introduced to achieve better image-text alignment by disentangling the causal and non-casual parts of image features. Numerical experiments demonstrate that Bayes-CAL achieved state-of-the-art OoD generalization performances on two-dimensional distribution shifts. Moreover, compared with CLIP-like models, Bayes-CAL yields more stable generalization performances on unseen classes. Our code is available at https://github.com/LinLLLL/BayesCAL.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2504.06220.pdf' target='_blank'>https://arxiv.org/pdf/2504.06220.pdf</a></span>   <span><a href='https://github.com/VisionXLab/Earth-Adapter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxing Hu, Ziyang Gong, Yupei Wang, Yuru Jia, Gen Luo, Xue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06220">Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt powerful Foundation Models (FMs) to diverse downstream tasks while preserving and unleashing their inherent capabilities. However, we have observed that existing PEFT methods, which are often designed with natural imagery in mind, struggle when applied to Remote Sensing (RS) scenarios. This is primarily due to their inability to handle artifact influences, a problem particularly severe in RS image features. To tackle this challenge, we introduce Earth-Adapter, the first PEFT method specifically designed for RS artifacts conquering. Earth-Adapter introduces a novel Mixture of Frequency Adaptation process that combines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT). By utilizing DFT, Earth-Adapter can decompose features into different frequency components, precisely separating artifacts from original features. The MoA then dynamically assigns weights to each adapter expert, allowing for the combination of features across various frequency domains. These simple-yet-effective approaches enable Earth-Adapter to more efficiently overcome the disturbances caused by artifacts than previous PEFT methods, significantly enhancing the FMs' performance on RS scenarios. Experiments on Domain Adaptation (DA), and Domain Generalization (DG) semantic segmentation benchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline Rein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG benchmarks. Our code will be released at https://github.com/VisionXLab/Earth-Adapter.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2504.04517.pdf' target='_blank'>https://arxiv.org/pdf/2504.04517.pdf</a></span>   <span><a href='https://github.com/jaychempan/ETS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiancheng Pan, Yanxing Liu, Xiao He, Long Peng, Jiahao Li, Yuze Sun, Xiaomeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04517">Enhance Then Search: An Augmentation-Search Strategy with Foundation Models for Cross-Domain Few-Shot Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models pretrained on extensive datasets, such as GroundingDINO and LAE-DINO, have performed remarkably in the cross-domain few-shot object detection (CD-FSOD) task. Through rigorous few-shot training, we found that the integration of image-based data augmentation techniques and grid-based sub-domain search strategy significantly enhances the performance of these foundation models. Building upon GroundingDINO, we employed several widely used image augmentation methods and established optimization objectives to effectively navigate the expansive domain space in search of optimal sub-domains. This approach facilitates efficient few-shot object detection and introduces an approach to solving the CD-FSOD problem by efficiently searching for the optimal parameter configuration from the foundation model. Our findings substantially advance the practical deployment of vision-language models in data-scarce environments, offering critical insights into optimizing their cross-domain generalization capabilities without labor-intensive retraining. Code is available at https://github.com/jaychempan/ETS.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2504.04034.pdf' target='_blank'>https://arxiv.org/pdf/2504.04034.pdf</a></span>   <span><a href='https://github.com/kylechuuuuu/UCS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhu, Li Chen, Dianshuo Li, Yunxiang Cao, Jun Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04034">UCS: A Universal Model for Curvilinear Structure Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Curvilinear structure segmentation (CSS) is essential in various domains, including medical imaging, landscape analysis, industrial surface inspection, and plant analysis. While existing methods achieve high performance within specific domains, their generalizability is limited. On the other hand, large-scale models such as Segment Anything Model (SAM) exhibit strong generalization but are not optimized for curvilinear structures. Existing adaptations of SAM primarily focus on general object segmentation and lack specialized design for CSS tasks. To bridge this gap, we propose the Universal Curvilinear structure Segmentation (UCS) model, which adapts SAM to CSS tasks while further enhancing its cross-domain generalization. UCS features a novel encoder architecture integrating a pretrained SAM encoder with two innovations: a Sparse Adapter, strategically inserted to inherit the pre-trained SAM encoder's generalization capability while minimizing the number of fine-tuning parameters, and a Prompt Generation module, which leverages Fast Fourier Transform with a high-pass filter to generate curve-specific prompts. Furthermore, the UCS incorporates a mask decoder that eliminates reliance on manual interaction through a dual-compression module: a Hierarchical Feature Compression module, which aggregates the outputs of the sampled encoder to enhance detail preservation, and a Guidance Feature Compression module, which extracts and compresses image-driven guidance features. Evaluated on a comprehensive multi-domain dataset, including an in-house dataset covering eight natural curvilinear structures, UCS demonstrates state-of-the-art generalization and open-set segmentation performance across medical, engineering, natural, and plant imagery, establishing a new benchmark for universal CSS. The source code is available at https://github.com/kylechuuuuu/UCS.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2504.02298.pdf' target='_blank'>https://arxiv.org/pdf/2504.02298.pdf</a></span>   <span><a href='https://github.com/ethanxyluo/SPACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Luo, Kecheng Chen, Pao-Sheng Vincent Sun, Chris Xing Tian, Arindam Basu, Haoliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02298">SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility. However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios. Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior. To address these challenges, we propose SPike-Aware Consistency Enhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data. We evaluate SPACE on multiple datasets. Furthermore, SPACE exhibits robust generalization across diverse network architectures, consistently enhancing the performance of SNNs on CNNs, Transformer, and ConvLSTM architectures. Experimental results show that SPACE outperforms state-of-the-art ANN methods while maintaining lower computational cost, highlighting its effectiveness and robustness for SNNs in real-world settings. The code will be available at https://github.com/ethanxyluo/SPACE.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2504.02272.pdf' target='_blank'>https://arxiv.org/pdf/2504.02272.pdf</a></span>   <span><a href='https://github.com/longshaocong/GCDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Long, Qianyu Zhou, Xiangtai Li, Chenhao Ying, Yunhai Tong, Lizhuang Ma, Yuan Luo, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02272">Generative Classifier for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to improve the generalizability of computer vision models toward distribution shifts. The mainstream DG methods focus on learning domain invariance, however, such methods overlook the potential inherent in domain-specific information. While the prevailing practice of discriminative linear classifier has been tailored to domain-invariant features, it struggles when confronted with diverse domain-specific information, e.g., intra-class shifts, that exhibits multi-modality. To address these issues, we explore the theoretical implications of relying on domain invariance, revealing the crucial role of domain-specific information in mitigating the target risk for DG. Drawing from these insights, we propose Generative Classifier-driven Domain Generalization (GCDG), introducing a generative paradigm for the DG classifier based on Gaussian Mixture Models (GMMs) for each class across domains. GCDG consists of three key modules: Heterogeneity Learning Classifier~(HLC), Spurious Correlation Blocking~(SCB), and Diverse Component Balancing~(DCB). Concretely, HLC attempts to model the feature distributions and thereby capture valuable domain-specific information via GMMs. SCB identifies the neural units containing spurious correlations and perturbs them, mitigating the risk of HLC learning spurious patterns. Meanwhile, DCB ensures a balanced contribution of components in HLC, preventing the underestimation or neglect of critical components. In this way, GCDG excels in capturing the nuances of domain-specific information characterized by diverse distributions. GCDG demonstrates the potential to reduce the target risk and encourage flat minima, improving the generalizability. Extensive experiments show GCDG's comparable performance on five DG benchmarks and one face anti-spoofing dataset, seamlessly integrating into existing DG methods with consistent improvements.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2503.22748.pdf' target='_blank'>https://arxiv.org/pdf/2503.22748.pdf</a></span>   <span><a href='https://github.com/yin-gz/SPARK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gongzhu Yin, Hongli Zhang, Yi Luo, Yuchen Yang, Kun Lu, Chao Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22748">Ignite Forecasting with SPARK: An Efficient Generative Framework for Refining LLMs in Temporal Knowledge Graph Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal Knowledge Graph (TKG) forecasting is crucial for predicting future events using historical data. With the surge of Large Language Models (LLMs), recent studies have begun exploring their integration into TKG forecasting and achieved some success. However, they still face limitations such as limited input length, inefficient output generation, and resource-intensive refinement, which undermine their performance and practical applicability. To address these limitations, we introduce SPARK, a Sequence-level Proxy-Adapting framework for Refining LLMs in TKG forecasting. Inspired by inference-time algorithms adopted in controlling generation, SPARK offers a cost-effective, plug-and-play solution through two key innovations: (1) Beam Sequence-Level Generation, which reframes TKG forecasting as a top-K sequence-level generation task, using beam search for efficiently generating next-entity distribution in a single forward pass. (2) TKG Adapter for Refinement, which employs traditional TKG models as trainable proxy adapters to leverage global graph information and refine LLM outputs, overcoming both the input length and the resource-intensive fine-tuning problems. Experiments across diverse datasets validate SPARK's forecasting performance, robust generalization capabilities, and high efficiency. We release source codes at https://github.com/yin-gz/SPARK.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2503.21847.pdf' target='_blank'>https://arxiv.org/pdf/2503.21847.pdf</a></span>   <span><a href='https://yong-xie-xy.github.io/ReCoM/' target='_blank'>  GitHub</a></span> <span><a href='https://yong-xie-xy.github.io/ReCoM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Xie, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21847">ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ReCoM, an efficient framework for generating high-fidelity and generalizable human body motions synchronized with speech. The core innovation lies in the Recurrent Embedded Transformer (RET), which integrates Dynamic Embedding Regularization (DER) into a Vision Transformer (ViT) core architecture to explicitly model co-speech motion dynamics. This architecture enables joint spatial-temporal dependency modeling, thereby enhancing gesture naturalness and fidelity through coherent motion synthesis. To enhance model robustness, we incorporate the proposed DER strategy, which equips the model with dual capabilities of noise resistance and cross-domain generalization, thereby improving the naturalness and fluency of zero-shot motion generation for unseen speech inputs. To mitigate inherent limitations of autoregressive inference, including error accumulation and limited self-correction, we propose an iterative reconstruction inference (IRI) strategy. IRI refines motion sequences via cyclic pose reconstruction, driven by two key components: (1) classifier-free guidance improves distribution alignment between generated and real gestures without auxiliary supervision, and (2) a temporal smoothing process eliminates abrupt inter-frame transitions while ensuring kinematic continuity. Extensive experiments on benchmark datasets validate ReCoM's effectiveness, achieving state-of-the-art performance across metrics. Notably, it reduces the FrÃ©chet Gesture Distance (FGD) from 18.70 to 2.48, demonstrating an 86.7% improvement in motion realism. Our project page is https://yong-xie-xy.github.io/ReCoM/.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2503.18483.pdf' target='_blank'>https://arxiv.org/pdf/2503.18483.pdf</a></span>   <span><a href='https://github.com/joeyz0z/LanCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zequn Zeng, Yudi Su, Jianqiao Sun, Tiansheng Wen, Hao Zhang, Zhengjue Wang, Bo Chen, Hongwei Liu, Jiawei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18483">Explaining Domain Shifts in Language: Concept erasing for Interpretable Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Concept-based models can map black-box representations to human-understandable concepts, which makes the decision-making process more transparent and then allows users to understand the reason behind predictions. However, domain-specific concepts often impact the final predictions, which subsequently undermine the model generalization capabilities, and prevent the model from being used in high-stake applications. In this paper, we propose a novel Language-guided Concept-Erasing (LanCE) framework. In particular, we empirically demonstrate that pre-trained vision-language models (VLMs) can approximate distinct visual domain shifts via domain descriptors while prompting large Language Models (LLMs) can easily simulate a wide range of descriptors of unseen visual domains. Then, we introduce a novel plug-in domain descriptor orthogonality (DDO) regularizer to mitigate the impact of these domain-specific concepts on the final predictions. Notably, the DDO regularizer is agnostic to the design of concept-based models and we integrate it into several prevailing models. Through evaluation of domain generalization on four standard benchmarks and three newly introduced benchmarks, we demonstrate that DDO can significantly improve the out-of-distribution (OOD) generalization over the previous state-of-the-art concept-based models.Our code is available at https://github.com/joeyz0z/LanCE.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2503.18294.pdf' target='_blank'>https://arxiv.org/pdf/2503.18294.pdf</a></span>   <span><a href='https://github.com/Falmi/LGPS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fiseha B. Tesema, Alejandro Guerra Manzanares, Tianxiang Cui, Qian Zhang, Moses Solomon, Sean He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18294">LGPS: A Lightweight GAN-Based Approach for Polyp Segmentation in Colonoscopy Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Colorectal cancer (CRC) is a major global cause of cancer-related deaths, with early polyp detection and removal during colonoscopy being crucial for prevention. While deep learning methods have shown promise in polyp segmentation, challenges such as high computational costs, difficulty in segmenting small or low-contrast polyps, and limited generalizability across datasets persist. To address these issues, we propose LGPS, a lightweight GAN-based framework for polyp segmentation. LGPS incorporates three key innovations: (1) a MobileNetV2 backbone enhanced with modified residual blocks and Squeeze-and-Excitation (ResE) modules for efficient feature extraction; (2) Convolutional Conditional Random Fields (ConvCRF) for precise boundary refinement; and (3) a hybrid loss function combining Binary Cross-Entropy, Weighted IoU Loss, and Dice Loss to address class imbalance and enhance segmentation accuracy. LGPS is validated on five benchmark datasets and compared with state-of-the-art(SOTA) methods. On the largest and challenging PolypGen test dataset, LGPS achieves a Dice of 0.7299 and an IoU of 0.7867, outperformed all SOTA works and demonstrating robust generalization. With only 1.07 million parameters, LGPS is 17 times smaller than the smallest existing model, making it highly suitable for real-time clinical applications. Its lightweight design and strong performance underscore its potential for improving early CRC diagnosis. Code is available at https://github.com/Falmi/LGPS/.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2503.14526.pdf' target='_blank'>https://arxiv.org/pdf/2503.14526.pdf</a></span>   <span><a href='https://yuffish.github.io/rebot/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Fang, Yue Yang, Xinghao Zhu, Kaiyuan Zheng, Gedas Bertasius, Daniel Szafir, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14526">ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models present a promising paradigm by training policies directly on real robot datasets like Open X-Embodiment. However, the high cost of real-world data collection hinders further data scaling, thereby restricting the generalizability of VLAs. In this paper, we introduce ReBot, a novel real-to-sim-to-real approach for scaling real robot datasets and adapting VLA models to target domains, which is the last-mile deployment challenge in robot manipulation. Specifically, ReBot replays real-world robot trajectories in simulation to diversify manipulated objects (real-to-sim), and integrates the simulated movements with inpainted real-world background to synthesize physically realistic and temporally consistent robot videos (sim-to-real). Our approach has several advantages: 1) it enjoys the benefit of real data to minimize the sim-to-real gap; 2) it leverages the scalability of simulation; and 3) it can generalize a pretrained VLA to a target domain with fully automated data pipelines. Extensive experiments in both simulation and real-world environments show that ReBot significantly enhances the performance and robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot improved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and out-of-domain generalization by 19.9% and 9.4%, respectively. For real-world evaluation with a Franka robot, ReBot increased the success rates of Octo by 17% and OpenVLA by 20%. More information can be found at: https://yuffish.github.io/rebot/
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2503.13915.pdf' target='_blank'>https://arxiv.org/pdf/2503.13915.pdf</a></span>   <span><a href='https://github.com/dongkwani/UPCSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongkwan Lee, Kyomin Hwang, Nojun Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13915">Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of semi-supervised domain generalization (SSDG), where the distributions of train and test data differ, and only a small amount of labeled data along with a larger amount of unlabeled data are available during training. Existing SSDG methods that leverage only the unlabeled samples for which the model's predictions are highly confident (confident-unlabeled samples), limit the full utilization of the available unlabeled data. To the best of our knowledge, we are the first to explore a method for incorporating the unconfident-unlabeled samples that were previously disregarded in SSDG setting. To this end, we propose UPCSC to utilize these unconfident-unlabeled samples in SSDG that consists of two modules: 1) Unlabeled Proxy-based Contrastive learning (UPC) module, treating unconfident-unlabeled samples as additional negative pairs and 2) Surrogate Class learning (SC) module, generating positive pairs for unconfident-unlabeled samples using their confusing class set. These modules are plug-and-play and do not require any domain labels, which can be easily integrated into existing approaches. Experiments on four widely used SSDG benchmarks demonstrate that our approach consistently improves performance when attached to baselines and outperforms competing plug-and-play methods. We also analyze the role of our method in SSDG, showing that it enhances class-level discriminability and mitigates domain gaps. The code is available at https://github.com/dongkwani/UPCSC.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2503.13579.pdf' target='_blank'>https://arxiv.org/pdf/2503.13579.pdf</a></span>   <span><a href='https://seokhyeonhong.github.io/projects/asmr/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokhyeon Hong, Soojin Choi, Chaelin Kim, Sihun Cha, Junyong Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13579">ASMR: Adaptive Skeleton-Mesh Rigging and Skinning via 2D Generative Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the growing accessibility of skeletal motion data, integrating it for animating character meshes remains challenging due to diverse configurations of both skeletons and meshes. Specifically, the body scale and bone lengths of the skeleton should be adjusted in accordance with the size and proportions of the mesh, ensuring that all joints are accurately positioned within the character mesh. Furthermore, defining skinning weights is complicated by variations in skeletal configurations, such as the number of joints and their hierarchy, as well as differences in mesh configurations, including their connectivity and shapes. While existing approaches have made efforts to automate this process, they hardly address the variations in both skeletal and mesh configurations. In this paper, we present a novel method for the automatic rigging and skinning of character meshes using skeletal motion data, accommodating arbitrary configurations of both meshes and skeletons. The proposed method predicts the optimal skeleton aligned with the size and proportion of the mesh as well as defines skinning weights for various mesh-skeleton configurations, without requiring explicit supervision tailored to each of them. By incorporating Diffusion 3D Features (Diff3F) as semantic descriptors of character meshes, our method achieves robust generalization across different configurations. To assess the performance of our method in comparison to existing approaches, we conducted comprehensive evaluations encompassing both quantitative and qualitative analyses, specifically examining the predicted skeletons, skinning weights, and deformation quality.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2503.13012.pdf' target='_blank'>https://arxiv.org/pdf/2503.13012.pdf</a></span>   <span><a href='https://github.com/Yore0/TTDG-MGM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingguo Lv, Xingbo Dong, Liwen Wang, Jiewen Yang, Lei Zhao, Bin Pu, Zhe Jin, Xuejun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13012">Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite domain generalization (DG) has significantly addressed the performance degradation of pre-trained models caused by domain shifts, it often falls short in real-world deployment. Test-time adaptation (TTA), which adjusts a learned model using unlabeled test data, presents a promising solution. However, most existing TTA methods struggle to deliver strong performance in medical image segmentation, primarily because they overlook the crucial prior knowledge inherent to medical images. To address this challenge, we incorporate morphological information and propose a framework based on multi-graph matching. Specifically, we introduce learnable universe embeddings that integrate morphological priors during multi-source training, along with novel unsupervised test-time paradigms for domain adaptation. This approach guarantees cycle-consistency in multi-matching while enabling the model to more effectively capture the invariant priors of unseen data, significantly mitigating the effects of domain shifts. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches on two medical image segmentation benchmarks for both multi-source and single-source domain generalization tasks. The source code is available at https://github.com/Yore0/TTDG-MGM.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2503.12797.pdf' target='_blank'>https://arxiv.org/pdf/2503.12797.pdf</a></span>   <span><a href='https://github.com/thunlp/DeepPerception' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12797">DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\% accuracy improvements on KVG-Bench and exhibiting +4.60\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2503.12049.pdf' target='_blank'>https://arxiv.org/pdf/2503.12049.pdf</a></span>   <span><a href='https://jason-aplp.github.io/TACO' target='_blank'>  GitHub</a></span> <span><a href='https://jason-aplp.github.io/TACO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Lu, Yixin Chen, Yu Liu, Jiaxiang Tang, Junfeng Ni, Diwen Wan, Gang Zeng, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12049">TACO: Taming Diffusion for in-the-wild Video Amodal Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans can infer complete shapes and appearances of objects from limited visual cues, relying on extensive prior knowledge of the physical world. However, completing partially observable objects while ensuring consistency across video frames remains challenging for existing models, especially for unstructured, in-the-wild videos. This paper tackles the task of Video Amodal Completion (VAC), which aims to generate the complete object consistently throughout the video given a visual prompt specifying the object of interest. Leveraging the rich, consistent manifolds learned by pre-trained video diffusion models, we propose a conditional diffusion model, TACO, that repurposes these manifolds for VAC. To enable its effective and robust generalization to challenging in-the-wild scenarios, we curate a large-scale synthetic dataset with multiple difficulty levels by systematically imposing occlusions onto un-occluded videos. Building on this, we devise a progressive fine-tuning paradigm that starts with simpler recovery tasks and gradually advances to more complex ones. We demonstrate TACO's versatility on a wide range of in-the-wild videos from Internet, as well as on diverse, unseen datasets commonly used in autonomous driving, robotic manipulation, and scene understanding. Moreover, we show that TACO can be effectively applied to various downstream tasks like object reconstruction and pose estimation, highlighting its potential to facilitate physical world understanding and reasoning. Our project page is available at https://jason-aplp.github.io/TACO.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2503.10996.pdf' target='_blank'>https://arxiv.org/pdf/2503.10996.pdf</a></span>   <span><a href='https://github.com/GaotangLi/JUICE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaotang Li, Yuzhong Chen, Hanghang Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10996">Taming Knowledge Conflicts in Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language Models (LMs) often encounter knowledge conflicts when parametric memory contradicts contextual knowledge. Previous works attribute this conflict to the interplay between "memory heads" and "context heads", attention heads assumed to promote either memory or context exclusively. In this study, we go beyond this fundamental assumption by uncovering a critical phenomenon we term the superposition of contextual information and parametric memory, where highly influential attention heads simultaneously contribute to both memory and context. Building upon this insight, we propose Just Run Twice (JuICE), a test-time attention intervention method that steers LMs toward either parametric beliefs or contextual knowledge without requiring fine-tuning. JuICE identifies a set of reliable attention heads and leverages a dual-run approach to mitigate the superposition effects. Extensive experiments across 11 datasets and 6 model architectures demonstrate that JuICE sets the new state-of-the-art performance and robust generalization, achieving significant and consistent improvement across different domains under various conflict types. Finally, we theoretically analyze knowledge conflict and the superposition of contextual information and parametric memory in attention heads, which further elucidates the effectiveness of JuICE in these settings. Our code is available at https://github.com/GaotangLi/JUICE.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2503.10615.pdf' target='_blank'>https://arxiv.org/pdf/2503.10615.pdf</a></span>   <span><a href='https://github.com/Fancy-MLLM/R1-onevision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10615">R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2503.10526.pdf' target='_blank'>https://arxiv.org/pdf/2503.10526.pdf</a></span>   <span><a href='https://github.com/zzezze/NeighborRetr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengrong Lin, Zheng Wang, Tianwen Qian, Pan Mu, Sixian Chan, Cong Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10526">NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal retrieval aims to bridge the semantic gap between different modalities, such as visual and textual data, enabling accurate retrieval across them. Despite significant advancements with models like CLIP that align cross-modal representations, a persistent challenge remains: the hubness problem, where a small subset of samples (hubs) dominate as nearest neighbors, leading to biased representations and degraded retrieval accuracy. Existing methods often mitigate hubness through post-hoc normalization techniques, relying on prior data distributions that may not be practical in real-world scenarios. In this paper, we directly mitigate hubness during training and introduce NeighborRetr, a novel method that effectively balances the learning of hubs and adaptively adjusts the relations of various kinds of neighbors. Our approach not only mitigates the hubness problem but also enhances retrieval performance, achieving state-of-the-art results on multiple cross-modal retrieval benchmarks. Furthermore, NeighborRetr demonstrates robust generalization to new domains with substantial distribution shifts, highlighting its effectiveness in real-world applications. We make our code publicly available at: https://github.com/zzezze/NeighborRetr .
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2503.10460.pdf' target='_blank'>https://arxiv.org/pdf/2503.10460.pdf</a></span>   <span><a href='https://github.com/Qihoo360/Light-R1' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Qihoo360/Light-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10460">Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.
  Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.
  Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.
  Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at https://github.com/Qihoo360/Light-R1.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2503.10216.pdf' target='_blank'>https://arxiv.org/pdf/2503.10216.pdf</a></span>   <span><a href='https://github.com/kk42yy/CoStoDet-DDPM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixiang Yang, Xin Li, Qiang Li, Zhiwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10216">CoStoDet-DDPM: Collaborative Training of Stochastic and Deterministic Models Improves Surgical Workflow Anticipation and Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anticipating and recognizing surgical workflows are critical for intelligent surgical assistance systems. However, existing methods rely on deterministic decision-making, struggling to generalize across the large anatomical and procedural variations inherent in real-world surgeries.In this paper, we introduce an innovative framework that incorporates stochastic modeling through a denoising diffusion probabilistic model (DDPM) into conventional deterministic learning for surgical workflow analysis. At the heart of our approach is a collaborative co-training paradigm: the DDPM branch captures procedural uncertainties to enrich feature representations, while the task branch focuses on predicting surgical phases and instrument usage.Theoretically, we demonstrate that this mutual refinement mechanism benefits both branches: the DDPM reduces prediction errors in uncertain scenarios, and the task branch directs the DDPM toward clinically meaningful representations. Notably, the DDPM branch is discarded during inference, enabling real-time predictions without sacrificing accuracy.Experiments on the Cholec80 dataset show that for the anticipation task, our method achieves a 16% reduction in eMAE compared to state-of-the-art approaches, and for phase recognition, it improves the Jaccard score by 1.0%. Additionally, on the AutoLaparo dataset, our method achieves a 1.5% improvement in the Jaccard score for phase recognition, while also exhibiting robust generalization to patient-specific variations. Our code and weight are available at https://github.com/kk42yy/CoStoDet-DDPM.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2503.10149.pdf' target='_blank'>https://arxiv.org/pdf/2503.10149.pdf</a></span>   <span><a href='https://github.com/peakpang/UGP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenxuan Zeng, Qiao Wu, Xiyu Zhang, Lin Yuanbo Wu, Pei An, Jiaqi Yang, Ji Wang, Peng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10149">Unlocking Generalization Power in LiDAR Point Cloud Registration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world environments, a LiDAR point cloud registration method with robust generalization capabilities (across varying distances and datasets) is crucial for ensuring safety in autonomous driving and other LiDAR-based applications. However, current methods fall short in achieving this level of generalization. To address these limitations, we propose UGP, a pruned framework designed to enhance generalization power for LiDAR point cloud registration. The core insight in UGP is the elimination of cross-attention mechanisms to improve generalization, allowing the network to concentrate on intra-frame feature extraction. Additionally, we introduce a progressive self-attention module to reduce ambiguity in large-scale scenes and integrate Bird's Eye View (BEV) features to incorporate semantic information about scene elements. Together, these enhancements significantly boost the network's generalization performance. We validated our approach through various generalization experiments in multiple outdoor scenes. In cross-distance generalization experiments on KITTI and nuScenes, UGP achieved state-of-the-art mean Registration Recall rates of 94.5% and 91.4%, respectively. In cross-dataset generalization from nuScenes to KITTI, UGP achieved a state-of-the-art mean Registration Recall of 90.9%. Code will be available at https://github.com/peakpang/UGP.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2503.08906.pdf' target='_blank'>https://arxiv.org/pdf/2503.08906.pdf</a></span>   <span><a href='https://github.com/ChongQingNoSubway/Prompt-OT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiwen Chen, Wenhui Zhu, Peijie Qiu, Hao Wang, Huayu Li, Haiyu Wu, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08906">Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT)-guided prompt learning framework that mitigates forgetting by preserving the structural consistency of feature distributions between pre-trained and fine-tuned models. Unlike conventional point-wise constraints, OT naturally captures cross-instance relationships and expands the feasible parameter space for prompt tuning, allowing a better trade-off between adaptation and generalization. Our approach enforces joint constraints on both vision and text representations, ensuring a holistic feature alignment. Extensive experiments on benchmark datasets demonstrate that our simple yet effective method can outperform existing prompt learning strategies in base-to-novel generalization, cross-dataset evaluation, and domain generalization without additional augmentation or ensemble techniques. The code is available at https://github.com/ChongQingNoSubway/Prompt-OT
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2503.06520.pdf' target='_blank'>https://arxiv.org/pdf/2503.06520.pdf</a></span>   <span><a href='https://github.com/dvlab-research/Seg-Zero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, Jiaya Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06520">Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2503.03417.pdf' target='_blank'>https://arxiv.org/pdf/2503.03417.pdf</a></span>   <span><a href='https://github.com/JabezNzomo99/claim-matching-robustness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jabez Magomere, Emanuele La Malfa, Manuel Tonneau, Ashkan Kazemi, Scott Hale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03417">When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online misinformation remains a critical challenge, and fact-checkers increasingly rely on claim matching systems that use sentence embedding models to retrieve relevant fact-checks. However, as users interact with claims online, they often introduce edits, and it remains unclear whether current embedding models used in retrieval are robust to such edits. To investigate this, we introduce a perturbation framework that generates valid and natural claim variations, enabling us to assess the robustness of a wide-range of sentence embedding models in a multi-stage retrieval pipeline and evaluate the effectiveness of various mitigation approaches. Our evaluation reveals that standard embedding models exhibit notable performance drops on edited claims, while LLM-distilled embedding models offer improved robustness at a higher computational cost. Although a strong reranker helps to reduce the performance drop, it cannot fully compensate for first-stage retrieval gaps. To address these retrieval gaps, we evaluate train- and inference-time mitigation approaches, demonstrating that they can improve in-domain robustness by up to 17 percentage points and boost out-of-domain generalization by 10 percentage points. Overall, our findings provide practical improvements to claim-matching systems, enabling more reliable fact-checking of evolving misinformation. Code and data are available at https://github.com/JabezNzomo99/claim-matching-robustness.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2503.03222.pdf' target='_blank'>https://arxiv.org/pdf/2503.03222.pdf</a></span>   <span><a href='https://wangzhumei.github.io/mocap-2-to-3/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou, Mingtao Pei, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03222">Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering absolute human motion from monocular inputs is challenging due to two main issues. First, existing methods depend on 3D training data collected from limited environments, constraining out-of-distribution generalization. The second issue is the difficulty of estimating metric-scale poses from monocular input. To address these challenges, we introduce Mocap-2-to-3, a novel framework that performs multi-view lifting from monocular input by leveraging 2D data pre-training, enabling the reconstruction of metrically accurate 3D motions with absolute positions. To leverage abundant 2D data, we decompose complex 3D motion into multi-view syntheses. We first pretrain a single-view diffusion model on extensive 2D datasets, then fine-tune a multi-view model using public 3D data to enable view-consistent motion generation from monocular input, allowing the model to acquire action priors and diversity through 2D data. Furthermore, to recover absolute poses, we propose a novel human motion representation that decouples the learning of local pose and global movements, while encoding geometric priors of the ground to accelerate convergence. This enables progressive recovery of motion in absolute space during inference. Experimental results on in-the-wild benchmarks demonstrate that our method surpasses state-of-the-art approaches in both camera-space motion realism and world-grounded human positioning, while exhibiting superior generalization capability. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2503.02101.pdf' target='_blank'>https://arxiv.org/pdf/2503.02101.pdf</a></span>   <span><a href='https://github.com/heboyong/Generalized-Diffusion-Detector' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyong He, Yuxiang Ji, Qianwen Ye, Zhuoyue Tan, Liaoni Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02101">Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) for object detection aims to enhance detectors' performance in unseen scenarios. This task remains challenging due to complex variations in real-world applications. Recently, diffusion models have demonstrated remarkable capabilities in diverse scene generation, which inspires us to explore their potential for improving DG tasks. Instead of generating images, our method extracts multi-step intermediate features during the diffusion process to obtain domain-invariant features for generalized detection. Furthermore, we propose an efficient knowledge transfer framework that enables detectors to inherit the generalization capabilities of diffusion models through feature and object-level alignment, without increasing inference time. We conduct extensive experiments on six challenging DG benchmarks. The results demonstrate that our method achieves substantial improvements of 14.0% mAP over existing DG approaches across different domains and corruption types. Notably, our method even outperforms most domain adaptation methods without accessing any target domain data. Moreover, the diffusion-guided detectors show consistent improvements of 15.9% mAP on average compared to the baseline. Our work aims to present an effective approach for domain-generalized detection and provide potential insights for robust visual recognition in real-world scenarios. The code is available at https://github.com/heboyong/Generalized-Diffusion-Detector.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2503.00986.pdf' target='_blank'>https://arxiv.org/pdf/2503.00986.pdf</a></span>   <span><a href='https://github.com/OpenRobotLab/EgoHOD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baoqi Pei, Yifei Huang, Jilan Xu, Guo Chen, Yuping He, Lijin Yang, Yali Wang, Weidi Xie, Yu Qiao, Fei Wu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00986">Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In egocentric video understanding, the motion of hands and objects as well as their interactions play a significant role by nature. However, existing egocentric video representation learning methods mainly focus on aligning video representation with high-level narrations, overlooking the intricate dynamics between hands and objects. In this work, we aim to integrate the modeling of fine-grained hand-object dynamics into the video representation learning process. Since no suitable data is available, we introduce HOD, a novel pipeline employing a hand-object detector and a large language model to generate high-quality narrations with detailed descriptions of hand-object dynamics. To learn these fine-grained dynamics, we propose EgoVideo, a model with a new lightweight motion adapter to capture fine-grained hand-object motion information. Through our co-training strategy, EgoVideo effectively and efficiently leverages the fine-grained hand-object dynamics in the HOD data. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple egocentric downstream tasks, including improvements of 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and 16.3% in EGTEA classification in zero-shot settings. Furthermore, our model exhibits robust generalization capabilities in hand-object interaction and robot manipulation tasks. Code and data are available at https://github.com/OpenRobotLab/EgoHOD/.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2503.00429.pdf' target='_blank'>https://arxiv.org/pdf/2503.00429.pdf</a></span>   <span><a href='https://github.com/yjyddq/DADM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Yang, Xun Lin, Zitong Yu, Liepiao Zhang, Xin Liu, Hui Li, Xiaochen Yuan, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00429">DADM: Dual Alignment of Domain and Modality for Face Anti-spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the availability of diverse sensor modalities (i.e., RGB, Depth, Infrared) and the success of multi-modal learning, multi-modal face anti-spoofing (FAS) has emerged as a prominent research focus. The intuition behind it is that leveraging multiple modalities can uncover more intrinsic spoofing traces. However, this approach presents more risk of misalignment. We identify two main types of misalignment: (1) \textbf{Intra-domain modality misalignment}, where the importance of each modality varies across different attacks. For instance, certain modalities (e.g., Depth) may be non-defensive against specific attacks (e.g., 3D mask), indicating that each modality has unique strengths and weaknesses in countering particular attacks. Consequently, simple fusion strategies may fall short. (2) \textbf{Inter-domain modality misalignment}, where the introduction of additional modalities exacerbates domain shifts, potentially overshadowing the benefits of complementary fusion. To tackle (1), we propose a alignment module between modalities based on mutual information, which adaptively enhances favorable modalities while suppressing unfavorable ones. To address (2), we employ a dual alignment optimization method that aligns both sub-domain hyperplanes and modality angle margins, thereby mitigating domain gaps. Our method, dubbed \textbf{D}ual \textbf{A}lignment of \textbf{D}omain and \textbf{M}odality (DADM), achieves state-of-the-art performance in extensive experiments across four challenging protocols demonstrating its robustness in multi-modal domain generalization scenarios. The codes will be released soon.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2502.20619.pdf' target='_blank'>https://arxiv.org/pdf/2502.20619.pdf</a></span>   <span><a href='https://github.com/Senyh/StyCona' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Shen, Peng Cao, Jinzhu Yang, Osmar R. Zaiane, Zhaolin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20619">Style Content Decomposition-based Data Augmentation for Domain Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to domain shifts across diverse medical imaging modalities, learned segmentation models often suffer significant performance degradation during deployment. These domain shifts, typically caused by variations in imaging systems, generally comprise two principal components: 1) \textbf{"style" shifts}, referring to global disparities in image properties such as illumination, contrast, and color; and 2) \textbf{"content" shifts}, which involve local discrepancies in anatomical structures. To address domain shifts in medical image segmentation, a core challenge arises: how can we decouple the factors within images that determine their "style" and "content" components? To this end, we first propose a linear style-content decomposition method that factorizes an image into style codes and content maps, explicitly modeling the "style" and "content" components. Building on this, we introduce a \textbf{Sty}le-\textbf{Con}tent decomposition-based data \textbf{a}ugmentation algorithm (StyCona), which leverages this decomposition strategy to guide augmentation of both the global style and local content of source-domain images, enabling the training of a well-generalized model for domain-generalizable medical image segmentation. StyCona is a simple yet effective plug-and-play module that substantially improves model generalization without requiring additional training parameters or modifications to segmentation model architectures. Experiments on cardiac magnetic resonance imaging and fundus photography segmentation tasks, with single and multiple target domains respectively, demonstrate the effectiveness of StyCona and its superiority over state-of-the-art domain generalization methods. The code will be released at https://github.com/Senyh/StyCona.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2502.20158.pdf' target='_blank'>https://arxiv.org/pdf/2502.20158.pdf</a></span>   <span><a href='https://github.com/Mia-YatingYu/Open-MeDe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yating Yu, Congqi Cao, Yifan Zhang, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20158">Learning to Generalize without Bias for Open-Vocabulary Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging the effective visual-text alignment and static generalizability from CLIP, recent video learners adopt CLIP initialization with further regularization or recombination for generalization in open-vocabulary action recognition in-context. However, due to the static bias of CLIP, such video learners tend to overfit on shortcut static features, thereby compromising their generalizability, especially to novel out-of-context actions. To address this issue, we introduce Open-MeDe, a novel Meta-optimization framework with static Debiasing for Open-vocabulary action recognition. From a fresh perspective of generalization, Open-MeDe adopts a meta-learning approach to improve known-to-open generalizing and image-to-video debiasing in a cost-effective manner. Specifically, Open-MeDe introduces a cross-batch meta-optimization scheme that explicitly encourages video learners to quickly generalize to arbitrary subsequent data via virtual evaluation, steering a smoother optimization landscape. In effect, the free of CLIP regularization during optimization implicitly mitigates the inherent static bias of the video meta-learner. We further apply self-ensemble over the optimization trajectory to obtain generic optimal parameters that can achieve robust generalization to both in-context and out-of-context novel data. Extensive evaluations show that Open-MeDe not only surpasses state-of-the-art regularization methods tailored for in-context open-vocabulary action recognition but also substantially excels in out-of-context scenarios.Code is released at https://github.com/Mia-YatingYu/Open-MeDe.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2502.19167.pdf' target='_blank'>https://arxiv.org/pdf/2502.19167.pdf</a></span>   <span><a href='https://github.com/AI4HealthUOL/ppg-ood-generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Moulaeifard, Peter H. Charlton, Nils Strodthoff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19167">Generalizable deep learning for photoplethysmography-based blood pressure estimation -- A Benchmarking Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photoplethysmography (PPG)-based blood pressure (BP) estimation represents a promising alternative to cuff-based BP measurements. Recently, an increasing number of deep learning models have been proposed to infer BP from the raw PPG waveform. However, these models have been predominantly evaluated on in-distribution test sets, which immediately raises the question of the generalizability of these models to external datasets. To investigate this question, we trained five deep learning models on the recently released PulseDB dataset, provided in-distribution benchmarking results on this dataset, and then assessed out-of-distribution performance on several external datasets. The best model (XResNet1d101) achieved in-distribution MAEs of 9.4 and 6.0 mmHg for systolic and diastolic BP respectively on PulseDB (with subject-specific calibration), and 14.0 and 8.5 mmHg respectively without calibration. Equivalent MAEs on external test datasets without calibration ranged from 15.0 to 25.1 mmHg (SBP) and 7.0 to 10.4 mmHg (DBP). Our results indicate that the performance is strongly influenced by the differences in BP distributions between datasets. We investigated a simple way of improving performance through sample-based domain adaptation and put forward recommendations for training models with good generalization properties. With this work, we hope to educate more researchers for the importance and challenges of out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2502.18104.pdf' target='_blank'>https://arxiv.org/pdf/2502.18104.pdf</a></span>   <span><a href='https://github.com/HanNieWHU/PromptMID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Nie, Bin Luo, Jun Liu, Zhitao Fu, Huan Zhou, Shuo Zhang, Weixing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18104">PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ideal goal of image matching is to achieve stable and efficient performance in unseen domains. However, many existing learning-based optical-SAR image matching methods, despite their effectiveness in specific scenarios, exhibit limited generalization and struggle to adapt to practical applications. Repeatedly training or fine-tuning matching models to address domain differences is not only not elegant enough but also introduces additional computational overhead and data production costs. In recent years, general foundation models have shown great potential for enhancing generalization. However, the disparity in visual domains between natural and remote sensing images poses challenges for their direct application. Therefore, effectively leveraging foundation models to improve the generalization of optical-SAR image matching remains challenge. To address the above challenges, we propose PromptMID, a novel approach that constructs modality-invariant descriptors using text prompts based on land use classification as priors information for optical and SAR image matching. PromptMID extracts multi-scale modality-invariant features by leveraging pre-trained diffusion models and visual foundation models (VFMs), while specially designed feature aggregation modules effectively fuse features across different granularities. Extensive experiments on optical-SAR image datasets from four diverse regions demonstrate that PromptMID outperforms state-of-the-art matching methods, achieving superior results in both seen and unseen domains and exhibiting strong cross-domain generalization capabilities. The source code will be made publicly available https://github.com/HanNieWHU/PromptMID.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2502.13061.pdf' target='_blank'>https://arxiv.org/pdf/2502.13061.pdf</a></span>   <span><a href='https://github.com/JingbiaoMei/RGCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13061">Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While Large Multimodal Models (LMMs) have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both supervised fine-tuning (SFT) and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Analysis reveals that our approach achieves improved robustness under adversarial attacks compared to SFT models. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems. Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability. Code available at https://github.com/JingbiaoMei/RGCL
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2502.12413.pdf' target='_blank'>https://arxiv.org/pdf/2502.12413.pdf</a></span>   <span><a href='https://github.com/kokolerk/DivIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Wang, Yuhang Zhou, Zhixiong Zhang, Qiguang Chen, Yongqiang Chen, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12413">DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution generalization is a common problem that expects the model to perform well in the different distributions even far from the train data. A popular approach to addressing this issue is invariant learning (IL), in which the model is compiled to focus on invariant features instead of spurious features by adding strong constraints during training. However, there are some potential pitfalls of strong invariant constraints. Due to the limited number of diverse environments and over-regularization in the feature space, it may lead to a loss of important details in the invariant features while alleviating the spurious correlations, namely the over-invariance, which can also degrade the generalization performance. We theoretically define the over-invariance and observe that this issue occurs in various classic IL methods. To alleviate this issue, we propose a simple approach Diverse Invariant Learning (DivIL) by adding the unsupervised contrastive learning and the random masking mechanism compensatory for the invariant constraints, which can be applied to various IL methods. Furthermore, we conduct experiments across multiple modalities across 12 datasets and 6 classic models, verifying our over-invariance insight and the effectiveness of our DivIL framework. Our code is available at https://github.com/kokolerk/DivIL.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2502.07200.pdf' target='_blank'>https://arxiv.org/pdf/2502.07200.pdf</a></span>   <span><a href='https://github.com/RaviShah1/DCIN-CQG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravi Shah, Atsushi Fukuda, Quan Huu Cap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07200">Color-Quality Invariance for Robust Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization (SDG) in medical image segmentation remains a significant challenge, particularly for images with varying color distributions and qualities. Previous approaches often struggle when models trained on high-quality images fail to generalize to low-quality test images due to these color and quality shifts. In this work, we propose two novel techniques to enhance generalization: dynamic color image normalization (DCIN) module and color-quality generalization (CQG) loss. The DCIN dynamically normalizes the color of test images using two reference image selection strategies. Specifically, the DCIN utilizes a global reference image selection (GRIS), which finds a universal reference image, and a local reference image selection (LRIS), which selects a semantically similar reference image per test sample. Additionally, CQG loss enforces invariance to color and quality variations by ensuring consistent segmentation predictions across transformed image pairs. Experimental results show that our proposals significantly improve segmentation performance over the baseline on two target domain datasets, despite being trained solely on a single source domain. Notably, our model achieved up to a 32.3-point increase in Dice score compared to the baseline, consistently producing robust and usable results even under substantial domain shifts. Our work contributes to the development of more robust medical image segmentation models that generalize across unseen domains. The implementation code is available at https://github.com/RaviShah1/DCIN-CQG.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2502.06593.pdf' target='_blank'>https://arxiv.org/pdf/2502.06593.pdf</a></span>   <span><a href='https://mever-team.github.io/SAGI/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Paschalis Giakoumoglou, Dimitrios Karageorgiou, Symeon Papadopoulos, Panagiotis C. Petrantonakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06593">SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in generative AI have made text-guided image inpainting - adding, removing, or altering image regions using textual prompts - widely accessible. However, generating semantically correct photorealistic imagery, typically requires carefully-crafted prompts and iterative refinement by evaluating the realism of the generated content - tasks commonly performed by humans. To automate the generative process, we propose Semantically Aligned and Uncertainty Guided AI Image Inpainting (SAGI), a model-agnostic pipeline, to sample prompts from a distribution that closely aligns with human perception and to evaluate the generated content and discard instances that deviate from such a distribution, which we approximate using pretrained large language models and vision-language models. By applying this pipeline on multiple state-of-the-art inpainting models, we create the SAGI Dataset (SAGI-D), currently the largest and most diverse dataset of AI-generated inpaintings, comprising over 95k inpainted images and a human-evaluated subset. Our experiments show that semantic alignment significantly improves image quality and aesthetics, while uncertainty guidance effectively identifies realistic manipulations - human ability to distinguish inpainted images from real ones drops from 74% to 35% in terms of accuracy, after applying our pipeline. Moreover, using SAGI-D for training several image forensic approaches increases in-domain detection performance on average by 37.4% and out-of-domain generalization by 26.1% in terms of IoU, also demonstrating its utility in countering malicious exploitation of generative AI. Code and dataset are available at https://mever-team.github.io/SAGI/
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2502.04204.pdf' target='_blank'>https://arxiv.org/pdf/2502.04204.pdf</a></span>   <span><a href='https://github.com/fshp971/adv-icl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaopeng Fu, Liang Ding, Jingfeng Zhang, Di Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04204">Short-length Adversarial Training Helps LLMs Defend Long-length Jailbreak Attacks: Theoretical and Empirical Evidence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Jailbreak attacks against large language models (LLMs) aim to induce harmful behaviors in LLMs through carefully crafted adversarial prompts. To mitigate attacks, one way is to perform adversarial training (AT)-based alignment, i.e., training LLMs on some of the most adversarial prompts to help them learn how to behave safely under attacks. During AT, the length of adversarial prompts plays a critical role in the robustness of aligned LLMs. While long-length adversarial prompts during AT might lead to strong LLM robustness, their synthesis however is very resource-consuming, which may limit the application of LLM AT. This paper focuses on adversarial suffix jailbreak attacks and unveils that to defend against a jailbreak attack with an adversarial suffix of length $Î(M)$, it is enough to align LLMs on prompts with adversarial suffixes of length $Î(\sqrt{M})$. Theoretically, we analyze the adversarial in-context learning of linear transformers on linear regression tasks and prove a robust generalization bound for trained transformers. The bound depends on the term $Î(\sqrt{M_{\text{test}}}/M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the numbers of adversarially perturbed in-context samples during training and testing. Empirically, we conduct AT on popular open-source LLMs and evaluate their robustness against jailbreak attacks of different adversarial suffix lengths. Results confirm a positive correlation between the attack success rate and the ratio of the square root of the adversarial suffix length during jailbreaking to the length during AT. Our findings show that it is practical to defend against ``long-length'' jailbreak attacks via efficient ``short-length'' AT. The code is available at https://github.com/fshp971/adv-icl.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2502.02525.pdf' target='_blank'>https://arxiv.org/pdf/2502.02525.pdf</a></span>   <span><a href='https://github.com/CNJianLiu/Diff9D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Liu, Wei Sun, Hui Yang, Pengchao Deng, Chongpei Liu, Nicu Sebe, Hossein Rahmani, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02525">Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nine-degrees-of-freedom (9-DoF) object pose and size estimation is crucial for enabling augmented reality and robotic manipulation. Category-level methods have received extensive research attention due to their potential for generalization to intra-class unknown objects. However, these methods require manual collection and labeling of large-scale real-world training data. To address this problem, we introduce a diffusion-based paradigm for domain-generalized category-level 9-DoF object pose estimation. Our motivation is to leverage the latent generalization ability of the diffusion model to address the domain generalization challenge in object pose estimation. This entails training the model exclusively on rendered synthetic data to achieve generalization to real-world scenes. We propose an effective diffusion model to redefine 9-DoF object pose estimation from a generative perspective. Our model does not require any 3D shape priors during training or inference. By employing the Denoising Diffusion Implicit Model, we demonstrate that the reverse diffusion process can be executed in as few as 3 steps, achieving near real-time performance. Finally, we design a robotic grasping system comprising both hardware and software components. Through comprehensive experiments on two benchmark datasets and the real-world robotic system, we show that our method achieves state-of-the-art domain generalization performance. Our code will be made public at https://github.com/CNJianLiu/Diff9D.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2501.18739.pdf' target='_blank'>https://arxiv.org/pdf/2501.18739.pdf</a></span>   <span><a href='https://github.com/Zehong-Wang/GPM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehong Wang, Zheyuan Zhang, Tianyi Ma, Nitesh V Chawla, Chuxu Zhang, Yanfang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18739">Beyond Message Passing: Neural Graph Pattern Machine</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph learning tasks often hinge on identifying key substructure patterns -- such as triadic closures in social networks or benzene rings in molecular graphs -- that underpin downstream performance. However, most existing graph neural networks (GNNs) rely on message passing, which aggregates local neighborhood information iteratively and struggles to explicitly capture such fundamental motifs, like triangles, k-cliques, and rings. This limitation hinders both expressiveness and long-range dependency modeling. In this paper, we introduce the Neural Graph Pattern Machine (GPM), a novel framework that bypasses message passing by learning directly from graph substructures. GPM efficiently extracts, encodes, and prioritizes task-relevant graph patterns, offering greater expressivity and improved ability to capture long-range dependencies. Empirical evaluations across four standard tasks -- node classification, link prediction, graph classification, and graph regression -- demonstrate that GPM outperforms state-of-the-art baselines. Further analysis reveals that GPM exhibits strong out-of-distribution generalization, desirable scalability, and enhanced interpretability. Code and datasets are available at: https://github.com/Zehong-Wang/GPM.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2501.18592.pdf' target='_blank'>https://arxiv.org/pdf/2501.18592.pdf</a></span>   <span><a href='https://github.com/donghao51/Awesome-Multimodal-Adaptation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/donghao51/Awesome-Multimodal-Adaptation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18592">Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world scenarios, achieving domain adaptation and generalization poses significant challenges, as models must adapt to or generalize across unknown target distributions. Extending these capabilities to unseen multimodal distributions, i.e., multimodal domain adaptation and generalization, is even more challenging due to the distinct characteristics of different modalities. Significant progress has been made over the years, with applications ranging from action recognition to semantic segmentation. Besides, the recent advent of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired works leveraging these models to enhance adaptation and generalization performances or adapting them to downstream tasks. This survey provides the first comprehensive review of recent advances from traditional approaches to foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal test-time adaptation; (3) Multimodal domain generalization; (4) Domain adaptation and generalization with the help of multimodal foundation models; and (5) Adaptation of multimodal foundation models. For each topic, we formally define the problem and thoroughly review existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We maintain an active repository that contains up-to-date literature at https://github.com/donghao51/Awesome-Multimodal-Adaptation.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2501.14693.pdf' target='_blank'>https://arxiv.org/pdf/2501.14693.pdf</a></span>   <span><a href='https://github.com/MichiganNLP/TAMA,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naihao Deng, Rada Mihalcea
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14693">Rethinking Table Instruction Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2501.10080.pdf' target='_blank'>https://arxiv.org/pdf/2501.10080.pdf</a></span>   <span><a href='https://github.com/AIT-Assistive-Autonomous-Systems/Hopomop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Schwingshackl, Fabio Francisco Oberweger, Markus Murschitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10080">Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel approach to few-shot semantic segmentation for machinery with multiple parts that exhibit spatial and hierarchical relationships. Our method integrates the foundation models CLIPSeg and Segment Anything Model (SAM) with the interest point detector SuperPoint and a graph convolutional network (GCN) to accurately segment machinery parts. By providing 1 to 25 annotated samples, our model, evaluated on a purely synthetic dataset depicting a truck-mounted loading crane, achieves effective segmentation across various levels of detail. Training times are kept under five minutes on consumer GPUs. The model demonstrates robust generalization to real data, achieving a qualitative synthetic-to-real generalization with a $J\&F$ score of 92.2 on real data using 10 synthetic support samples. When benchmarked on the DAVIS 2017 dataset, it achieves a $J\&F$ score of 71.5 in semi-supervised video segmentation with three support samples. This method's fast training times and effective generalization to real data make it a valuable tool for autonomous systems interacting with machinery and infrastructure, and illustrate the potential of combined and orchestrated foundation models for few-shot segmentation tasks.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2501.09877.pdf' target='_blank'>https://arxiv.org/pdf/2501.09877.pdf</a></span>   <span><a href='https://github.com/Jingchensun/clap-s' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingchen Sun, Shaobo Han, Wataru Kohno, Changyou Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09877">CLAP-S: Support Set Based Adaptation for Downstream Fiber-optic Acoustic Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive Language-Audio Pretraining (CLAP) models have demonstrated unprecedented performance in various acoustic signal recognition tasks. Fiber-optic-based acoustic recognition is one of the most important downstream tasks and plays a significant role in environmental sensing. Adapting CLAP for fiber-optic acoustic recognition has become an active research area. As a non-conventional acoustic sensor, fiber-optic acoustic recognition presents a challenging, domain-specific, low-shot deployment environment with significant domain shifts due to unique frequency response and noise characteristics. To address these challenges, we propose a support-based adaptation method, CLAP-S, which linearly interpolates a CLAP Adapter with the Support Set, leveraging both implicit knowledge through fine-tuning and explicit knowledge retrieved from memory for cross-domain generalization. Experimental results show that our method delivers competitive performance on both laboratory-recorded fiber-optic ESC-50 datasets and a real-world fiber-optic gunshot-firework dataset. Our research also provides valuable insights for other downstream acoustic recognition tasks. The code and gunshot-firework dataset are available at https://github.com/Jingchensun/clap-s.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2501.04958.pdf' target='_blank'>https://arxiv.org/pdf/2501.04958.pdf</a></span>   <span><a href='https://github.com/yinghemedical/imbalance-aware_domain_adaptation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Li, Xinglin Zhang, Jun Liang, Tao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04958">Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo Development Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models in medical imaging face dual challenges: domain shift, where models perform poorly when deployed in settings different from their training environment, and class imbalance, where certain disease conditions are naturally underrepresented. We present Imbalance-Aware Domain Adaptation (IADA), a novel framework that simultaneously tackles both challenges through three key components: (1) adaptive feature learning with class-specific attention mechanisms, (2) balanced domain alignment with dynamic weighting, and (3) adaptive threshold optimization. Our theoretical analysis establishes convergence guarantees and complexity bounds. Through extensive experiments on embryo development assessment across four imaging modalities, IADA demonstrates significant improvements over existing methods, achieving up to 25.19\% higher accuracy while maintaining balanced performance across classes. In challenging scenarios with low-quality imaging systems, IADA shows robust generalization with AUC improvements of up to 12.56\%. These results demonstrate IADA's potential for developing reliable and equitable medical imaging systems for diverse clinical settings. The code is made public available at \url{https://github.com/yinghemedical/imbalance-aware_domain_adaptation}
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2501.02048.pdf' target='_blank'>https://arxiv.org/pdf/2501.02048.pdf</a></span>   <span><a href='https://yuanpengtu.github.io/Dreammask-Page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu, Xi Chen, Ser-Nam Lim, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02048">DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary panoptic segmentation has received significant attention due to its applicability in the real world. Despite claims of robust generalization, we find that the advancements of previous works are attributed mainly on trained categories, exposing a lack of generalization to novel classes. In this paper, we explore boosting existing models from a data-centric perspective. We propose DreamMask, which systematically explores how to generate training data in the open-vocabulary setting, and how to train the model with both real and synthetic data. For the first part, we propose an automatic data generation pipeline with off-the-shelf models. We propose crucial designs for vocabulary expansion, layout arrangement, data filtering, etc. Equipped with these techniques, our generated data could significantly outperform the manually collected web data. To train the model with generated data, a synthetic-real alignment loss is designed to bridge the representation gap, bringing noticeable improvements across multiple benchmarks. In general, DreamMask significantly simplifies the collection of large-scale training data, serving as a plug-and-play enhancement for existing methods. For instance, when trained on COCO and tested on ADE20K, the model equipped with DreamMask outperforms the previous state-of-the-art by a substantial margin of 2.1% mIoU.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2501.02012.pdf' target='_blank'>https://arxiv.org/pdf/2501.02012.pdf</a></span>   <span><a href='https://github.com/jh-liang/Information-Subtraction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keng Hou Leong, Yuxuan Xiu, Wai Kin, Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02012">Information Subtraction: Learning Representations for Conditional Entropy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The representations of conditional entropy and conditional mutual information are significant in explaining the unique effects among variables. While previous studies based on conditional contrastive sampling have effectively removed information regarding discrete sensitive variables, they have not yet extended their scope to continuous cases. This paper introduces Information Subtraction, a framework designed to generate representations that preserve desired information while eliminating the undesired. We implement a generative-based architecture that outputs these representations by simultaneously maximizing an information term and minimizing another. With its flexibility in disentangling information, we can iteratively apply Information Subtraction to represent arbitrary information components between continuous variables, thereby explaining the various relationships that exist between them. Our results highlight the representations' ability to provide semantic features of conditional entropy. By subtracting sensitive and domain-specific information, our framework demonstrates effective performance in fair learning and domain generalization. The code for this paper is available at https://github.com/jh-liang/Information-Subtraction
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2501.00895.pdf' target='_blank'>https://arxiv.org/pdf/2501.00895.pdf</a></span>   <span><a href='https://chen-yang-liu.github.io/Text2Earth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyang Liu, Keyan Chen, Rui Zhao, Zhengxia Zou, Zhenwei Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00895">Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative foundation models have advanced large-scale text-driven natural image generation, becoming a prominent research trend across various vertical domains. However, in the remote sensing field, there is still a lack of research on large-scale text-to-image (text2image) generation technology. Existing remote sensing image-text datasets are small in scale and confined to specific geographic areas and scene types. Besides, existing text2image methods have struggled to achieve global-scale, multi-resolution controllable, and unbounded image generation. To address these challenges, this paper presents two key contributions: the Git-10M dataset and the Text2Earth foundation model. Git-10M is a global-scale image-text dataset comprising 10.5 million image-text pairs, 5 times larger than the previous largest one. The dataset covers a wide range of geographic scenes and contains resolution information, significantly surpassing existing datasets in both size and diversity. Building on Git-10M, we propose Text2Earth, a 1.3 billion parameter generative foundation model based on the diffusion framework to model global-scale remote sensing scenes. Text2Earth integrates a resolution guidance mechanism, enabling users to specify image resolutions. A dynamic condition adaptation strategy is proposed for training and inference to improve image quality. Text2Earth excels in zero-shot text2image generation and demonstrates robust generalization and flexibility across multiple tasks, including unbounded scene construction, image editing, and cross-modal image generation. This robust capability surpasses previous models restricted to the basic fixed size and limited scene types. On the previous benchmark dataset, Text2Earth outperforms previous models with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA metric.Our project page is https://chen-yang-liu.github.io/Text2Earth
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2412.18342.pdf' target='_blank'>https://arxiv.org/pdf/2412.18342.pdf</a></span>   <span><a href='https://github.com/KPeng9510/HyProMeta' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/KPeng9510/HyProMeta' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Peng, Di Wen, Sarfraz M. Saquib, Yufan Chen, Junwei Zheng, David Schneider, Kailun Yang, Jiamin Wu, Alina Roitberg, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18342">Mitigating Label Noise using Prompt-Based Hyperbolic Meta-Learning in Open-Set Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Set Domain Generalization (OSDG) is a challenging task requiring models to accurately predict familiar categories while minimizing confidence for unknown categories to effectively reject them in unseen domains. While the OSDG field has seen considerable advancements, the impact of label noise--a common issue in real-world datasets--has been largely overlooked. Label noise can mislead model optimization, thereby exacerbating the challenges of open-set recognition in novel domains. In this study, we take the first step towards addressing Open-Set Domain Generalization under Noisy Labels (OSDG-NL) by constructing dedicated benchmarks derived from widely used OSDG datasets, including PACS and DigitsDG. We evaluate baseline approaches by integrating techniques from both label denoising and OSDG methodologies, highlighting the limitations of existing strategies in handling label noise effectively. To address these limitations, we propose HyProMeta, a novel framework that integrates hyperbolic category prototypes for label noise-aware meta-learning alongside a learnable new-category agnostic prompt designed to enhance generalization to unseen classes. Our extensive experiments demonstrate the superior performance of HyProMeta compared to state-of-the-art methods across the newly established benchmarks. The source code of this work is released at https://github.com/KPeng9510/HyProMeta.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2412.18303.pdf' target='_blank'>https://arxiv.org/pdf/2412.18303.pdf</a></span>   <span><a href='https://github.com/Yushu-Li/ECALP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushu Li, Yongyi Su, Adam Goodge, Kui Jia, Xun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18303">Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have revolutionized machine learning by leveraging large pre-trained models to tackle various downstream tasks. Although label, training, and data efficiency have improved, many state-of-the-art VLMs still require task-specific hyperparameter tuning and fail to fully exploit test samples. To overcome these challenges, we propose a graph-based approach for label-efficient adaptation and inference. Our method dynamically constructs a graph over text prompts, few-shot examples, and test samples, using label propagation for inference without task-specific tuning. Unlike existing zero-shot label propagation techniques, our approach requires no additional unlabeled support set and effectively leverages the test sample manifold through dynamic graph expansion. We further introduce a context-aware feature re-weighting mechanism to improve task adaptation accuracy. Additionally, our method supports efficient graph expansion, enabling real-time inductive inference. Extensive evaluations on downstream tasks, such as fine-grained categorization and out-of-distribution generalization, demonstrate the effectiveness of our approach. The source code is available at https://github.com/Yushu-Li/ECALP.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2412.15544.pdf' target='_blank'>https://arxiv.org/pdf/2412.15544.pdf</a></span>   <span><a href='https://zilin-huang.github.io/VLM-RL-website' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, Sikai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15544">VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, reinforcement learning (RL)-based methods for learning driving policies have gained increasing attention in the autonomous driving community and have achieved remarkable progress in various driving scenarios. However, traditional RL approaches rely on manually engineered rewards, which require extensive human effort and often lack generalizability. To address these limitations, we propose \textbf{VLM-RL}, a unified framework that integrates pre-trained Vision-Language Models (VLMs) with RL to generate reward signals using image observation and natural language goals. The core of VLM-RL is the contrasting language goal (CLG)-as-reward paradigm, which uses positive and negative language goals to generate semantic rewards. We further introduce a hierarchical reward synthesis approach that combines CLG-based semantic rewards with vehicle state information, improving reward stability and offering a more comprehensive reward signal. Additionally, a batch-processing technique is employed to optimize computational efficiency during training. Extensive experiments in the CARLA simulator demonstrate that VLM-RL outperforms state-of-the-art baselines, achieving a 10.5\% reduction in collision rate, a 104.6\% increase in route completion rate, and robust generalization to unseen driving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any standard RL algorithms, potentially revolutionizing the existing RL paradigm that relies on manual reward engineering and enabling continuous performance improvements. The demo video and code can be accessed at: https://zilin-huang.github.io/VLM-RL-website.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2412.15380.pdf' target='_blank'>https://arxiv.org/pdf/2412.15380.pdf</a></span>   <span><a href='https://github.com/Meghnak13/UG-CEMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meghana Karri, Amit Soni Arya, Koushik Biswas, Nicol`o Gennaro, Vedat Cicek, Gorkem Durak, Yuri S. Velichko, Ulas Bagci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15380">Uncertainty-Guided Cross Attention Ensemble Mean Teacher for Semi-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a novel framework, Uncertainty-Guided Cross Attention Ensemble Mean Teacher (UG-CEMT), for achieving state-of-the-art performance in semi-supervised medical image segmentation. UG-CEMT leverages the strengths of co-training and knowledge distillation by combining a Cross-attention Ensemble Mean Teacher framework (CEMT) inspired by Vision Transformers (ViT) with uncertainty-guided consistency regularization and Sharpness-Aware Minimization emphasizing uncertainty. UG-CEMT improves semi-supervised performance while maintaining a consistent network architecture and task setting by fostering high disparity between sub-networks. Experiments demonstrate significant advantages over existing methods like Mean Teacher and Cross-pseudo Supervision in terms of disparity, domain generalization, and medical image segmentation performance. UG-CEMT achieves state-of-the-art results on multi-center prostate MRI and cardiac MRI datasets, where object segmentation is particularly challenging. Our results show that using only 10\% labeled data, UG-CEMT approaches the performance of fully supervised methods, demonstrating its effectiveness in exploiting unlabeled data for robust medical image segmentation. The code is publicly available at \url{https://github.com/Meghnak13/UG-CEMT}
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2412.13742.pdf' target='_blank'>https://arxiv.org/pdf/2412.13742.pdf</a></span>   <span><a href='https://github.com/taozh2017/KnowSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiwen Huang, Tao Zhou, Huazhu Fu, Yizhe Zhang, Yi Zhou, Chen Gong, Dong Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13742">Learnable Prompting SAM-induced Knowledge Distillation for Semi-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The limited availability of labeled data has driven advancements in semi-supervised learning for medical image segmentation. Modern large-scale models tailored for general segmentation, such as the Segment Anything Model (SAM), have revealed robust generalization capabilities. However, applying these models directly to medical image segmentation still exposes performance degradation. In this paper, we propose a learnable prompting SAM-induced Knowledge distillation framework (KnowSAM) for semi-supervised medical image segmentation. Firstly, we propose a Multi-view Co-training (MC) strategy that employs two distinct sub-networks to employ a co-teaching paradigm, resulting in more robust outcomes. Secondly, we present a Learnable Prompt Strategy (LPS) to dynamically produce dense prompts and integrate an adapter to fine-tune SAM specifically for medical image segmentation tasks. Moreover, we propose SAM-induced Knowledge Distillation (SKD) to transfer useful knowledge from SAM to two sub-networks, enabling them to learn from SAM's predictions and alleviate the effects of incorrect pseudo-labels during training. Notably, the predictions generated by our subnets are used to produce mask prompts for SAM, facilitating effective inter-module information exchange. Extensive experimental results on various medical segmentation tasks demonstrate that our model outperforms the state-of-the-art semi-supervised segmentation approaches. Crucially, our SAM distillation framework can be seamlessly integrated into other semi-supervised segmentation methods to enhance performance. The code will be released upon acceptance of this manuscript at: https://github.com/taozh2017/KnowSAM
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2412.12456.pdf' target='_blank'>https://arxiv.org/pdf/2412.12456.pdf</a></span>   <span><a href='https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunkai Li, Zhengyu Wu, Jiayi Wu, Hanwen Cui, Jishuo Jia, Rong-Hua Li, Guoren Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12456">Graph Learning in the Era of LLMs: A Survey from the Perspective of Data, Models, and Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing prevalence of cross-domain Text-Attributed Graph (TAG) Data (e.g., citation networks, recommendation systems, social networks, and ai4science), the integration of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as collaborators, LLM as predictor) has emerged as a promising technological paradigm. The core of this new graph learning paradigm lies in the synergistic combination of GNNs' ability to capture complex structural relationships and LLMs' proficiency in understanding informative contexts from the rich textual descriptions of graphs. Therefore, we can leverage graph description texts with rich semantic context to fundamentally enhance Data quality, thereby improving the representational capacity of model-centric approaches in line with data-centric machine learning principles. By leveraging the strengths of these distinct neural network architectures, this integrated approach addresses a wide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph question answering), particularly in complex industrial scenarios (e.g., supervised, few-shot, and zero-shot settings). In other words, we can treat text as a medium to enable cross-domain generalization of graph learning Model, allowing a single graph model to effectively handle the diversity of downstream graph-based Task across different data domains. This work serves as a foundational reference for researchers and practitioners looking to advance graph learning methodologies in the rapidly evolving landscape of LLM. We consistently maintain the related open-source materials at \url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2412.10680.pdf' target='_blank'>https://arxiv.org/pdf/2412.10680.pdf</a></span>   <span><a href='https://github.com/fine68/UCDR2024' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Jiang, Zhi-Qi Cheng, Gabriel Moreira, Jiawen Zhu, Jingdong Sun, Bukun Ren, Jun-Yan He, Qi Dai, Xian-Sheng Hua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10680">UCDR-Adapter: Exploring Adaptation of Pre-Trained Vision-Language Models for Universal Cross-Domain Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Universal Cross-Domain Retrieval (UCDR) retrieves relevant images from unseen domains and classes without semantic labels, ensuring robust generalization. Existing methods commonly employ prompt tuning with pre-trained vision-language models but are inherently limited by static prompts, reducing adaptability. We propose UCDR-Adapter, which enhances pre-trained models with adapters and dynamic prompt generation through a two-phase training strategy. First, Source Adapter Learning integrates class semantics with domain-specific visual knowledge using a Learnable Textual Semantic Template and optimizes Class and Domain Prompts via momentum updates and dual loss functions for robust alignment. Second, Target Prompt Generation creates dynamic prompts by attending to masked source prompts, enabling seamless adaptation to unseen domains and classes. Unlike prior approaches, UCDR-Adapter dynamically adapts to evolving data distributions, enhancing both flexibility and generalization. During inference, only the image branch and generated prompts are used, eliminating reliance on textual inputs for highly efficient retrieval. Extensive benchmark experiments show that UCDR-Adapter consistently outperforms ProS in most cases and other state-of-the-art methods on UCDR, U(c)CDR, and U(d)CDR settings.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2412.10061.pdf' target='_blank'>https://arxiv.org/pdf/2412.10061.pdf</a></span>   <span><a href='https://tuurstuyck.github.io/quaffure/quaffure.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuur Stuyck, Gene Wei-Chin Lin, Egor Larionov, Hsiao-yu Chen, Aljaz Bozic, Nikolaos Sarafianos, Doug Roble
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10061">Quaffure: Real-Time Quasi-Static Neural Hair Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic hair motion is crucial for high-quality avatars, but it is often limited by the computational resources available for real-time applications. To address this challenge, we propose a novel neural approach to predict physically plausible hair deformations that generalizes to various body poses, shapes, and hairstyles. Our model is trained using a self-supervised loss, eliminating the need for expensive data generation and storage. We demonstrate our method's effectiveness through numerous results across a wide range of pose and shape variations, showcasing its robust generalization capabilities and temporally smooth results. Our approach is highly suitable for real-time applications with an inference time of only a few milliseconds on consumer hardware and its ability to scale to predicting the drape of 1000 grooms in 0.3 seconds.
  Please see our project page here following https://tuurstuyck.github.io/quaffure/quaffure.html
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2412.09074.pdf' target='_blank'>https://arxiv.org/pdf/2412.09074.pdf</a></span>   <span><a href='https://github.com/jinsuby/DomCLP' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/jinsuby/DomCLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin-Seop Lee, Noo-ri Kim, Jee-Hyong Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09074">DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL) methods based on the instance discrimination tasks with InfoNCE have achieved remarkable success. Despite their success, SSL models often struggle to generate effective representations for unseen-domain data. To address this issue, research on unsupervised domain generalization (UDG), which aims to develop SSL models that can generate domain-irrelevant features, has been conducted. Most UDG approaches utilize contrastive learning with InfoNCE to generate representations, and perform feature alignment based on strong assumptions to generalize domain-irrelevant common features from multi-source domains. However, existing methods that rely on instance discrimination tasks are not effective at extracting domain-irrelevant common features. This leads to the suppression of domain-irrelevant common features and the amplification of domain-relevant features, thereby hindering domain generalization. Furthermore, strong assumptions underlying feature alignment can lead to biased feature learning, reducing the diversity of common features. In this paper, we propose a novel approach, DomCLP, Domain-wise Contrastive Learning with Prototype Mixup. We explore how InfoNCE suppresses domain-irrelevant common features and amplifies domain-relevant features. Based on this analysis, we propose Domain-wise Contrastive Learning (DCon) to enhance domain-irrelevant common features. We also propose Prototype Mixup Learning (PMix) to generalize domain-irrelevant common features across multiple domains without relying on strong assumptions. The proposed method consistently outperforms state-of-the-art methods on the PACS and DomainNet datasets across various label fractions, showing significant improvements. Our code will be released. Our project page is available at https://github.com/jinsuby/DomCLP.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2412.04245.pdf' target='_blank'>https://arxiv.org/pdf/2412.04245.pdf</a></span>   <span><a href='https://github.com/berndprach/IntriguingProperties' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernd Prach, Christoph H. Lampert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04245">Intriguing Properties of Robust Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite extensive research since the community learned about adversarial examples 10 years ago, we still do not know how to train high-accuracy classifiers that are guaranteed to be robust to small perturbations of their inputs. Previous works often argued that this might be because no classifier exists that is robust and accurate at the same time. However, in computer vision this assumption does not match reality where humans are usually accurate and robust on most tasks of interest. We offer an alternative explanation and show that in certain settings robust generalization is only possible with unrealistically large amounts of data. Specifically, we find a setting where a robust classifier exists, it is easy to learn an accurate classifier, yet it requires an exponential amount of data to learn a robust classifier. Based on this theoretical result, we evaluate the influence of the amount of training data on datasets such as CIFAR-10. Our findings indicate that the amount of training data is the main factor determining the robust performance. Furthermore we show that there are low magnitude directions in the data which are useful for non-robust generalization but are not available for robust classifiers. We provide code at https://github.com/berndprach/IntriguingProperties.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2412.04077.pdf' target='_blank'>https://arxiv.org/pdf/2412.04077.pdf</a></span>   <span><a href='https://ysj9909.github.io/SoRA.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokju Yun, Seunghye Chae, Dongheon Lee, Youngmin Ro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04077">SoMA: Singular Value Decomposed Minor Components Adaptation for Domain Generalizable Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to adapt a model using one or multiple source domains to ensure robust performance in unseen target domains. Recently, Parameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising results in the context of DG problem. Nevertheless, existing PEFT methods still struggle to strike a balance between preserving generalizable components of the pre-trained model and learning task-specific features. To gain insights into the distribution of generalizable components, we begin by analyzing the pre-trained weights through the lens of singular value decomposition. Building on these insights, we introduce Singular Value Decomposed Minor Components Adaptation (SoMA), an approach that selectively tunes minor singular components while keeping the residual parts frozen. SoMA effectively retains the generalization ability of the pre-trained model while efficiently acquiring task-specific skills. Moreover, we freeze domain-generalizable blocks and employ an annealing weight decay strategy, thereby achieving an optimal balance in the delicate trade-off between generalizability and discriminability. SoMA attains state-of-the-art results on multiple benchmarks that span both domain generalized semantic segmentation to domain generalized object detection. In addition, our methods introduce no additional inference overhead or regularization loss, maintain compatibility with any backbone or head, and are designed to be versatile, allowing easy integration into a wide range of tasks.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2412.02837.pdf' target='_blank'>https://arxiv.org/pdf/2412.02837.pdf</a></span>   <span><a href='https://github.com/sarthaxxxxx/BATCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarthak Kumar Maharana, Baoming Zhang, Leonid Karlinsky, Rogerio Feris, Yunhui Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02837">$\texttt{BATCLIP}$: Bimodal Online Test-Time Adaptation for CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although open-vocabulary classification models like Contrastive Language Image Pretraining (CLIP) have demonstrated strong zero-shot learning capabilities, their robustness to common image corruptions remains poorly understood. Through extensive experiments, we show that zero-shot CLIP lacks robustness to common image corruptions during test-time, necessitating the adaptation of CLIP to unlabeled corrupted images using test-time adaptation (TTA). However, we found that existing TTA methods have severe limitations in adapting CLIP due to their unimodal nature. To address these limitations, we propose $\texttt{BATCLIP}$, a bimodal $\textbf{online}$ TTA method designed to improve CLIP's robustness to common image corruptions. The key insight of our approach is not only to adapt the visual encoders for improving image features but also to strengthen the alignment between image and text features by promoting a stronger association between the image class prototype, computed using pseudo-labels, and the corresponding text feature. We evaluate our approach on benchmark image corruption datasets and achieve state-of-the-art results in online TTA for CLIP. Furthermore, we evaluate our proposed TTA approach on various domain generalization datasets to demonstrate its generalization capabilities. Our code is available at https://github.com/sarthaxxxxx/BATCLIP
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2411.17763.pdf' target='_blank'>https://arxiv.org/pdf/2411.17763.pdf</a></span>   <span><a href='https://ryanxli.github.io/reflect3d/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Zixuan Huang, Anh Thai, James M. Rehg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17763">Symmetry Strikes Back: From Single-Image Symmetry Detection to 3D Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Symmetry is a ubiquitous and fundamental property in the visual world, serving as a critical cue for perception and structure interpretation. This paper investigates the detection of 3D reflection symmetry from a single RGB image, and reveals its significant benefit on single-image 3D generation. We introduce Reflect3D, a scalable, zero-shot symmetry detector capable of robust generalization to diverse and real-world scenarios. Inspired by the success of foundation models, our method scales up symmetry detection with a transformer-based architecture. We also leverage generative priors from multi-view diffusion models to address the inherent ambiguity in single-view symmetry detection. Extensive evaluations on various data sources demonstrate that Reflect3D establishes a new state-of-the-art in single-image symmetry detection. Furthermore, we show the practical benefit of incorporating detected symmetry into single-image 3D generation pipelines through a symmetry-aware optimization process. The integration of symmetry significantly enhances the structural accuracy, cohesiveness, and visual fidelity of the reconstructed 3D geometry and textures, advancing the capabilities of 3D content creation.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2411.13284.pdf' target='_blank'>https://arxiv.org/pdf/2411.13284.pdf</a></span>   <span><a href='https://github.com/StrohmayerJ/DATTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Strohmayer, Rafael Sterzinger, Matthias WÃ¶dlinger, Martin Kampel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13284">DATTA: Domain-Adversarial Test-Time Adaptation for Cross-Domain WiFi-Based Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain generalization is an open problem in WiFi-based sensing due to variations in environments, devices, and subjects, causing domain shifts in channel state information. To address this, we propose Domain-Adversarial Test-Time Adaptation (DATTA), a novel framework combining domain-adversarial training (DAT), test-time adaptation (TTA), and weight resetting to facilitate adaptation to unseen target domains and to prevent catastrophic forgetting. DATTA is integrated into a lightweight, flexible architecture optimized for speed. We conduct a comprehensive evaluation of DATTA, including an ablation study on all key components using publicly available data, and verify its suitability for real-time applications such as human activity recognition. When combining a SotA video-based variant of TTA with WiFi-based DAT and comparing it to DATTA, our method achieves an 8.1% higher F1-Score. The PyTorch implementation of DATTA is publicly available at: https://github.com/StrohmayerJ/DATTA.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2411.10745.pdf' target='_blank'>https://arxiv.org/pdf/2411.10745.pdf</a></span>   <span><a href='https://kaist-viclab.github.io/TDSM_site/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeonghyeok Do, Munchurl Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10745">Bridging the Skeleton-Text Modality Gap: Diffusion-Powered Modality Alignment for Zero-shot Skeleton-based Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In zero-shot skeleton-based action recognition (ZSAR), aligning skeleton features with the text features of action labels is essential for accurately predicting unseen actions. ZSAR faces a fundamental challenge in bridging the modality gap between the two-kind features, which severely limits generalization to unseen actions. Previous methods focus on direct alignment between skeleton and text latent spaces, but the modality gaps between these spaces hinder robust generalization learning. Motivated by the success of diffusion models in multi-modal alignment (e.g., text-to-image, text-to-video), we firstly present a diffusion-based skeleton-text alignment framework for ZSAR. Our approach, Triplet Diffusion for Skeleton-Text Matching (TDSM), focuses on cross-alignment power of diffusion models rather than their generative capability. Specifically, TDSM aligns skeleton features with text prompts by incorporating text features into the reverse diffusion process, where skeleton features are denoised under text guidance, forming a unified skeleton-text latent space for robust matching. To enhance discriminative power, we introduce a triplet diffusion (TD) loss that encourages our TDSM to correct skeleton-text matches while pushing them apart for different action classes. Our TDSM significantly outperforms very recent state-of-the-art methods with significantly large margins of 2.36%-point to 13.05%-point, demonstrating superior accuracy and scalability in zero-shot settings through effective skeleton-text matching.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2411.06040.pdf' target='_blank'>https://arxiv.org/pdf/2411.06040.pdf</a></span>   <span><a href='https://github.com/hasanjawad001/CGLearn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jawad Chowdhury, Gabriel Terejanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06040">CGLearn: Consistent Gradient-Based Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving generalization and achieving highly predictive, robust machine learning models necessitates learning the underlying causal structure of the variables of interest. A prominent and effective method for this is learning invariant predictors across multiple environments. In this work, we introduce a simple yet powerful approach, CGLearn, which relies on the agreement of gradients across various environments. This agreement serves as a powerful indication of reliable features, while disagreement suggests less reliability due to potential differences in underlying causal mechanisms. Our proposed method demonstrates superior performance compared to state-of-the-art methods in both linear and nonlinear settings across various regression and classification tasks. CGLearn shows robust applicability even in the absence of separate environments by exploiting invariance across different subsamples of observational data. Comprehensive experiments on both synthetic and real-world datasets highlight its effectiveness in diverse scenarios. Our findings underscore the importance of leveraging gradient agreement for learning causal invariance, providing a significant step forward in the field of robust machine learning. The source code of the linear and nonlinear implementation of CGLearn is open-source and available at: https://github.com/hasanjawad001/CGLearn.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2411.05223.pdf' target='_blank'>https://arxiv.org/pdf/2411.05223.pdf</a></span>   <span><a href='https://github.com/ratschlab/ICMSeg' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ratschlab/ICMSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boqi Chen, Yuanzhi Zhu, Yunke Ao, Sebastiano Caprara, Reto Sutter, Gunnar RÃ¤tsch, Ender Konukoglu, Anna Susmelj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05223">Generalizable Single-Source Cross-modality Medical Image Segmentation via Invariant Causal Mechanisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization (SDG) aims to learn a model from a single source domain that can generalize well on unseen target domains. This is an important task in computer vision, particularly relevant to medical imaging where domain shifts are common. In this work, we consider a challenging yet practical setting: SDG for cross-modality medical image segmentation. We combine causality-inspired theoretical insights on learning domain-invariant representations with recent advancements in diffusion-based augmentation to improve generalization across diverse imaging modalities. Guided by the ``intervention-augmentation equivariant'' principle, we use controlled diffusion models (DMs) to simulate diverse imaging styles while preserving the content, leveraging rich generative priors in large-scale pretrained DMs to comprehensively perturb the multidimensional style variable. Extensive experiments on challenging cross-modality segmentation tasks demonstrate that our approach consistently outperforms state-of-the-art SDG methods across three distinct anatomies and imaging modalities. The source code is available at \href{https://github.com/ratschlab/ICMSeg}{https://github.com/ratschlab/ICMSeg}.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2411.04224.pdf' target='_blank'>https://arxiv.org/pdf/2411.04224.pdf</a></span>   <span><a href='https://github.com/StrohmayerJ/WiFlexFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Strohmayer, Matthias WÃ¶dlinger, Martin Kampel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04224">WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose WiFlexFormer, a highly efficient Transformer-based architecture designed for WiFi Channel State Information (CSI)-based person-centric sensing. We benchmark WiFlexFormer against state-of-the-art vision and specialized architectures for processing radio frequency data and demonstrate that it achieves comparable Human Activity Recognition (HAR) performance while offering a significantly lower parameter count and faster inference times. With an inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is optimized for real-time inference. Additionally, its low parameter count contributes to improved cross-domain generalization, where it often outperforms larger models. Our comprehensive evaluation shows that WiFlexFormer is a potential solution for efficient, scalable WiFi-based sensing applications. The PyTorch implementation of WiFlexFormer is publicly available at: https://github.com/StrohmayerJ/WiFlexFormer.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2411.04219.pdf' target='_blank'>https://arxiv.org/pdf/2411.04219.pdf</a></span>   <span><a href='https://github.com/divelab/AIRS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Xu, Haiyang Yu, Montgomery Bohde, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04219">Equivariant Graph Network Approximations of High-Degree Polynomials for Force Field Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in equivariant deep models have shown promise in accurately predicting atomic potentials and force fields in molecular dynamics simulations. Using spherical harmonics (SH) and tensor products (TP), these equivariant networks gain enhanced physical understanding, like symmetries and many-body interactions. Beyond encoding physical insights, SH and TP are also crucial to represent equivariant polynomial functions. In this work, we analyze the equivariant polynomial functions for the equivariant architecture, and introduce a novel equivariant network, named PACE. The proposed PACE utilizes edge booster and the Atomic Cluster Expansion (ACE) technique to approximate a greater number of $SE(3) \times S_n$ equivariant polynomial functions with enhanced degrees. As experimented in commonly used benchmarks, PACE demonstrates state-of-the-art performance in predicting atomic energy and force fields, with robust generalization capability across various geometric distributions under molecular dynamics (MD) across different temperature conditions. Our code is publicly available as part of the AIRS library https://github.com/divelab/AIRS/.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2411.03829.pdf' target='_blank'>https://arxiv.org/pdf/2411.03829.pdf</a></span>   <span><a href='https://github.com/gaozhitong/MultiShiftSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitong Gao, Bingnan Li, Mathieu Salzmann, Xuming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03829">Generalize or Detect? Towards Robust Semantic Segmentation Under Multiple Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In open-world scenarios, where both novel classes and domains may exist, an ideal segmentation model should detect anomaly classes for safety and generalize to new domains. However, existing methods often struggle to distinguish between domain-level and semantic-level distribution shifts, leading to poor out-of-distribution (OOD) detection or domain generalization performance. In this work, we aim to equip the model to generalize effectively to covariate-shift regions while precisely identifying semantic-shift regions. To achieve this, we design a novel generative augmentation method to produce coherent images that incorporate both anomaly (or novel) objects and various covariate shifts at both image and object levels. Furthermore, we introduce a training strategy that recalibrates uncertainty specifically for semantic shifts and enhances the feature extractor to align features associated with domain shifts. We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts. Our method achieves state-of-the-art performance across all benchmarks for both OOD detection and domain generalization. Code is available at https://github.com/gaozhitong/MultiShiftSeg.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2411.02614.pdf' target='_blank'>https://arxiv.org/pdf/2411.02614.pdf</a></span>   <span><a href='https://github.com/sharonchokuwa/dg-adr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharon Chokuwa, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02614">Divergent Domains, Convergent Grading: Enhancing Generalization in Diabetic Retinopathy Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic Retinopathy (DR) constitutes 5% of global blindness cases. While numerous deep learning approaches have sought to enhance traditional DR grading methods, they often falter when confronted with new out-of-distribution data thereby impeding their widespread application. In this study, we introduce a novel deep learning method for achieving domain generalization (DG) in DR grading and make the following contributions. First, we propose a new way of generating image-to-image diagnostically relevant fundus augmentations conditioned on the grade of the original fundus image. These augmentations are tailored to emulate the types of shifts in DR datasets thus increase the model's robustness. Second, we address the limitations of the standard classification loss in DG for DR fundus datasets by proposing a new DG-specific loss, domain alignment loss; which ensures that the feature vectors from all domains corresponding to the same class converge onto the same manifold for better domain generalization. Third, we tackle the coupled problem of data imbalance across DR domains and classes by proposing to employ Focal loss which seamlessly integrates with our new alignment loss. Fourth, due to inevitable observer variability in DR diagnosis that induces label noise, we propose leveraging self-supervised pretraining. This approach ensures that our DG model remains robust against early susceptibility to label noise, even when only a limited dataset of non-DR fundus images is available for pretraining. Our method demonstrates significant improvements over the strong Empirical Risk Minimization baseline and other recently proposed state-of-the-art DG methods for DR grading. Code is available at https://github.com/sharonchokuwa/dg-adr.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2410.22629.pdf' target='_blank'>https://arxiv.org/pdf/2410.22629.pdf</a></span>   <span><a href='https://github.com/Cuzyoung/CrossEarth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Gong, Zhixiang Wei, Di Wang, Xiaoxing Hu, Xianzheng Ma, Hongruixuan Chen, Yuru Jia, Yupeng Deng, Zhenming Ji, Xiangwei Zhu, Xue Yang, Naoto Yokoya, Jing Zhang, Bo Du, Junchi Yan, Liangpei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22629">CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. Despite the substantial domain gaps in RS images that are characterized by variabilities such as location, wavelength, and sensor type, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies targeting the RSDG issue, especially for semantic segmentation tasks, where existing models are developed for specific unknown domains, struggling with issues of underfitting on other unknown scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 cross-domain settings across various regions, spectral bands, platforms, and climates, providing a comprehensive framework for testing the generalizability of future RSDG models. Extensive experiments on this benchmark demonstrate the superiority of CrossEarth over existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2410.22622.pdf' target='_blank'>https://arxiv.org/pdf/2410.22622.pdf</a></span>   <span><a href='https://github.com/judydnguyen/PARDON-FedDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dung Thuy Nguyen, Taylor T. Johnson, Kevin Leach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22622">PARDON: Privacy-Aware and Robust Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) shows promise in preserving privacy and enabling collaborative learning. However, most current solutions focus on private data collected from a single domain. A significant challenge arises when client data comes from diverse domains (i.e., domain shift), leading to poor performance on unseen domains. Existing Federated Domain Generalization approaches address this problem but assume each client holds data for an entire domain, limiting their practicality in real-world scenarios with domain-based heterogeneity and client sampling. In addition, certain methods enable information sharing among clients, raising privacy concerns as this information could be used to reconstruct sensitive private data.
  To overcome this, we introduce FISC, a novel FedDG paradigm designed to robustly handle more complicated domain distributions between clients while ensuring security. FISC enables learning across domains by extracting an interpolative style from local styles and employing contrastive learning. This strategy gives clients multi-domain representations and unbiased convergent targets. Empirical results on multiple datasets, including PACS, Office-Home, and IWildCam, show FISC outperforms state-of-the-art (SOTA) methods. Our method achieves accuracy on unseen domains, with improvements ranging from 3.64% to 57.22% on unseen domains. Our code is available at https://github.com/judydnguyen/PARDON-FedDG.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2410.22228.pdf' target='_blank'>https://arxiv.org/pdf/2410.22228.pdf</a></span>   <span><a href='https://github.com/Nanolbw/SuGAr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Liu, Haoyang Li, Shuning Wang, Shuo Nie, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22228">Subgraph Aggregation for Out-of-Distribution Generalization on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization in Graph Neural Networks (GNNs) has gained significant attention due to its critical importance in graph-based predictions in real-world scenarios. Existing methods primarily focus on extracting a single causal subgraph from the input graph to achieve generalizable predictions. However, relying on a single subgraph can lead to susceptibility to spurious correlations and is insufficient for learning invariant patterns behind graph data. Moreover, in many real-world applications, such as molecular property prediction, multiple critical subgraphs may influence the target label property. To address these challenges, we propose a novel framework, SubGraph Aggregation (SuGAr), designed to learn a diverse set of subgraphs that are crucial for OOD generalization on graphs. Specifically, SuGAr employs a tailored subgraph sampler and diversity regularizer to extract a diverse set of invariant subgraphs. These invariant subgraphs are then aggregated by averaging their representations, which enriches the subgraph signals and enhances coverage of the underlying causal structures, thereby improving OOD generalization. Extensive experiments on both synthetic and real-world datasets demonstrate that \ours outperforms state-of-the-art methods, achieving up to a 24% improvement in OOD generalization on graphs. To the best of our knowledge, this is the first work to study graph OOD generalization by learning multiple invariant subgraphs. code: https://github.com/Nanolbw/SuGAr
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2410.21331.pdf' target='_blank'>https://arxiv.org/pdf/2410.21331.pdf</a></span>   <span><a href='https://github.com/PKU-ML/Beyond_Interpretability' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Zhang, Yifei Wang, Jingyi Cui, Xiang Pan, Qi Lei, Stefanie Jegelka, Yisen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21331">Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models often suffer from a lack of interpretability due to polysemanticity, where individual neurons are activated by multiple unrelated semantics, resulting in unclear attributions of model behavior. Recent advances in monosemanticity, where neurons correspond to consistent and distinct semantics, have significantly improved interpretability but are commonly believed to compromise accuracy. In this work, we challenge the prevailing belief of the accuracy-interpretability tradeoff, showing that monosemantic features not only enhance interpretability but also bring concrete gains in model performance. Across multiple robust learning scenarios-including input and label noise, few-shot learning, and out-of-domain generalization-our results show that models leveraging monosemantic features significantly outperform those relying on polysemantic features. Furthermore, we provide empirical and theoretical understandings on the robustness gains of feature monosemanticity. Our preliminary analysis suggests that monosemanticity, by promoting better separation of feature representations, leads to more robust decision boundaries. This diverse evidence highlights the generality of monosemanticity in improving model robustness. As a first step in this new direction, we embark on exploring the learning benefits of monosemanticity beyond interpretability, supporting the long-standing hypothesis of linking interpretability and robustness. Code is available at \url{https://github.com/PKU-ML/Beyond_Interpretability}.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2410.20542.pdf' target='_blank'>https://arxiv.org/pdf/2410.20542.pdf</a></span>   <span><a href='https://github.com/nokia-bell-labs/papagei-foundation-model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arvind Pillai, Dimitris Spathis, Fahim Kawsar, Mohammad Malekzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20542">PaPaGei: Open Foundation Models for Optical Physiological Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Photoplethysmography (PPG) is the leading non-invasive technique for monitoring biosignals and cardiovascular health, with widespread adoption in both clinical settings and consumer wearable devices. While machine learning models trained on PPG signals have shown promise, they tend to be task-specific and struggle with generalization. Current research is limited by the use of single-device datasets, insufficient exploration of out-of-domain generalization, and a lack of publicly available models, which hampers reproducibility. To address these limitations, we present PaPaGei, the first open foundation model for PPG signals. The model is pre-trained on over 57,000 hours of data, comprising 20 million unlabeled PPG segments from publicly available datasets. We introduce a novel representation learning approach that leverages domain knowledge of PPG signal morphology across individuals, enabling the capture of richer representations compared to traditional contrastive learning methods. We evaluate PaPaGei against state-of-the-art time-series foundation models and self-supervised learning benchmarks across 20 tasks from 10 diverse datasets, spanning cardiovascular health, sleep disorders, pregnancy monitoring, and wellbeing assessment. Our model demonstrates superior performance, improving classification and regression metrics by 6.3% and 2.9% respectively in at least 14 tasks. Notably, PaPaGei achieves these results while being more data- and parameter-efficient, outperforming models that are 70x larger. Beyond accuracy, we examine model robustness across different skin tones, establishing a benchmark for bias evaluation in future models. PaPaGei can serve as both a feature extractor and an encoder for multimodal models, opening up new opportunities for multimodal health monitoring.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2410.20406.pdf' target='_blank'>https://arxiv.org/pdf/2410.20406.pdf</a></span>   <span><a href='https://github.com/auniquesun/Point-PRC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Sun, Qiuhong Ke, Yongcai Wang, Wang Chen, Kang Yang, Deying Li, Jianfei Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20406">Point-PRC: A Prompt Learning Based Regulation Framework for Generalizable Point Cloud Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the 3D domain generalization (3DDG) ability of large 3D models based on prevalent prompt learning. Recent works demonstrate the performances of 3D point cloud recognition can be boosted remarkably by parameter-efficient prompt tuning. However, we observe that the improvement on downstream tasks comes at the expense of a severe drop in 3D domain generalization. To resolve this challenge, we present a comprehensive regulation framework that allows the learnable prompts to actively interact with the well-learned general knowledge in large 3D models to maintain good generalization. Specifically, the proposed framework imposes multiple explicit constraints on the prompt learning trajectory by maximizing the mutual agreement between task-specific predictions and task-agnostic knowledge. We design the regulation framework as a plug-and-play module to embed into existing representative large 3D models. Surprisingly, our method not only realizes consistently increasing generalization ability but also enhances task-specific 3D recognition performances across various 3DDG benchmarks by a clear margin. Considering the lack of study and evaluation on 3DDG, we also create three new benchmarks, namely base-to-new, cross-dataset and few-shot generalization benchmarks, to enrich the field and inspire future research. Code and benchmarks are available at \url{https://github.com/auniquesun/Point-PRC}.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2410.19265.pdf' target='_blank'>https://arxiv.org/pdf/2410.19265.pdf</a></span>   <span><a href='https://github.com/kaize0409/Awesome-Graph-OOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexin Zhang, Shuhan Liu, Song Wang, Weili Shi, Chen Chen, Pan Li, Sheng Li, Jundong Li, Kaize Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19265">A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution Generalization to Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distribution shifts on graphs -- the discrepancies in data distribution between training and employing a graph machine learning model -- are ubiquitous and often unavoidable in real-world scenarios. These shifts may severely deteriorate model performance, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph machine learning under distribution shifts, aiming to train models to achieve satisfactory performance on out-of-distribution (OOD) test data. In our survey, we provide an up-to-date and forward-looking review of deep graph learning under distribution shifts. Specifically, we cover three primary scenarios: graph OOD generalization, training-time graph OOD adaptation, and test-time graph OOD adaptation. We begin by formally formulating the problems and discussing various types of distribution shifts that can affect graph learning, such as covariate shifts and concept shifts. To provide a better understanding of the literature, we introduce a systematic taxonomy that classifies existing methods into model-centric and data-centric approaches, investigating the techniques used in each category. We also summarize commonly used datasets in this research area to facilitate further investigation. Finally, we point out promising research directions and the corresponding challenges to encourage further study in this vital domain. We also provide a continuously updated reading list at https://github.com/kaize0409/Awesome-Graph-OOD.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2410.18122.pdf' target='_blank'>https://arxiv.org/pdf/2410.18122.pdf</a></span>   <span><a href='https://github.com/ioverho/misinfo-general' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivo Verhoeven, Pushkar Mishra, Ekaterina Shutova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18122">Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article introduces misinfo-general, a benchmark dataset for evaluating misinformation models' ability to perform out-of-distribution generalization. Misinformation changes rapidly, much more quickly than moderators can annotate at scale, resulting in a shift between the training and inference data distributions. As a result, misinformation detectors need to be able to perform out-of-distribution generalization, an attribute they currently lack. Our benchmark uses distant labelling to enable simulating covariate shifts in misinformation content. We identify time, event, topic, publisher, political bias, misinformation type as important axes for generalization, and we evaluate a common class of baseline models on each. Using article metadata, we show how this model fails desiderata, which is not necessarily obvious from classification metrics. Finally, we analyze properties of the data to ensure limited presence of modelling shortcuts. We make the dataset and accompanying code publicly available: https://github.com/ioverho/misinfo-general
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2410.17146.pdf' target='_blank'>https://arxiv.org/pdf/2410.17146.pdf</a></span>   <span><a href='https://github.com/wang-kee/LiNeS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Wang, Nikolaos Dimitriadis, Alessandro Favero, Guillermo Ortiz-Jimenez, Francois Fleuret, Pascal Frossard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17146">LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning pre-trained models has become the standard approach to endow them with specialized knowledge, but it poses fundamental challenges. In particular, \textit{(i)} fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks, and \textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to significant performance loss. To address these challenges, we introduce LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. In multi-task model merging scenarios, layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Our method is simple to implement, computationally efficient and complementary to many existing techniques. Our source code is available at https://github.com/wang-kee/LiNeS
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2410.17020.pdf' target='_blank'>https://arxiv.org/pdf/2410.17020.pdf</a></span>   <span><a href='https://github.com/liangchen527/LFME' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Chen, Yong Zhang, Yibing Song, Zhiqiang Shen, Lingqiao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17020">LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) methods aim to maintain good performance in an unseen target domain by using training data from multiple source domains. While success on certain occasions are observed, enhancing the baseline across most scenarios remains challenging. This work introduces a simple yet effective framework, dubbed learning from multiple experts (LFME), that aims to make the target model an expert in all source domains to improve DG. Specifically, besides learning the target model used in inference, LFME will also train multiple experts specialized in different domains, whose output probabilities provide professional guidance by simply regularizing the logit of the target model. Delving deep into the framework, we reveal that the introduced logit regularization term implicitly provides effects of enabling the target model to harness more information, and mining hard samples from the experts during training. Extensive experiments on benchmarks from different DG tasks demonstrate that LFME is consistently beneficial to the baseline and can achieve comparable performance to existing arts. Code is available at~\url{https://github.com/liangchen527/LFME}.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2410.16845.pdf' target='_blank'>https://arxiv.org/pdf/2410.16845.pdf</a></span>   <span><a href='https://github.com/draym28/FGSAM_NeurIPS24' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Luo, Yuhan Chen, Siya Qiu, Yiwei Wang, Chen Zhang, Yan Zhou, Xiaochun Cao, Jing Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16845">Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have shown superior performance in node classification. However, GNNs perform poorly in the Few-Shot Node Classification (FSNC) task that requires robust generalization to make accurate predictions for unseen classes with limited labels. To tackle the challenge, we propose the integration of Sharpness-Aware Minimization (SAM)--a technique designed to enhance model generalization by finding a flat minimum of the loss landscape--into GNN training. The standard SAM approach, however, consists of two forward-backward steps in each training iteration, doubling the computational cost compared to the base optimizer (e.g., Adam). To mitigate this drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM), that integrates the rapid training of Multi-Layer Perceptrons (MLPs) with the superior performance of GNNs. Specifically, we utilize GNNs for parameter perturbation while employing MLPs to minimize the perturbed loss so that we can find a flat minimum with good generalization more efficiently. Moreover, our method reutilizes the gradient from the perturbation phase to incorporate graph topology into the minimization process at almost zero additional cost. To further enhance training efficiency, we develop FGSAM+ that executes exact perturbations periodically. Extensive experiments demonstrate that our proposed algorithm outperforms the standard SAM with lower computational costs in FSNC tasks. In particular, our FGSAM+ as a SAM variant offers a faster optimization than the base optimizer in most cases. In addition to FSNC, our proposed methods also demonstrate competitive performance in the standard node classification task for heterophilic graphs, highlighting the broad applicability. The code is available at https://github.com/draym28/FGSAM_NeurIPS24.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2410.16146.pdf' target='_blank'>https://arxiv.org/pdf/2410.16146.pdf</a></span>   <span><a href='https://github.com/C0notSilly/AdvFrequency' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilin He, Jingyu Hu, Qinliang Lin, Cheng Luo, Weicheng Xie, Siyang Song, Muhammad Haris Khan, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16146">Towards Combating Frequency Simplicity-biased Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization methods aim to learn transferable knowledge from source domains that can generalize well to unseen target domains. Recent studies show that neural networks frequently suffer from a simplicity-biased learning behavior which leads to over-reliance on specific frequency sets, namely as frequency shortcuts, instead of semantic information, resulting in poor generalization performance. Despite previous data augmentation techniques successfully enhancing generalization performances, they intend to apply more frequency shortcuts, thereby causing hallucinations of generalization improvement. In this paper, we aim to prevent such learning behavior of applying frequency shortcuts from a data-driven perspective. Given the theoretical justification of models' biased learning behavior on different spatial frequency components, which is based on the dataset frequency properties, we argue that the learning behavior on various frequency components could be manipulated by changing the dataset statistical structure in the Fourier domain. Intuitively, as frequency shortcuts are hidden in the dominant and highly dependent frequencies of dataset structure, dynamically perturbating the over-reliance frequency components could prevent the application of frequency shortcuts. To this end, we propose two effective data augmentation modules designed to collaboratively and adaptively adjust the frequency characteristic of the dataset, aiming to dynamically influence the learning behavior of the model and ultimately serving as a strategy to mitigate shortcut learning. Code is available at AdvFrequency (https://github.com/C0notSilly/AdvFrequency).
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2410.16020.pdf' target='_blank'>https://arxiv.org/pdf/2410.16020.pdf</a></span>   <span><a href='https://github.com/lingeringlight/START' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lingeringlight/START' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Guo, Lei Qi, Yinghuan Shi, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16020">START: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to enable models to generalize to unseen target domains by learning from multiple source domains. Existing DG methods primarily rely on convolutional neural networks (CNNs), which inherently learn texture biases due to their limited receptive fields, making them prone to overfitting source domains. While some works have introduced transformer-based methods (ViTs) for DG to leverage the global receptive field, these methods incur high computational costs due to the quadratic complexity of self-attention. Recently, advanced state space models (SSMs), represented by Mamba, have shown promising results in supervised learning tasks by achieving linear complexity in sequence length during training and fast RNN-like computation during inference. Inspired by this, we investigate the generalization ability of the Mamba model under domain shifts and find that input-dependent matrices within SSMs could accumulate and amplify domain-specific features, thus hindering model generalization. To address this issue, we propose a novel SSM-based architecture with saliency-based token-aware transformation (namely START), which achieves state-of-the-art (SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our START can selectively perturb and suppress domain-specific features in salient tokens within the input-dependent matrices of SSMs, thus effectively reducing the discrepancy between different domains. Extensive experiments on five benchmarks demonstrate that START outperforms existing SOTA DG methods with efficient linear complexity. Our code is available at https://github.com/lingeringlight/START.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2410.14817.pdf' target='_blank'>https://arxiv.org/pdf/2410.14817.pdf</a></span>   <span><a href='https://github.com/EricElmoznino/complexity_compositionality' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, Guillaume Lajoie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14817">A Complexity-Based Theory of Compositionality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, compositional representations can enable a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, we lack satisfying formal definitions for it that are measurable and mathematical. Here, we propose such a definition, which we call representational compositionality, that accounts for and extends our intuitions about compositionality. The definition is conceptually simple, quantitative, grounded in algorithmic information theory, and applicable to any representation. Intuitively, representational compositionality states that a compositional representation satisfies three properties. First, it must be expressive. Second, it must be possible to re-describe the representation as a function of discrete symbolic sequences with re-combinable parts, analogous to sentences in natural language. Third, the function that relates these symbolic sequences to the representation, analogous to semantics in natural language, must be simple. Through experiments on both synthetic and real world data, we validate our definition of compositionality and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools. We hope that our definition can inspire the design of novel, theoretically-driven models that better capture the mechanisms of compositional thought. We make our code available at https://github.com/EricElmoznino/complexity_compositionality.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2410.11397.pdf' target='_blank'>https://arxiv.org/pdf/2410.11397.pdf</a></span>   <span><a href='https://github.com/XeniaLLL/FOOGD-main.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinting Liao, Weiming Liu, Pengyang Zhou, Fengyuan Yu, Jiahe Xu, Jun Wang, Wenjie Wang, Chaochao Chen, Xiaolin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11397">FOOGD: Federated Collaboration for Both Out-of-distribution Generalization and Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) is a promising machine learning paradigm that collaborates with client models to capture global knowledge. However, deploying FL models in real-world scenarios remains unreliable due to the coexistence of in-distribution data and unexpected out-of-distribution (OOD) data, such as covariate-shift and semantic-shift data. Current FL researches typically address either covariate-shift data through OOD generalization or semantic-shift data via OOD detection, overlooking the simultaneous occurrence of various OOD shifts. In this work, we propose FOOGD, a method that estimates the probability density of each client and obtains reliable global distribution as guidance for the subsequent FL process. Firstly, SM3D in FOOGD estimates score model for arbitrary distributions without prior constraints, and detects semantic-shift data powerfully. Then SAG in FOOGD provides invariant yet diverse knowledge for both local covariate-shift generalization and client performance generalization. In empirical validations, FOOGD significantly enjoys three main advantages: (1) reliably estimating non-normalized decentralized distributions, (2) detecting semantic shift data via score values, and (3) generalizing to covariate-shift data by regularizing feature extractor. The prejoct is open in https://github.com/XeniaLLL/FOOGD-main.git.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2410.11267.pdf' target='_blank'>https://arxiv.org/pdf/2410.11267.pdf</a></span>   <span><a href='https://github.com/sanphouwang/fedccrl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinpeng Wang, Yongxin Guo, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11267">FedCCRL: Federated Domain Generalization with Cross-Client Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to train models that can effectively generalize to unseen domains. However, in the context of Federated Learning (FL), where clients collaboratively train a model without directly sharing their data, most existing DG algorithms are not directly applicable to the FL setting due to privacy constraints, as well as the limited data quantity and domain diversity at each client. To tackle these challenges, we propose FedCCRL, a lightweight federated domain generalization method that significantly improves the model's generalization ability while preserving privacy and ensuring computational and communication efficiency. Specifically, FedCCRL comprises two principal modules: the first is a cross-client feature extension module, which increases local domain diversity via cross-client domain transfer and domain-invariant feature perturbation; the second is a representation and prediction dual-stage alignment module, which enables the model to effectively capture domain-invariant features. Extensive experimental results demonstrate that FedCCRL achieves the state-of-the-art performance on the PACS, OfficeHome and miniDomainNet datasets across FL settings of varying numbers of clients. Code is available at https://github.com/sanphouwang/fedccrl
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2410.11160.pdf' target='_blank'>https://arxiv.org/pdf/2410.11160.pdf</a></span>   <span><a href='https://github.com/sstary/SSRS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianping Ma, Xiaokang Zhang, Man-On Pun, Bo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11160">A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal remote sensing data, acquired from diverse sensors, offer a comprehensive and integrated perspective of the Earth's surface. Leveraging multimodal fusion techniques, semantic segmentation enables detailed and accurate analysis of geographic scenes, surpassing single-modality approaches. Building on advancements in vision foundation models, particularly the Segment Anything Model (SAM), this study proposes a unified framework incorporating a novel Multimodal Fine-tuning Network (MFNet) for remote sensing semantic segmentation. The proposed framework is designed to seamlessly integrate with various fine-tuning mechanisms, demonstrated through the inclusion of Adapter and Low-Rank Adaptation (LoRA) as representative examples. This extensibility ensures the framework's adaptability to other emerging fine-tuning strategies, allowing models to retain SAM's general knowledge while effectively leveraging multimodal data. Additionally, a pyramid-based Deep Fusion Module (DFM) is introduced to integrate high-level geographic features across multiple scales, enhancing feature representation prior to decoding. This work also highlights SAM's robust generalization capabilities with Digital Surface Model (DSM) data, a novel application. Extensive experiments on three benchmark multimodal remote sensing datasets, ISPRS Vaihingen, ISPRS Potsdam and MMHunan, demonstrate that the proposed MFNet significantly outperforms existing methods in multimodal semantic segmentation, setting a new standard in the field while offering a versatile foundation for future research and applications. The source code for this work is accessible at https://github.com/sstary/SSRS.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2410.10105.pdf' target='_blank'>https://arxiv.org/pdf/2410.10105.pdf</a></span>   <span><a href='https://github.com/qianyu-dlut/DiffDIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Yu, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Lihe Zhang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10105">High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of high-resolution (HR), fine-grained image segmentation, the primary challenge is balancing broad contextual awareness with the precision required for detailed object delineation, capturing intricate details and the finest edges of objects. Diffusion models, trained on vast datasets comprising billions of image-text pairs, such as SD V2.1, have revolutionized text-to-image synthesis by delivering exceptional quality, fine detail resolution, and strong contextual awareness, making them an attractive solution for high-resolution image segmentation. To this end, we propose DiffDIS, a diffusion-driven segmentation model that taps into the potential of the pre-trained U-Net within diffusion models, specifically designed for high-resolution, fine-grained object segmentation. By leveraging the robust generalization capabilities and rich, versatile image representation prior of the SD models, coupled with a task-specific stable one-step denoising approach, we significantly reduce the inference time while preserving high-fidelity, detailed generation. Additionally, we introduce an auxiliary edge generation task to not only enhance the preservation of fine details of the object boundaries, but reconcile the probabilistic nature of diffusion with the deterministic demands of segmentation. With these refined strategies in place, DiffDIS serves as a rapid object mask generation model, specifically optimized for generating detailed binary maps at high resolutions, while demonstrating impressive accuracy and swift processing. Experiments on the DIS5K dataset demonstrate the superiority of DiffDIS, achieving state-of-the-art results through a streamlined inference process. The source code will be publicly available at https://github.com/qianyu-dlut/DiffDIS.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2410.08000.pdf' target='_blank'>https://arxiv.org/pdf/2410.08000.pdf</a></span>   <span><a href='https://github.com/HaoyueBaiZJU/aha' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyue Bai, Jifan Zhang, Robert Nowak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08000">AHA: Human-Assisted Out-of-Distribution Generalization and Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern machine learning models deployed often encounter distribution shifts in real-world applications, manifesting as covariate or semantic out-of-distribution (OOD) shifts. These shifts give rise to challenges in OOD generalization and OOD detection. This paper introduces a novel, integrated approach AHA (Adaptive Human-Assisted OOD learning) to simultaneously address both OOD generalization and detection through a human-assisted framework by labeling data in the wild. Our approach strategically labels examples within a novel maximum disambiguation region, where the number of semantic and covariate OOD data roughly equalizes. By labeling within this region, we can maximally disambiguate the two types of OOD data, thereby maximizing the utility of the fixed labeling budget. Our algorithm first utilizes a noisy binary search algorithm that identifies the maximal disambiguation region with high probability. The algorithm then continues with annotating inside the identified labeling region, reaping the full benefit of human feedback. Extensive experiments validate the efficacy of our framework. We observed that with only a few hundred human annotations, our method significantly outperforms existing state-of-the-art methods that do not involve human assistance, in both OOD generalization and OOD detection. Code is publicly available at \url{https://github.com/HaoyueBaiZJU/aha}.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2410.07408.pdf' target='_blank'>https://arxiv.org/pdf/2410.07408.pdf</a></span>   <span><a href='https://digital-cousins.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyuan Dai, Josiah Wong, Yunfan Jiang, Chen Wang, Cem Gokmen, Ruohan Zhang, Jiajun Wu, Li Fei-Fei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07408">Automated Creation of Digital Cousins for Robust Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training robot policies in the real world can be unsafe, costly, and difficult to scale. Simulation serves as an inexpensive and potentially limitless source of training data, but suffers from the semantics and physics disparity between simulated and real-world environments. These discrepancies can be minimized by training in digital twins, which serve as virtual replicas of a real scene but are expensive to generate and cannot produce cross-domain generalization. To address these limitations, we propose the concept of digital cousins, a virtual asset or scene that, unlike a digital twin, does not explicitly model a real-world counterpart but still exhibits similar geometric and semantic affordances. As a result, digital cousins simultaneously reduce the cost of generating an analogous virtual environment while also facilitating better robustness during sim-to-real domain transfer by providing a distribution of similar training scenes. Leveraging digital cousins, we introduce a novel method for their automated creation, and propose a fully automated real-to-sim-to-real pipeline for generating fully interactive scenes and training robot policies that can be deployed zero-shot in the original scene. We find that digital cousin scenes that preserve geometric and semantic affordances can be produced automatically, and can be used to train policies that outperform policies trained on digital twins, achieving 90% vs. 25% success rates under zero-shot sim-to-real transfer. Additional details are available at https://digital-cousins.github.io/.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2410.06020.pdf' target='_blank'>https://arxiv.org/pdf/2410.06020.pdf</a></span>   <span><a href='https://saqibjaved1.github.io/QT_DoG/' target='_blank'>  GitHub</a></span> <span><a href='https://saqibjaved1.github.io/QT_DoG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saqib Javed, Hieu Le, Mathieu Salzmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06020">QT-DoG: Quantization-aware Training for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key challenge in Domain Generalization (DG) is preventing overfitting to source domains, which can be mitigated by finding flatter minima in the loss landscape. In this work, we propose Quantization-aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization effectively leads to flatter minima in the loss landscape, thereby enhancing domain generalization. Unlike traditional quantization methods focused on model compression, QT-DoG exploits quantization as an implicit regularizer by inducing noise in model weights, guiding the optimization process toward flatter minima that are less sensitive to perturbations and overfitting. We provide both an analytical perspective and empirical evidence demonstrating that quantization inherently encourages flatter minima, leading to better generalization across domains. Moreover, with the benefit of reducing the model size through quantization, we demonstrate that an ensemble of multiple quantized models further yields superior accuracy than the state-of-the-art DG approaches with no computational or memory overheads. Code is released at: https://saqibjaved1.github.io/QT_DoG/.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2410.05270.pdf' target='_blank'>https://arxiv.org/pdf/2410.05270.pdf</a></span>   <span><a href='https://github.com/astra-vision/ProLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick PÃ©rez, Raoul de Charette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05270">CLIP's Visual Embedding Projector is a Few-shot Cornucopia</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of adapting a contrastively pretrained vision-language model like CLIP (Radford et al., 2021) for few-shot classification. The literature addresses this problem by learning a linear classifier of the frozen visual features, optimizing word embeddings, or learning external feature adapters. We introduce an alternative way for few-shot CLIP adaptation without adding ''external'' parameters to optimize. We find that simply fine-tuning the embedding projection matrix of the vision encoder leads to better performance than all baselines. Furthermore, we show that regularizing training with the distance between the fine-tuned and pretrained matrices adds reliability for adapting CLIP, making the results stable across different learning rates in the ''validation-free'' setting. This simple approach, coined ProLIP, yields state-of-the-art performance on 11 few-shot classification benchmarks, few-shot cross-dataset transfer, domain generalization, and base-to-new class generalization. We also show that ProLIP significantly outperforms prompt tuning when extended to another task of test-time adaptation, while being one order of magnitude faster to train. Code will be made available at: https://github.com/astra-vision/ProLIP .
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2410.04671.pdf' target='_blank'>https://arxiv.org/pdf/2410.04671.pdf</a></span>   <span><a href='https://github.com/MiracleDance/CAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Yao, Jialin Li, Yifeng Zhou, Yong Liu, Xi Jiang, Chengjie Wang, Feng Zheng, Yuexian Zou, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04671">CAR: Controllable Autoregressive Modeling for Visual Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllable generation, which enables fine-grained control over generated outputs, has emerged as a critical focus in visual generative models. Currently, there are two primary technical approaches in visual generation: diffusion models and autoregressive models. Diffusion models, as exemplified by ControlNet and T2I-Adapter, offer advanced control mechanisms, whereas autoregressive models, despite showcasing impressive generative quality and scalability, remain underexplored in terms of controllability and flexibility. In this study, we introduce Controllable AutoRegressive Modeling (CAR), a novel, plug-and-play framework that integrates conditional control into multi-scale latent variable modeling, enabling efficient control generation within a pre-trained visual autoregressive model. CAR progressively refines and captures control representations, which are injected into each autoregressive step of the pre-trained model to guide the generation process. Our approach demonstrates excellent controllability across various types of conditions and delivers higher image quality compared to previous methods. Additionally, CAR achieves robust generalization with significantly fewer training resources compared to those required for pre-training the model. To the best of our knowledge, we are the first to propose a control framework for pre-trained autoregressive visual generation models.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2410.04372.pdf' target='_blank'>https://arxiv.org/pdf/2410.04372.pdf</a></span>   <span><a href='https://github.com/skJack/DiffusionFake.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Sun, Shen Chen, Taiping Yao, Hong Liu, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04372">DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of Deepfake technology has made face swapping highly realistic, raising concerns about the malicious use of fabricated facial content. Existing methods often struggle to generalize to unseen domains due to the diverse nature of facial manipulations. In this paper, we revisit the generation process and identify a universal principle: Deepfake images inherently contain information from both source and target identities, while genuine faces maintain a consistent identity. Building upon this insight, we introduce DiffusionFake, a novel plug-and-play framework that reverses the generative process of face forgeries to enhance the generalization of detection models. DiffusionFake achieves this by injecting the features extracted by the detection model into a frozen pre-trained Stable Diffusion model, compelling it to reconstruct the corresponding target and source images. This guided reconstruction process constrains the detection network to capture the source and target related features to facilitate the reconstruction, thereby learning rich and disentangled representations that are more resilient to unseen forgeries. Extensive experiments demonstrate that DiffusionFake significantly improves cross-domain generalization of various detector architectures without introducing additional parameters during inference. Our Codes are available in https://github.com/skJack/DiffusionFake.git.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2410.03499.pdf' target='_blank'>https://arxiv.org/pdf/2410.03499.pdf</a></span>   <span><a href='https://github.com/sunnyinAI/FedStein' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Gupta, Nikita Jangid, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03499">FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) facilitates data privacy by enabling collaborative in-situ training across decentralized clients. Despite its inherent advantages, FL faces significant challenges of performance and convergence when dealing with data that is not independently and identically distributed (non-i.i.d.). While previous research has primarily addressed the issue of skewed label distribution across clients, this study focuses on the less explored challenge of multi-domain FL, where client data originates from distinct domains with varying feature distributions. We introduce a novel method designed to address these challenges FedStein: Enhancing Multi-Domain Federated Learning Through the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS) estimates of batch normalization (BN) statistics across clients, while maintaining local BN parameters. The non-BN layer parameters are exchanged via standard FL techniques. Extensive experiments conducted across three datasets and multiple models demonstrate that FedStein surpasses existing methods such as FedAvg and FedBN, with accuracy improvements exceeding 14% in certain domains leading to enhanced domain generalization. The code is available at https://github.com/sunnyinAI/FedStein
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2410.02505.pdf' target='_blank'>https://arxiv.org/pdf/2410.02505.pdf</a></span>   <span><a href='https://github.com/Kai-Liu001/Dog-IQA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Kai-Liu001/Dog-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Liu, Ziqing Zhang, Wenbo Li, Renjing Pei, Fenglong Song, Xiaohong Liu, Linghe Kong, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02505">Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image quality assessment (IQA) serves as the golden standard for all models' performance in nearly all computer vision fields. However, it still suffers from poor out-of-distribution generalization ability and expensive training costs. To address these problems, we propose Dog-IQA, a standard-guided zero-shot mix-grained IQA method, which is training-free and utilizes the exceptional prior knowledge of multimodal large language models (MLLMs). To obtain accurate IQA scores, namely scores consistent with humans, we design an MLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA applies two techniques. First, Dog-IQA objectively scores with specific standards that utilize MLLM's behavior pattern and minimize the influence of subjective factors. Second, Dog-IQA comprehensively takes local semantic objects and the whole image as input and aggregates their scores, leveraging local and global information. Our proposed Dog-IQA achieves state-of-the-art (SOTA) performance compared with training-free methods, and competitive performance compared with training-based methods in cross-dataset scenarios. Our code will be available at https://github.com/Kai-Liu001/Dog-IQA.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2410.02396.pdf' target='_blank'>https://arxiv.org/pdf/2410.02396.pdf</a></span>   <span><a href='https://github.com/duguodong7/pcb-merging' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02396">Parameter Competition Balancing for Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \url{https://github.com/duguodong7/pcb-merging}.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2409.19774.pdf' target='_blank'>https://arxiv.org/pdf/2409.19774.pdf</a></span>   <span><a href='https://github.com/NikosEfth/crafting-shifts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikos Efthymiadis, Giorgos Tolias, OndÅej Chum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19774">Crafting Distribution Shifts for Validation and Training in Single Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization attempts to learn a model on a source domain and deploy it to unseen target domains. Limiting access only to source domain data imposes two key challenges - how to train a model that can generalize and how to verify that it does. The standard practice of validation on the training distribution does not accurately reflect the model's generalization ability, while validation on the test distribution is a malpractice to avoid. In this work, we construct an independent validation set by transforming source domain images with a comprehensive list of augmentations, covering a broad spectrum of potential distribution shifts in target domains. We demonstrate a high correlation between validation and test performance for multiple methods and across various datasets. The proposed validation achieves a relative accuracy improvement over the standard validation equal to 15.4% or 1.6% when used for method selection or learning rate tuning, respectively. Furthermore, we introduce a novel family of methods that increase the shape bias through enhanced edge maps. To benefit from the augmentations during training and preserve the independence of the validation set, a k-fold validation process is designed to separate the augmentation types used in training and validation. The method that achieves the best performance on the augmented validation is selected from the proposed family. It achieves state-of-the-art performance on various standard benchmarks. Code at: https://github.com/NikosEfth/crafting-shifts
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2409.19663.pdf' target='_blank'>https://arxiv.org/pdf/2409.19663.pdf</a></span>   <span><a href='https://github.com/xpq-tech/KETI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaopeng Li, Shasha Li, Shangwen Wang, Shezheng Song, Bin Ji, Huijun Liu, Jun Ma, Jie Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19663">Identifying Knowledge Editing Types in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge editing has emerged as an efficient technique for updating the knowledge of large language models (LLMs), attracting increasing attention in recent years. However, there is a lack of effective measures to prevent the malicious misuse of this technique, which could lead to harmful edits in LLMs. These malicious modifications could cause LLMs to generate toxic content, misleading users into inappropriate actions. In front of this risk, we introduce a new task, $\textbf{K}$nowledge $\textbf{E}$diting $\textbf{T}$ype $\textbf{I}$dentification (KETI), aimed at identifying different types of edits in LLMs, thereby providing timely alerts to users when encountering illicit edits. As part of this task, we propose KETIBench, which includes five types of harmful edits covering the most popular toxic types, as well as one benign factual edit. We develop five classical classification models and three BERT-based models as baseline identifiers for both open-source and closed-source LLMs. Our experimental results, across 92 trials involving four models and three knowledge editing methods, demonstrate that all eight baseline identifiers achieve decent identification performance, highlighting the feasibility of identifying malicious edits in LLMs. Additional analyses reveal that the performance of the identifiers is independent of the reliability of the knowledge editing methods and exhibits cross-domain generalization, enabling the identification of edits from unknown sources. All data and code are available in https://github.com/xpq-tech/KETI.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2409.17555.pdf' target='_blank'>https://arxiv.org/pdf/2409.17555.pdf</a></span>   <span><a href='https://github.com/KPeng9510/EBiL-HaDS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/KPeng9510/EBiL-HaDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Peng, Di Wen, Kailun Yang, Ao Luo, Yufan Chen, Jia Fu, M. Saquib Sarfraz, Alina Roitberg, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17555">Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Open-Set Domain Generalization (OSDG), the model is exposed to both new variations of data appearance (domains) and open-set conditions, where both known and novel categories are present at test time. The challenges of this task arise from the dual need to generalize across diverse domains and accurately quantify category novelty, which is critical for applications in dynamic environments. Recently, meta-learning techniques have demonstrated superior results in OSDG, effectively orchestrating the meta-train and -test tasks by employing varied random categories and predefined domain partition strategies. These approaches prioritize a well-designed training schedule over traditional methods that focus primarily on data augmentation and the enhancement of discriminative feature learning. The prevailing meta-learning models in OSDG typically utilize a predefined sequential domain scheduler to structure data partitions. However, a crucial aspect that remains inadequately explored is the influence brought by strategies of domain schedulers during training. In this paper, we observe that an adaptive domain scheduler benefits more in OSDG compared with prefixed sequential and random domain schedulers. We propose the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) to achieve an adaptive domain scheduler. This method strategically sequences domains by assessing their reliabilities in utilizing a follower network, trained with confidence scores learned in an evidential manner, regularized by max rebiasing discrepancy, and optimized in a bi-level manner. The results show that our method substantially improves OSDG performance and achieves more discriminative embeddings for both the seen and unseen categories. The source code is publicly available at https://github.com/KPeng9510/EBiL-HaDS.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2409.15730.pdf' target='_blank'>https://arxiv.org/pdf/2409.15730.pdf</a></span>   <span><a href='https://github.com/Sephirex-X/LatentDriver' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyu Xiao, Jiang-Jiang Liu, Sen Yang, Xiaofan Li, Xiaoqing Ye, Wankou Yang, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15730">Learning Multiple Probabilistic Decisions from Latent World Model in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The autoregressive world model exhibits robust generalization capabilities in vectorized scene understanding but encounters difficulties in deriving actions due to insufficient uncertainty modeling and self-delusion. In this paper, we explore the feasibility of deriving decisions from an autoregressive world model by addressing these challenges through the formulation of multiple probabilistic hypotheses. We propose LatentDriver, a framework models the environment's next states and the ego vehicle's possible actions as a mixture distribution, from which a deterministic control signal is then derived. By incorporating mixture modeling, the stochastic nature of decisionmaking is captured. Additionally, the self-delusion problem is mitigated by providing intermediate actions sampled from a distribution to the world model. Experimental results on the recently released close-loop benchmark Waymax demonstrate that LatentDriver surpasses state-of-the-art reinforcement learning and imitation learning methods, achieving expert-level performance. The code and models will be made available at https://github.com/Sephirex-X/LatentDriver.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2409.14396.pdf' target='_blank'>https://arxiv.org/pdf/2409.14396.pdf</a></span>   <span><a href='https://github.com/nblt/Flat-LoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Li, Zhengbao He, Yujun Li, Yasheng Wang, Lifeng Shang, Xiaolin Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14396">Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning large-scale pre-trained models is prohibitively expensive in terms of computation and memory costs. Low-Rank Adaptation (LoRA), a popular Parameter-Efficient Fine-Tuning (PEFT) method, offers an efficient solution by optimizing only low-rank matrices. Despite recent progress in improving LoRA's performance, the relationship between the LoRA optimization space and the full parameter space is often overlooked. A solution that appears flat in the loss landscape of the LoRA space may still exhibit sharp directions in the full parameter space, potentially compromising generalization. We introduce Flat-LoRA, which aims to identify a low-rank adaptation situated in a flat region of the full parameter space. Instead of adopting the well-established sharpness-aware minimization approach, which incurs significant computation and memory overheads, we employ a Bayesian expectation loss objective to preserve training efficiency. Further, we design a refined random perturbation generation strategy for improved performance and carefully manage memory overhead using random seeds. Experiments across diverse tasks-including mathematical reasoning, coding abilities, dialogue generation, instruction following, and text-to-image generation-demonstrate that Flat-LoRA improves both in-domain and out-of-domain generalization. Code is available at https://github.com/nblt/Flat-LoRA.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2409.14163.pdf' target='_blank'>https://arxiv.org/pdf/2409.14163.pdf</a></span>   <span><a href='https://github.com/zhanghr2001/PromptTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Jingwen Fu, Badong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14163">PromptTA: Prompt-driven Text Adapter for Source-free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Source-free domain generalization (SFDG) tackles the challenge of adapting models to unseen target domains without access to source domain data. To deal with this challenging task, recent advances in SFDG have primarily focused on leveraging the text modality of vision-language models such as CLIP. These methods involve developing a transferable linear classifier based on diverse style features extracted from the text and learned prompts or deriving domain-unified text representations from domain banks. However, both style features and domain banks have limitations in capturing comprehensive domain knowledge. In this work, we propose Prompt-Driven Text Adapter (PromptTA) method, which is designed to better capture the distribution of style features and employ resampling to ensure thorough coverage of domain knowledge. To further leverage this rich domain information, we introduce a text adapter that learns from these style features for efficient domain information storage. Extensive experiments conducted on four benchmark datasets demonstrate that PromptTA achieves state-of-the-art performance. The code is available at https://github.com/zhanghr2001/PromptTA.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2409.12522.pdf' target='_blank'>https://arxiv.org/pdf/2409.12522.pdf</a></span>   <span><a href='https://github.com/wkklavis/DAPSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhikai Wei, Wenhui Dong, Peilin Zhou, Yuliang Gu, Zhou Zhao, Yongchao Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12522">Prompting Segment Anything Model with Domain-Adaptive Prototype for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning based methods often suffer from performance degradation caused by domain shift. In recent years, many sophisticated network structures have been designed to tackle this problem. However, the advent of large model trained on massive data, with its exceptional segmentation capability, introduces a new perspective for solving medical segmentation problems. In this paper, we propose a novel Domain-Adaptive Prompt framework for fine-tuning the Segment Anything Model (termed as DAPSAM) to address single-source domain generalization (SDG) in segmenting medical images. DAPSAM not only utilizes a more generalization-friendly adapter to fine-tune the large model, but also introduces a self-learning prototype-based prompt generator to enhance model's generalization ability. Specifically, we first merge the important low-level features into intermediate features before feeding to each adapter, followed by an attention filter to remove redundant information. This yields more robust image embeddings. Then, we propose using a learnable memory bank to construct domain-adaptive prototypes for prompt generation, helping to achieve generalizable medical image segmentation. Extensive experimental results demonstrate that our DAPSAM achieves state-of-the-art performance on two SDG medical image segmentation tasks with different modalities. The code is available at https://github.com/wkklavis/DAPSAM.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2409.12293.pdf' target='_blank'>https://arxiv.org/pdf/2409.12293.pdf</a></span>   <span><a href='https://github.com/LuGroupUMN/ICL_Linear_Systems' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank Cole, Yulong Lu, Wuzhe Xu, Tianhao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12293">In-Context Learning of Linear Systems: Generalization Theory and Applications to Operator Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study theoretical guarantees for solving linear systems in-context using a linear transformer architecture. For in-domain generalization, we provide neural scaling laws that bound the generalization error in terms of the number of tasks and sizes of samples used in training and inference. For out-of-domain generalization, we find that the behavior of trained transformers under task distribution shifts depends crucially on the distribution of the tasks seen during training. We introduce a novel notion of task diversity and show that it defines a necessary and sufficient condition for pre-trained transformers generalize under task distribution shifts. We also explore applications of learning linear systems in-context, such as to in-context operator learning for PDEs. Finally, we provide some numerical experiments to validate the established theory.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2409.12040.pdf' target='_blank'>https://arxiv.org/pdf/2409.12040.pdf</a></span>   <span><a href='https://github.com/XieYiping66/SFDA-rPPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiping Xie, Zitong Yu, Bingjie Wu, Weicheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12040">SFDA-rPPG: Source-Free Domain Adaptive Remote Physiological Measurement with Spatio-Temporal Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote Photoplethysmography (rPPG) is a non-contact method that uses facial video to predict changes in blood volume, enabling physiological metrics measurement. Traditional rPPG models often struggle with poor generalization capacity in unseen domains. Current solutions to this problem is to improve its generalization in the target domain through Domain Generalization (DG) or Domain Adaptation (DA). However, both traditional methods require access to both source domain data and target domain data, which cannot be implemented in scenarios with limited access to source data, and another issue is the privacy of accessing source domain data. In this paper, we propose the first Source-free Domain Adaptation benchmark for rPPG measurement (SFDA-rPPG), which overcomes these limitations by enabling effective domain adaptation without access to source domain data. Our framework incorporates a Three-Branch Spatio-Temporal Consistency Network (TSTC-Net) to enhance feature consistency across domains. Furthermore, we propose a new rPPG distribution alignment loss based on the Frequency-domain Wasserstein Distance (FWD), which leverages optimal transport to align power spectrum distributions across domains effectively and further enforces the alignment of the three branches. Extensive cross-domain experiments and ablation studies demonstrate the effectiveness of our proposed method in source-free domain adaptation settings. Our findings highlight the significant contribution of the proposed FWD loss for distributional alignment, providing a valuable reference for future research and applications. The source code is available at https://github.com/XieYiping66/SFDA-rPPG
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2409.11570.pdf' target='_blank'>https://arxiv.org/pdf/2409.11570.pdf</a></span>   <span><a href='https://github.com/mhnazeri/VertiCoder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Nazeri, Aniket Datar, Anuj Pokhrel, Chenhui Pan, Garrett Warnell, Xuesu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11570">VertiCoder: Self-Supervised Kinodynamic Representation Learning on Vertically Challenging Terrain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present VertiCoder, a self-supervised representation learning approach for robot mobility on vertically challenging terrain. Using the same pre-training process, VertiCoder can handle four different downstream tasks, including forward kinodynamics learning, inverse kinodynamics learning, behavior cloning, and patch reconstruction with a single representation. VertiCoder uses a TransformerEncoder to learn the local context of its surroundings by random masking and next patch reconstruction. We show that VertiCoder achieves better performance across all four different tasks compared to specialized End-to-End models with 77% fewer parameters. We also show VertiCoder's comparable performance against state-of-the-art kinodynamic modeling and planning approaches in real-world robot deployment. These results underscore the efficacy of VertiCoder in mitigating overfitting and fostering more robust generalization across diverse environmental contexts and downstream vehicle kinodynamic tasks.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2409.11206.pdf' target='_blank'>https://arxiv.org/pdf/2409.11206.pdf</a></span>   <span><a href='https://github.com/Addy-1998/High_Order_Graphs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Humnabadkar, Arindam Sikdar, Benjamin Cave, Huaizhong Zhang, Paul Bakaki, Ardhendu Behera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11206">High-Order Evolving Graphs for Enhanced Representation of Traffic Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an innovative framework for traffic dynamics analysis using High-Order Evolving Graphs, designed to improve spatio-temporal representations in autonomous driving contexts. Our approach constructs temporal bidirectional bipartite graphs that effectively model the complex interactions within traffic scenes in real-time. By integrating Graph Neural Networks (GNNs) with high-order multi-aggregation strategies, we significantly enhance the modeling of traffic scene dynamics, providing a more accurate and detailed analysis of these interactions. Additionally, we incorporate inductive learning techniques inspired by the GraphSAGE framework, enabling our model to adapt to new and unseen traffic scenarios without the need for retraining, thus ensuring robust generalization. Through extensive experiments on the ROAD and ROAD Waymo datasets, we establish a comprehensive baseline for further developments, demonstrating the potential of our method in accurately capturing traffic behavior. Our results emphasize the value of high-order statistical moments and feature-gated attention mechanisms in improving traffic behavior analysis, laying the groundwork for advancing autonomous driving technologies. Our source code is available at: https://github.com/Addy-1998/High_Order_Graphs
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2409.07960.pdf' target='_blank'>https://arxiv.org/pdf/2409.07960.pdf</a></span>   <span><a href='https://github.com/kerem-cekmeceli/Foundation-Models-for-Medical-Imagery' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kerem Cekmeceli, Meva Himmetoglu, Guney I. Tombak, Anna Susmelj, Ertunc Erdil, Ender Konukoglu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07960">Do Vision Foundation Models Enhance Domain Generalization in Medical Image Segmentation?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks achieve state-of-the-art performance in many supervised learning tasks when the training data distribution matches the test data distribution. However, their performance drops significantly under domain (covariate) shift, a prevalent issue in medical image segmentation due to varying acquisition settings across different scanner models and protocols. Recently, foundational models (FMs) trained on large datasets have gained attention for their ability to be adapted for downstream tasks and achieve state-of-the-art performance with excellent generalization capabilities on natural images. However, their effectiveness in medical image segmentation remains underexplored. In this paper, we investigate the domain generalization performance of various FMs, including DinoV2, SAM, MedSAM, and MAE, when fine-tuned using various parameter-efficient fine-tuning (PEFT) techniques such as Ladder and Rein (+LoRA) and decoder heads. We introduce a novel decode head architecture, HQHSAM, which simply integrates elements from two state-of-the-art decoder heads, HSAM and HQSAM, to enhance segmentation performance. Our extensive experiments on multiple datasets, encompassing various anatomies and modalities, reveal that FMs, particularly with the HQHSAM decode head, improve domain generalization for medical image segmentation. Moreover, we found that the effectiveness of PEFT techniques varies across different FMs. These findings underscore the potential of FMs to enhance the domain generalization performance of neural networks in medical image segmentation across diverse clinical settings, providing a solid foundation for future research. Code and models are available for research purposes at \url{https://github.com/kerem-cekmeceli/Foundation-Models-for-Medical-Imagery}.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2409.04699.pdf' target='_blank'>https://arxiv.org/pdf/2409.04699.pdf</a></span>   <span><a href='https://github.com/alusi123/DFA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Wang, ALuSi, Xun Yang, Ke Xu, Huibin Tan, Xingyi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04699">Dual-stream Feature Augmentation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) task aims to learn a robust model from source domains that could handle the out-of-distribution (OOD) issue. In order to improve the generalization ability of the model in unseen domains, increasing the diversity of training samples is an effective solution. However, existing augmentation approaches always have some limitations. On the one hand, the augmentation manner in most DG methods is not enough as the model may not see the perturbed features in approximate the worst case due to the randomness, thus the transferability in features could not be fully explored. On the other hand, the causality in discriminative features is not involved in these methods, which harms the generalization ability of model due to the spurious correlations. To address these issues, we propose a Dual-stream Feature Augmentation~(DFA) method by constructing some hard features from two perspectives. Firstly, to improve the transferability, we construct some targeted features with domain related augmentation manner. Through the guidance of uncertainty, some hard cross-domain fictitious features are generated to simulate domain shift. Secondly, to take the causality into consideration, the spurious correlated non-causal information is disentangled by an adversarial mask, then the more discriminative features can be extracted through these hard causal related information. Different from previous fixed synthesizing strategy, the two augmentations are integrated into a unified learnable feature disentangle model. Based on these hard features, contrastive learning is employed to keep the semantic consistency and improve the robustness of the model. Extensive experiments on several datasets demonstrated that our approach could achieve state-of-the-art performance for domain generalization. Our code is available at: https://github.com/alusi123/DFA.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2409.03501.pdf' target='_blank'>https://arxiv.org/pdf/2409.03501.pdf</a></span>   <span><a href='https://github.com/RizhaoCai/FAS_Aug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rizhao Cai, Cecelia Soh, Zitong Yu, Haoliang Li, Wenhan Yang, Alex Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03501">Towards Data-Centric Face Anti-Spoofing: Improving Cross-domain Generalization via Physics-based Data Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Anti-Spoofing (FAS) research is challenged by the cross-domain problem, where there is a domain gap between the training and testing data. While recent FAS works are mainly model-centric, focusing on developing domain generalization algorithms for improving cross-domain performance, data-centric research for face anti-spoofing, improving generalization from data quality and quantity, is largely ignored. Therefore, our work starts with data-centric FAS by conducting a comprehensive investigation from the data perspective for improving cross-domain generalization of FAS models. More specifically, at first, based on physical procedures of capturing and recapturing, we propose task-specific FAS data augmentation (FAS-Aug), which increases data diversity by synthesizing data of artifacts, such as printing noise, color distortion, moirÃ© pattern, \textit{etc}. Our experiments show that using our FAS augmentation can surpass traditional image augmentation in training FAS models to achieve better cross-domain performance. Nevertheless, we observe that models may rely on the augmented artifacts, which are not environment-invariant, and using FAS-Aug may have a negative effect. As such, we propose Spoofing Attack Risk Equalization (SARE) to prevent models from relying on certain types of artifacts and improve the generalization performance. Last but not least, our proposed FAS-Aug and SARE with recent Vision Transformer backbones can achieve state-of-the-art performance on the FAS cross-domain generalization protocols. The implementation is available at https://github.com/RizhaoCai/FAS_Aug.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2409.01128.pdf' target='_blank'>https://arxiv.org/pdf/2409.01128.pdf</a></span>   <span><a href='https://github.com/jinglin-liang/DDDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinglin Liang, Jin Zhong, Hanlin Gu, Zhongqi Lu, Xingxing Tang, Gang Dai, Shuangping Huang, Lixin Fan, Qiang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01128">Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Class Continual Learning (FCCL) merges the challenges of distributed client learning with the need for seamless adaptation to new classes without forgetting old ones. The key challenge in FCCL is catastrophic forgetting, an issue that has been explored to some extent in Continual Learning (CL). However, due to privacy preservation requirements, some conventional methods, such as experience replay, are not directly applicable to FCCL. Existing FCCL methods mitigate forgetting by generating historical data through federated training of GANs or data-free knowledge distillation. However, these approaches often suffer from unstable training of generators or low-quality generated data, limiting their guidance for the model. To address this challenge, we propose a novel method of data replay based on diffusion models. Instead of training a diffusion model, we employ a pre-trained conditional diffusion model to reverse-engineer each class, searching the corresponding input conditions for each class within the model's input space, significantly reducing computational resources and time consumption while ensuring effective generation. Furthermore, we enhance the classifier's domain generalization ability on generated and real data through contrastive learning, indirectly improving the representational capability of generated data for real data. Comprehensive experiments demonstrate that our method significantly outperforms existing baselines. Code is available at https://github.com/jinglin-liang/DDDR.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2508.03700.pdf' target='_blank'>https://arxiv.org/pdf/2508.03700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liujian Tang, Shaokang Dong, Yijia Huang, Minqi Xiang, Hongtao Ruan, Bin Wang, Shuo Li, Zhiheng Xi, Zhihui Cao, Hailiang Pang, Heng Kong, He Yang, Mingxu Chai, Zhilin Gao, Xingyu Liu, Yingnan Fu, Jiaming Liu, Xuanjing Huang, Yu-Gang Jiang, Tao Gui, Qi Zhang, Kang Wang, Yunke Zhang, Yuran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03700">MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents MagicGUI, a foundational mobile GUI agent designed to address critical challenges in perception, grounding, and reasoning within real-world mobile GUI environments. The framework is underpinned by following six key components: (1) a comprehensive and accurate dataset, constructed via the scalable GUI Data Pipeline, which aggregates the largest and most diverse GUI-centric multimodal data to date from open-source repositories, automated crawling, and targeted manual annotation; (2) enhanced perception and grounding capabilities, facilitating fine-grained multimodal alignment for UI element referencing, grounding, and screen comprehension; (3) a comprehensive and unified action space, encompassing both fundamental UI operations and complex interactive intents to support human-agent interactions; (4) planning-oriented reasoning mechanisms that enable the model to decompose complex user instructions into sequential actions with explicit intermediate meta-paln reasoning; (5) an iterative two-stage training procedure, combining large-scale continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing a spatially enhanced composite reward and dual filtering strategy; and (6) competitive performance on both the proprietary Magic-RICH benchmark and over a dozen public benchmarks, achieving superior performance across GUI perception and agent tasks, while demonstrating robust generalization and real-world deployment potential in practical mobile GUI scenarios, as detailed in Figure 1.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2507.05197.pdf' target='_blank'>https://arxiv.org/pdf/2507.05197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shihan Dou, Shichun Liu, Yuming Yang, Yicheng Zou, Yunhua Zhou, Shuhao Xing, Chenhao Huang, Qiming Ge, Demin Song, Haijun Lv, Songyang Gao, Chengqi Lv, Enyu Zhou, Honglin Guo, Zhiheng Xi, Wenwei Zhang, Qipeng Guo, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Tao Gui, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05197">Pre-Trained Policy Discriminators are General Reward Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2509.21871.pdf' target='_blank'>https://arxiv.org/pdf/2509.21871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyang Liu, Yifan Hu, Senjie Jin, Shihan Dou, Gonglei Shi, Jie Shao, Tao Gui, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21871">Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) are well suited to image aesthetic assessment, as they can capture high-level aesthetic features leveraging their cross-modal understanding capacity. However, the scarcity of multimodal aesthetic reasoning data and the inherently subjective nature of aesthetic judgment make it difficult for MLLMs to generate accurate aesthetic judgments with interpretable rationales. To this end, we propose Aes-R1, a comprehensive aesthetic reasoning framework with reinforcement learning (RL). Concretely, Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality chain-of-thought aesthetic reasoning data used for cold-start. After teaching the model to generate structured explanations prior to scoring, we then employ the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that jointly optimizes absolute score regression and relative ranking order, improving both per-image accuracy and cross-image preference judgments. Aes-R1 enables MLLMs to generate grounded explanations alongside faithful scores, thereby enhancing aesthetic scoring and reasoning in a unified framework. Extensive experiments demonstrate that Aes-R1 improves the backbone's average PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar size. More ablation studies validate Aes-R1's robust generalization under limited supervision and in out-of-distribution scenarios.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2510.16880.pdf' target='_blank'>https://arxiv.org/pdf/2510.16880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weida Wang, Benteng Chen, Di Zhang, Wanhao Liu, Shuchen Pu, Ben Gao, Jin Zeng, Lei Bai, Wanli Ouyang, Xiaoyong Wei, Tianshu Yu, Tianfan Fu, Shuzhou Sun, Jiatong Li, Zifu Wang, Yuqiang Li, Shufei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16880">Chem-R: Learning to Reason as a Chemist</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although large language models (LLMs) have significant potential to advance chemical discovery, current LLMs lack core chemical knowledge, produce unreliable reasoning trajectories, and exhibit suboptimal performance across diverse chemical tasks. To address these challenges, we propose Chem-R, a generalizable Chemical Reasoning model designed to emulate the deliberative processes of chemists. Chem-R is trained through a three-phase framework that progressively builds advanced reasoning capabilities, including: 1) Chemical Foundation Training, which establishes core chemical knowledge. 2) Chemical Reasoning Protocol Distillation, incorporating structured, expert-like reasoning traces to guide systematic and reliable problem solving. 3) Multi-task Group Relative Policy Optimization that optimizes the model for balanced performance across diverse molecular- and reaction-level tasks. This structured pipeline enables Chem-R to achieve state-of-the-art performance on comprehensive benchmarks, surpassing leading large language models, including Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing chemical foundation models across both molecular and reaction level tasks. These results highlight Chem-R's robust generalization, interpretability, and potential as a foundation for next-generation AI-driven chemical discovery.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2505.19797.pdf' target='_blank'>https://arxiv.org/pdf/2505.19797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Zhang, Hao Li, Chenxu Wang, Linyao Chen, Qiaosheng Zhang, Peng Ye, Shi Feng, Daling Wang, Zhen Wang, Xinrun Wang, Jia Xu, Lei Bai, Wanli Ouyang, Shuyue Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19797">The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proprietary giants are increasingly dominating the race for ever-larger language models. Can open-source, smaller models remain competitive across a broad range of tasks? In this paper, we present the Avengers -- a simple recipe that leverages the collective intelligence of these smaller models. The Avengers builds upon four lightweight operations: (i) embedding: encode queries using a text embedding model; (ii) clustering: group queries based on their semantic similarity; (iii) scoring: scores each model's performance within each cluster; and (iv) voting: improve outputs via repeated sampling and voting. At inference time, each query is embedded and assigned to its nearest cluster. The top-performing model(s) within that cluster are selected to generate the response with repeated sampling. Remarkably, with 10 open-source models (~7B parameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in average performance across 15 diverse datasets spanning mathematics, coding, logical reasoning, general knowledge, and affective tasks. In particular, it surpasses GPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the Avengers delivers superior out-of-distribution generalization, and remains robust across various embedding models, clustering algorithms, ensemble strategies, and values of its sole parameter -- the number of clusters.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2508.17715.pdf' target='_blank'>https://arxiv.org/pdf/2508.17715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Huang, Keping Bi, Yinqiong Cai, Wei Chen, Jiafeng Guo, Xueqi Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17715">How Do LLM-Generated Texts Impact Term-Based Retrieval Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As more content generated by large language models (LLMs) floods into the Internet, information retrieval (IR) systems now face the challenge of distinguishing and handling a blend of human-authored and machine-generated texts. Recent studies suggest that neural retrievers may exhibit a preferential inclination toward LLM-generated content, while classic term-based retrievers like BM25 tend to favor human-written documents. This paper investigates the influence of LLM-generated content on term-based retrieval models, which are valued for their efficiency and robust generalization across domains. Our linguistic analysis reveals that LLM-generated texts exhibit smoother high-frequency and steeper low-frequency Zipf slopes, higher term specificity, and greater document-level diversity. These traits are aligned with LLMs being trained to optimize reader experience through diverse and precise expressions. Our study further explores whether term-based retrieval models demonstrate source bias, concluding that these models prioritize documents whose term distributions closely correspond to those of the queries, rather than displaying an inherent source bias. This work provides a foundation for understanding and addressing potential biases in term-based IR systems managing mixed-source content.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2601.07470.pdf' target='_blank'>https://arxiv.org/pdf/2601.07470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07470">Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2505.07596.pdf' target='_blank'>https://arxiv.org/pdf/2505.07596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07596">Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2601.16206.pdf' target='_blank'>https://arxiv.org/pdf/2601.16206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16206">LLM-in-Sandbox Elicits General Agentic Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2512.22857.pdf' target='_blank'>https://arxiv.org/pdf/2512.22857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shihao Cai, Runnan Fang, Jialong Wu, Baixuan Li, Xinyu Wang, Yong Jiang, Liangcai Su, Liwen Zhang, Wenbiao Yin, Zhen Zhang, Fuli Feng, Pengjun Xie, Xiaobin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22857">AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2509.22295.pdf' target='_blank'>https://arxiv.org/pdf/2509.22295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Wu, Jianxin Jin, Wanghui Qiu, Peng Chen, Yang Shu, Bin Yang, Chenjuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22295">Aurora: Towards Universal Generative Multimodal Time Series Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain generalization is very important in Time Series Forecasting because similar historical information may lead to distinct future trends due to the domain-specific characteristics. Recent works focus on building unimodal time series foundation models and end-to-end multimodal supervised models. Since domain-specific knowledge is often contained in modalities like texts, the former lacks the explicit utilization of them, thus hindering the performance. The latter is tailored for end-to-end scenarios and does not support zero-shot inference for cross-domain scenarios. In this work, we introduce Aurora, a Multimodal Time Series Foundation Model, which supports multimodal inputs and zero-shot inference. Pretrained on Corss-domain Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key domain knowledge contained in corrsponding text or image modalities, thus possessing strong Cross-domain generalization capability. Through tokenization, encoding, and distillation, Aurora can extract multimodal domain knowledge as guidance and then utilizes a Modality-Guided Multi-head Self-Attention to inject them into the modeling of temporal representations. In the decoding phase, the multimodal representations are used to generate the conditions and prototypes of future tokens, contributing to a novel Prototype-Guided Flow Matching for generative probabilistic forecasting. Comprehensive experiments on well-recognized benchmarks, including TimeMMD, TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art performance of Aurora on both unimodal and multimodal scenarios.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2509.22403.pdf' target='_blank'>https://arxiv.org/pdf/2509.22403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanjin Meng, Yuan Yuan, Jingtao Ding, Jie Feng, Chonghua Han, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22403">MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobility Foundation Models (MFMs) have advanced the modeling of human movement patterns, yet they face a ceiling due to limitations in data scale and semantic understanding. While Large Language Models (LLMs) offer powerful semantic reasoning, they lack the innate understanding of spatio-temporal statistics required for generating physically plausible mobility trajectories. To address these gaps, we propose MoveFM-R, a novel framework that unlocks the full potential of mobility foundation models by leveraging language-driven semantic reasoning capabilities. It tackles two key challenges: the vocabulary mismatch between continuous geographic coordinates and discrete language tokens, and the representation gap between the latent vectors of MFMs and the semantic world of LLMs. MoveFM-R is built on three core innovations: a semantically enhanced location encoding to bridge the geography-language gap, a progressive curriculum to align the LLM's reasoning with mobility patterns, and an interactive self-reflection mechanism for conditional trajectory generation. Extensive experiments demonstrate that MoveFM-R significantly outperforms existing MFM-based and LLM-based baselines. It also shows robust generalization in zero-shot settings and excels at generating realistic trajectories from natural language instructions. By synthesizing the statistical power of MFMs with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm that enables a more comprehensive, interpretable, and powerful modeling of human mobility. The implementation of MoveFM-R is available online at https://anonymous.4open.science/r/MoveFM-R-CDE7/.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2512.17661.pdf' target='_blank'>https://arxiv.org/pdf/2512.17661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17661">Vidarc: Embodied Video Diffusion Model for Closed-loop Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2507.18484.pdf' target='_blank'>https://arxiv.org/pdf/2507.18484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yang, Lingxuan Wu, Lizhong Wang, Chengyang Ying, Hang Su, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18484">Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2505.14681.pdf' target='_blank'>https://arxiv.org/pdf/2505.14681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14681">Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2510.03896.pdf' target='_blank'>https://arxiv.org/pdf/2510.03896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Liu, Zheng Huang, Xiaoyi Lin, Muzhi Zhu, Canyu Zhao, Zongze Du, Yating Wang, Haoyi Zhu, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03896">Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although Vision-Language Models (VLM) have demonstrated impressive planning and reasoning capabilities, translating these abilities into the physical world introduces significant challenges. Conventional Vision-Language-Action (VLA) models, which integrate reasoning and action into a monolithic architecture, generalize poorly because they are constrained by scarce, narrow-domain data. While recent dual-system approaches attempt to decouple "thinking" from "acting", they are often constrained by semantic ambiguities within the action module. This ambiguity makes large-scale, cross-task training infeasible. Consequently, these systems typically necessitate fine-tuning on newly collected data when deployed to novel environments, and the cooperation mechanism between the two systems remains ill-defined. To address these limitations, we introduce, for the first time, a framework centered around a generalizable action expert. Our approach utilizes sparse 3D trajectories as an intermediate representation, effectively bridging the high-level planning capabilities of the VLM with the low-level physical action module. During the planning phase, the VLM is only required to generate coarse 3D waypoints. These waypoints are then processed by our generalizable action expert, which refines them into dense, executable action sequences by sampling real-time point cloud observations of the environment. To promote training efficiency and robust generalization, we introduce a novel "Action Pre-training, Pointcloud Fine-tuning" paradigm. Our method combines the broad generalization capabilities of VLMs in visual understanding and planning with the fine-grained, action-level generalization of action expert.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2508.04117.pdf' target='_blank'>https://arxiv.org/pdf/2508.04117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwen Ruan, Yun Chen, Yutao Hou, Peng Li, Yang Liu, Guanhua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04117">Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pretrained large language models (LLMs) are finetuned with labeled data for better instruction following ability and alignment with human values. In this paper, we study the learning dynamics of LLM finetuning on reasoning tasks and reveal the uncovered over-memorization phenomenon during a specific stage of LLM finetuning. At this stage, the LLMs have excessively memorized training data and exhibit high test perplexity while maintaining good test accuracy. We investigate the conditions that lead to LLM over-memorization and find that training epochs and large learning rates contribute to this issue. Although models with over-memorization demonstrate comparable test accuracy to normal models, they suffer from reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. Our experiments unveil the over-memorization to be broadly applicable across different tasks, models, and finetuning methods. Our research highlights that overparameterized, extensively finetuned LLMs exhibit unique learning dynamics distinct from traditional machine learning models. Based on our observations of over-memorization, we provide recommendations on checkpoint and learning rate selection during finetuning.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2505.11166.pdf' target='_blank'>https://arxiv.org/pdf/2505.11166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huashan Sun, Shengyi Liao, Yansen Han, Yu Bai, Yang Gao, Cheng Fu, Weizhou Shen, Fanqi Wan, Ming Yan, Ji Zhang, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11166">SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite advances in pretraining with extended context lengths, large language models (LLMs) still face challenges in effectively utilizing real-world long-context information, primarily due to insufficient long-context alignment caused by data quality issues, training inefficiencies, and the lack of well-designed optimization objectives. To address these limitations, we propose a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng $\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling long-context preference optimization (PO) into two components: short-context PO and short-to-long reward alignment (SoLo-RA), supported by both theoretical and empirical evidence. Specifically, short-context PO leverages preference pairs sampled from short contexts to enhance the model's contextual knowledge utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score consistency utilization for the responses when conditioned on both short and long contexts that contain identical task-relevant information. This facilitates transferring the model's ability to handle short contexts into long-context scenarios. SoLoPO is compatible with mainstream preference optimization algorithms, while substantially improving the efficiency of data construction and training processes. Experimental results show that SoLoPO enhances all these algorithms with respect to stronger length and domain generalization abilities across various long-context benchmarks, while achieving notable improvements in both computational and memory efficiency.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2501.03271.pdf' target='_blank'>https://arxiv.org/pdf/2501.03271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amitava Das, Suranjana Trivedy, Danush Khanna, Rajarshi Roy, Gurpreet Singh, Basab Ghosh, Yaswanth Narsupalli, Vinija Jain, Vasu Sharma, Aishwarya Naresh Reganti, Aman Chadha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03271">DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid rise of large language models (LLMs) has unlocked many applications but also underscores the challenge of aligning them with diverse values and preferences. Direct Preference Optimization (DPO) is central to alignment but constrained by fixed divergences and limited feature transformations. We propose DPO-Kernels, which integrates kernel methods to address these issues through four key contributions: (i) Kernelized Representations with polynomial, RBF, Mahalanobis, and spectral kernels for richer transformations, plus a hybrid loss combining embedding-based and probability-based objectives; (ii) Divergence Alternatives (Jensen-Shannon, Hellinger, Renyi, Bhattacharyya, Wasserstein, and f-divergences) for greater stability; (iii) Data-Driven Selection metrics that automatically choose the best kernel-divergence pair; and (iv) a Hierarchical Mixture of Kernels for both local precision and global modeling. Evaluations on 12 datasets demonstrate state-of-the-art performance in factuality, safety, reasoning, and instruction following. Grounded in Heavy-Tailed Self-Regularization, DPO-Kernels maintains robust generalization for LLMs, offering a comprehensive resource for further alignment research.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2510.07755.pdf' target='_blank'>https://arxiv.org/pdf/2510.07755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyu Wu, Yinlin Zhu, Xunkai Li, Ziang Qiu, Rong-Hua Li, Guoren Wang, Chenghu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07755">FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models have shown remarkable cross-domain generalization in language and vision, inspiring the development of graph foundation models (GFMs). However, existing GFMs typically assume centralized access to multi-domain graphs, which is often infeasible due to privacy and institutional constraints. Federated Graph Foundation Models (FedGFMs) address this limitation, but their effectiveness fundamentally hinges on constructing a robust global codebook that achieves intra-domain coherence by consolidating mutually reinforcing semantics within each domain, while also maintaining inter-domain diversity by retaining heterogeneous knowledge across domains. To this end, we propose FedBook, a unified federated graph foundation codebook that systematically aggregates clients' local codebooks during server-side federated pre-training. FedBook follows a two-phase process: (1) Intra-domain Collaboration, where low-frequency tokens are refined by referencing more semantically reliable high-frequency tokens across clients to enhance domain-specific coherence; and (2) Inter-domain Integration, where client contributions are weighted by the semantic distinctiveness of their codebooks during the aggregation of the global GFM, thereby preserving cross-domain diversity. Extensive experiments on 8 benchmarks across multiple domains and tasks demonstrate that FedBook consistently outperforms 21 baselines, including isolated supervised learning, FL/FGL, federated adaptations of centralized GFMs, and FedGFM techniques.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2509.08401.pdf' target='_blank'>https://arxiv.org/pdf/2509.08401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08401">Two Facets of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the success of LLMs, GFMs are designed to learn the optimal embedding functions from multi-domain text-attributed graphs for the downstream cross-task generalization capability. Among the diverse architectures, graph VQ-MAE stands out among the increasingly diverse landscape of GFM. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2507.17001.pdf' target='_blank'>https://arxiv.org/pdf/2507.17001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Li, Guangyi Chen, Yunlong Deng, Zijian Li, Zeyu Tang, Anpeng Wu, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17001">Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing methods for adapting models to out-of-distribution (OOD) domains rely on invariant representation learning to eliminate the influence of biased features. However, should bias always be eliminated -- and if not, when should it be retained, and how can it be leveraged? To address these questions, we first present a theoretical analysis that explores the conditions under which biased features can be identified and effectively utilized. Building on this theoretical foundation, we introduce a novel framework that strategically leverages bias to complement invariant representations during inference. The framework comprises two key components that leverage bias in both direct and indirect ways: (1) using invariance as guidance to extract predictive ingredients from bias, and (2) exploiting identified bias to estimate the environmental condition and then use it to explore appropriate bias-aware predictors to alleviate environment gaps. We validate our approach through experiments on both synthetic datasets and standard domain generalization benchmarks. Results consistently demonstrate that our method outperforms existing approaches, underscoring its robustness and adaptability.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2510.24216.pdf' target='_blank'>https://arxiv.org/pdf/2510.24216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Xu, Hao Wu, Kun Wang, Nan Wang, Qingsong Wen, Xian Wu, Wei Gong, Xibin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24216">Unlocking Out-of-Distribution Generalization in Dynamics through Physics-Guided Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In dynamical system modeling, traditional numerical methods are limited by high computational costs, while modern data-driven approaches struggle with data scarcity and distribution shifts. To address these fundamental limitations, we first propose SPARK, a physics-guided quantitative augmentation plugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate physical parameters into a physics-rich discrete state dictionary. This state dictionary then acts as a structured dictionary of physical states, enabling the creation of new, physically-plausible training samples via principled interpolation in the latent space. Further, for downstream prediction, these augmented representations are seamlessly integrated with a Fourier-enhanced Graph ODE, a combination designed to robustly model the enriched data distribution while capturing long-term temporal dependencies. Extensive experiments on diverse benchmarks demonstrate that SPARK significantly outperforms state-of-the-art baselines, particularly in challenging out-of-distribution scenarios and data-scarce regimes, proving the efficacy of our physics-guided augmentation paradigm.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2511.09980.pdf' target='_blank'>https://arxiv.org/pdf/2511.09980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Li, Tian Tian, Zhenghua Xu, Hao Cheng, Shikun Zhang, Wei Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09980">Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic retrieval-augmented generation (RAG) allows large language models (LLMs) to fetch external knowledge on demand, offering greater adaptability than static RAG. A central challenge in this setting lies in determining the optimal timing for retrieval. Existing methods often trigger retrieval based on low token-level confidence, which may lead to delayed intervention after errors have already propagated. We introduce Entropy-Trend Constraint (ETC), a training-free method that determines optimal retrieval timing by modeling the dynamics of token-level uncertainty. Specifically, ETC utilizes first- and second-order differences of the entropy sequence to detect emerging uncertainty trends, enabling earlier and more precise retrieval. Experiments on six QA benchmarks with three LLM backbones demonstrate that ETC consistently outperforms strong baselines while reducing retrieval frequency. ETC is particularly effective in domain-specific scenarios, exhibiting robust generalization capabilities. Ablation studies and qualitative analyses further confirm that trend-aware uncertainty modeling yields more effective retrieval timing. The method is plug-and-play, model-agnostic, and readily integrable into existing decoding pipelines. Implementation code is included in the supplementary materials.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2508.06026.pdf' target='_blank'>https://arxiv.org/pdf/2508.06026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, Wei Ye, Shikun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06026">Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose \textbf{Temporal Self-Rewarding Language Models} that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) \textit{Anchored Rejection} - fixing rejected responses using the past initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2507.17849.pdf' target='_blank'>https://arxiv.org/pdf/2507.17849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyue Yin, Qiushi Sun, Zhiyuan Zeng, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17849">Dynamic and Generalizable Process Reward Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Process Reward Models (PRMs) are crucial for guiding Large Language Models (LLMs) in complex scenarios by providing dense reward signals. However, existing PRMs primarily rely on heuristic approaches, which struggle with cross-domain generalization. While LLM-as-judge has been proposed to provide generalized rewards, current research has focused mainly on feedback results, overlooking the meaningful guidance embedded within the text. Additionally, static and coarse-grained evaluation criteria struggle to adapt to complex process supervision. To tackle these challenges, we propose Dynamic and Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to capture and store fine-grained, multi-dimensional reward criteria. DG-PRM dynamically selects reward signals for step-wise reward scoring. To handle multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation to identify discriminative positive and negative pairs. Experimental results show that DG-PRM achieves stunning performance on prevailing benchmarks, significantly boosting model performance across tasks with dense rewards. Further analysis reveals that DG-PRM adapts well to out-of-distribution scenarios, demonstrating exceptional generalizability.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2506.10966.pdf' target='_blank'>https://arxiv.org/pdf/2506.10966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Gao, Yilun Chen, Shuai Yang, Xinyi Chen, Yang Tian, Hao Li, Haifeng Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10966">GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation in real-world settings remains challenging, especially regarding robust generalization. Existing simulation platforms lack sufficient support for exploring how policies adapt to varied instructions and scenarios. Thus, they lag behind the growing interest in instruction-following foundation models like LLMs, whose adaptability is crucial yet remains underexplored in fair comparisons. To bridge this gap, we introduce GenManip, a realistic tabletop simulation platform tailored for policy generalization studies. It features an automatic pipeline via LLM-driven task-oriented scene graph to synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To systematically assess generalization, we present GenManip-Bench, a benchmark of 200 scenarios refined via human-in-the-loop corrections. We evaluate two policy types: (1) modular manipulation systems integrating foundation models for perception, reasoning, and planning, and (2) end-to-end policies trained through scalable data collection. Results show that while data scaling benefits end-to-end methods, modular systems enhanced with foundation models generalize more effectively across diverse scenarios. We anticipate this platform to facilitate critical insights for advancing policy generalization in realistic conditions. Project Page: https://genmanip.axi404.top/.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2505.12629.pdf' target='_blank'>https://arxiv.org/pdf/2505.12629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchang Sun, Yanxi Chen, Yaliang Li, Bolin Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12629">Enhancing Latent Computation in Transformers with Latent Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmenting large language models (LLMs) with auxiliary tokens has emerged as a promising strategy for enhancing model performance. In this work, we introduce a lightweight method termed latent tokens; these are dummy tokens that may be non-interpretable in natural language but steer the autoregressive decoding process of a Transformer-based LLM via the attention mechanism. The proposed latent tokens can be seamlessly integrated with a pre-trained Transformer, trained in a parameter-efficient manner, and applied flexibly at inference time, while adding minimal complexity overhead to the existing infrastructure of standard Transformers. We propose several hypotheses about the underlying mechanisms of latent tokens and design synthetic tasks accordingly to verify them. Numerical results confirm that the proposed method noticeably outperforms the baselines, particularly in the out-of-distribution generalization scenarios, highlighting its potential in improving the adaptability of LLMs.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2511.21701.pdf' target='_blank'>https://arxiv.org/pdf/2511.21701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang, Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Junhao Song, Ziqian Bi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21701">47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2511.13648.pdf' target='_blank'>https://arxiv.org/pdf/2511.13648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziang Cao, Fangzhou Hong, Zhaoxi Chen, Liang Pan, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13648">PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2505.07395.pdf' target='_blank'>https://arxiv.org/pdf/2505.07395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07395">ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2507.02288.pdf' target='_blank'>https://arxiv.org/pdf/2507.02288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>De Cheng, Zhipeng Xu, Xinyang Jiang, Dongsheng Li, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02288">Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) seeks to develop a versatile model capable of performing effectively on unseen target domains. Notably, recent advances in pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated considerable potential in enhancing the generalization capabilities of deep learning models. Despite the increasing attention toward VFM-based domain prompt tuning within DG, the effective design of prompts capable of disentangling invariant features across diverse domains remains a critical challenge. In this paper, we propose addressing this challenge by leveraging the controllable and flexible language prompt of the VFM. Noting that the text modality of VFMs is naturally easier to disentangle, we introduce a novel framework for text feature-guided visual prompt tuning. This framework first automatically disentangles the text prompt using a large language model (LLM) and then learns domain-invariant visual representation guided by the disentangled text feature. However, relying solely on language to guide visual feature disentanglement has limitations, as visual features can sometimes be too complex or nuanced to be fully captured by descriptive text. To address this, we introduce Worst Explicit Representation Alignment (WERA), which extends text-guided visual prompts by incorporating an additional set of abstract prompts. These prompts enhance source domain diversity through stylized image augmentations, while alignment constraints ensure that visual representations remain consistent across both the original and augmented distributions. Experiments conducted on major DG datasets, including PACS, VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method outperforms state-of-the-art DG methods.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2505.22855.pdf' target='_blank'>https://arxiv.org/pdf/2505.22855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruining Deng, Junchao Zhu, Juming Xiong, Can Cui, Tianyuan Yao, Junlin Guo, Siqi Lu, Marilyn Lionts, Mengmeng Yin, Yu Wang, Shilin Zhao, Yucheng Tang, Yihe Yang, Paul Dennis Simonson, Mert R. Sabuncu, Haichun Yang, Yuankai Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22855">IRS: Incremental Relationship-guided Segmentation for Digital Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual learning is rapidly emerging as a key focus in computer vision, aiming to develop AI systems capable of continuous improvement, thereby enhancing their value and practicality in diverse real-world applications. In healthcare, continual learning holds great promise for continuously acquired digital pathology data, which is collected in hospitals on a daily basis. However, panoramic segmentation on digital whole slide images (WSIs) presents significant challenges, as it is often infeasible to obtain comprehensive annotations for all potential objects, spanning from coarse structures (e.g., regions and unit objects) to fine structures (e.g., cells). This results in temporally and partially annotated data, posing a major challenge in developing a holistic segmentation framework. Moreover, an ideal segmentation model should incorporate new phenotypes, unseen diseases, and diverse populations, making this task even more complex. In this paper, we introduce a novel and unified Incremental Relationship-guided Segmentation (IRS) learning scheme to address temporally acquired, partially annotated data while maintaining out-of-distribution (OOD) continual learning capacity in digital pathology. The key innovation of IRS lies in its ability to realize a new spatial-temporal OOD continual learning paradigm by mathematically modeling anatomical relationships between existing and newly introduced classes through a simple incremental universal proposition matrix. Experimental results demonstrate that the IRS method effectively handles the multi-scale nature of pathological segmentation, enabling precise kidney segmentation across various structures (regions, units, and cells) as well as OOD disease lesions at multiple magnifications. This capability significantly enhances domain generalization, making IRS a robust approach for real-world digital pathology applications.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2505.05785.pdf' target='_blank'>https://arxiv.org/pdf/2505.05785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henan Sun, Xunkai Li, Lei Zhu, Junyi Han, Guang Zeng, Ronghua Li, Guoren Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05785">Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-Of-Distribution (OOD) generalization has gained increasing attentions for machine learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation under distribution shifts. Existing graph OOD methods tend to follow the basic ideas of invariant risk minimization and structural causal models, interpreting the invariant knowledge across datasets under various distribution shifts as graph topology or graph spectrum. However, these interpretations may be inconsistent with real-world scenarios, as neither invariant topology nor spectrum is assured. In this paper, we advocate the learnable random walk (LRW) perspective as the instantiation of invariant knowledge, and propose LRW-OOD to realize graph OOD generalization learning. Instead of employing fixed probability transition matrix (i.e., degree-normalized adjacency matrix), we parameterize the transition matrix with an LRW-sampler and a path encoder. Furthermore, we propose the kernel density estimation (KDE)-based mutual information (MI) loss to generate random walk sequences that adhere to OOD principles. Extensive experiment demonstrates that our model can effectively enhance graph OOD generalization under various types of distribution shifts and yield a significant accuracy improvement of 3.87% over state-of-the-art graph OOD generalization baselines.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2506.17718.pdf' target='_blank'>https://arxiv.org/pdf/2506.17718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo He, Shuang Li, Wenze Song, Longhui Yuan, Jian Liang, Han Li, Kun Gai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17718">Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Endowing deep models with the ability to generalize in dynamic scenarios is of vital significance for real-world deployment, given the continuous and complex changes in data distribution. Recently, evolving domain generalization (EDG) has emerged to address distribution shifts over time, aiming to capture evolving patterns for improved model generalization. However, existing EDG methods may suffer from spurious correlations by modeling only the dependence between data and targets across domains, creating a shortcut between task-irrelevant factors and the target, which hinders generalization. To this end, we design a time-aware structural causal model (SCM) that incorporates dynamic causal factors and the causal mechanism drifts, and propose \textbf{S}tatic-D\textbf{YN}amic \textbf{C}ausal Representation Learning (\textbf{SYNC}), an approach that effectively learns time-aware causal representations. Specifically, it integrates specially designed information-theoretic objectives into a sequential VAE framework which captures evolving patterns, and produces the desired representations by preserving intra-class compactness of causal factors both across and within domains. Moreover, we theoretically show that our method can yield the optimal causal predictor for each time domain. Results on both synthetic and real-world datasets exhibit that SYNC can achieve superior temporal generalization performance.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2506.20923.pdf' target='_blank'>https://arxiv.org/pdf/2506.20923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, Qian Chen, Youcheng Pan, Yang Xiang, Meishan Zhang, Haofen Wang, Jun Yu, Baotian Hu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20923">KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose KaLM-Embedding-V2, a versatile and compact embedding model, which achieves impressive performance in general-purpose text embedding tasks by leveraging superior training techniques and data. Our key innovations include: (1) To better align the architecture with representation learning, we remove the causal attention mask and adopt a fully bidirectional transformer with simple yet effective mean-pooling to produce fixed-length embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on large-scale weakly supervised open-source corpora; (ii) fine-tuning on high-quality retrieval and non-retrieval datasets; and (iii) model-soup parameter averaging for robust generalization. Besides, we introduce a focal-style reweighting mechanism that concentrates learning on difficult samples and an online hard-negative mixing strategy to continuously enrich hard negatives without expensive offline mining; (3) We collect over 20 categories of data for pre-training and 100 categories of data for fine-tuning, to boost both the performance and generalization of the embedding model. Extensive evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English show that our model significantly outperforms others of comparable size, and competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new standard for a versatile and compact embedding model with less than 1B parameters.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2506.03167.pdf' target='_blank'>https://arxiv.org/pdf/2506.03167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Tan Le, Senura Hansaja Wanasekara, Zerun Niu, Yansong Shi, Nguyen H. Tran, Phuong Vo, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong, H. Vincent Poor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03167">Distributionally Robust Wireless Semantic Communication with Large AI Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>6G wireless systems are expected to support massive volumes of data with ultra-low latency. However, conventional bit-level transmission strategies cannot support the efficiency and adaptability required by modern, data-intensive applications. The concept of semantic communication (SemCom) addresses this limitation by focusing on transmitting task-relevant semantic information instead of raw data. While recent efforts incorporating deep learning and large-scale AI models have improved SemCom's performance, existing systems remain vulnerable to both semantic-level and transmission-level noise because they often rely on domain-specific architectures that hinder generalizability. In this paper, a novel and generalized semantic communication framework called WaSeCom is proposed to systematically address uncertainty and enhance robustness. In particular, Wasserstein distributionally robust optimization is employed to provide resilience against semantic misinterpretation and channel perturbations. A rigorous theoretical analysis is performed to establish the robust generalization guarantees of the proposed framework. Experimental results on image and text transmission demonstrate that WaSeCom achieves improved robustness under noise and adversarial perturbations. These results highlight its effectiveness in preserving semantic fidelity across varying wireless conditions.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2411.02149.pdf' target='_blank'>https://arxiv.org/pdf/2411.02149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanqi Yao, Gang Wu, Kui Jiang, Siao Liu, Jian Kuai, Xianming Liu, Junjun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02149">Improving Domain Generalization in Self-supervised Monocular Depth Estimation via Stabilized Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a self-supervised Monocular Depth Estimation (MDE) model with great generalization remains significantly challenging. Despite the success of adversarial augmentation in the supervised learning generalization, naively incorporating it into self-supervised MDE models potentially causes over-regularization, suffering from severe performance degradation. In this paper, we conduct qualitative analysis and illuminate the main causes: (i) inherent sensitivity in the UNet-alike depth network and (ii) dual optimization conflict caused by over-regularization. To tackle these issues, we propose a general adversarial training framework, named Stabilized Conflict-optimization Adversarial Training (SCAT), integrating adversarial data augmentation into self-supervised MDE methods to achieve a balance between stability and generalization. Specifically, we devise an effective scaling depth network that tunes the coefficients of long skip connection and effectively stabilizes the training process. Then, we propose a conflict gradient surgery strategy, which progressively integrates the adversarial gradient and optimizes the model toward a conflict-free direction. Extensive experiments on five benchmarks demonstrate that SCAT can achieve state-of-the-art performance and significantly improve the generalization capability of existing self-supervised MDE methods.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2601.21067.pdf' target='_blank'>https://arxiv.org/pdf/2601.21067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Li, Haibo Chen, Xin Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21067">Out-of-Distribution Generalization in Graph Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2510.12497.pdf' target='_blank'>https://arxiv.org/pdf/2510.12497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jincheng Zhong, Boyuan Jiang, Xin Tao, Pengfei Wan, Kun Gai, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12497">Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue in this family of models: a misalignment between the pre-defined noise level and the actual noise level encoded in intermediate states during sampling. We refer to this misalignment as noise shift. Through empirical analysis, we demonstrate that noise shift is widespread in modern diffusion models and exhibits a systematic bias, leading to sub-optimal generation due to both out-of-distribution generalization and inaccurate denoising updates. To address this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective correction method that explicitly steers sampling trajectories to remain consistent with the pre-defined noise schedule. We further introduce a classifier-free variant of NAG, which jointly trains a noise-conditional and a noise-unconditional model via noise-condition dropout, thereby eliminating the need for external classifiers. Extensive experiments, including ImageNet generation and various supervised fine-tuning tasks, show that NAG consistently mitigates noise shift and substantially improves the generation quality of mainstream diffusion models.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2510.09035.pdf' target='_blank'>https://arxiv.org/pdf/2510.09035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitong Kong, Zichao Zeng, Di Wen, Jiale Wei, Kunyu Peng, June Moh Goo, Jan Boehm, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09035">Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate perception is critical for vehicle safety, with LiDAR as a key enabler in autonomous driving. To ensure robust performance across environments, sensor types, and weather conditions without costly re-annotation, domain generalization in LiDAR-based 3D semantic segmentation is essential. However, LiDAR annotations are often noisy due to sensor imperfections, occlusions, and human errors. Such noise degrades segmentation accuracy and is further amplified under domain shifts, threatening system reliability. While noisy-label learning is well-studied in images, its extension to 3D LiDAR segmentation under domain generalization remains largely unexplored, as the sparse and irregular structure of point clouds limits direct use of 2D methods. To address this gap, we introduce the novel task Domain Generalization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL) and establish the first benchmark by adapting three representative noisy-label learning strategies from image classification to 3D segmentation. However, we find that existing noisy-label learning approaches adapt poorly to LiDAR data. We therefore propose DuNe, a dual-view framework with strong and weak branches that enforce feature-level consistency and apply cross-entropy loss based on confidence-aware filtering of predictions. Our approach shows state-of-the-art performance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and 52.58% on SemanticPOSS under 10% symmetric label noise, with an overall Arithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby demonstrating robust domain generalization in DGLSS-NL tasks. The code is available on our project page.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2411.15421.pdf' target='_blank'>https://arxiv.org/pdf/2411.15421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Hu, Kun Yuan, Yaling Shen, Feilong Tang, Xiaohao Xu, Lin Zhou, Wei Li, Ying Chen, Zhongxing Xu, Zelin Peng, Siyuan Yan, Vinkle Srivastav, Diping Song, Tianbin Li, Danli Shi, Jin Ye, Nicolas Padoy, Nassir Navab, Junjun He, Zongyuan Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15421">OphCLIP: Hierarchical Retrieval-Augmented Learning for Ophthalmic Surgical Video-Language Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical practice involves complex visual interpretation, procedural skills, and advanced medical knowledge, making surgical vision-language pretraining (VLP) particularly challenging due to this complexity and the limited availability of annotated data. To address the gap, we propose OphCLIP, a hierarchical retrieval-augmented vision-language pretraining framework specifically designed for ophthalmic surgical workflow understanding. OphCLIP leverages the OphVL dataset we constructed, a large-scale and comprehensive collection of over 375K hierarchically structured video-text pairs with tens of thousands of different combinations of attributes (surgeries, phases/operations/actions, instruments, medications, as well as more advanced aspects like the causes of eye diseases, surgical objectives, and postoperative recovery recommendations, etc). These hierarchical video-text correspondences enable OphCLIP to learn both fine-grained and long-term visual representations by aligning short video clips with detailed narrative descriptions and full videos with structured titles, capturing intricate surgical details and high-level procedural insights, respectively. Our OphCLIP also designs a retrieval-augmented pretraining framework to leverage the underexplored large-scale silent surgical procedure videos, automatically retrieving semantically relevant content to enhance the representation learning of narrative videos. Evaluation across 11 datasets for phase recognition and multi-instrument identification shows OphCLIP's robust generalization and superior performance.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2411.17772.pdf' target='_blank'>https://arxiv.org/pdf/2411.17772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Liu, Xiaomei Zhang, Zhiyuan Ma, Xiangyu Zhu, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17772">MVBoost: Boost 3D Reconstruction with Multi-View Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D object reconstruction have been remarkable, yet most current 3D models rely heavily on existing 3D datasets. The scarcity of diverse 3D datasets results in limited generalization capabilities of 3D reconstruction models. In this paper, we propose a novel framework for boosting 3D reconstruction with multi-view refinement (MVBoost) by generating pseudo-GT data. The key of MVBoost is combining the advantages of the high accuracy of the multi-view generation model and the consistency of the 3D reconstruction model to create a reliable data source. Specifically, given a single-view input image, we employ a multi-view diffusion model to generate multiple views, followed by a large 3D reconstruction model to produce consistent 3D data. MVBoost then adaptively refines these multi-view images, rendered from the consistent 3D data, to build a large-scale multi-view dataset for training a feed-forward 3D reconstruction model. Additionally, the input view optimization is designed to optimize the corresponding viewpoints based on the user's input image, ensuring that the most important viewpoint is accurately tailored to the user's needs. Extensive evaluations demonstrate that our method achieves superior reconstruction results and robust generalization compared to prior works.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2511.08423.pdf' target='_blank'>https://arxiv.org/pdf/2511.08423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuncheng Guo, Junyan Ye, Chenjue Zhang, Hengrui Kang, Haohuan Fu, Conghui He, Weijia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08423">OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A truly universal AI-Generated Image (AIGI) detector must simultaneously generalize across diverse generative models and varied semantic content. Current state-of-the-art methods learn a single, entangled forgery representation--conflating content-dependent flaws with content-agnostic artifacts--and are further constrained by outdated benchmarks. To overcome these limitations, we propose OmniAID, a novel framework centered on a decoupled Mixture-of-Experts (MoE) architecture. The core of our method is a hybrid expert system engineered to decouple: (1) semantic flaws across distinct content domains, and (2) these content-dependent flaws from content-agnostic universal artifacts. This system employs a set of Routable Specialized Semantic Experts, each for a distinct domain (e.g., human, animal), complemented by a Fixed Universal Artifact Expert. This architecture is trained using a bespoke two-stage strategy: we first train the experts independently with domain-specific hard-sampling to ensure specialization, and subsequently train a lightweight gating network for effective input routing. By explicitly decoupling "what is generated" (content-specific flaws) from "how it is generated" (universal artifacts), OmniAID achieves robust generalization. To address outdated benchmarks and validate real-world applicability, we introduce Mirage, a new large-scale, contemporary dataset. Extensive experiments, using both traditional benchmarks and our Mirage dataset, demonstrate our model surpasses existing monolithic detectors, establishing a new, robust standard for AIGI authentication against modern, in-the-wild threats.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2504.03450.pdf' target='_blank'>https://arxiv.org/pdf/2504.03450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Van-Anh Nguyen, Thanh-Toan Do, Mehrtash Harandi, Dinh Phung, Trung Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03450">Optimizing Specific and Shared Parameters for Efficient Parameter Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models, with a vast number of parameters and pretraining on massive datasets, achieve state-of-the-art performance across various applications. However, efficiently adapting them to downstream tasks with minimal computational overhead remains a challenge. Parameter-Efficient Transfer Learning (PETL) addresses this by fine-tuning only a small subset of parameters while preserving pre-trained knowledge. In this paper, we propose SaS, a novel PETL method that effectively mitigates distributional shifts during fine-tuning. SaS integrates (1) a shared module that captures common statistical characteristics across layers using low-rank projections and (2) a layer-specific module that employs hypernetworks to generate tailored parameters for each layer. This dual design ensures an optimal balance between performance and parameter efficiency while introducing less than 0.05% additional parameters, making it significantly more compact than existing methods. Extensive experiments on diverse downstream tasks, few-shot settings and domain generalization demonstrate that SaS significantly enhances performance while maintaining superior parameter efficiency compared to existing methods, highlighting the importance of capturing both shared and layer-specific information in transfer learning. Code and data are available at https://anonymous.4open.science/r/SaS-PETL-3565.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2502.10716.pdf' target='_blank'>https://arxiv.org/pdf/2502.10716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long-Tung Vuong, Vy Vo, Hien Dang, Van-Anh Nguyen, Thanh-Toan Do, Mehrtash Harandi, Trung Le, Dinh Phung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10716">Why Domain Generalization Fail? A View of Necessity and Sufficiency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite a strong theoretical foundation, empirical experiments reveal that existing domain generalization (DG) algorithms often fail to consistently outperform the ERM baseline. We argue that this issue arises because most DG studies focus on establishing theoretical guarantees for generalization under unrealistic assumptions, such as the availability of sufficient, diverse (or even infinite) domains or access to target domain knowledge. As a result, the extent to which domain generalization is achievable in scenarios with limited domains remains largely unexplored. This paper seeks to address this gap by examining generalization through the lens of the conditions necessary for its existence and learnability. Specifically, we systematically establish a set of necessary and sufficient conditions for generalization. Our analysis highlights that existing DG methods primarily act as regularization mechanisms focused on satisfying sufficient conditions, while often neglecting necessary ones. However, sufficient conditions cannot be verified in settings with limited training domains. In such cases, regularization targeting sufficient conditions aims to maximize the likelihood of generalization, whereas regularization targeting necessary conditions ensures its existence. Using this analysis, we reveal the shortcomings of existing DG algorithms by showing that, while they promote sufficient conditions, they inadvertently violate necessary conditions. To validate our theoretical insights, we propose a practical method that promotes the sufficient condition while maintaining the necessary conditions through a novel subspace representation alignment strategy. This approach highlights the advantages of preserving the necessary conditions on well-established DG benchmarks.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2510.15349.pdf' target='_blank'>https://arxiv.org/pdf/2510.15349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baode Wang, Biao Wu, Weizhen Li, Meng Fang, Zuming Huang, Jun Huang, Haozhe Wang, Yanjie Liang, Ling Chen, Wei Chu, Yuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15349">Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Document parsing from scanned images into structured formats remains a significant challenge due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Existing supervised fine-tuning methods often struggle to generalize across diverse document types, leading to poor performance, particularly on out-of-distribution data. This issue is further exacerbated by the limited availability of high-quality training data for layout-aware parsing tasks. To address these challenges, we introduce LayoutRL, a reinforcement learning framework that optimizes layout understanding through composite rewards integrating normalized edit distance, paragraph count accuracy, and reading order preservation. To support this training, we construct the Infinity-Doc-400K dataset, which we use to train Infinity-Parser, a vision-language model demonstrating robust generalization across various domains. Extensive evaluations on benchmarks including OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser consistently achieves state-of-the-art performance across a broad range of document types, languages, and structural complexities, substantially outperforming both specialized document parsing systems and general-purpose vision-language models. We will release our code, dataset, and model to facilitate reproducible research in document parsing.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2510.08558.pdf' target='_blank'>https://arxiv.org/pdf/2510.08558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianwei Yang, Ning Zhang, Xian Li, Ashish Shah, Dat Huynh, Hengduo Li, Zi Yang, Sara Cao, Lawrence Jang, Shuyan Zhou, Jiacheng Zhu, Huan Sun, Jason Weston, Yu Su, Yifan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08558">Agent Learning via Early Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2510.06955.pdf' target='_blank'>https://arxiv.org/pdf/2510.06955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06955">High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensembling fine-tuned models initialized from powerful pre-trained weights is a common strategy to improve robustness under distribution shifts, but it comes with substantial computational costs due to the need to train and store multiple models. Dropout offers a lightweight alternative by simulating ensembles through random neuron deactivation; however, when applied to pre-trained models, it tends to over-regularize and disrupt critical representations necessary for generalization. In this work, we investigate Mixout, a stochastic regularization technique that provides an alternative to Dropout for domain generalization. Rather than deactivating neurons, Mixout mitigates overfitting by probabilistically swapping a subset of fine-tuned weights with their pre-trained counterparts during training, thereby maintaining a balance between adaptation and retention of prior knowledge. Our study reveals that achieving strong performance with Mixout on domain generalization benchmarks requires a notably high masking probability of 0.9 for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it yields two key advantages for domain generalization: (1) higher masking rates more strongly penalize deviations from the pre-trained parameters, promoting better generalization to unseen domains; and (2) high-rate masking substantially reduces computational overhead, cutting gradient computation by up to 45% and gradient memory usage by up to 90%. Experiments across five domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet, using ResNet and ViT architectures, show that our approach, High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based methods while significantly reducing training costs.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2504.10957.pdf' target='_blank'>https://arxiv.org/pdf/2504.10957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongkang Li, Yihua Zhang, Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10957">When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2507.17512.pdf' target='_blank'>https://arxiv.org/pdf/2507.17512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17512">Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2505.12762.pdf' target='_blank'>https://arxiv.org/pdf/2505.12762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenlin Ming, Chendi Qu, Mengzhang Cai, Qizhi Pei, Zhuoshi Pan, Yu Li, Xiaoming Duan, Lijun Wu, Conghui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12762">IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have achieved impressive performance through Supervised Fine-tuning (SFT) on diverse instructional datasets. When training on multiple capabilities simultaneously, the mixture training dataset, governed by volumes of data from different domains, is a critical factor that directly impacts the final model's performance. Unlike many studies that focus on enhancing the quality of training datasets through data selection methods, few works explore the intricate relationship between the compositional quantity of mixture training datasets and the emergent capabilities of LLMs. Given the availability of a high-quality multi-domain training dataset, understanding the impact of data from each domain on the model's overall capabilities is crucial for preparing SFT data and training a well-balanced model that performs effectively across diverse domains. In this work, we introduce IDEAL, an innovative data equilibrium adaptation framework designed to effectively optimize volumes of data from different domains within mixture SFT datasets, thereby enhancing the model's alignment and performance across multiple capabilities. IDEAL employs a gradient-based approach to iteratively refine the training data distribution, dynamically adjusting the volumes of domain-specific data based on their impact on downstream task performance. By leveraging this adaptive mechanism, IDEAL ensures a balanced dataset composition, enabling the model to achieve robust generalization and consistent proficiency across diverse tasks. Experiments across different capabilities demonstrate that IDEAL outperforms conventional uniform data allocation strategies, achieving a comprehensive improvement of approximately 7% in multi-task evaluation scores.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2508.14076.pdf' target='_blank'>https://arxiv.org/pdf/2508.14076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, Di Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14076">PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward models (RMs), which are central to existing post-training methods, aim to align LLM outputs with human values by providing feedback signals during fine-tuning. However, existing RMs struggle to capture nuanced, user-specific preferences, especially under limited data and across diverse domains. Thus, we introduce PersRM-R1, the first reasoning-based reward modeling framework specifically designed to identify and represent personal factors from only one or a few personal exemplars. To address challenges including limited data availability and the requirement for robust generalization, our approach combines synthetic data generation with a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement fine-tuning. Experimental results demonstrate that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability, paving the way for more effective personalized LLMs.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2601.20209.pdf' target='_blank'>https://arxiv.org/pdf/2601.20209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyang Wu, Shuo Yang, Changpeng Yang, Yuhao Shen, Shuai Zhang, Zhengqi Wen, Jianhua Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20209">Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2601.03872.pdf' target='_blank'>https://arxiv.org/pdf/2601.03872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyang Wu, Guocheng Zhai, Ruihan Jin, Jiahao Yuan, Yuhao Shen, Shuai Zhang, Zhengqi Wen, Jianhua Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03872">Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) \textbf{training-free cluster-based routing} that exploits empirical priors for domain-specific alignment, and (2) \textbf{RL-based multi-step routing} that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2505.15692.pdf' target='_blank'>https://arxiv.org/pdf/2505.15692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Haoran Luo, Ling Yang, Huazhe Xu, Jianhua Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15692">TemplateRL: Structured Template-Guided Reinforcement Learning for LLM Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has emerged as an effective paradigm for enhancing model reasoning. However, existing RL methods like GRPO often rely on unstructured self-sampling to fit scalar rewards, often producing inefficient rollouts that fail to capture transferable problem-solving strategies. To address these limitations, we propose **TemplateRL**, a structured template-guided RL framework that augments policy optimization with explicit template guidance. Our approach first constructs a problem-solving template library via MCTS on a small seed set, then seamlessly integrates this high-level structured guidance into RL training. By guiding rollout generation to align with proven template structures, TemplateRL significantly improves high-quality trajectory hit rates while reducing ineffective exploration. This structure-guided design steers the policy toward validated strategic patterns, stabilizing training dynamics, and enhancing RL sampling efficiency. Notably, the explicit template library is interpretable, editable, and supports online updates-enabling continuous updates during both training and inference. Extensive experiments demonstrate that TemplateRL outperforms GRPO by 99% on AIME and 41% on AMC, with superior stability on weak models and remarkable cross-domain generalization, highlighting its potential for broader tasks.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2505.02152.pdf' target='_blank'>https://arxiv.org/pdf/2505.02152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02152">Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown great promise for generalist robotic manipulation in the physical world. However, existing models are restricted to robot observations and text-only instructions, lacking the flexibility of interleaved multimodal instructions enabled by recent advances in foundation models in the digital world. In this paper, we present Interleave-VLA, the first framework capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world. It offers a flexible, model-agnostic paradigm that extends state-of-the-art VLA models with minimal modifications and strong zero-shot generalization. A key challenge in realizing Interleave-VLA is the absence of large-scale interleaved embodied datasets. To bridge this gap, we develop an automatic pipeline that converts text-only instructions from real-world datasets in Open X-Embodiment into interleaved image-text instructions, resulting in the first large-scale real-world interleaved embodied dataset with 210k episodes. Through comprehensive evaluation on simulation benchmarks and real-robot experiments, we demonstrate that Interleave-VLA offers significant benefits: 1) it improves out-of-domain generalization to unseen objects by 2-3x compared to state-of-the-art baselines, 2) supports flexible task interfaces, and 3) handles diverse user-provided image instructions in a zero-shot manner, such as hand-drawn sketches. We further analyze the factors behind Interleave-VLA's strong zero-shot performance, showing that the interleaved paradigm effectively leverages heterogeneous datasets and diverse instruction images, including those from the Internet, which demonstrates strong potential for scaling up. Our model and dataset will be open-sourced.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2511.18865.pdf' target='_blank'>https://arxiv.org/pdf/2511.18865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Haoan Ping, Yuchen Li, Zhenshan Bing, Fuchun Sun, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18865">DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\% higher inference speed and 53.4\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2510.24505.pdf' target='_blank'>https://arxiv.org/pdf/2510.24505.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Zong, Jiayu Liu, Tianshi Zheng, Chunyang Li, Baixuan Xu, Haochen Shi, Weiqi Wang, Zhaowei Wang, Chunkit Chan, Yangqiu Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24505">CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate confidence calibration in Large Language Models (LLMs) is critical for safe use in high-stakes domains, where clear verbalized confidence enhances user trust. Traditional methods that mimic reference confidence expressions often fail to capture the reasoning needed for accurate confidence assessment. We propose natural language critiques as a solution, ideally suited for confidence calibration, as precise gold confidence labels are hard to obtain and often require multiple generations. This paper studies how natural language critiques can enhance verbalized confidence, addressing: (1) What to critique: uncertainty (question-focused) or confidence (answer-specific)? Analysis shows confidence suits multiple-choice tasks, while uncertainty excels in open-ended scenarios. (2) How to critique: self-critique or critique calibration training? We propose Self-Critique, enabling LLMs to critique and optimize their confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration training method that leverages natural language critiques to improve confidence calibration, moving beyond direct numerical optimization. Experiments show that CritiCal significantly outperforms Self-Critique and other competitive baselines, even surpassing its teacher model, GPT-4o, in complex reasoning tasks. CritiCal also shows robust generalization in out-of-distribution settings, advancing LLM's reliability.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2510.17845.pdf' target='_blank'>https://arxiv.org/pdf/2510.17845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jusheng Zhang, Kaitong Cai, Yijia Fan, Ningyuan Liu, Keze Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17845">MAT-Agent: Adaptive Multi-Agent Training Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-label image classification demands adaptive training strategies to navigate complex, evolving visual-semantic landscapes, yet conventional methods rely on static configurations that falter in dynamic settings. We propose MAT-Agent, a novel multi-agent framework that reimagines training as a collaborative, real-time optimization process. By deploying autonomous agents to dynamically tune data augmentation, optimizers, learning rates, and loss functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to balance exploration and exploitation, guided by a composite reward harmonizing accuracy, rare-class performance, and training stability. Enhanced with dual-rate exponential moving average smoothing and mixed-precision training, it ensures robustness and efficiency. Extensive experiments across Pascal VOC, COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of 97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated convergence and robust cross-domain generalization, MAT-Agent offers a scalable, intelligent solution for optimizing complex visual models, paving the way for adaptive deep learning advancements.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2509.22407.pdf' target='_blank'>https://arxiv.org/pdf/2509.22407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhehao Dong, Xiaofeng Wang, Zheng Zhu, Yirui Wang, Yang Wang, Yukun Zhou, Boyuan Wang, Chaojun Ni, Runqi Ouyang, Wenkang Qin, Xinze Chen, Yun Ye, Guan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22407">EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2505.18531.pdf' target='_blank'>https://arxiv.org/pdf/2505.18531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhou, Jiaming Ji, Boyuan Chen, Jiapeng Sun, Wenqi Chen, Donghai Hong, Sirui Han, Yike Guo, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18531">Generative RLHF-V: Learning Principles from Multi-modal Human Preference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training multi-modal large language models (MLLMs) that align with human intentions is a long-term challenge. Traditional score-only reward models for alignment suffer from low accuracy, weak generalization, and poor interpretability, blocking the progress of alignment methods, e.g., reinforcement learning from human feedback (RLHF). Generative reward models (GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate pair-wise responses, but their pair-wise paradigm makes it hard to generalize to learnable rewards. We introduce Generative RLHF-V, a novel alignment framework that integrates GRMs with multi-modal RLHF. We propose a two-stage pipeline: $\textbf{multi-modal generative reward modeling from RL}$, where RL guides GRMs to actively capture human intention, then predict the correct pair-wise scores; and $\textbf{RL optimization from grouped comparison}$, which enhances multi-modal RL scoring precision by grouped responses comparison. Experimental results demonstrate that, besides out-of-distribution generalization of RM discrimination, our framework improves 4 MLLMs' performance across 7 benchmarks by $18.1\%$, while the baseline RLHF is only $5.3\%$. We further validate that Generative RLHF-V achieves a near-linear improvement with an increasing number of candidate responses. Our code and models can be found at https://generative-rlhf-v.github.io.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2503.03480.pdf' target='_blank'>https://arxiv.org/pdf/2503.03480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03480">SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, this exploration yields an 83.58% safety improvement compared to the current state-of-the-art method, while also maintaining task performance (+3.85%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2501.17384.pdf' target='_blank'>https://arxiv.org/pdf/2501.17384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengpeng Xie, Jiahang Cao, Yulong Zhang, Qiang Zhang, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17384">A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent's policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2501.00759.pdf' target='_blank'>https://arxiv.org/pdf/2501.00759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianshi Zheng, Jiazheng Wang, Zihao Wang, Jiaxin Bai, Hang Yin, Zheye Deng, Yangqiu Song, Jianxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00759">Enhancing Transformers for Generalizable First-Order Logical Entailment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformers, as the fundamental deep learning architecture, have demonstrated great capability in reasoning. This paper studies the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and how to improve it. Transformers' capability of first-order reasoning is further captured by whether they can conduct first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish the connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) unseen knowledge and query settings discussed in the task of knowledge graph query answering, which makes it possible to characterize the fine-grained generalizability. Results on our comprehensive dataset showed that transformers \textit{outperform} previous methods designed particularly for this task and provided detailed empirical evidence about the impact of the input query syntax, token embedding, and transformer architectures on their reasoning capability. Interestingly, our results revealed the mismatch of positional encoding and other design choices of transformer architectures in previous practices. Motivated by this, we propose TEGA, a logic-aware architecture that significantly improves the performance in generalizable first-order logical entailment.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2508.01850.pdf' target='_blank'>https://arxiv.org/pdf/2508.01850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lala Shakti Swarup Ray, Vitor Fortes Rey, Bo Zhou, Paul Lukowicz, Sungho Suh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01850">ChairPose: Pressure-based Chair Morphology Grounded Sitting Pose Estimation through Simulation-Assisted Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prolonged seated activity is increasingly common in modern environments, raising concerns around musculoskeletal health, ergonomics, and the design of responsive interactive systems. Existing posture sensing methods such as vision-based or wearable approaches face limitations including occlusion, privacy concerns, user discomfort, and restricted deployment flexibility. We introduce ChairPose, the first full body, wearable free seated pose estimation system that relies solely on pressure sensing and operates independently of chair geometry. ChairPose employs a two stage generative model trained on pressure maps captured from a thin, chair agnostic sensing mattress. Unlike prior approaches, our method explicitly incorporates chair morphology into the inference process, enabling accurate, occlusion free, and privacy preserving pose estimation. To support generalization across diverse users and chairs, we introduce a physics driven data augmentation pipeline that simulates realistic variations in posture and seating conditions. Evaluated across eight users and four distinct chairs, ChairPose achieves a mean per joint position error of 89.4 mm when both the user and the chair are unseen, demonstrating robust generalization to novel real world generalizability. ChairPose expands the design space for posture aware interactive systems, with potential applications in ergonomics, healthcare, and adaptive user interfaces.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2501.07408.pdf' target='_blank'>https://arxiv.org/pdf/2501.07408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07408">Initial Findings on Sensor based Open Vocabulary Activity Recognition via Text Embedding Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional human activity recognition (HAR) relies on classifiers trained to predict discrete activity classes, inherently limiting recognition to activities explicitly present in the training set. Such classifiers would invariably fail, putting zero likelihood, when encountering unseen activities. We propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this limitation by first converting each activity into natural language and breaking it into a sequence of elementary motions. This descriptive text is then encoded into a fixed-size embedding. The model is trained to regress this embedding, which is subsequently decoded back into natural language using a pre-trained embedding inversion model. Unlike other works that rely on auto-regressive large language models (LLMs) at their core, OV-HAR achieves open vocabulary recognition without the computational overhead of such models. The generated text can be transformed into a single activity class using LLM prompt engineering. We have evaluated our approach on different modalities, including vision (pose), IMU, and pressure sensors, demonstrating robust generalization across unseen activities and modalities, offering a fundamentally different paradigm from contemporary classifiers.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2410.04587.pdf' target='_blank'>https://arxiv.org/pdf/2410.04587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04587">Hammer: Robust Function-Calling for On-Device Language Models via Function Masking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function calling capabilities. This paper identifies a critical gap in existing function calling models, where performance varies significantly across benchmarks, often due to being misled by specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models' sensitivity to irrelevant functions and incorporates function masking techniques to minimize misleading. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving sota results. Our open source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function calling performance.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2511.07800.pdf' target='_blank'>https://arxiv.org/pdf/2511.07800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Xia, Zekun Xu, Jiajun Chai, Wentian Fan, Yan Song, Xiaohan Wang, Guojun Yin, Wei Lin, Haifeng Zhang, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07800">From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2509.12275.pdf' target='_blank'>https://arxiv.org/pdf/2509.12275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghua Zhao, Hang Su, Lichun Fan, Zhenbo Luo, Hui Wang, Haoqin Sun, Yong Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12275">Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio question answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid progress of large audio-language models (LALMs), audio question answering (AQA) has emerged as a challenging task requiring both fine-grained audio understanding and complex reasoning. While current methods mainly rely on constructing new datasets via captioning or reasoning traces, existing high-quality AQA data remains underutilized. To address this, we propose Omni-CLST, an error-aware Curriculum Learning framework with guided Selective Chain-of-Thought. The framework efficiently leverages existing high-quality dataset through two key strategies: an error-aware curriculum that organizes samples by difficulty, and a guided thought dropout mechanism that focuses reasoning on challenging cases. Experiments show that Omni-CLST achieves 73.80% on MMAU-mini and a new state of the art of 64.30% on MMAR, demonstrating robust generalization in multimodal audio-language understanding.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2503.18987.pdf' target='_blank'>https://arxiv.org/pdf/2503.18987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiran Wang, Jian Zhang, Lei Qi, Yinghuan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18987">Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization is proposed to address distribution shift, arising from statistical disparities between training source and unseen target domains. The widely used first-order meta-learning algorithms demonstrate strong performance for domain generalization by leveraging the gradient matching theory, which aims to establish balanced parameters across source domains to reduce overfitting to any particular domain. However, our analysis reveals that there are actually numerous directions to achieve gradient matching, with current methods representing just one possible path. These methods actually overlook another critical factor that the balanced parameters should be close to the centroid of optimal parameters of each source domain. To address this, we propose a simple yet effective arithmetic meta-learning with arithmetic-weighted gradients. This approach, while adhering to the principles of gradient matching, promotes a more precise balance by estimating the centroid between domain-specific optimal parameters. Experimental results validate the effectiveness of our strategy.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2511.02354.pdf' target='_blank'>https://arxiv.org/pdf/2511.02354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyun Sun, Jiayi Luo, Haonan Yuan, Xingcheng Fu, Hao Peng, Jianxin Li, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02354">Evolving Graph Learning for Out-of-Distribution Generalization in Non-stationary Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural networks have shown remarkable success in exploiting the spatial and temporal patterns on dynamic graphs. However, existing GNNs exhibit poor generalization ability under distribution shifts, which is inevitable in dynamic scenarios. As dynamic graph generation progresses amid evolving latent non-stationary environments, it is imperative to explore their effects on out-of-distribution (OOD) generalization. This paper proposes a novel Evolving Graph Learning framework for OOD generalization (EvoOOD) by environment-aware invariant pattern recognition. Specifically, we first design an environment sequential variational auto-encoder to model environment evolution and infer the underlying environment distribution. Then, we introduce a mechanism for environment-aware invariant pattern recognition, tailored to address environmental diversification through inferred distributions. Finally, we conduct fine-grained causal interventions on individual nodes using a mixture of instantiated environment samples. This approach helps to distinguish spatio-temporal invariant patterns for OOD prediction, especially in non-stationary environments. Experimental results demonstrate the superiority of EvoGOOD on both real-world and synthetic dynamic datasets under distribution shifts. To the best of our knowledge, it is the first attempt to study the dynamic graph OOD generalization problem from the environment evolution perspective.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2506.17137.pdf' target='_blank'>https://arxiv.org/pdf/2506.17137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuonan Liang, Dongnan Liu, Jianan Fan, Yaxuan Song, Qiang Qu, Yu Yao, Peng Fu, Weidong Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17137">On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object counting models suffer when deployed across domains with differing density variety, since density shifts are inherently task-relevant and violate standard domain adaptation assumptions. To address this, we propose a theoretical framework of conditional feature alignment. We first formalize the notion of conditional divergence by partitioning each domain into subsets (e.g., object vs. background) and measuring divergences per condition. We then derive a joint error bound showing that, under discrete label spaces treated as condition sets, aligning distributions conditionally leads to tighter bounds on the combined source-target decision error than unconditional alignment. These insights motivate a general conditional adaptation principle: by preserving task-relevant variations while filtering out nuisance shifts, one can achieve superior cross-domain generalization for counting. We provide both defining conditional divergence then proving its benefit in lowering joint error and a practical adaptation strategy that preserves task-relevant information in unsupervised domain-adaptive counting. We demonstrate the effectiveness of our approach through extensive experiments on multiple counting datasets with varying density distributions. The results show that our method outperforms existing unsupervised domain adaptation methods, empirically validating the theoretical insights on conditional feature alignment.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2505.18819.pdf' target='_blank'>https://arxiv.org/pdf/2505.18819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guofeng Mei, Bin Ren, Juan Liu, Luigi Riz, Xiaoshui Huang, Xu Zheng, Yongshun Gong, Ming-Hsuan Yang, Nicu Sebe, Fabio Poiesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18819">Self-Supervised and Generalizable Tokenization for CLIP-Based 3D Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models like CLIP can offer a promising foundation for 3D scene understanding when extended with 3D tokenizers. However, standard approaches, such as k-nearest neighbor or radius-based tokenization, struggle with cross-domain generalization due to sensitivity to dataset-specific spatial scales. We present a universal 3D tokenizer designed for scale-invariant representation learning with a frozen CLIP backbone. We show that combining superpoint-based grouping with coordinate scale normalization consistently outperforms conventional methods through extensive experimental analysis. Specifically, we introduce S4Token, a tokenization pipeline that produces semantically-informed tokens regardless of scene scale. Our tokenizer is trained without annotations using masked point modeling and clustering-based objectives, along with cross-modal distillation to align 3D tokens with 2D multi-view image features. For dense prediction tasks, we propose a superpoint-level feature propagation module to recover point-level detail from sparse tokens.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2507.23110.pdf' target='_blank'>https://arxiv.org/pdf/2507.23110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheyuan Zhang, Linkai Peng, Wanying Dou, Cuiling Sun, Halil Ertugrul Aktas, Andrea M. Bejar, Elif Keles, Gorkem Durak, Ulas Bagci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23110">Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences whose appearance differs more than the acquisition sites that produce them. Existing domain-generalization benchmarks focus almost on cross-center shifts and overlook this dominant source of variability. Pancreas segmentation remains a major challenge in abdominal imaging: the gland is small, irregularly, surrounded by organs and fat, and often suffers from low T1 contrast. State-of-the-art deep networks that already achieve >90% Dice on the liver or kidneys still miss 20-30% of the pancreas. The organ is also systematically under-represented in public cross-domain benchmarks, despite its clinical importance in early cancer detection, surgery, and diabetes research. To close this gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas segmentation dataset for investigating domain generalization in medical imaging. The dataset comprises 563 MRI scans from six institutions, spanning both venous phase and out-of-phase sequences, enabling study of both cross-center and cross-sequence variations with pixel-accurate pancreas masks created by a double-blind, two-pass protocol. Through comprehensive analysis, we reveal three insights: (i) limited sampling introduces significant variance that may be mistaken for distribution shifts, (ii) cross-center performance correlates with source domain performance for identical sequences, and (iii) cross-sequence shifts require specialized solutions. We also propose a semi-supervised approach that leverages anatomical invariances, significantly outperforming state-of-the-art domain generalization techniques with 61.63% Dice score improvements and 87.00% on two test centers for cross-sequence segmentation. PancreasDG sets a new benchmark for domain generalization in medical imaging. Dataset, code, and models will be available at https://pancreasdg.netlify.app.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2501.07769.pdf' target='_blank'>https://arxiv.org/pdf/2501.07769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Ming Yang, Lan-Zhe Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07769">BMIP: Bi-directional Modality Interaction Prompt Learning for VLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have exhibited remarkable generalization capabilities, and prompt learning for VLMs has attracted great attention for the ability to adapt pre-trained VLMs to specific downstream tasks. However, existing studies mainly focus on single-modal prompts or uni-directional modality interaction, overlooking the powerful alignment effects resulting from the interaction between the vision and language modalities. To this end, we propose a novel prompt learning method called $\underline{\textbf{B}}i-directional \underline{\textbf{M}}odality \underline{\textbf{I}}nteraction \underline{\textbf{P}}rompt (BMIP)$, which dynamically weights bi-modal information through learning the information of the attention layer, enhancing trainability and inter-modal consistency compared to simple information aggregation methods. To evaluate the effectiveness of prompt learning methods, we propose a more realistic evaluation paradigm called open-world generalization complementing the widely adopted cross-dataset transfer and domain generalization tasks. Comprehensive experiments on various datasets reveal that BMIP not only outperforms current state-of-the-art methods across all three evaluation paradigms but is also flexible enough to be combined with other prompt-based methods for consistent performance enhancement.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2512.22120.pdf' target='_blank'>https://arxiv.org/pdf/2512.22120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22120">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2504.06235.pdf' target='_blank'>https://arxiv.org/pdf/2504.06235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahryar Zehtabi, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06235">Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Much of federated learning (FL) focuses on settings where local dataset statistics remain the same between training and testing. However, this assumption often does not hold in practice due to distribution shifts, motivating the development of domain generalization (DG) approaches that leverage source domain data to train models capable of generalizing to unseen target domains. In this paper, we are motivated by two major gaps in existing work on FL and DG: (1) the lack of formal mathematical analysis of DG objectives; and (2) DG research in FL being limited to the star-topology architecture. We develop Decentralized Federated Domain Generalization with Style Sharing ($\textit{StyleDDG}$), a decentralized DG algorithm which allows devices in a peer-to-peer network to achieve DG based on sharing style information inferred from their datasets. Additionally, we provide the first systematic approach to analyzing style-based DG training in decentralized networks. We cast existing centralized DG algorithms within our framework, and employ their formalisms to model $\textit{StyleDDG}$. We then obtain analytical conditions under which convergence of $\textit{StyleDDG}$ can be guaranteed. Through experiments on popular DG datasets, we demonstrate that $\textit{StyleDDG}$ can obtain significant improvements in accuracy across target domains with minimal communication overhead compared to baseline decentralized gradient methods.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2509.21261.pdf' target='_blank'>https://arxiv.org/pdf/2509.21261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng-Qi Cui, Jinyang Huang, Anyang Tong, Ziyu Jia, Jie Zhang, Zhi Liu, Dan Guo, Jianwei Lu, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21261">Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Micro-action Recognition is vital for psychological assessment and human-computer interaction. However, existing methods often fail in real-world scenarios because inter-person variability causes the same action to manifest differently, hindering robust generalization. To address this, we propose the Person Independence Universal Micro-action Recognition Framework, which integrates Distributionally Robust Optimization principles to learn person-agnostic representations. Our framework contains two plug-and-play components operating at the feature and loss levels. At the feature level, the Temporal-Frequency Alignment Module normalizes person-specific motion characteristics with a dual-branch design: the temporal branch applies Wasserstein-regularized alignment to stabilize dynamic trajectories, while the frequency branch introduces variance-guided perturbations to enhance robustness against person-specific spectral differences. A consistency-driven fusion mechanism integrates both branches. At the loss level, the Group-Invariant Regularized Loss partitions samples into pseudo-groups to simulate unseen person-specific distributions. By up-weighting boundary cases and regularizing subgroup variance, it forces the model to generalize beyond easy or frequent samples, thus enhancing robustness to difficult variations. Experiments on the large-scale MA-52 dataset demonstrate that our framework outperforms existing methods in both accuracy and robustness, achieving stable generalization under fine-grained conditions.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2508.05234.pdf' target='_blank'>https://arxiv.org/pdf/2508.05234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Shangguan, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Ge Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05234">Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The surge in rich multimodal content on social media platforms has greatly advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs) further accelerating progress in this field. Current approaches primarily leverage the knowledge and reasoning capabilities of parameter-heavy (Multimodal) LLMs for sentiment classification, overlooking autonomous multimodal sentiment reasoning generation in resource-constrained environments. Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment Reasoning and Classification task, JMSRC, which simultaneously performs multimodal sentiment reasoning chain generation and sentiment classification only with a lightweight model. We propose a Multimodal Chain-of-Thought Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a "Teacher-Assistant-Student" distillation paradigm to address deployment constraints in resource-limited environments. We first leverage a high-performance Multimodal Large Language Model (MLLM) to generate the initial reasoning dataset and train a medium-sized assistant model with a multi-task learning mechanism. A lightweight student model is jointly trained to perform efficient multimodal sentiment reasoning generation and classification. Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B parameters achieves strong performance on JMSRC, while exhibiting robust generalization and enhanced interpretability.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2506.18939.pdf' target='_blank'>https://arxiv.org/pdf/2506.18939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui An, Yifeng Zhang, Ziran Liang, Wenqi Fan, Yuxuan Liang, Xuequn Shang, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18939">Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training urban spatio-temporal foundation models that generalize well across diverse regions and cities is critical for deploying urban services in unseen or data-scarce regions. Recent studies have typically focused on fusing cross-domain spatio-temporal data to train unified Transformer-based models. However, these models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment. Inspired by the efficiency of Mamba, a state space model with linear time complexity, we explore its potential for efficient urban spatio-temporal prediction. However, directly applying Mamba as a spatio-temporal backbone leads to negative transfer and severe performance degradation. This is primarily due to spatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden state updates, which limit cross-domain generalization. To overcome these challenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for efficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear complexity advantage while significantly enhancing its adaptability to heterogeneous domains. Specifically, we introduce two core innovations: (1) a domain-adaptive state space model that partitions the latent representation space into a shared subspace for learning cross-domain commonalities and independent, domain-specific subspaces for capturing intra-domain discriminative features; (2) three distinct Domain Adapters, which serve as domain-aware proxies to bridge disparate domain distributions and facilitate the alignment of cross-domain commonalities. Extensive experiments demonstrate the generalization and efficiency of Damba-ST. It achieves state-of-the-art performance on prediction tasks and demonstrates strong zero-shot generalization, enabling seamless deployment in new urban environments without extensive retraining or fine-tuning.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2506.07309.pdf' target='_blank'>https://arxiv.org/pdf/2506.07309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Huang, Yifan Ethan Xu, Kai Sun, Vera Yan, Alicia Sun, Haidar Khan, Jimmy Nguyen, Mohammad Kachuee, Zhaojiang Lin, Yue Liu, Aaron Colak, Anuj Kumar, Wen-tau Yih, Xin Luna Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07309">ConfQA: Answer Only If You Are Confident</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit "I am unsure". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt "answer only if you are confident" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA's confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2505.16675.pdf' target='_blank'>https://arxiv.org/pdf/2505.16675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenwen Qiang, Jingyao Wang, Zeen Song, Jiangmeng Li, Changwen Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16675">On the Out-of-Distribution Generalization of Self-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we focus on the out-of-distribution (OOD) generalization of self-supervised learning (SSL). By analyzing the mini-batch construction during the SSL training phase, we first give one plausible explanation for SSL having OOD generalization. Then, from the perspective of data generation and causal inference, we analyze and conclude that SSL learns spurious correlations during the training process, which leads to a reduction in OOD generalization. To address this issue, we propose a post-intervention distribution (PID) grounded in the Structural Causal Model. PID offers a scenario where the spurious variable and label variable is mutually independent. Besides, we demonstrate that if each mini-batch during SSL training satisfies PID, the resulting SSL model can achieve optimal worst-case OOD performance. This motivates us to develop a batch sampling strategy that enforces PID constraints through the learning of a latent variable model. Through theoretical analysis, we demonstrate the identifiability of the latent variable model and validate the effectiveness of the proposed sampling strategy. Experiments conducted on various downstream OOD tasks demonstrate the effectiveness of the proposed sampling strategy.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2510.17790.pdf' target='_blank'>https://arxiv.org/pdf/2510.17790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17790">UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer-use agents face a fundamental limitation. They rely exclusively on primitive GUI actions (click, type, scroll), creating brittle execution chains prone to cascading failures. While API-driven agents harness rich capabilities through structured interfaces and tools, computer-use agents remain constrained to low-level visual interactions. We present UltraCUA, a foundation model that transcends this limitation through hybrid action-seamlessly unifying primitive GUI operations with high-level tool execution. Our innovation rests on four critical advances. First, an automated pipeline extracts and scales tool capabilities from software documentation and code repositories. Second, a synthetic data engine produces 17,000+ verifiable tasks capturing real-world computer-use complexity. Third, comprehensive hybrid action trajectory collection incorporates both GUI primitives and strategic tool calls. Fourth, a two-stage training methodology combines supervised fine-tuning with online reinforcement learning, enabling intelligent action selection between GUI and API. Evaluation with our 7B and 32B UltraCUA models reveals transformative performance gains. On OSWorld, UltraCUA achieves 22% relative improvement while executing 11% faster than existing approaches, averagely. Cross-domain validation on WindowsAgentArena demonstrates robust generalization with 21.7% success rate, surpassing Windows-trained baselines. The hybrid action paradigm proves essential, reducing error propagation while improving execution efficiency. This work establishes a scalable paradigm bridging primitive GUI interactions and high-level tool intelligence, enabling more resilient and adaptable computer use agents for diverse environments and complex real-world tasks.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2508.05008.pdf' target='_blank'>https://arxiv.org/pdf/2508.05008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xusheng Liang, Lihua Zhou, Nianxin Li, Miao Xu, Ziyang Song, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05008">Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable zero-shot capabilities in various computer vision tasks. However, their application to medical imaging remains challenging due to the high variability and complexity of medical data. Specifically, medical images often exhibit significant domain shifts caused by various confounders, including equipment differences, procedure artifacts, and imaging modes, which can lead to poor generalization when models are applied to unseen domains. To address this limitation, we propose Multimodal Causal-Driven Representation Learning (MCDRL), a novel framework that integrates causal inference with the VLM to tackle domain generalization in medical image segmentation. MCDRL is implemented in two steps: first, it leverages CLIP's cross-modal capabilities to identify candidate lesion regions and construct a confounder dictionary through text prompts, specifically designed to represent domain-specific variations; second, it trains a causal intervention network that utilizes this dictionary to identify and eliminate the influence of these domain-specific variations while preserving the anatomical structural information critical for segmentation tasks. Extensive experiments demonstrate that MCDRL consistently outperforms competing methods, yielding superior segmentation accuracy and exhibiting robust generalizability.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2507.09961.pdf' target='_blank'>https://arxiv.org/pdf/2507.09961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lihua Zhou, Mao Ye, Nianxin Li, Shuaifeng Li, Jinlin Wu, Xiatian Zhu, Lei Deng, Hongbin Liu, Jiebo Luo, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09961">Text-Driven Causal Representation Learning for Source-Free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning often struggles when training and test data distributions differ. Traditional domain generalization (DG) tackles this by including data from multiple source domains, which is impractical due to expensive data collection and annotation. Recent vision-language models like CLIP enable source-free domain generalization (SFDG) by using text prompts to simulate visual representations, reducing data demands. However, existing SFDG methods struggle with domain-specific confounders, limiting their generalization capabilities. To address this issue, we propose TDCRL (\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation \textbf{L}earning), the first method to integrate causal inference into the SFDG setting. TDCRL operates in two steps: first, it employs data augmentation to generate style word vectors, combining them with class information to generate text embeddings to simulate visual representations; second, it trains a causal intervention network with a confounder dictionary to extract domain-invariant features. Grounded in causal learning, our approach offers a clear and effective mechanism to achieve robust, domain-invariant features, ensuring robust generalization. Extensive experiments on PACS, VLCS, OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL effectiveness in SFDG.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2411.00553.pdf' target='_blank'>https://arxiv.org/pdf/2411.00553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Mancusi, Mattia Bernardi, Aniello Panariello, Angelo Porrello, Rita Cucchiara, Simone Calderara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00553">Is Multiple Object Tracking a Matter of Specialization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end transformer-based trackers have achieved remarkable performance on most human-related datasets. However, training these trackers in heterogeneous scenarios poses significant challenges, including negative interference - where the model learns conflicting scene-specific parameters - and limited domain generalization, which often necessitates expensive fine-tuning to adapt the models to new domains. In response to these challenges, we introduce Parameter-efficient Scenario-specific Tracking Architecture (PASTA), a novel framework that combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL). Specifically, we define key scenario attributes (e.g, camera-viewpoint, lighting condition) and train specialized PEFT modules for each attribute. These expert modules are combined in parameter space, enabling systematic generalization to new domains without increasing inference time. Extensive experiments on MOTSynth, along with zero-shot evaluations on MOT17 and PersonPath22 demonstrate that a neural tracker built from carefully selected modules surpasses its monolithic counterpart. We release models and code.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2510.10464.pdf' target='_blank'>https://arxiv.org/pdf/2510.10464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Dong, Dejia Liu, Ruiqi Ding, Zongxing Chen, Yingjie Huang, Zhu Meng, Jianbo Zhao, Zhicheng Zhao, Fei Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10464">Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transjugular intrahepatic portosystemic shunt (TIPS) is an established procedure for portal hypertension, but provides variable survival outcomes and frequent overt hepatic encephalopathy (OHE), indicating the necessity of accurate preoperative prognostic modeling. Current studies typically build machine learning models from preoperative CT images or clinical characteristics, but face three key challenges: (1) labor-intensive region-of-interest (ROI) annotation, (2) poor reliability and generalizability of unimodal methods, and (3) incomplete assessment from single-endpoint prediction. Moreover, the lack of publicly accessible datasets constrains research in this field. Therefore, we present MultiTIPS, the first public multi-center dataset for TIPS prognosis, and propose a novel multimodal prognostic framework based on it. The framework comprises three core modules: (1) dual-option segmentation, which integrates semi-supervised and foundation model-based pipelines to achieve robust ROI segmentation with limited annotations and facilitate subsequent feature extraction; (2) multimodal interaction, where three techniques, multi-grained radiomics attention (MGRA), progressive orthogonal disentanglement (POD), and clinically guided prognostic enhancement (CGPE), are introduced to enable cross-modal feature interaction and complementary representation integration, thus improving model accuracy and robustness; and (3) multi-task prediction, where a staged training strategy is used to perform stable optimization of survival, portal pressure gradient (PPG), and OHE prediction for comprehensive prognostic assessment. Extensive experiments on MultiTIPS demonstrate the superiority of the proposed method over state-of-the-art approaches, along with strong cross-domain generalization and interpretability, indicating its promise for clinical application. The dataset and code are available.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2508.17784.pdf' target='_blank'>https://arxiv.org/pdf/2508.17784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17784">Proximal Supervised Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised fine-tuning (SFT) of foundation models often leads to poor generalization, where prior capabilities deteriorate after tuning on new tasks or domains. Inspired by trust-region policy optimization (TRPO) and proximal policy optimization (PPO) in reinforcement learning (RL), we propose Proximal SFT (PSFT). This fine-tuning objective incorporates the benefits of trust-region, effectively constraining policy drift during SFT while maintaining competitive tuning. By viewing SFT as a special case of policy gradient methods with constant positive advantages, we derive PSFT that stabilizes optimization and leads to generalization, while leaving room for further optimization in subsequent post-training stages. Experiments across mathematical and human-value domains show that PSFT matches SFT in-domain, outperforms it in out-of-domain generalization, remains stable under prolonged training without causing entropy collapse, and provides a stronger foundation for the subsequent optimization.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2506.08772.pdf' target='_blank'>https://arxiv.org/pdf/2506.08772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Song, Kaiyu Li, Xiangyong Cao, Deyu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08772">RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic segmentation in remote sensing images is crucial for various applications, yet its performance is heavily reliant on large-scale, high-quality pixel-wise annotations, which are notoriously expensive and time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a promising alternative to mitigate this data dependency. However, existing SSS methods often struggle with the inherent distribution mismatch between limited labeled data and abundant unlabeled data, leading to suboptimal generalization. To alleviate this issue, we attempt to introduce the Vision Foundation Models (VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs possess robust generalization capabilities that can effectively bridge this distribution gap and provide strong semantic priors for SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework that leverages the powerful semantic knowledge embedded in VFMs to guide semi-supervised learning in remote sensing. Specifically, RS-MTDF employs multiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing feature-level distillation to align student features with their robust representations. To further enhance discriminative power, the distilled knowledge is seamlessly fused into the student decoder. Extensive experiments on three challenging remote sensing datasets demonstrate that RS-MTDF consistently achieves state-of-the-art performance. Notably, our method outperforms existing approaches across various label ratios on LoveDA and secures the highest IoU in the majority of semantic categories. These results underscore the efficacy of multi-teacher VFM guidance in significantly enhancing both generalization and semantic understanding for remote sensing segmentation. Ablation studies further validate the contribution of each proposed module.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2505.06575.pdf' target='_blank'>https://arxiv.org/pdf/2505.06575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengfeng Wang, Wei Zhai, Yuhang Yang, Yang Cao, Zhengjun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06575">GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the geometry level of human-scene contact aims to ground specific contact surface points at 3D human geometries, which provides a spatial prior and bridges the interaction between human and scene, supporting applications such as human behavior analysis, embodied AI, and AR/VR. To complete the task, existing approaches predominantly rely on parametric human models (e.g., SMPL), which establish correspondences between images and contact regions through fixed SMPL vertex sequences. This actually completes the mapping from image features to an ordered sequence. However, this approach lacks consideration of geometry, limiting its generalizability in distinct human geometries. In this paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates a point cloud encoder-decoder architecture along with a hierarchical feature extraction and fusion module, enabling the effective integration of 3D human geometric structures with 2D interaction semantics derived from images. Guided by visual cues, GRACE establishes an implicit mapping from geometric features to the vertex space of the 3D human mesh, thereby achieving accurate modeling of contact regions. This design ensures high prediction accuracy and endows the framework with strong generalization capability across diverse human geometries. Extensive experiments on multiple benchmark datasets demonstrate that GRACE achieves state-of-the-art performance in contact estimation, with additional results further validating its robust generalization to unstructured human point clouds.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2509.24803.pdf' target='_blank'>https://arxiv.org/pdf/2509.24803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Guan, Zijie Meng, Dianqi Li, Shiyu Wang, Chao-Han Huck Yang, Qingsong Wen, Zuozhu Liu, Sabato Marco Siniscalchi, Ming Jin, Shirui Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24803">TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2506.08989.pdf' target='_blank'>https://arxiv.org/pdf/2506.08989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, Weizhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08989">SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2504.21414.pdf' target='_blank'>https://arxiv.org/pdf/2504.21414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Fan, Kaiqi Liu, Nian Liu, Hisham Cholakkal, Rao Muhammad Anwer, Wenbin Li, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21414">Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2511.07051.pdf' target='_blank'>https://arxiv.org/pdf/2511.07051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zhou, Tao Yu, Wen Huang, Yuheng Zhang, Tao Dai, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07051">Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization capability of deepfake detectors is critical for real-world use. Data augmentation via synthetic fake face generation effectively enhances generalization, yet current SoTA methods rely on fixed strategies-raising a key question: Is a single static augmentation sufficient, or does the diversity of forgery features demand dynamic approaches? We argue existing methods overlook the evolving complexity of real-world forgeries (e.g., facial warping, expression manipulation), which fixed policies cannot fully simulate. To address this, we propose CRDA (Curriculum Reinforcement-Learning Data Augmentation), a novel framework guiding detectors to progressively master multi-domain forgery features from simple to complex. CRDA synthesizes augmented samples via a configurable pool of forgery operations and dynamically generates adversarial samples tailored to the detector's current learning state. Central to our approach is integrating reinforcement learning (RL) and causal inference. An RL agent dynamically selects augmentation actions based on detector performance to efficiently explore the vast augmentation space, adapting to increasingly challenging forgeries. Simultaneously, the agent introduces action space variations to generate heterogeneous forgery patterns, guided by causal inference to mitigate spurious correlations-suppressing task-irrelevant biases and focusing on causally invariant features. This integration ensures robust generalization by decoupling synthetic augmentation patterns from the model's learned representations. Extensive experiments show our method significantly improves detector generalizability, outperforming SOTA methods across multiple cross-domain datasets.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2503.01422.pdf' target='_blank'>https://arxiv.org/pdf/2503.01422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, Rui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01422">Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time scaling improves large language model performance by adding extra compute during decoding. Best-of-N (BoN) sampling serves as a common scaling technique, broadening the search space for finding better solutions from the model distribution. However, traditional BoN requires N full generations, leading to high GPU memory overhead and time latency. Moreover, some methods depend on reward models, adding computational cost and limiting domain generalization.
  In this paper, we propose Self-Truncation Best-of-N (ST-BoN), a novel decoding method that avoids fully generating all samplings and eliminates the need for reward models. ST-BoN introduces early sampling consistency to estimate the most promising sample, truncating suboptimal ones to free memory and accelerate inference. This pushes the sampling-efficient test-time scaling. Compared to traditional BoN, ST-BoN can reduce dynamic GPU memory overhead by over 90% and time latency by 50%, while achieving comparable or even better performance across reasoning and open-ended domains.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2502.19655.pdf' target='_blank'>https://arxiv.org/pdf/2502.19655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, Hoifung Poon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19655">Med-RLVR: Emerging Medical Reasoning from a 3B base model via reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning from verifiable rewards (RLVR) has recently gained attention for its ability to elicit self-evolved reasoning capabilitie from base language models without explicit reasoning supervisions, as demonstrated by DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical and coding domains, its applicability to other tasks and domains remains unexplored. In this work, we investigate whether medical reasoning can emerge from RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical domain leveraging medical multiple-choice question answering (MCQA) data as verifiable labels. Our results demonstrate that RLVR is not only effective for math and coding but also extends successfully to medical question answering. Notably, Med-RLVR achieves performance comparable to traditional supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization, with an 8-point accuracy gain. Further analysis of training dynamics reveals that, with no explicit reasoning supervision, reasoning emerges from the 3B-parameter base model. These findings underscore the potential of RLVR in domains beyond math and coding, opening new avenues for its application in knowledge-intensive fields such as medicine.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2501.09783.pdf' target='_blank'>https://arxiv.org/pdf/2501.09783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiliang Tang, Jia-Hui Pan, Yun-Hui Liu, Masayoshi Tomizuka, Li Erran Li, Chi-Wing Fu, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09783">GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GeoManip, a framework to enable generalist robots to leverage essential conditions derived from object and part relationships, as geometric constraints, for robot manipulation. For example, cutting the carrot requires adhering to a geometric constraint: the blade of the knife should be perpendicular to the carrot's direction. By interpreting these constraints through symbolic language representations and translating them into low-level actions, GeoManip bridges the gap between natural language and robotic execution, enabling greater generalizability across diverse even unseen tasks, objects, and scenarios. Unlike vision-language-action models that require extensive training, operates training-free by utilizing large foundational models: a constraint generation module that predicts stage-specific geometric constraints and a geometry parser that identifies object parts involved in these constraints. A solver then optimizes trajectories to satisfy inferred constraints from task descriptions and the scene. Furthermore, GeoManip learns in-context and provides five appealing human-robot interaction features: on-the-fly policy adaptation, learning from human demonstrations, learning from failure cases, long-horizon action planning, and efficient data collection for imitation learning. Extensive evaluations on both simulations and real-world scenarios demonstrate GeoManip's state-of-the-art performance, with superior out-of-distribution generalization while avoiding costly model training.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2601.00352.pdf' target='_blank'>https://arxiv.org/pdf/2601.00352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuxiang Qiu, Hui Da, Yuzhen Niu, Tiesong Zhao, Yang Cao, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.00352">OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2509.20807.pdf' target='_blank'>https://arxiv.org/pdf/2509.20807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhan Wu, Xiaoyang Qu, Zhangcheng Huang, Jianzong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20807">Federated Domain Generalization with Domain-specific Soft Prompts Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has become an efficient paradigm for adapting CLIP to downstream tasks. Compared with traditional fine-tuning, prompt learning optimizes a few parameters yet yields highly competitive results, especially appealing in federated learning for computational efficiency. engendering domain shift among clients and posing a formidable challenge for downstream-task adaptation. Existing federated domain generalization (FDG) methods based on prompt learning typically learn soft prompts from training samples, replacing manually designed prompts to enhance the generalization ability of federated models. However, these learned prompts exhibit limited diversity and tend to ignore information from unknown domains. We propose a novel and effective method from a generative perspective for handling FDG tasks, namely federated domain generalization with domain-specific soft prompts generation (FedDSPG). Specifically, during training, we introduce domain-specific soft prompts (DSPs) for each domain and integrate content and domain knowledge into the generative model among clients. In the inference phase, the generator is utilized to obtain DSPs for unseen target domains, thus guiding downstream tasks in unknown domains. Comprehensive evaluations across several public datasets confirm that our method outperforms existing strong baselines in FDG, achieving state-of-the-art results.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2508.07917.pdf' target='_blank'>https://arxiv.org/pdf/2508.07917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07917">MolmoAct: Action Reasoning Models that can Reason in Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of robotic foundation models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1.5; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2504.11218.pdf' target='_blank'>https://arxiv.org/pdf/2504.11218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11218">3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2503.19469.pdf' target='_blank'>https://arxiv.org/pdf/2503.19469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fred Philippy, Siwen Guo, Cedric Lothritz, Jacques Klein, TegawendÃ© F. BissyandÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19469">Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. While pretrained language models (PLMs) have shown promise in ZSC, they often rely on large training datasets or external knowledge, limiting their applicability in multilingual and low-resource scenarios. Recent approaches leveraging natural language prompts reduce the dependence on large training datasets but struggle to effectively incorporate available labeled data from related classification tasks, especially when these datasets originate from different languages or distributions. Moreover, existing prompt-based methods typically rely on manually crafted prompts in a specific language, limiting their adaptability and effectiveness in cross-lingual settings. To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts. RoSPrompt is designed for small multilingual PLMs, enabling them to leverage high-resource languages to improve performance in low-resource settings without requiring extensive fine-tuning or high computational costs. We evaluate our approach on multiple multilingual PLMs across datasets covering 106 languages, demonstrating strong cross-lingual transfer performance and robust generalization capabilities over unseen classes.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2502.11617.pdf' target='_blank'>https://arxiv.org/pdf/2502.11617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarthak Mittal, Yoshua Bengio, Nikolay Malkin, Guillaume Lajoie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11617">In-Context Parametric Inference: Point or Distribution Estimators?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian and frequentist inference are two fundamental paradigms in statistical estimation. Bayesian methods treat hypotheses as random variables, incorporating priors and updating beliefs via Bayes' theorem, whereas frequentist methods assume fixed but unknown hypotheses, relying on estimators like maximum likelihood. While extensive research has compared these approaches, the frequentist paradigm of obtaining point estimates has become predominant in deep learning, as Bayesian inference is challenging due to the computational complexity and the approximation gap of posterior estimation methods. However, a good understanding of trade-offs between the two approaches is lacking in the regime of amortized estimators, where in-context learners are trained to estimate either point values via maximum likelihood or maximum a posteriori estimation, or full posteriors using normalizing flows, score-based diffusion samplers, or diagonal Gaussian approximations, conditioned on observations. To help resolve this, we conduct a rigorous comparative analysis spanning diverse problem settings, from linear models to shallow neural networks, with a robust evaluation framework assessing both in-distribution and out-of-distribution generalization on tractable tasks. Our experiments indicate that amortized point estimators generally outperform posterior inference, though the latter remain competitive in some low-dimensional problems, and we further discuss why this might be the case.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2412.20895.pdf' target='_blank'>https://arxiv.org/pdf/2412.20895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, Tieniu Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20895">Towards Compatible Fine-tuning for Vision-Language Model Updates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>So far, efficient fine-tuning has become a popular strategy for enhancing the capabilities of foundation models on downstream tasks by learning plug-and-play modules. However, existing methods overlook a crucial issue: if the underlying foundation model is updated, are these plug-and-play modules still effective? In this paper, we first conduct a detailed analysis of various fine-tuning methods on the CLIP in terms of their compatibility with model updates. The study reveals that many high-performing fine-tuning methods fail to be compatible with the upgraded models. To address this, we propose a novel approach, Class-conditioned Context Optimization (ContCoOp), which integrates learnable prompts with class embeddings using an attention layer before inputting them into the text encoder. Consequently, the prompts can dynamically adapt to the changes in embedding space (due to model updates), ensuring continued effectiveness. Extensive experiments over 15 datasets show that our ContCoOp achieves the highest compatibility over the baseline methods, and exhibits robust out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2510.10197.pdf' target='_blank'>https://arxiv.org/pdf/2510.10197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Lu, Zechuan Wang, Hongxuan Zhang, Qintong Wu, Leilei Gan, Chenyi Zhuang, Jinjie Gu, Tao Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10197">Don't Just Fine-tune the Agent, Tune the Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (RL) struggles with a critical cold-start problem and training instability. To address these challenges, we introduce $\textbf{Environment Tuning}$, a novel training paradigm that enables agents to learn complex behaviors directly from problem instances without relying on pre-collected expert trajectories. $\textbf{Environment Tuning}$ orchestrates this learning process through a structured curriculum, actionable environment augmentation that provides corrective feedback, and fine-grained progress rewards to ensure stable and efficient exploration. Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Our work presents a paradigm shift from supervised fine-tuning on static trajectories to dynamic, environment-based exploration, paving the way for training more robust and data-efficient agents.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2504.09439.pdf' target='_blank'>https://arxiv.org/pdf/2504.09439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Xu, Jingjing Chen, Yang Jiao, Jiacheng Zhang, Zhiyu Tan, Hao Li, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09439">Identity-Aware Vision-Language Model for Explainable Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative artificial intelligence have enabled the creation of highly realistic image forgeries, raising significant concerns about digital media authenticity. While existing detection methods demonstrate promising results on benchmark datasets, they face critical limitations in real-world applications. First, existing detectors typically fail to detect semantic inconsistencies with the person's identity, such as implausible behaviors or incompatible environmental contexts in given images. Second, these methods rely heavily on low-level visual cues, making them effective for known forgeries but less reliable against new or unseen manipulation techniques. To address these challenges, we present a novel personalized vision-language model (VLM) that integrates low-level visual artifact analysis and high-level semantic inconsistency detection. Unlike previous VLM-based methods, our approach avoids resource-intensive supervised fine-tuning that often struggles to preserve distinct identity characteristics. Instead, we employ a lightweight method that dynamically encodes identity-specific information into specialized identifier tokens. This design enables the model to learn distinct identity characteristics while maintaining robust generalization capabilities. We further enhance detection capabilities through a lightweight detection adapter that extracts fine-grained information from shallow features of the vision encoder, preserving critical low-level evidence. Comprehensive experiments demonstrate that our approach achieves 94.25% accuracy and 94.08% F1 score, outperforming both traditional forgery detectors and general VLMs while requiring only 10 extra tokens.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2411.16898.pdf' target='_blank'>https://arxiv.org/pdf/2411.16898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyi Li, Michael Niemeyer, Zeyu Chen, Nassir Navab, Federico Tombari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16898">MonoGSDF: Exploring Monocular Geometric Cues for Gaussian Splatting-Guided Implicit Surface Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate meshing from monocular images remains a key challenge in 3D vision. While state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at synthesizing photorealistic novel views through rasterization-based rendering, their reliance on sparse, explicit primitives severely limits their ability to recover watertight and topologically consistent 3D surfaces.We introduce MonoGSDF, a novel method that couples Gaussian-based primitives with a neural Signed Distance Field (SDF) for high-quality reconstruction. During training, the SDF guides Gaussians' spatial distribution, while at inference, Gaussians serve as priors to reconstruct surfaces, eliminating the need for memory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a scaling strategy for robust generalization. A multi-resolution training scheme further refines details and monocular geometric cues from off-the-shelf estimators enhance reconstruction quality. Experiments on real-world datasets show MonoGSDF outperforms prior methods while maintaining efficiency.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2411.02920.pdf' target='_blank'>https://arxiv.org/pdf/2411.02920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengkun Jiao, Na Zhao, Jingjing Chen, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02920">Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts. The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition. In this paper, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes. Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples. Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information. Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers. In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization. Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2510.00413.pdf' target='_blank'>https://arxiv.org/pdf/2510.00413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Liu, Junyi Li, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00413">PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graphical User Interface (GUI) agents powered by Multimodal Large Language Models (MLLMs) promise human-like interaction with software applications, yet long-horizon tasks remain challenging due to memory limitations. Existing approaches either truncate history or rely on simple textual summaries, which risk losing critical information when past visual details become necessary for future decisions. In this paper, we propose \textbf{PAL-UI} (\textbf{P}lanning with \textbf{A}ctive \textbf{L}ook-back), a novel framework that enables GUI agents to adaptively retrieve past observations when required. PAL-UI combines a dual-level summarization agent, capturing both observation-level cues and action-level outcomes, with a dedicated retrieval tool that allows the agent to recall specific historical screenshots during planning. We curate a step-level instruction dataset of 8.6K samples from mobile GUI navigation trajectories and train \textbf{PAL-UI-3B} and \textbf{PAL-UI-7B} models based on Qwen2.5-VL. Extensive experiments demonstrate that PAL-UI significantly outperforms baseline models and prior methods in mobile GUI navigation tasks, even under data-efficient settings. Moreover, PAL-UI exhibits strong cross-domain generalization, achieving notable improvements in web navigation without additional training. Our work highlights the potential of active memory retrieval for long-horizon planning capabilities of vision-based GUI agents.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2509.03131.pdf' target='_blank'>https://arxiv.org/pdf/2509.03131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sashuai Zhou, Weinan Gan, Qijiong Liu, Ke Lei, Jieming Zhu, Hai Huang, Yan Xia, Ruiming Tang, Zhenhua Dong, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03131">RecBase: Generative Foundation Model Pretraining for Zero-Shot Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in LLM-based recommendation have shown promise, yet their cross-domain generalization is hindered by a fundamental mismatch between language-centric pretraining and the recommendation task. Existing methods, relying on language-level knowledge, fail to capture dynamic, item-level user interests across domains. To bridge this gap, we propose RecBase, a domain-agnostic foundational model pretrained with a recommendation-oriented objective. RecBase leverages a large-scale, heterogeneous, cross-domain corpus with unified textual representations and feature mappings to enhance cross-domain generalization. To further align item semantics across domains, we introduce a unified item tokenizer that encodes items into hierarchical concept identifiers, enabling structured representation and efficient vocabulary sharing. The model is trained using an autoregressive objective to capture complex item-level sequential patterns. On eight real-world datasets, our 1.5B-parameter model matches or surpasses the performance of LLM baselines up to 7B parameters in zero-shot and cross-domain recommendation tasks.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2507.12821.pdf' target='_blank'>https://arxiv.org/pdf/2507.12821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lance Ying, Katherine M. Collins, Prafull Sharma, Cedric Colas, Kaiya Ivy Zhao, Adrian Weller, Zenna Tavares, Phillip Isola, Samuel J. Gershman, Jacob D. Andreas, Thomas L. Griffiths, Francois Chollet, Kelsey R. Allen, Joshua B. Tenenbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12821">Assessing Adaptive World Models in Machines with Novel Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on massive corpora of data, instead of the efficiency and efficacy in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this class of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2506.12307.pdf' target='_blank'>https://arxiv.org/pdf/2506.12307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotian Zhang, Yuan Wang, Zhaopeng Feng, Ruizhe Chen, Zhijie Zhou, Yan Zhang, Hongxia Xu, Jian Wu, Zuozhu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12307">Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve comprehensive medical understanding is still largely unexplored. In this paper, we present Med-U1, a unified framework for robust reasoning across medical QA tasks with diverse output formats, ranging from MCQs to complex generation and computation tasks. Med-U1 employs pure large-scale reinforcement learning with mixed rule-based binary reward functions, incorporating a length penalty to manage output verbosity. With multi-objective reward optimization, Med-U1 directs LLMs to produce concise and verifiable reasoning chains. Empirical results reveal that Med-U1 significantly improves performance across multiple challenging Med-QA benchmarks, surpassing even larger specialized and proprietary models. Furthermore, Med-U1 demonstrates robust generalization to out-of-distribution (OOD) tasks. Extensive analysis presents insights into training strategies, reasoning chain length control, and reward design for medical LLMs. Our code is available here.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2412.10345.pdf' target='_blank'>https://arxiv.org/pdf/2412.10345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal DaumÃ©, Andrey Kolobov, Furong Huang, Jianwei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10345">TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation. In this work, we introduce visual trace prompting, a simple yet effective approach to facilitate VLA models' spatial-temporal awareness for action prediction by encoding state-action trajectories visually. We develop a new TraceVLA model by finetuning OpenVLA on our own collected dataset of 150K robot manipulation trajectories using visual trace prompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and 3.5x on real-robot tasks and exhibiting robust generalization across diverse embodiments and scenarios. To further validate the effectiveness and generality of our method, we present a compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while significantly improving inference efficiency.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2411.05824.pdf' target='_blank'>https://arxiv.org/pdf/2411.05824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixian Su, Jingwei Guo, Xi Yang, Qiufeng Wang, Frans Coenen, Kaizhu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05824">Navigating Distribution Shifts in Medical Image Analysis: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical Image Analysis (MedIA) has become indispensable in modern healthcare, enhancing clinical diagnostics and personalized treatment. Despite the remarkable advancements supported by deep learning (DL) technologies, their practical deployment faces challenges due to distribution shifts, where models trained on specific datasets underperform across others from varying hospitals, regions, or patient populations. To navigate this issue, researchers have been actively developing strategies to increase the adaptability and robustness of DL models, enabling their effective use in unfamiliar and diverse environments. This paper systematically reviews approaches that apply DL techniques to MedIA systems affected by distribution shifts. Unlike traditional categorizations based on technical specifications, our approach is grounded in the real-world operational constraints faced by healthcare institutions. Specifically, we categorize the existing body of work into Joint Training, Federated Learning, Fine-tuning, and Domain Generalization, with each method tailored to distinct scenarios caused by Data Accessibility, Privacy Concerns, and Collaborative Protocols. This perspective equips researchers with a nuanced understanding of how DL can be strategically deployed to address distribution shifts in MedIA, ensuring diverse and robust medical applications. By delving deeper into these topics, we highlight potential pathways for future research that not only address existing limitations but also push the boundaries of deployable MedIA technologies.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2410.05345.pdf' target='_blank'>https://arxiv.org/pdf/2410.05345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Ghaznavi, Hesam Asadollahzadeh, Fahimeh Hosseini Noohdani, Soroush Vafaie Tabar, Hosein Hasani, Taha Akbari Alvanagh, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05345">Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or 'minority') groups that lack these attributes, posing significant challenges for both out-of-distribution generalization and fairness objectives. Many studies aim to enhance robustness to spurious correlation, but they sometimes depend on group annotations for training. Additionally, a common limitation in previous research is the reliance on group-annotated validation datasets for model selection. This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are not available. To enhance model robustness with minimal group annotation assumptions, we propose Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses from an ERM-trained model to construct a balanced dataset of high-loss and low-loss samples, mitigating group imbalance in data. This significantly enhances robustness to group shifts when equipped with a simple post-training last layer retraining. By using environment inference methods to create diverse environments with correlation shifts, EVaLS can potentially eliminate the need for group annotation in validation data. In this context, the worst environment accuracy acts as a reliable surrogate throughout the retraining process for tuning hyperparameters and finding a model that performs well across diverse group shifts. EVaLS effectively achieves group robustness, showing that group annotation is not necessary even for validation. It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2410.04492.pdf' target='_blank'>https://arxiv.org/pdf/2410.04492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04492">Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2509.14921.pdf' target='_blank'>https://arxiv.org/pdf/2509.14921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tahar Chettaoui, Naser Damer, Fadi Boutros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14921">Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models such as CLIP have demonstrated exceptional zero- and few-shot transfer capabilities across diverse vision tasks. However, when fine-tuned for highly specialized biometric tasks, face recognition (FR), morphing attack detection (MAD), and presentation attack detection (PAD), these models may suffer from over-specialization. Thus, they may lose one of their foundational strengths, cross-domain generalization. In this work, we systematically quantify these trade-offs by evaluating three instances of CLIP fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the original CLIP baseline on 14 general vision datasets under zero-shot and linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our results indicate that fine-tuned models suffer from over-specialization, especially when fine-tuned for complex tasks of FR. Also, our results pointed out that task complexity and classification head design, multi-class (FR) vs. binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The FRoundation model with the ViT-L backbone outperforms other approaches on the large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%. However, it experiences a substantial performance drop on ImageNetV2, reaching only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover, the larger CLIP architecture consistently preserves more of the model's original generalization ability than the smaller variant, indicating that increased model capacity may help mitigate over-specialization.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2502.07968.pdf' target='_blank'>https://arxiv.org/pdf/2502.07968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Wang, Zhen Tan, Yaochen Zhu, Chuxu Zhang, Jundong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07968">Generative Risk Minimization for Out-of-Distribution Generalization on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2502.01778.pdf' target='_blank'>https://arxiv.org/pdf/2502.01778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stavros Orfanoudakis, Nanda Kishor Panda, Peter Palensky, Pedro P. Vergara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01778">GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient exploration of the solution space. This study introduces GNN-DT, a novel Decision Transformer (DT) architecture that integrates Graph Neural Network (GNN) embedders with a novel residual connection between input and output tokens crucial for handling dynamic environments. By learning from previously collected trajectories, GNN-DT tackles the sparse rewards limitations of online RL algorithms and delivers high-quality solutions in real-time. We evaluate GNN-DT on the complex electric vehicle (EV) charging optimization problem and prove that its performance is superior and requires significantly fewer training trajectories, thus improving sample efficiency compared to existing DT and offline RL baselines. Furthermore, GNN-DT exhibits robust generalization to unseen environments and larger action spaces, addressing a critical gap in prior offline and online RL approaches.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2501.04102.pdf' target='_blank'>https://arxiv.org/pdf/2501.04102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Wang, Xiaodong Yang, Rashidul Islam, Huiyuan Chen, Minghua Xu, Jundong Li, Yiwei Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04102">Enhancing Distribution and Label Consistency for Graph Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To deal with distribution shifts in graph data, various graph out-of-distribution (OOD) generalization techniques have been recently proposed. These methods often employ a two-step strategy that first creates augmented environments and subsequently identifies invariant subgraphs to improve generalizability. Nevertheless, this approach could be suboptimal from the perspective of consistency. First, the process of augmenting environments by altering the graphs while preserving labels may lead to graphs that are not realistic or meaningfully related to the origin distribution, thus lacking distribution consistency. Second, the extracted subgraphs are obtained from directly modifying graphs, and may not necessarily maintain a consistent predictive relationship with their labels, thereby impacting label consistency. In response to these challenges, we introduce an innovative approach that aims to enhance these two types of consistency for graph OOD generalization. We propose a modifier to obtain both augmented and invariant graphs in a unified manner. With the augmented graphs, we enrich the training data without compromising the integrity of label-graph relationships. The label consistency enhancement in our framework further preserves the supervision information in the invariant graph. We conduct extensive experiments on real-world datasets to demonstrate the superiority of our framework over other state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2412.02825.pdf' target='_blank'>https://arxiv.org/pdf/2412.02825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Wenhui Zhu, Xuanzhao Dong, Yanxi Chen, Xin Li, Peijie Qiu, Xiwen Chen, Vamsi Krishna Vasa, Yujian Xiong, Oana M. Dumitrascu, Abolfazl Razi, Yalin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02825">Many-MobileNet: Multi-Model Augmentation for Robust Retinal Disease Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose Many-MobileNet, an efficient model fusion strategy for retinal disease classification using lightweight CNN architecture. Our method addresses key challenges such as overfitting and limited dataset variability by training multiple models with distinct data augmentation strategies and different model complexities. Through this fusion technique, we achieved robust generalization in data-scarce domains while balancing computational efficiency with feature extraction capabilities.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2601.09413.pdf' target='_blank'>https://arxiv.org/pdf/2601.09413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Wan, Chao-Han Huck Yang, Jinchuan Tian, Hanrong Ye, Ankita Pasad, Szu-wei Fu, Arushi Goel, Ryo Hachiuma, Shizhe Diao, Kunal Dhawan, Sreyan Ghosh, Yusuke Hirota, Zhehuai Chen, Rafael Valle, Ehsan Hosseini Asl, Chenhui Chu, Shinji Watanabe, Yu-Chiang Frank Wang, Boris Ginsburg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09413">Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a voice-agentic framework that learns one critical omni-understanding skill: knowing when to trust itself versus when to consult external audio perception. Our work is motivated by a crucial yet counterintuitive finding: naively fine-tuning an omni-model on both speech recognition and external sound understanding tasks often degrades performance, as the model can be easily misled by noisy hypotheses. To address this, our framework, Speech-Hands, recasts the problem as an explicit self-reflection decision. This learnable reflection primitive proves effective in preventing the model from being derailed by flawed external candidates. We show that this agentic action mechanism generalizes naturally from speech recognition to complex, multiple-choice audio reasoning. Across the OpenASR leaderboard, Speech-Hands consistently outperforms strong baselines by 12.1% WER on seven benchmarks. The model also achieves 77.37% accuracy and high F1 on audio QA decisions, showing robust generalization and reliability across diverse audio question answering datasets. By unifying perception and decision-making, our work offers a practical path toward more reliable and resilient audio intelligence.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2505.16590.pdf' target='_blank'>https://arxiv.org/pdf/2505.16590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renyi Zhong, Yichen Li, Guangba Yu, Wenwei Gu, Jinxi Kuang, Yintong Huo, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16590">Larger Is Not Always Better: Exploring Small Open-source Language Models in Logging Statement Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developers use logging statements to create logs that document system behavior and aid in software maintenance. As such, high-quality logging is essential for effective maintenance; however, manual logging often leads to errors and inconsistency. Recent methods emphasize using large language models (LLMs) for automated logging statement generation, but these present privacy and resource issues, hindering their suitability for enterprise use. This paper presents the first large-scale empirical study evaluating small open-source language models (SOLMs) for automated logging statement generation. We evaluate four prominent SOLMs using various prompt strategies and parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG). Our results show that fine-tuned SOLMs with LoRA and RAG prompts, particularly Qwen2.5-coder-14B, outperform existing tools and LLM baselines in predicting logging locations and generating high-quality statements, with robust generalization across diverse repositories. These findings highlight SOLMs as a privacy-preserving, efficient alternative for automated logging.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2504.04470.pdf' target='_blank'>https://arxiv.org/pdf/2504.04470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Guo, Ajian Liu, Yunfeng Diao, Jin Zhang, Hui Ma, Bo Zhao, Richang Hong, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04470">Domain Generalization for Face Anti-spoofing via Content-aware Composite Prompt Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The challenge of Domain Generalization (DG) in Face Anti-Spoofing (FAS) is the significant interference of domain-specific signals on subtle spoofing clues. Recently, some CLIP-based algorithms have been developed to alleviate this interference by adjusting the weights of visual classifiers. However, our analysis of this class-wise prompt engineering suffers from two shortcomings for DG FAS: (1) The categories of facial categories, such as real or spoof, have no semantics for the CLIP model, making it difficult to learn accurate category descriptions. (2) A single form of prompt cannot portray the various types of spoofing. In this work, instead of class-wise prompts, we propose a novel Content-aware Composite Prompt Engineering (CCPE) that generates instance-wise composite prompts, including both fixed template and learnable prompts. Specifically, our CCPE constructs content-aware prompts from two branches: (1) Inherent content prompt explicitly benefits from abundant transferred knowledge from the instruction-based Large Language Model (LLM). (2) Learnable content prompts implicitly extract the most informative visual content via Q-Former. Moreover, we design a Cross-Modal Guidance Module (CGM) that dynamically adjusts unimodal features for fusion to achieve better generalized FAS. Finally, our CCPE has been validated for its effectiveness in multiple cross-domain experiments and achieves state-of-the-art (SOTA) results.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2511.09141.pdf' target='_blank'>https://arxiv.org/pdf/2511.09141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuetao Li, Wenke Huang, Nengyuan Pan, Kaiyan Zhao, Songhua Yang, Yiming Wang, Mengde Li, Mang Ye, Jifeng Xuan, Miao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09141">RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2507.14783.pdf' target='_blank'>https://arxiv.org/pdf/2507.14783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Derek Li, Jiaming Zhou, Amirreza Kazemi, Qianyi Sun, Abbas Ghaddar, Mohammad Ali Alomrani, Liheng Ma, Yu Luo, Dong Li, Feng Wen, Jianye Hao, Mark Coates, Yingxue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14783">Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of general-purpose artificial intelligence relies on large language models (LLMs) that excel across a wide range of tasks, from structured reasoning to creative generation. However, post-training methods like Supervised Fine-Tuning (SFT) often struggle with generalization, favoring memorization over transferable learning. In this work, we introduce Omni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM performance across diverse tasks by combining rule-based verifiable rewards with generative preference signals via LLM-as-a-Judge evaluations. Our approach enables consistent optimization across task types and scales RL-based training to subjective domains. We further investigate training strategies, demonstrating that a curriculum-based progression that orders tasks from structured to open-ended improves performance and reduces forgetting. Experimental results across four domains reveal that curriculum learning improves performance by 5.2% over joint training and 9.1% over model merging. These results highlight the importance of task-aware sampling and hybrid supervision in scaling RL-based post-training for general-purpose LLMs.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2507.11969.pdf' target='_blank'>https://arxiv.org/pdf/2507.11969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaohong Huang, Yuxin Zhang, Jingjing Xie, Fei Chao, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11969">GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in test-time adaptation (TTA) for Vision-Language Models (VLMs) have garnered increasing attention, particularly through the use of multiple augmented views of a single image to boost zero-shot generalization. Unfortunately, existing methods fail to strike a satisfactory balance between performance and efficiency, either due to excessive overhead of tuning text prompts or unstable benefits from handcrafted, training-free visual feature enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias), an efficient and effective TTA paradigm that incorporates two learnable biases during TTA, unfolded as the global bias and spatial bias. Particularly, the global bias captures the global semantic features of a test image by learning consistency across augmented views, while spatial bias learns the semantic coherence between regions in the image's spatial visual representation. It is worth highlighting that these two sets of biases are directly added to the logits outputed by the pretrained VLMs, which circumvent the full backpropagation through VLM that hinders the efficiency of existing TTA methods. This endows GS-Bias with extremely high efficiency while achieving state-of-the-art performance on 15 benchmark datasets. For example, it achieves a 2.23% improvement over TPT in cross-dataset generalization and a 2.72% improvement in domain generalization, while requiring only 6.5% of TPT's memory usage on ImageNet.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2507.10281.pdf' target='_blank'>https://arxiv.org/pdf/2507.10281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Tian, Liyao Li, Wentao Ye, Haobo Wang, Lingxin Wang, Lihua Yu, Zujie Ren, Gang Chen, Junbo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10281">Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tables are fundamental in domains such as finance, healthcare, and public administration, yet real-world table tasks often involve noise, structural heterogeneity, and semantic complexity--issues underexplored in existing research that primarily targets clean academic datasets. This survey focuses on LLM-based Table Agents, which aim to automate table-centric workflows by integrating preprocessing, reasoning, and domain adaptation. We define five core competencies--C1: Table Structure Understanding, C2: Table and Query Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze and compare current approaches. In addition, a detailed examination of the Text-to-SQL Agent reveals a performance gap between academic benchmarks and real-world scenarios, especially for open-source models. Finally, we provide actionable insights to improve the robustness, generalization, and efficiency of LLM-based Table Agents in practical settings.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2505.11883.pdf' target='_blank'>https://arxiv.org/pdf/2505.11883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihuan Qiu, Yi Xu, Chiyuan He, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11883">MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual model merging integrates independently fine-tuned models sequentially without access to original training data, providing a scalable and efficient solution to continual learning. However, current methods still face critical challenges, notably parameter interference among tasks and limited adaptability to evolving test distributions. The former causes catastrophic forgetting of integrated tasks, while the latter hinders effective adaptation to new tasks. To address these, we propose MINGLE, a novel framework for test-time continual model merging, which leverages test-time adaptation using a small set of unlabeled test samples from the current task to dynamically guide the merging process. MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation. Extensive experiments on standard continual merging benchmarks demonstrate that MINGLE achieves robust generalization, reduces forgetting significantly, and consistently surpasses previous state-of-the-art methods by 7-9\% on average across diverse task orders.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2502.19634.pdf' target='_blank'>https://arxiv.org/pdf/2502.19634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19634">MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice. Inference model is available at: https://huggingface.co/JZPeterPan/MedVLM-R1.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2510.14532.pdf' target='_blank'>https://arxiv.org/pdf/2510.14532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14532">Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2412.08906.pdf' target='_blank'>https://arxiv.org/pdf/2412.08906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengchao Chen, Guodong Long, Jing Jiang, Chengqi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08906">Federated Foundation Models on Heterogeneous Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a general-purpose time series foundation models with robust generalization capabilities across diverse applications from scratch is still an open challenge. Efforts are primarily focused on fusing cross-domain time series datasets to extract shared subsequences as tokens for training models on Transformer architecture. However, due to significant statistical heterogeneity across domains, this cross-domain fusing approach doesn't work effectively as the same as fusing texts and images. To tackle this challenge, this paper proposes a novel federated learning approach to address the heterogeneity in time series foundation models training, namely FFTS. Specifically, each data-holding organization is treated as an independent client in a collaborative learning framework with federated settings, and then many client-specific local models will be trained to preserve the unique characteristics per dataset. Moreover, a new regularization mechanism will be applied to both client-side and server-side, thus to align the shared knowledge across heterogeneous datasets from different domains. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed federated learning approach. The newly learned time series foundation models achieve superior generalization capabilities on cross-domain time series analysis tasks, including forecasting, imputation, and anomaly detection.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2409.12370.pdf' target='_blank'>https://arxiv.org/pdf/2409.12370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Yifan Peng, Yichen Lu, Xuankai Chang, Ruihua Song, Shinji Watanabe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12370">Robust Audiovisual Speech Recognition Models with Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual signals can enhance audiovisual speech recognition accuracy by providing additional contextual information. Given the complexity of visual signals, an audiovisual speech recognition model requires robust generalization capabilities across diverse video scenarios, presenting a significant challenge. In this paper, we introduce EVA, leveraging the mixture-of-Experts for audioVisual ASR to perform robust speech recognition for ``in-the-wild'' videos. Specifically, we first encode visual information into visual tokens sequence and map them into speech space by a lightweight projection. Then, we build EVA upon a robust pretrained speech recognition model, ensuring its generalization ability. Moreover, to incorporate visual information effectively, we inject visual information into the ASR model through a mixture-of-experts module. Experiments show our model achieves state-of-the-art results on three benchmarks, which demonstrates the generalization ability of EVA across diverse video domains.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2510.04225.pdf' target='_blank'>https://arxiv.org/pdf/2510.04225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikun Ji, Yan Hong, Bowen Deng, jun lan, Huijia Zhu, Weiqiang Wang, Liqing Zhang, Jianfu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04225">Zoom-In to Sort AI-Generated Images Out</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of AI-generated imagery has blurred the boundary between real and synthetic content, raising critical concerns for digital integrity. Vision-language models (VLMs) offer interpretability through explanations but often fail to detect subtle artifacts in high-quality synthetic images. We propose ZoomIn, a two-stage forensic framework that improves both accuracy and interpretability. Mimicking human visual inspection, ZoomIn first scans an image to locate suspicious regions and then performs a focused analysis on these zoomed-in areas to deliver a grounded verdict. To support training, we introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images annotated with bounding boxes and forensic explanations, generated through an automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust generalization, while providing human-understandable explanations grounded in visual evidence.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2509.24704.pdf' target='_blank'>https://arxiv.org/pdf/2509.24704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guibin Zhang, Muxin Fu, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24704">MemGen: Weaving Generative Latent Memory for Self-Evolving Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent memory shapes how Large Language Model (LLM)-powered agents, akin to the human brain, progressively refine themselves through environment interactions. Existing paradigms remain constrained: parametric memory forcibly adjusts model parameters, and retrieval-based memory externalizes experience into structured databases, yet neither captures the fluid interweaving of reasoning and memory that underlies human cognition. To address this gap, we propose MemGen, a dynamic generative memory framework that equips agents with a human-esque cognitive faculty. It consists of a \textit{memory trigger}, which monitors the agent's reasoning state to decide explicit memory invocation, and a \textit{memory weaver}, which takes the agent's current state as stimulus to construct a latent token sequence as machine-native memory to enrich its reasoning. In this way, MemGen enables agents to recall and augment latent memory throughout reasoning, producing a tightly interwoven cycle of memory and cognition. Extensive experiments across eight benchmarks show that MemGen surpasses leading external memory systems such as ExpeL and AWM by up to $38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain generalization ability. More importantly, we find that without explicit supervision, MemGen spontaneously evolves distinct human-like memory faculties, including planning memory, procedural memory, and working memory, suggesting an emergent trajectory toward more naturalistic forms of machine cognition.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2506.07603.pdf' target='_blank'>https://arxiv.org/pdf/2506.07603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhui Wei, Zikai Xiao, Danyu Sun, Luqi Gong, Zongxin Yang, Zuozhu Liu, Jian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07603">SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical video understanding is pivotal for enabling automated intraoperative decision-making, skill assessment, and postoperative quality improvement. However, progress in developing surgical video foundation models (FMs) remains hindered by the scarcity of large-scale, diverse datasets for pretraining and systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a unified surgical video benchmarking framework comprising a pretraining dataset, \textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}. SurgBench offers extensive coverage of diverse surgical scenarios, with SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11 specialties, and SurgBench-E providing robust evaluation across six categories (phase classification, camera motion, tool recognition, disease diagnosis, action classification, and organ detection) spanning 72 fine-grained tasks. Extensive experiments reveal that existing video FMs struggle to generalize across varied surgical video analysis tasks, whereas pretraining on SurgBench-P yields substantial performance improvements and superior cross-domain generalization to unseen procedures and modalities. Our dataset and code are available upon request.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2411.09837.pdf' target='_blank'>https://arxiv.org/pdf/2411.09837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kirill Vasilevski, Dayi Lin, Ahmed E. Hassan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09837">Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To balance the quality and inference cost of a Foundation Model (FM, such as large language models (LLMs)) powered software, people often opt to train a routing model that routes requests to FMs with different sizes and capabilities. Existing routing models rely on learning the optimal routing decision from carefully curated data, require complex computations to be updated, and do not consider the potential evolution of weaker FMs. In this paper, we propose Real-time Adaptive Routing (RAR), an approach to continuously adapt FM routing decisions while using guided in-context learning to enhance the capabilities of weaker FM. The goal is to reduce reliance on stronger, more expensive FMs. We evaluate our approach on different subsets of the popular MMLU benchmark. Over time, our approach routes 50.2% fewer requests to computationally expensive models while maintaining around 90.5% of the general response quality. In addition, the guides generated from stronger models have shown intra-domain generalization and led to a better quality of responses compared to an equivalent approach with a standalone weaker FM.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2601.19394.pdf' target='_blank'>https://arxiv.org/pdf/2601.19394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Han, Senkang Hu, Yihang Tao, Yu Guo, Philip Birch, Sam Tak Wu Kwong, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19394">DSP-Reg: Domain-Sensitive Parameter Regularization for Robust Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) is a critical area that focuses on developing models capable of performing well on data from unseen distributions, which is essential for real-world applications. Existing approaches primarily concentrate on learning domain-invariant features, which assume that a model robust to variations in the source domains will generalize well to unseen target domains. However, these approaches neglect a deeper analysis at the parameter level, which makes the model hard to explicitly differentiate between parameters sensitive to domain shifts and those robust, potentially hindering its overall ability to generalize. In order to address these limitations, we first build a covariance-based parameter sensitivity analysis framework to quantify the sensitivity of each parameter in a model to domain shifts. By computing the covariance of parameter gradients across multiple source domains, we can identify parameters that are more susceptible to domain variations, which serves as our theoretical foundation. Based on this, we propose Domain-Sensitive Parameter Regularization (DSP-Reg), a principled framework that guides model optimization by a soft regularization technique that encourages the model to rely more on domain-invariant parameters while suppressing those that are domain-specific. This approach provides a more granular control over the model's learning process, leading to improved robustness and generalization to unseen domains. Extensive experiments on benchmarks, such as PACS, VLCS, OfficeHome, and DomainNet, demonstrate that DSP-Reg outperforms state-of-the-art approaches, achieving an average accuracy of 66.7\% and surpassing all baselines.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2511.05092.pdf' target='_blank'>https://arxiv.org/pdf/2511.05092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruolin Li, Min Liu, Yuan Bian, Zhaoyang Li, Yuzhen Li, Xueping Wang, Yaonan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05092">A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With growing concerns over data privacy, researchers have started using virtual data as an alternative to sensitive real-world images for training person re-identification (Re-ID) models. However, existing virtual datasets produced by game engines still face challenges such as complex construction and poor domain generalization, making them difficult to apply in real scenarios. To address these challenges, we propose a Dual-stage Prompt-driven Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich prompts incorporating multi-dimensional attributes such as pedestrian appearance, illumination, and viewpoint that drive the diffusion model to synthesize diverse data end-to-end, building a large-scale virtual dataset named GenePerson with 130,519 images of 6,641 identities. In the second stage, we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn domain-invariant generalization features. With the aid of contrastive learning, we employ two textual inversion networks to map images into pseudo-words representing style and content, respectively, thereby constructing style-disentangled content prompts to guide the model in learning domain-invariant content features at the image level. Experiments demonstrate that models trained on GenePerson with PDM achieve state-of-the-art generalization performance, surpassing those on popular real and virtual Re-ID datasets.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2509.10503.pdf' target='_blank'>https://arxiv.org/pdf/2509.10503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolin Yuan, Jingtao Li, Weiming Zhuang, Chen Chen, Lingjuan Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10503">FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Object Detection (FOD) enables clients to collaboratively train a global object detection model without accessing their local data from diverse domains. However, significant variations in environment, weather, and other domain specific factors hinder performance, making cross domain generalization a key challenge. Existing FOD methods often overlook the hardware constraints of edge devices and introduce local training regularizations that incur high computational costs, limiting real-world applicability. In this paper, we propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without introducing additional local computational overhead. FEDEXCHANGE employs a server side dynamic model exchange strategy that enables each client to gain insights from other clients' domain data without direct data sharing. Specifically, FEDEXCHANGE allows the server to alternate between model aggregation and model exchange. During aggregation rounds, the server aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters and exchanges local models based on distance measures, allowing local models to learn from a variety of domains. As all operations are performed on the server side, clients can achieve improved cross domain utility without any additional computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE enhances FOD performance, achieving 1.6X better mean average precision in challenging domains, such as rainy conditions, while requiring only 0.8X the computational resources compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2503.13617.pdf' target='_blank'>https://arxiv.org/pdf/2503.13617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Yubin Xiao, Ke Liang, Mengzhu Wang, Long Lan, Kenli Li, Xinwang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13617">Let Synthetic Data Shine: Domain Reassembly and Soft-Fusion for Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Domain Generalization (SDG) aims to train models with consistent performance across diverse scenarios using data from a single source. While using latent diffusion models (LDMs) show promise in augmenting limited source data, we demonstrate that directly using synthetic data can be detrimental due to significant feature distribution discrepancies between synthetic and real target domains, leading to performance degradation. To address this issue, we propose Discriminative Domain Reassembly and Soft-Fusion (DRSF), a training framework leveraging synthetic data to improve model generalization. We employ LDMs to produce diverse pseudo-target domain samples and introduce two key modules to handle distribution bias. First, Discriminative Feature Decoupling and Reassembly (DFDR) module uses entropy-guided attention to recalibrate channel-level features, suppressing synthetic noise while preserving semantic consistency. Second, Multi-pseudo-domain Soft Fusion (MDSF) module uses adversarial training with latent-space feature interpolation, creating continuous feature transitions between domains. Extensive SDG experiments on object detection and semantic segmentation tasks demonstrate that DRSF achieves substantial performance gains with only marginal computational overhead. Notably, DRSF's plug-and-play architecture enables seamless integration with unsupervised domain adaptation paradigms, underscoring its broad applicability in addressing diverse and real-world domain challenges.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2502.20900.pdf' target='_blank'>https://arxiv.org/pdf/2502.20900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Zhang Chen, Tianrui Guan, Fanlian Zeng, Ka Num Lui, Yuyao Ye, Yitao Liang, Yaodong Yang, Yuanpei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20900">DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on restrictive assumptions, such as single-object settings or limited environments, showing constrained generalization. We present DexGraspVLA, a hierarchical framework for robust generalization in language-guided general dexterous grasping and beyond. It utilizes a pre-trained Vision-Language model as the high-level planner and learns a diffusion-based low-level Action controller. The key insight to achieve generalization lies in iteratively transforming diverse language and visual inputs into domain-invariant representations via foundation models, where imitation learning can be effectively applied due to the alleviation of domain shift. Notably, our method achieves a 90+% dexterous grasping success rate under thousands of challenging unseen cluttered scenes. Empirical analysis confirms the consistency of internal model behavior across environmental variations, validating our design. DexGraspVLA also, for the first time, simultaneously demonstrates free-form long-horizon prompt execution, robustness to adversarial objects and human disturbance, and failure recovery. Extended application to nonprehensile grasping further proves its generality. Project website: https://dexgraspvla.github.io.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2502.03393.pdf' target='_blank'>https://arxiv.org/pdf/2502.03393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewen Liu, Juntong Ni, Max S. Y. Lau, Wei Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03393">Pre-training Epidemic Time Series Forecasters with Compartmental Prototypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate epidemic forecasting is crucial for outbreak preparedness, but existing data-driven models are often brittle. Typically trained on a single pathogen, they struggle with data scarcity during new outbreaks and fail under distribution shifts caused by viral evolution or interventions. However, decades of surveillance data from diverse diseases offer an untapped source of transferable knowledge. To leverage the collective lessons from history, we propose CAPE, the first open-source pre-trained model for epidemic forecasting. Unlike existing time series foundation models that overlook epidemiological challenges, CAPE models epidemic dynamics as mixtures of latent population states, termed compartmental prototypes. It discovers a flexible dictionary of compartment prototypes directly from surveillance data, enabling each outbreak to be expressed as a time-varying mixture that links observed infections to latent population states. To promote robust generalization, CAPE combines self-supervised pre-training objectives with lightweight epidemic-aware regularizers that align the learned prototypes with epidemiological semantics. On a comprehensive benchmark spanning 17 diseases and 50+ regions, CAPE significantly outperforms strong baselines in zero-shot, few-shot, and full-shot forecasting. This work represents a principled step toward pre-trained epidemic models that are both transferable and epidemiologically grounded.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2412.18281.pdf' target='_blank'>https://arxiv.org/pdf/2412.18281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhou Jin, Li You, Huibin Zhou, Yuanshuo Wang, Xiaofeng Liu, Xinrui Gong, Xiqi Gao, Derrick Wing Kwan Ng, Xiang-Gen Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18281">GDM4MMIMO: Generative Diffusion Models for Massive MIMO Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Massive multiple-input multiple-output (MIMO) offers significant advantages in spectral and energy efficiencies, positioning it as a cornerstone technology of fifth-generation (5G) wireless communication systems and a promising solution for the burgeoning data demands anticipated in sixth-generation (6G) networks. In recent years, with the continuous advancement of artificial intelligence (AI), a multitude of task-oriented generative foundation models (GFMs) have emerged, achieving remarkable performance in various fields such as computer vision (CV), natural language processing (NLP), and autonomous driving. As a pioneering force, these models are driving the paradigm shift in AI towards generative AI (GenAI). Among them, the generative diffusion model (GDM), as one of state-of-the-art families of generative models, demonstrates an exceptional capability to learn implicit prior knowledge and robust generalization capabilities, thereby enhancing its versatility and effectiveness across diverse applications. In this paper, we delve into the potential applications of GDM in massive MIMO communications. Specifically, we first provide an overview of massive MIMO communication, the framework of GFMs, and the working mechanism of GDM. Following this, we discuss recent research advancements in the field and present a case study of near-field channel estimation based on GDM, demonstrating its promising potential for facilitating efficient ultra-dimensional channel statement information (CSI) acquisition in the context of massive MIMO communications. Finally, we highlight several pressing challenges in future mobile communications and identify promising research directions surrounding GDM.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2412.13815.pdf' target='_blank'>https://arxiv.org/pdf/2412.13815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Xiangyuan Yang, Mengzhu Wang, Long Lan, Ke Liang, Xinwang Liu, Kenli Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13815">Object Style Diffusion for Generalized Object Detection in Urban Scene</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection is a critical task in computer vision, with applications in various domains such as autonomous driving and urban scene monitoring. However, deep learning-based approaches often demand large volumes of annotated data, which are costly and difficult to acquire, particularly in complex and unpredictable real-world environments. This dependency significantly hampers the generalization capability of existing object detection techniques. To address this issue, we introduce a novel single-domain object detection generalization method, named GoDiff, which leverages a pre-trained model to enhance generalization in unseen domains. Central to our approach is the Pseudo Target Data Generation (PTDG) module, which employs a latent diffusion model to generate pseudo-target domain data that preserves source domain characteristics while introducing stylistic variations. By integrating this pseudo data with source domain data, we diversify the training dataset. Furthermore, we introduce a cross-style instance normalization technique to blend style features from different domains generated by the PTDG module, thereby increasing the detector's robustness. Experimental results demonstrate that our method not only enhances the generalization ability of existing detectors but also functions as a plug-and-play enhancement for other single-domain generalization methods, achieving state-of-the-art performance in autonomous driving scenarios.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2511.22445.pdf' target='_blank'>https://arxiv.org/pdf/2511.22445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikai Tang, Haoran Geng, Sheng Zang, Pieter Abbeel, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22445">Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has emerged as a crucial ap proach for acquiring visuomotor skills from demonstrations, where designing effective observation encoders is essential for policy generalization. However, existing methods often struggle to generalize under spatial and visual randomizations, instead tending to overfit. To address this challenge, we propose Visual Geometry Diffusion Policy (VGDP), a multimodal imitation learning framework built around a Complementarity-Aware Fusion Module where modality-wise dropout enforces balanced use of RGB and point-cloud cues, with cross-attention serving only as a lightweight interaction layer. Our experiments show that the expressiveness of the fused latent space is largely induced by the enforced complementarity from modality-wise dropout, with cross-attention serving primarily as a lightweight interaction mechanism rather than the main source of robustness. Across a benchmark of 18 simulated tasks and 4 real-world tasks, VGDP outperforms seven baseline policies with an average performance improvement of 39.1%. More importantly, VGDP demonstrates strong robustness under visual and spatial per turbations, surpassing baselines with an average improvement of 41.5% in different visual conditions and 15.2% in different spatial settings.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2501.14315.pdf' target='_blank'>https://arxiv.org/pdf/2501.14315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Shao-Hua Sun, Hung-yi Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14315">Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and three additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2412.05551.pdf' target='_blank'>https://arxiv.org/pdf/2412.05551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Jiang, Yuan Meng, Chen Tang, Han Yu, Qun Li, Zhi Wang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05551">GAQAT: gradient-adaptive quantization-aware training for domain generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research on loss surface geometry, such as Sharpness-Aware Minimization (SAM), shows that flatter minima improve generalization. Recent studies further reveal that flatter minima can also reduce the domain generalization (DG) gap. However, existing flatness-based DG techniques predominantly operate within a full-precision training process, which is impractical for deployment on resource-constrained edge devices that typically rely on lower bit-width representations (e.g., 4 bits, 3 bits). Consequently, low-precision quantization-aware training is critical for optimizing these techniques in real-world applications. In this paper, we observe a significant degradation in performance when applying state-of-the-art DG-SAM methods to quantized models, suggesting that current approaches fail to preserve generalizability during the low-precision training process. To address this limitation, we propose a novel Gradient-Adaptive Quantization-Aware Training (GAQAT) framework for DG. Our approach begins by identifying the scale-gradient conflict problem in low-precision quantization, where the task loss and smoothness loss induce conflicting gradients for the scaling factors of quantizers, with certain layers exhibiting opposing gradient directions. This conflict renders the optimization of quantized weights highly unstable. To mitigate this, we further introduce a mechanism to quantify gradient inconsistencies and selectively freeze the gradients of scaling factors, thereby stabilizing the training process and enhancing out-of-domain generalization. Extensive experiments validate the effectiveness of the proposed GAQAT framework. On PACS, our 3-bit and 4-bit models outperform direct DG-QAT integration by up to 4.5%. On DomainNet, the 4-bit model achieves near-lossless performance compared to full precision, with improvements of 1.39% (4-bit) and 1.06% (3-bit) over the SOTA QAT baseline.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2510.04861.pdf' target='_blank'>https://arxiv.org/pdf/2510.04861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing Chu, Xinke Zhang, Xueyi Zheng, Ke Zheng, Xiaobo Wen, Jiabo Ma, Yihui Wang, Jiewei Chen, Chengyou Zheng, Jiangyu Zhang, Yongqin Wen, Jiajia Meng, Ziqi Zeng, Xiaoqing Li, Jing Li, Dan Xie, Yaping Ye, Yu Wang, Hao Chen, Muyan Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04861">A Clinical-grade Universal Foundation Model for Intraoperative Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intraoperative pathology is pivotal to precision surgery, yet its clinical impact is constrained by diagnostic complexity and the limited availability of high-quality frozen-section data. While computational pathology has made significant strides, the lack of large-scale, prospective validation has impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a clinical-grade foundation model developed on over 100,000 frozen sections from eight medical centers, specifically designed to provide Clinical-grade Robust Intraoperative Support for Pathology (CRISP). CRISP was comprehensively evaluated on more than 15,000 intraoperative slides across nearly 100 retrospective diagnostic tasks, including benign-malignant discrimination, key intraoperative decision-making, and pan-cancer detection, etc. The model demonstrated robust generalization across diverse institutions, tumor types, and anatomical sites-including previously unseen sites and rare cancers. In a prospective cohort of over 2,000 patients, CRISP sustained high diagnostic accuracy under real-world conditions, directly informing surgical decisions in 92.6% of cases. Human-AI collaboration further reduced diagnostic workload by 35%, avoided 105 ancillary tests and enhanced detection of micrometastases with 87.5% accuracy. Together, these findings position CRISP as a clinical-grade paradigm for AI-driven intraoperative pathology, bridging computational advances with surgical precision and accelerating the translation of artificial intelligence into routine clinical practice.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2508.13401.pdf' target='_blank'>https://arxiv.org/pdf/2508.13401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Dumitriu, Florin Miron, Florin Tatui, Radu Tudor Ionescu, Radu Timofte, Aakash Ralhan, Florin-Alexandru Vasluianu, Shenyang Qian, Mitchell Harley, Imran Razzak, Yang Song, Pu Luo, Yumei Li, Cong Xu, Jinming Chai, Kexin Zhang, Licheng Jiao, Lingling Li, Siqi Yu, Chao Zhang, Kehuan Song, Fang Liu, Puhua Chen, Xu Liu, Jin Hu, Jinyang Xu, Biao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13401">AIM 2025 Rip Current Segmentation (RipSeg) Challenge Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents an overview of the AIM 2025 RipSeg Challenge, a competition designed to advance techniques for automatic rip current segmentation in still images. Rip currents are dangerous, fast-moving flows that pose a major risk to beach safety worldwide, making accurate visual detection an important and underexplored research task. The challenge builds on RipVIS, the largest available rip current dataset, and focuses on single-class instance segmentation, where precise delineation is critical to fully capture the extent of rip currents. The dataset spans diverse locations, rip current types, and camera orientations, providing a realistic and challenging benchmark. In total, $75$ participants registered for this first edition, resulting in $5$ valid test submissions. Teams were evaluated on a composite score combining $F_1$, $F_2$, $AP_{50}$, and $AP_{[50:95]}$, ensuring robust and application-relevant rankings. The top-performing methods leveraged deep learning architectures, domain adaptation techniques, pretrained models, and domain generalization strategies to improve performance under diverse conditions. This report outlines the dataset details, competition framework, evaluation metrics, and final results, providing insights into the current state of rip current segmentation. We conclude with a discussion of key challenges, lessons learned from the submissions, and future directions for expanding RipSeg.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2508.11265.pdf' target='_blank'>https://arxiv.org/pdf/2508.11265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei He, Lingling Li, Licheng Jiao, Ronghua Shang, Fang Liu, Shuang Wang, Xu Liu, Wenping Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11265">Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization in 3D segmentation is a critical challenge in deploying models to unseen environments. Current methods mitigate the domain shift by augmenting the data distribution of point clouds. However, the model learns global geometric patterns in point clouds while ignoring the category-level distribution and alignment. In this paper, a category-level geometry learning framework is proposed to explore the domain-invariant geometric features for domain generalized 3D semantic segmentation. Specifically, Category-level Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric properties of point cloud features, which constructs the geometric properties of each class and couples geometric embedding to semantic learning. Secondly, Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D distribution and align the category-level geometric embeddings, allowing the model to focus on the geometric invariant information to improve generalization. Experimental results verify the effectiveness of the proposed method, which has very competitive segmentation accuracy compared with the state-of-the-art domain generalized point cloud methods.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2504.15686.pdf' target='_blank'>https://arxiv.org/pdf/2504.15686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Phuong Quynh Le, Christin Seifert, JÃ¶rg SchlÃ¶tterer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15686">Invariant Learning with Annotation-free Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invariant learning is a promising approach to improve domain generalization compared to Empirical Risk Minimization (ERM). However, most invariant learning methods rely on the assumption that training examples are pre-partitioned into different known environments. We instead infer environments without the need for additional annotations, motivated by observations of the properties within the representation space of a trained ERM model. We show the preliminary effectiveness of our approach on the ColoredMNIST benchmark, achieving performance comparable to methods requiring explicit environment labels and on par with an annotation-free method that poses strong restrictions on the ERM reference model.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2601.02993.pdf' target='_blank'>https://arxiv.org/pdf/2601.02993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Zhiming Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02993">Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2511.00543.pdf' target='_blank'>https://arxiv.org/pdf/2511.00543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunchuan Guan, Yu Liu, Ke Zhou, Hui Li, Sen Jia, Zhiqi Shen, Ziyang Wang, Xinglin Zhang, Tao Chen, Jenq-Neng Hwang, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00543">Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative modeling enable neural networks to generate weights without relying on gradient-based optimization. However, current methods are limited by issues of over-coupling and long-horizon. The former tightly binds weight generation with task-specific objectives, thereby limiting the flexibility of the learned optimizer. The latter leads to inefficiency and low accuracy during inference, caused by the lack of local constraints. In this paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that enhances flexibility through learning various optimization policies. It adopts a hybrid-policy sub-trajectory balance objective, which integrates on-policy and off-policy learning to capture local optimization policies. Theoretically, we demonstrate that learning solely local optimization policies can address the long-horizon issue while enhancing the generation of global optimal weights. In addition, we validate Lo-Hp's superior accuracy and inference efficiency in tasks that require frequent weight updates, such as transfer learning, few-shot learning, domain generalization, and large language model adaptation.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2510.21090.pdf' target='_blank'>https://arxiv.org/pdf/2510.21090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingru Zhang, Liang Qiu, Ilgee Hong, Zhenghao Xu, Tianyi Liu, Shiyang Li, Rongzhi Zhang, Zheng Li, Lihong Li, Bing Yin, Chao Zhang, Jianshu Chen, Haoming Jiang, Tuo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21090">Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised fine-tuning (SFT) has emerged as a crucial method for aligning large language models (LLMs) with human-annotated demonstrations. However, SFT, being an off-policy approach similar to behavior cloning, often struggles with overfitting and poor out-of-domain generalization, especially in limited-data scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel fine-tuning method that leverages on-policy techniques to enhance generalization performance. Our approach combines the strengths of SFT and proximal policy optimization (PPO) to achieve more effective alignment from demonstration data. At its core is a reward function designed as the log policy ratio between the SFT model and the pretrained base model. This function serves as an implicit reward signal, using the pretrained policy as a baseline and the SFT policy as a target. By doing so, it enables on-policy fine-tuning without relying on human preference annotations. The integration of this self-rewarding mechanism with PPO addresses key limitations of SFT, improving generalization, data efficiency, and robustness. Our empirical evaluation across a range of natural language processing tasks demonstrates that Self-Rewarding PPO consistently outperforms traditional SFT methods. The results highlight the effectiveness of our approach in aligning LLMs using demonstration data, particularly in scenarios where high-quality annotated data is scarce.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2510.14449.pdf' target='_blank'>https://arxiv.org/pdf/2510.14449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jahidul Arafat, Fariha Tasmin, Md Kaosar Uddin, Sanjaya Poudel, Eftakhar Ahmed Arnob
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14449">Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-class wine classification presents fundamental trade-offs between model accuracy, feature dimensionality, and interpretability - critical factors for production deployment in analytical chemistry. This paper presents a comprehensive empirical study of One-vs-Rest logistic regression on the UCI Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing from-scratch gradient descent implementation against scikit-learn's optimized solvers and quantifying L1 regularization effects on feature sparsity. Manual gradient descent achieves 92.59 percent mean test accuracy with smooth convergence, validating theoretical foundations, though scikit-learn provides 24x training speedup and 98.15 percent accuracy. Class-specific analysis reveals distinct chemical signatures with heterogeneous patterns where color intensity varies dramatically (0.31 to 16.50) across cultivars. L1 regularization produces 54-69 percent feature reduction with only 4.63 percent accuracy decrease, demonstrating favorable interpretability-performance trade-offs. We propose an optimal 5-feature subset achieving 62 percent complexity reduction with estimated 92-94 percent accuracy, enabling cost-effective deployment with 80 dollars savings per sample and 56 percent time reduction. Statistical validation confirms robust generalization with sub-2ms prediction latency suitable for real-time quality control. Our findings provide actionable guidelines for practitioners balancing comprehensive chemical analysis against targeted feature measurement in resource-constrained environments.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2510.12866.pdf' target='_blank'>https://arxiv.org/pdf/2510.12866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dantong Niu, Yuvan Sharma, Baifeng Shi, Rachel Ding, Matteo Gioia, Haoru Xue, Henry Tsai, Konstantinos Kallidromitis, Anirudh Pai, Shankar Shastry, Trevor Darrell, Jitendra Malik, Roei Herzig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12866">Learning to Grasp Anything by Playing with Random Toys</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation policies often struggle to generalize to novel objects, limiting their real-world utility. In contrast, cognitive science suggests that children develop generalizable dexterous manipulation skills by mastering a small set of simple toys and then applying that knowledge to more complex items. Inspired by this, we study if similar generalization capabilities can also be achieved by robots. Our results indicate robots can learn generalizable grasping using randomly assembled objects that are composed from just four shape primitives: spheres, cuboids, cylinders, and rings. We show that training on these "toys" enables robust generalization to real-world objects, yielding strong zero-shot performance. Crucially, we find the key to this generalization is an object-centric visual representation induced by our proposed detection pooling mechanism. Evaluated in both simulation and on physical robots, our model achieves a 67% real-world grasping success rate on the YCB dataset, outperforming state-of-the-art approaches that rely on substantially more in-domain data. We further study how zero-shot generalization performance scales by varying the number and diversity of training toys and the demonstrations per toy. We believe this work offers a promising path to scalable and generalizable learning in robotic manipulation. Demonstration videos, code, checkpoints and our dataset are available on our project page: https://lego-grasp.github.io/ .
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2510.06274.pdf' target='_blank'>https://arxiv.org/pdf/2510.06274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Mahdi Samiei Paqaleh, Arash Marioriyad, Arman Tahmasebi-Zadeh, Mohamadreza Fereydooni, Mahdi Ghaznavai, Mahdieh Soleymani Baghshah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06274">Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2509.22812.pdf' target='_blank'>https://arxiv.org/pdf/2509.22812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhang, Christopher Malon, Lichao Sun, Martin Renqiang Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22812">EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiology report generation requires advanced medical image analysis, effective temporal reasoning, and accurate text generation. Although recent innovations, particularly multimodal large language models (MLLMs), have shown improved performance, their supervised fine-tuning (SFT) objective is not explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO, a mixed-policy reinforcement learning (RL) algorithm designed specifically to optimize the generation through clinically motivated rewards. EditGRPO integrates on-policy exploration with off-policy guidance by injecting sentence-level detailed corrections during training rollouts. This mixed-policy approach addresses the exploration dilemma and sampling efficiency issues typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO baselines, achieving an average improvement of 3.4% in CheXbert, GREEN, Radgraph, and RATEScore metrics across four major chest X-ray report generation datasets. Notably, EditGRPO also demonstrates superior out-of-domain generalization, with an average performance gain of 5.9% on unseen datasets.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2506.10097.pdf' target='_blank'>https://arxiv.org/pdf/2506.10097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoya Nishida, Noboru Harada, Daisuke Niizumi, Davide Albertini, Roberto Sannino, Simone Pradolini, Filippo Augusti, Keisuke Imoto, Kota Dohi, Harsh Purohit, Takashi Endo, Yohei Kawaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10097">Description and Discussion on DCASE 2025 Challenge Task 2: First-shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the task description for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2025 Challenge Task 2, titled "First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring." Building on the DCASE 2024 Challenge Task 2, this task is structured as a first-shot problem within a domain generalization framework. The primary objective of the first-shot approach is to facilitate the rapid deployment of ASD systems for new machine types without requiring machine-specific hyperparameter tunings. For DCASE 2025 Challenge Task 2, sounds from previously unseen machine types have been collected and provided as the evaluation dataset. Results and analysis of the challenge submissions will be added following the challenge's submission deadline.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2504.09532.pdf' target='_blank'>https://arxiv.org/pdf/2504.09532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congcong Wen, Geeta Chandra Raju Bethala, Yu Hao, Niraj Pudasaini, Hao Huang, Shuaihang Yuan, Baoru Huang, Anh Nguyen, Mengyu Wang, Anthony Tzes, Yi Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09532">Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions. Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings. In this paper, we propose Humanoid-COA, the first humanoid agent framework that integrates foundation model reasoning with an Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation. Within the perception--reasoning--action paradigm, our key contribution lies in the reasoning stage, where the proposed CoA mechanism decomposes high-level human instructions into structured sequences of locomotion and manipulation primitives through affordance analysis, spatial inference, and whole-body action reasoning. Extensive experiments on two humanoid robots, Unitree H1-2 and G1, in both an open test area and an apartment environment, demonstrate that our framework substantially outperforms prior baselines across manipulation, locomotion, and loco-manipulation tasks, achieving robust generalization to long-horizon and unstructured scenarios. Project page: https://humanoid-coa.github.io/
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2502.01117.pdf' target='_blank'>https://arxiv.org/pdf/2502.01117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Jenq-Neng Hwang, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01117">Learning to Learn Weight Generation via Local Consistency Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based algorithms have emerged as promising techniques for weight generation. However, existing solutions are limited by two challenges: generalizability and local target assignment. The former arises from the inherent lack of cross-task transferability in existing single-level optimization methods, limiting the model's performance on new tasks. The latter lies in existing research modeling only global optimal weights, neglecting the supervision signals in local target weights. Moreover, naively assigning local target weights causes local-global inconsistency. To address these issues, we propose Mc-Di, which integrates the diffusion algorithm with meta-learning for better generalizability. Furthermore, we extend the vanilla diffusion into a local consistency diffusion algorithm. Our theory and experiments demonstrate that it can learn from local targets while maintaining consistency with the global optima. We validate Mc-Di's superior accuracy and inference efficiency in tasks that require frequent weight updates, including transfer learning, few-shot learning, domain generalization, and large language model adaptation.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2512.18933.pdf' target='_blank'>https://arxiv.org/pdf/2512.18933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18933">Point What You Mean: Visually Grounded Instruction Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2512.03724.pdf' target='_blank'>https://arxiv.org/pdf/2512.03724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwen Li, Xin Wang, Hanlue Zhang, Runnan Chen, Runqi Lin, Xiao He, Han Huang, Yandong Guo, Fakhri Karray, Tongliang Liu, Mingming Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03724">PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2511.13469.pdf' target='_blank'>https://arxiv.org/pdf/2511.13469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyuan Luo, Chonghao Qiu, Runlong Yu, Yiqun Xie, Xiaowei Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13469">GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2510.03161.pdf' target='_blank'>https://arxiv.org/pdf/2510.03161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Huang, Zhipei Xu, Xuanyu Zhang, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03161">UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2509.25747.pdf' target='_blank'>https://arxiv.org/pdf/2509.25747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialei Huang, Zhaoheng Yin, Yingdong Hu, Shuo Wang, Xingyu Lin, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25747">Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sim-to-real transfer remains a fundamental challenge in robot manipulation due to the entanglement of perception and control in end-to-end learning. We present a decoupled framework that learns each component where it is most reliable: control policies are trained in simulation with privileged state to master spatial layouts and manipulation dynamics, while perception is adapted only at deployment to bridge real observations to the frozen control policy. Our key insight is that control strategies and action patterns are universal across environments and can be learned in simulation through systematic randomization, while perception is inherently domain-specific and must be learned where visual observations are authentic. Unlike existing end-to-end approaches that require extensive real-world data, our method achieves strong performance with only 10-20 real demonstrations by reducing the complex sim-to-real problem to a structured perception alignment task. We validate our approach on tabletop manipulation tasks, demonstrating superior data efficiency and out-of-distribution generalization compared to end-to-end baselines. The learned policies successfully handle object positions and scales beyond the training distribution, confirming that decoupling perception from control fundamentally improves sim-to-real transfer.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2509.10951.pdf' target='_blank'>https://arxiv.org/pdf/2509.10951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Wilkinghoff, Haici Yang, Janek Ebbers, FranÃ§ois G. Germain, Gordon Wichern, Jonathan Le Roux
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10951">Local Density-Based Anomaly Score Normalization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art anomalous sound detection (ASD) systems in domain-shifted conditions rely on projecting audio signals into an embedding space and using distance-based outlier detection to compute anomaly scores. One of the major difficulties to overcome is the so-called domain mismatch between the anomaly score distributions of a source domain and a target domain that differ acoustically and in terms of the amount of training data provided. A decision threshold that is optimal for one domain may be highly sub-optimal for the other domain and vice versa. This significantly degrades the performance when only using a single decision threshold, as is required when generalizing to multiple data domains that are possibly unseen during training while still using the same trained ASD system as in the source domain. To reduce this mismatch between the domains, we propose a simple local-density-based anomaly score normalization scheme. In experiments conducted on several ASD datasets, we show that the proposed normalization scheme consistently improves performance for various types of embedding-based ASD systems and yields better results than existing anomaly score normalization approaches.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2505.04979.pdf' target='_blank'>https://arxiv.org/pdf/2505.04979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Qi, Sijin Zhou, Lei Meng, Han Hu, Han Yu, Xiangxu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04979">Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attribute bias in federated learning (FL) typically leads local models to optimize inconsistently due to the learning of non-causal associations, resulting degraded performance. Existing methods either use data augmentation for increasing sample diversity or knowledge distillation for learning invariant representations to address this problem. However, they lack a comprehensive analysis of the inference paths, and the interference from confounding factors limits their performance. To address these limitations, we propose the \underline{Fed}erated \underline{D}econfounding and \underline{D}ebiasing \underline{L}earning (FedDDL) method. It constructs a structured causal graph to analyze the model inference process, and performs backdoor adjustment to eliminate confounding paths. Specifically, we design an intra-client deconfounding learning module for computer vision tasks to decouple background and objects, generating counterfactual samples that establish a connection between the background and any label, which stops the model from using the background to infer the label. Moreover, we design an inter-client debiasing learning module to construct causal prototypes to reduce the proportion of the background in prototype components. Notably, it bridges the gap between heterogeneous representations via causal prototypical regularization. Extensive experiments on 2 benchmarking datasets demonstrate that \methodname{} significantly enhances the model capability to focus on main objects in unseen data, leading to 4.5\% higher Top-1 Accuracy on average over 9 state-of-the-art existing methods.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2504.01512.pdf' target='_blank'>https://arxiv.org/pdf/2504.01512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01512">High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently single-view 3D generation via Gaussian splatting has emerged and developed quickly. They learn 3D Gaussians from 2D RGB images generated from pre-trained multi-view diffusion (MVD) models, and have shown a promising avenue for 3D generation through a single image. Despite the current progress, these methods still suffer from the inconsistency jointly caused by the geometric ambiguity in the 2D images, and the lack of structure of 3D Gaussians, leading to distorted and blurry 3D object generation. In this paper, we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian Reconstruction Model designed to generate high-fidelity 3D objects from single-view images. Our key insight is a structured 3D representation can simultaneously mitigate the afore-mentioned two issues. To this end, we propose a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation contains explicit 3D geometric information, eliminating the geometric ambiguity from 2D images. It also structures Gaussians during learning so that the optimization tends to find better local optima. Our 3D voxel representation is obtained by a fusion module that aligns RGB features and surface normal features, both of which can be estimated from 2D images. Extensive experiments demonstrate the superiority of our methods over prior works in terms of high-quality reconstruction results, robust generalization, and good efficiency.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2504.00850.pdf' target='_blank'>https://arxiv.org/pdf/2504.00850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Qi, Runhui Zhang, Lei Meng, Wei Wu, Yachong Zhang, Xiangxu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00850">Global Intervention and Distillation for Federated Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attribute skew in federated learning leads local models to focus on learning non-causal associations, guiding them towards inconsistent optimization directions, which inevitably results in performance degradation and unstable convergence. Existing methods typically leverage data augmentation to enhance sample diversity or employ knowledge distillation to learn invariant representations. However, the instability in the quality of generated data and the lack of domain information limit their performance on unseen samples. To address these issues, this paper presents a global intervention and distillation method, termed FedGID, which utilizes diverse attribute features for backdoor adjustment to break the spurious association between background and label. It includes two main modules, where the global intervention module adaptively decouples objects and backgrounds in images, injects background information into random samples to intervene in the sample distribution, which links backgrounds to all categories to prevent the model from treating background-label associations as causal. The global distillation module leverages a unified knowledge base to guide the representation learning of client models, preventing local models from overfitting to client-specific attributes. Experimental results on three datasets demonstrate that FedGID enhances the model's ability to focus on the main subjects in unseen data and outperforms existing methods in collaborative modeling.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2409.13527.pdf' target='_blank'>https://arxiv.org/pdf/2409.13527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avi Deb Raha, Apurba Adhikary, Mrityunjoy Gain, Yu Qiao, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13527">Boosting Federated Domain Generalization: Understanding the Role of Advanced Pre-Trained Architectures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we explore the efficacy of advanced pre-trained architectures, such as Vision Transformers (ViT), ConvNeXt, and Swin Transformers in enhancing Federated Domain Generalization. These architectures capture global contextual features and model long-range dependencies, making them promising candidates for improving cross-domain generalization. We conduct a broad study with in-depth analysis and systematically evaluate different variants of these architectures, using extensive pre-training datasets such as ImageNet-1K, ImageNet-21K, JFT-300M, and ImageNet-22K. Additionally, we compare self-supervised and supervised pre-training strategies to assess their impact on FDG performance. Our findings suggest that self-supervised techniques, which focus on reconstructing masked image patches, can better capture the intrinsic structure of images, thereby outperforming their supervised counterparts. Comprehensive evaluations on the Office-Home and PACS datasets demonstrate that adopting advanced architectures pre-trained on larger datasets establishes new benchmarks, achieving average accuracies of 84.46\% and 92.55\%, respectively. Additionally, we observe that certain variants of these advanced models, despite having fewer parameters, outperform larger ResNet models. This highlights the critical role of utilizing sophisticated architectures and diverse pre-training strategies to enhance FDG performance, especially in scenarios with limited computational resources where model efficiency is crucial. Our results indicate that federated learning systems can become more adaptable and efficient by leveraging these advanced methods, offering valuable insights for future research in FDG.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2601.19700.pdf' target='_blank'>https://arxiv.org/pdf/2601.19700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Su, Haoyuan Wang, Xiaohua Feng, Yunshan Ma, Xiaobo Xia, Yuyuan Li, Xiaolin Zheng, Jianmao Xiao, Chaochao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19700">Out-of-Distribution Generalization via Invariant Trajectories for Multimodal Large Language Model Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge editing emerges as a crucial technique for efficiently correcting incorrect or outdated knowledge in large language models (LLM). Existing editing methods for unimodal LLM rely on a rigid parameter-to-output mapping, which causes causal-underfit and causal-overfit in cascaded reasoning for Multimodal LLM (MLLM). In this paper, we reformulate MLLM editing as an out-of-distribution (OOD) generalization problem, where the goal is to discern semantic shift with factual shift and thus achieve robust editing among diverse cross-modal prompting. The key challenge of this OOD problem lies in identifying invariant causal trajectories that generalize accurately while suppressing spurious correlations. To address it, we propose ODEdit, a plug-and-play invariant learning based framework that optimizes the tripartite OOD risk objective to simultaneously enhance editing reliability, locality, and generality.We further introduce an edit trajectory invariant learning method, which integrates a total variation penalty into the risk minimization objective to stabilize edit trajectories against environmental variations. Theoretical analysis and extensive experiments demonstrate the effectiveness of ODEdit.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2512.03318.pdf' target='_blank'>https://arxiv.org/pdf/2512.03318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chandler Smith, Marwa Abdulhai, Manfred Diaz, Marko Tesic, Rakshit S. Trivedi, Alexander Sasha Vezhnevets, Lewis Hammond, Jesse Clifton, Minsuk Chang, Edgar A. Duéñez-Guzmán, John P. Agapiou, Jayd Matyas, Danny Karmon, Akash Kundu, Aliaksei Korshuk, Ananya Ananya, Arrasy Rahman, Avinaash Anand Kulandaivel, Bain McHale, Beining Zhang, Buyantuev Alexander, Carlos Saith Rodriguez Rojas, Caroline Wang, Chetan Talele, Chenao Liu, Chichen Lin, Diana Riazi, Di Yang Shi, Emanuel Tewolde, Elizaveta Tennant, Fangwei Zhong, Fuyang Cui, Gang Zhao, Gema Parreño Piqueras, Hyeonggeun Yun, Ilya Makarov, Jiaxun Cui, Jebish Purbey, Jim Dilkes, Jord Nguyen, Lingyun Xiao, Luis Felipe Giraldo, Manuela Chacon-Chamorro, Manuel Sebastian Rios Beltran, Marta Emili García Segura, Mengmeng Wang, Mogtaba Alim, Nicanor Quijano, Nico Schiavone, Olivia Macmillan-Scott, Oswaldo Peña, Peter Stone, Ram Mohan Rao Kadiyala, Rolando Fernandez, Ruben Manrique, Sunjia Lu, Sheila A. McIlraith, Shamika Dhuri, Shuqing Shi, Siddhant Gupta, Sneheel Sarangi, Sriram Ganapathi Subramanian, Taehun Cha, Toryn Q. Klassen, Wenming Tu, Weijian Fan, Wu Ruiyang, Xue Feng, Yali Du, Yang Liu, Yiding Wang, Yipeng Kang, Yoonchang Sung, Yuxuan Chen, Zhaowei Zhang, Zhihan Wang, Zhiqiang Wu, Ziang Chen, Zilong Zheng, Zixia Jia, Ziyan Wang, Dylan Hadfield-Menell, Natasha Jaques, Tim Baarslag, Jose Hernandez-Orallo, Joel Z. Leibo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03318">Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2511.18810.pdf' target='_blank'>https://arxiv.org/pdf/2511.18810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxia Fu, Zhizhen Zhang, Yuqi Zhang, Zijian Wang, Zi Huang, Yadan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18810">MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2509.26045.pdf' target='_blank'>https://arxiv.org/pdf/2509.26045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aoming Liu, Kevin Miller, Venkatesh Saligrama, Kate Saenko, Boqing Gong, Ser-Nam Lim, Bryan A. Plummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26045">Scaling Up Temporal Domain Generalization via Temporal Experts Averaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal Domain Generalization (TDG) aims to generalize across temporal distribution shifts, e.g., lexical change over time. Prior work often addresses this by predicting future model weights. However, full model prediction is prohibitively expensive for even reasonably sized models. Thus, recent methods only predict the classifier layer, limiting generalization by failing to adjust other model components. To address this, we propose Temporal Experts Averaging (TEA), a novel and scalable TDG framework that updates the entire model using weight averaging to maximize generalization potential while minimizing computational costs. Our theoretical analysis guides us to two steps that enhance generalization to future domains. First, we create expert models with functional diversity yet parameter similarity by fine-tuning a domain-agnostic base model on individual temporal domains while constraining weight changes. Second, we optimize the bias-variance tradeoff through adaptive averaging coefficients derived from modeling temporal weight trajectories in a principal component subspace. Expert's contributions are based on their projected proximity to future domains. Extensive experiments across 7 TDG benchmarks, 5 models, and 2 TDG settings shows TEA outperforms prior TDG methods by up to 69% while being up to 60x more efficient.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2508.11277.pdf' target='_blank'>https://arxiv.org/pdf/2508.11277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11277">Probing the Representational Power of Sparse Autoencoders in Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2503.13966.pdf' target='_blank'>https://arxiv.org/pdf/2503.13966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Zhang, Yanyuan Qiao, Qunbo Wang, Longteng Guo, Zhihua Wei, Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13966">FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aspiration of the Vision-and-Language Navigation (VLN) task has long been to develop an embodied agent with robust adaptability, capable of seamlessly transferring its navigation capabilities across various tasks. Despite remarkable advancements in recent years, most methods necessitate dataset-specific training, thereby lacking the capability to generalize across diverse datasets encompassing distinct types of instructions. Large language models (LLMs) have demonstrated exceptional reasoning and generalization abilities, exhibiting immense potential in robot action planning. In this paper, we propose FlexVLN, an innovative hierarchical approach to VLN that integrates the fundamental navigation ability of a supervised-learning-based Instruction Follower with the robust generalization ability of the LLM Planner, enabling effective generalization across diverse VLN datasets. Moreover, a verification mechanism and a multi-model integration mechanism are proposed to mitigate potential hallucinations by the LLM Planner and enhance execution accuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as out-of-domain datasets for assessing generalization ability. The generalization performance of FlexVLN surpasses that of all the previous methods to a large extent.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2503.13935.pdf' target='_blank'>https://arxiv.org/pdf/2503.13935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Yuan, Yuxia Fu, Zijian Wang, Yadan Luo, Zi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13935">SCORE: Soft Label Compression-Centric Dataset Condensation via Coding Rate Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dataset Condensation (DC) aims to obtain a condensed dataset that allows models trained on the condensed dataset to achieve performance comparable to those trained on the full dataset. Recent DC approaches increasingly focus on encoding knowledge into realistic images with soft labeling, for their scalability to ImageNet-scale datasets and strong capability of cross-domain generalization. However, this strong performance comes at a substantial storage cost which could significantly exceed the storage cost of the original dataset. We argue that the three key properties to alleviate this performance-storage dilemma are informativeness, discriminativeness, and compressibility of the condensed data. Towards this end, this paper proposes a \textbf{S}oft label compression-centric dataset condensation framework using \textbf{CO}ding \textbf{R}at\textbf{E} (SCORE). SCORE formulates dataset condensation as a min-max optimization problem, which aims to balance the three key properties from an information-theoretic perspective. In particular, we theoretically demonstrate that our coding rate-inspired objective function is submodular, and its optimization naturally enforces low-rank structure in the soft label set corresponding to each condensed data. Extensive experiments on large-scale datasets, including ImageNet-1K and Tiny-ImageNet, demonstrate that SCORE outperforms existing methods in most cases. Even with 30$\times$ compression of soft labels, performance decreases by only 5.5\% and 2.7\% for ImageNet-1K with IPC 10 and 50, respectively. Code will be released upon paper acceptance.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2412.02856.pdf' target='_blank'>https://arxiv.org/pdf/2412.02856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Bryan A. Plummer, Kate Saenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02856">Is Large-Scale Pretraining the Secret to Good Domain Generalization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Source Domain Generalization (DG) is the task of training on multiple source domains and achieving high classification performance on unseen target domains. Recent methods combine robust features from web-scale pretrained backbones with new features learned from source data, and this has dramatically improved benchmark results. However, it remains unclear if DG finetuning methods are becoming better over time, or if improved benchmark performance is simply an artifact of stronger pre-training. Prior studies have shown that perceptual similarity to pre-training data correlates with zero-shot performance, but we find the effect limited in the DG setting. Instead, we posit that having perceptually similar data in pretraining is not enough; and that it is how well these data were learned that determines performance. This leads us to introduce the Alignment Hypothesis, which states that the final DG performance will be high if and only if alignment of image and class label text embeddings is high. Our experiments confirm the Alignment Hypothesis is true, and we use it as an analysis tool of existing DG methods evaluated on DomainBed datasets by splitting evaluation data into In-pretraining (IP) and Out-of-pretraining (OOP). We show that all evaluated DG methods struggle on DomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, our findings highlight the need for DG methods which can generalize beyond pretraining alignment.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2409.13787.pdf' target='_blank'>https://arxiv.org/pdf/2409.13787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Hu, Chenwei Zhang, Min Yang, Xiaodan Liang, Chengming Li, Xiping Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13787">Learning to Generalize Unseen Domains via Multi-Source Meta Learning for Text Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of deep learning methods, there have been many breakthroughs in the field of text classification. Models developed for this task have been shown to achieve high accuracy. However, most of these models are trained using labeled data from seen domains. It is difficult for these models to maintain high accuracy in a new challenging unseen domain, which is directly related to the generalization of the model. In this paper, we study the multi-source Domain Generalization of text classification and propose a framework to use multiple seen domains to train a model that can achieve high accuracy in an unseen domain. Specifically, we propose a multi-source meta-learning Domain Generalization framework to simulate the process of model generalization to an unseen domain, so as to extract sufficient domain-related features. We introduced a memory mechanism to store domain-specific features, which coordinate with the meta-learning framework. Besides, we adopt the novel "jury" mechanism that enables the model to learn sufficient domain-invariant features. Experiments demonstrate that our meta-learning framework can effectively enhance the ability of the model to generalize to an unseen domain and can outperform the state-of-the-art methods on multi-source text classification datasets.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2512.21675.pdf' target='_blank'>https://arxiv.org/pdf/2512.21675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21675">UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2512.11686.pdf' target='_blank'>https://arxiv.org/pdf/2512.11686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhang, Han Wan, Yang Liu, Hao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11686">Stable spectral neural operator for learning stiff PDE systems from limited data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate modeling of spatiotemporal dynamics is crucial to understanding complex phenomena across science and engineering. However, this task faces a fundamental challenge when the governing equations are unknown and observational data are sparse. System stiffness, the coupling of multiple time-scales, further exacerbates this problem and hinders long-term prediction. Existing methods fall short: purely data-driven methods demand massive datasets, whereas physics-aware approaches are constrained by their reliance on known equations and fine-grained time steps. To overcome these limitations, we introduce an equation-free learning framework, namely, the Stable Spectral Neural Operator (SSNO), for modeling stiff partial differential equation (PDE) systems based on limited data. Instead of encoding specific equation terms, SSNO embeds spectrally inspired structures in its architecture, yielding strong inductive biases for learning the underlying physics. It automatically learns local and global spatial interactions in the frequency domain, while handling system stiffness with a robust integrating factor time-stepping scheme. Demonstrated across multiple 2D and 3D benchmarks in Cartesian and spherical geometries, SSNO achieves prediction errors one to two orders of magnitude lower than leading models. Crucially, it shows remarkable data efficiency, requiring only very few (2--5) training trajectories for robust generalization to out-of-distribution conditions. This work offers a robust and generalizable approach to learning stiff spatiotemporal dynamics from limited data without explicit \textit{a priori} knowledge of PDE terms.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2512.08071.pdf' target='_blank'>https://arxiv.org/pdf/2512.08071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingchuan Ma, Chengshuai Zhao, Bohan Jiang, Saketh Vishnubhatla, Ujun Jeong, Alimohammad Beigi, Adrienne Raglin, Huan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08071">CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2506.10145.pdf' target='_blank'>https://arxiv.org/pdf/2506.10145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajeev Yasarla, Shizhong Han, Hsin-Pai Cheng, Litian Liu, Shweta Mahajan, Apratim Bhattacharyya, Yunxiao Shi, Risheek Garrepalli, Hong Cai, Fatih Porikli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10145">RoCA: Robust Cross-Domain End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end (E2E) autonomous driving has recently emerged as a new paradigm, offering significant potential. However, few studies have looked into the practical challenge of deployment across domains (e.g., cities). Although several works have incorporated Large Language Models (LLMs) to leverage their open-world knowledge, LLMs do not guarantee cross-domain driving performance and may incur prohibitive retraining costs during domain adaptation. In this paper, we propose RoCA, a novel framework for robust cross-domain E2E autonomous driving. RoCA formulates the joint probabilistic distribution over the tokens that encode ego and surrounding vehicle information in the E2E pipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of basis tokens with corresponding trajectories, which span diverse driving scenarios. Then, given any driving scene, it is able to probabilistically infer the future trajectory. By using RoCA together with a base E2E model in source-domain training, we improve the generalizability of the base model, without requiring extra inference computation. In addition, RoCA enables robust adaptation on new target domains, significantly outperforming direct finetuning. We extensively evaluate RoCA on various cross-domain scenarios and show that it achieves strong domain generalization and adaptation performance.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2505.15734.pdf' target='_blank'>https://arxiv.org/pdf/2505.15734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav Srivastava, Zhenyu Bi, Meng Lu, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15734">DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on five reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2505.15447.pdf' target='_blank'>https://arxiv.org/pdf/2505.15447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, Chong Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15447">ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding is inherently intention-driven-humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15\% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2505.13519.pdf' target='_blank'>https://arxiv.org/pdf/2505.13519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Cai, Yiheng Yao, Guangji Bai, Renhe Jiang, Xuan Song, Ryosuke Shibasaki, Liang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13519">Continuous Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world data distributions often shift continuously across multiple latent factors such as time, geography, and socioeconomic context. However, existing domain generalization approaches typically treat domains as discrete or evolving along a single axis (e.g., time), which fails to capture the complex, multi-dimensional nature of real-world variation. This paper introduces the task of Continuous Domain Generalization (CDG), which aims to generalize predictive models to unseen domains defined by arbitrary combinations of continuous variation descriptors. We present a principled framework grounded in geometric and algebraic theory, showing that optimal model parameters across domains lie on a low-dimensional manifold. To model this structure, we propose a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter transitions by enforcing geometric continuity and algebraic consistency. To handle noisy or incomplete domain descriptors, we introduce a gating mechanism to suppress irrelevant dimensions and a local chart-based strategy for robust generalization. Extensive experiments on synthetic and real-world datasets-including remote sensing, scientific documents, and traffic forecasting-demonstrate that our method significantly outperforms existing baselines in generalization accuracy and robustness under descriptor imperfections.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2502.02247.pdf' target='_blank'>https://arxiv.org/pdf/2502.02247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bangzhen Liu, Chenxi Zheng, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02247">Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vulnerability of 3D point cloud analysis to unpredictable rotations poses an open yet challenging problem: orientation-aware 3D domain generalization. Cross-domain robustness and adaptability of 3D representations are crucial but not easily achieved through rotation augmentation. Motivated by the inherent advantages of intricate orientations in enhancing generalizability, we propose an innovative rotation-adaptive domain generalization framework for 3D point cloud analysis. Our approach aims to alleviate orientational shifts by leveraging intricate samples in an iterative learning process. Specifically, we identify the most challenging rotation for each point cloud and construct an intricate orientation set by optimizing intricate orientations. Subsequently, we employ an orientation-aware contrastive learning framework that incorporates an orientation consistency loss and a margin separation loss, enabling effective learning of categorically discriminative and generalizable features with rotation consistency. Extensive experiments and ablations conducted on 3D cross-domain benchmarks firmly establish the state-of-the-art performance of our proposed approach in the context of orientation-aware 3D domain generalization.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2601.19127.pdf' target='_blank'>https://arxiv.org/pdf/2601.19127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhilong Zhang, Lei Zhang, Qing He, Shuyin Xia, Guoyin Wang, Fuxiang Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19127">Implicit Non-Causal Factors are Out via Dataset Splitting for Domain Generalization Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2601.11266.pdf' target='_blank'>https://arxiv.org/pdf/2601.11266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aoshen Huang, Jiaming Chen, Jiyu Cheng, Ran Song, Wei Pan, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.11266">Skill-Aware Diffusion for Generalizable Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2512.08508.pdf' target='_blank'>https://arxiv.org/pdf/2512.08508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengmo Zhou, Feng Yu, Wenda Wang, Zhifeng Gao, Guolin Ke, Zhewei Wei, Zhen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08508">Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enzymes are crucial catalysts that enable a wide range of biochemical reactions. Efficiently identifying specific enzymes from vast protein libraries is essential for advancing biocatalysis. Traditional computational methods for enzyme screening and retrieval are time-consuming and resource-intensive. Recently, deep learning approaches have shown promise. However, these methods focus solely on the interaction between enzymes and reactions, overlooking the inherent hierarchical relationships within each domain. To address these limitations, we introduce FGW-CLIP, a novel contrastive learning framework based on optimizing the fused Gromov-Wasserstein distance. FGW-CLIP incorporates multiple alignments, including inter-domain alignment between reactions and enzymes and intra-domain alignment within enzymes and reactions. By introducing a tailored regularization term, our method minimizes the Gromov-Wasserstein distance between enzyme and reaction spaces, which enhances information integration across these domains. Extensive evaluations demonstrate the superiority of FGW-CLIP in challenging enzyme-reaction tasks. On the widely-used EnzymeMap benchmark, FGW-CLIP achieves state-of-the-art performance in enzyme virtual screening, as measured by BEDROC and EF metrics. Moreover, FGW-CLIP consistently outperforms across all three splits of ReactZyme, the largest enzyme-reaction benchmark, demonstrating robust generalization to novel enzymes and reactions. These results position FGW-CLIP as a promising framework for enzyme discovery in complex biochemical settings, with strong adaptability across diverse screening scenarios.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2510.16822.pdf' target='_blank'>https://arxiv.org/pdf/2510.16822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yahia Battach, Abdulwahab Felemban, Faizan Farooq Khan, Yousef A. Radwan, Xiang Li, Fabio Marchese, Sara Beery, Burton H. Jones, Francesca Benzoni, Mohamed Elhoseiny
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16822">ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coral reefs are rapidly declining due to anthropogenic pressures such as climate change, underscoring the urgent need for scalable, automated monitoring. We introduce ReefNet, a large public coral reef image dataset with point-label annotations mapped to the World Register of Marine Species (WoRMS). ReefNet aggregates imagery from 76 curated CoralNet sources and an additional site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level hard coral annotations with expert-verified labels. Unlike prior datasets, which are often limited by size, geography, or coarse labels and are not ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global scale to WoRMS. We propose two evaluation settings: (i) a within-source benchmark that partitions each source's images for localized evaluation, and (ii) a cross-source benchmark that withholds entire sources to test domain generalization. We analyze both supervised and zero-shot classification performance on ReefNet and find that while supervised within-source performance is promising, supervised performance drops sharply across domains, and performance is low across the board for zero-shot models, especially for rare and visually similar genera. This provides a challenging benchmark intended to catalyze advances in domain generalization and fine-grained coral classification. We will release our dataset, benchmarking code, and pretrained models to advance robust, domain-adaptive, global coral reef monitoring and conservation.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2509.22115.pdf' target='_blank'>https://arxiv.org/pdf/2509.22115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Wang, Tao Yang, Hongtao Tian, Yunsheng Shi, Qiyao Ma, Xiaotao Liu, Ting Yao, Wenbo Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22115">Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling (D$^3$S)} framework that prioritizes the most informative samples and tokens across groups to improve the efficient of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\text{Var}(A)$). We theoretically proven that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance and generalization while requiring \textit{fewer} samples and tokens across diverse reasoning benchmarks. Our code is added in the supplementary materials and will be made publicly available.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2509.13956.pdf' target='_blank'>https://arxiv.org/pdf/2509.13956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Yang, Zengqi Peng, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13956">SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous parking is a critical component for achieving safe and efficient urban autonomous driving. However, unstructured environments and dynamic interactions pose significant challenges to autonomous parking tasks. To address this problem, we propose SEG-Parking, a novel end-to-end offline reinforcement learning (RL) framework to achieve interaction-aware autonomous parking. Notably, a specialized parking dataset is constructed for parking scenarios, which include those without interference from the opposite vehicle (OV) and complex ones involving interactions with the OV. Based on this dataset, a goal-conditioned state encoder is pretrained to map the fused perception information into the latent space. Then, an offline RL policy is optimized with a conservative regularizer that penalizes out-of-distribution actions. Extensive closed-loop experiments are conducted in the high-fidelity CARLA simulator. Comparative results demonstrate the superior performance of our framework with the highest success rate and robust generalization to out-of-distribution parking scenarios. The related dataset and source code will be made publicly available after the paper is accepted.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2506.23726.pdf' target='_blank'>https://arxiv.org/pdf/2506.23726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bartlomiej Sobieski, Matthew Tivnan, Yuang Wang, Siyeop Yoon, Pengfei Jin, Dufan Wu, Quanzheng Li, Przemyslaw Biecek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23726">System-Embedded Diffusion Bridge Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solving inverse problems -- recovering signals from incomplete or noisy measurements -- is fundamental in science and engineering. Score-based generative models (SGMs) have recently emerged as a powerful framework for this task. Two main paradigms have formed: unsupervised approaches that adapt pretrained generative models to inverse problems, and supervised bridge methods that train stochastic processes conditioned on paired clean and corrupted data. While the former typically assume knowledge of the measurement model, the latter have largely overlooked this structural information. We introduce System embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge methods that explicitly embed the known linear measurement system into the coefficients of a matrix-valued SDE. This principled integration yields consistent improvements across diverse linear inverse problems and demonstrates robust generalization under system misspecification between training and deployment, offering a promising solution to real-world applications.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2501.05057.pdf' target='_blank'>https://arxiv.org/pdf/2501.05057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zengqi Peng, Yubin Wang, Xu Han, Lei Zheng, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05057">LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in reinforcement learning (RL) demonstrate the significant potential in autonomous driving. Despite this promise, challenges such as the manual design of reward functions and low sample efficiency in complex environments continue to impede the development of safe and effective driving policies. To tackle these issues, we introduce LearningFlow, an innovative automated policy learning workflow tailored to urban driving. This framework leverages the collaboration of multiple large language model (LLM) agents throughout the RL training process. LearningFlow includes a curriculum sequence generation process and a reward generation process, which work in tandem to guide the RL policy by generating tailored training curricula and reward functions. Particularly, each process is supported by an analysis agent that evaluates training progress and provides critical insights to the generation agent. Through the collaborative efforts of these LLM agents, LearningFlow automates policy learning across a series of complex driving tasks, and it significantly reduces the reliance on manual reward function design while enhancing sample efficiency. Comprehensive experiments are conducted in the high-fidelity CARLA simulator, along with comparisons with other existing methods, to demonstrate the efficacy of our proposed approach. The results demonstrate that LearningFlow excels in generating rewards and curricula. It also achieves superior performance and robust generalization across various driving tasks, as well as commendable adaptation to different RL algorithms.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2410.23156.pdf' target='_blank'>https://arxiv.org/pdf/2410.23156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, JoÃ£o F. Henriques, Kevin Ellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23156">VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2512.02369.pdf' target='_blank'>https://arxiv.org/pdf/2512.02369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingmei Li, Yang Zhang, Peifeng Zhang, Haohuan Fu, Juepeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02369">SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \textbf{S}tyle-\textbf{A}daptive \textbf{GE}neralization framework (\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2510.12266.pdf' target='_blank'>https://arxiv.org/pdf/2510.12266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Han, Huanyu Wang, Zeyu Zhang, Xiangxiang Dai, Xutong Liu, John C. S. Lui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12266">HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-Rank Adaptation (LoRA) has emerged as a widely used technique for adapting large language models (LLMs) to new domains, due to its modular design and broad availability on platforms such as HuggingFace. This availability has motivated efforts to reuse existing LoRAs for domain generalization. However, existing methods often rely on explicit task labels or additional training, which are impractical for deployment. Moreover, they typically activate a fixed number of entire LoRA modules, leading to parameter redundancy or insufficiency that degrade performance. In this paper, we propose \texttt{HiLoRA}, a training-free framework that performs adaptive hierarchical routing over LoRA pools. Drawing on structural properties of LoRA, we define rank-one components (ROCs), in which each rank parameter is regarded as an independent unit. For a given input sequence, \texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their ROC allocation based on Gaussian likelihoods at the sequence level. At the token level, it further refines routing by activating only the most informative ROCs. We further provide theoretical guarantees that \texttt{HiLoRA} selects the most relevant LoRAs with high probability. Extensive experiments show that \texttt{HiLoRA} achieves substantial improvements in domain generalization, with accuracy gains of up to {\small $55\%$} over state-of-the-art baselines, while maintaining comparable inference throughput.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2509.14420.pdf' target='_blank'>https://arxiv.org/pdf/2509.14420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Lin, Xiaolin Wu, Xi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14420">Class-invariant Test-Time Augmentation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep models often suffer significant performance degradation under distribution shifts. Domain generalization (DG) seeks to mitigate this challenge by enabling models to generalize to unseen domains. Most prior approaches rely on multi-domain training or computationally intensive test-time adaptation. In contrast, we propose a complementary strategy: lightweight test-time augmentation. Specifically, we develop a novel Class-Invariant Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple variants of each input image through elastic and grid deformations that nevertheless belong to the same class as the original input. Their predictions are aggregated through a confidence-guided filtering scheme that remove unreliable outputs, ensuring the final decision relies on consistent and trustworthy cues. Extensive Experiments on PACS and Office-Home datasets demonstrate consistent gains across different DG algorithms and backbones, highlighting the effectiveness and generality of our approach.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2507.02291.pdf' target='_blank'>https://arxiv.org/pdf/2507.02291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyu Zhang, Lingyi Wang, Wei Wu, Fuhui Zhou, Qihui Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02291">Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven semantic communication is based on superficial statistical patterns, thereby lacking interpretability and generalization, especially for applications with the presence of unseen data. To address these challenges, we propose a novel knowledge graph-enhanced zero-shot semantic communication (KGZS-SC) network. Guided by the structured semantic information from a knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides generalized semantic representations and enables reasoning for unseen cases. Specifically, the KG-SKB aligns the semantic features in a shared category semantics embedding space and enhances the generalization ability of the transmitter through aligned semantic features, thus reducing communication overhead by selectively transmitting compact visual semantics. At the receiver, zero-shot learning (ZSL) is leveraged to enable direct classification for unseen cases without the demand for retraining or additional computational overhead, thereby enhancing the adaptability and efficiency of the classification process in dynamic or resource-constrained environments. The simulation results conducted on the APY datasets show that the proposed KGZS-SC network exhibits robust generalization and significantly outperforms existing SC frameworks in classifying unseen categories across a range of SNR levels.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2506.06566.pdf' target='_blank'>https://arxiv.org/pdf/2506.06566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Bao, Chuanbing Huo, Qinyu Chen, Chang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06566">AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny, tailored for low-resource deployment on edge devices. Our approach introduces a hybrid training strategy that systematically combines standard and aphasic speech at varying ratios, enabling robust generalization, and a GPT-4-based reference enhancement method that refines noisy aphasic transcripts, improving supervision quality. We conduct extensive experiments across multiple data mixing configurations and evaluation settings. Results show that our fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30% while preserving performance on standard speech. The proposed framework offers a scalable, efficient solution for real-world disordered speech recognition.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2504.09601.pdf' target='_blank'>https://arxiv.org/pdf/2504.09601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Wei, Xiaoqi Zhao, Jonghye Woo, Jinsong Ouyang, Georges El Fakhri, Qingyu Chen, Xiaofeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09601">Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning methods that encode shape priors often suffer from limited representational power with a small set of offline computed shape elements, or overfitting when the dictionary size grows. Moreover, they are not readily compatible with large foundation models such as the Segment Anything Model (SAM). In this paper, we propose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly integrates the idea of mixture-of-experts (MoE) training into dictionary learning to efficiently capture diverse and robust shape priors. Our method conceptualizes each dictionary atom as a shape expert, which specializes in encoding distinct semantic shape information. A gating network dynamically fuses these shape experts into a robust shape map, with sparse activation guided by SAM encoding to prevent overfitting. We further provide this shape map as a prompt to SAM, utilizing the powerful generalization capability of SAM through bidirectional integration. All modules, including the shape dictionary, are trained in an end-to-end manner. Extensive experiments on multiple public datasets demonstrate its effectiveness.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2601.19461.pdf' target='_blank'>https://arxiv.org/pdf/2601.19461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19461">Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous UAV forestry operations require robust depth estimation with strong cross-domain generalization, yet existing evaluations focus on urban and indoor scenarios, leaving a critical gap for vegetation-dense environments. We present the first systematic zero-shot evaluation of eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms. All methods use officially released pretrained weights (trained on Scene Flow) and are evaluated on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury Tree Branches dataset ($1920 \times 1080$). Results reveal scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 4.65 px on Middlebury), while iterative methods show variable cross-benchmark performance (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury; IGEV: 0.33 px on ETH3D but 4.99 px on Middlebury). Qualitative evaluation on the Tree Branches dataset establishes DEFOM as the gold-standard baseline for vegetation depth estimation, with superior cross-domain consistency (consistently ranking 1st-2nd across benchmarks, average rank 1.75). DEFOM predictions will serve as pseudo-ground-truth for future benchmarking.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2601.08876.pdf' target='_blank'>https://arxiv.org/pdf/2601.08876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Chen, Hao Chen, Yuanchen Bei, Tianyang Zhao, Zhibo Zhou, Feiran Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08876">The Semantic Lifecycle in Embodied AI: Acquisition, Representation and Storage via Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic information in embodied AI is inherently multi-source and multi-stage, making it challenging to fully leverage for achieving stable perception-to-action loops in real-world environments. Early studies have combined manual engineering with deep neural networks, achieving notable progress in specific semantic-related embodied tasks. However, as embodied agents encounter increasingly complex environments and open-ended tasks, the demand for more generalizable and robust semantic processing capabilities has become imperative. Recent advances in foundation models (FMs) address this challenge through their cross-domain generalization abilities and rich semantic priors, reshaping the landscape of embodied AI research. In this survey, we propose the Semantic Lifecycle as a unified framework to characterize the evolution of semantic knowledge within embodied AI driven by foundation models. Departing from traditional paradigms that treat semantic processing as isolated modules or disjoint tasks, our framework offers a holistic perspective that captures the continuous flow and maintenance of semantic knowledge. Guided by this embodied semantic lifecycle, we further analyze and compare recent advances across three key stages: acquisition, representation, and storage. Finally, we summarize existing challenges and outline promising directions for future research.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2512.03427.pdf' target='_blank'>https://arxiv.org/pdf/2512.03427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03427">Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization. However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments. We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms. All methods are trained exclusively on Scene Flow and evaluated without fine-tuning on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera (1920x1080). Performance reveals scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D, 0.83-1.07 px on KITTI; DEFOM: 0.35-4.65 px across benchmarks), while iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px; IGEV: 0.33-21.91 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98 percent error rate) due to negative disparity predictions, while performing normally on KITTI (0.90-1.11 px). Qualitative evaluation on Canterbury forestry dataset identifies DEFOM as the optimal gold-standard baseline for vegetation depth estimation, exhibiting superior depth smoothness, occlusion handling, and cross-domain consistency compared to IGEV++, despite IGEV++'s finer detail preservation.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2510.10480.pdf' target='_blank'>https://arxiv.org/pdf/2510.10480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zishen Zhang, Xiangzhe Kong, Wenbing Huang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10480">Latent Retrieval Augmented Generation of Cross-Domain Protein Binders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing protein binders targeting specific sites, which requires to generate realistic and functional interaction patterns, is a fundamental challenge in drug discovery. Current structure-based generative models are limited in generating nterfaces with sufficient rationality and interpretability. In this paper, we propose Retrieval-Augmented Diffusion for Aligned interface (RADiAnce), a new framework that leverages known interfaces to guide the design of novel binders. By unifying retrieval and generation in a shared contrastive latent space, our model efficiently identifies relevant interfaces for a given binding site and seamlessly integrates them through a conditional latent diffusion generator, enabling cross-domain interface transfer. Extensive exeriments show that RADiAnce significantly outperforms baseline models across multiple metrics, including binding affinity and recovery of geometries and interactions. Additional experimental results validate cross-domain generalization, demonstrating that retrieving interfaces from diverse domains, such as peptides, antibodies, and protein fragments, enhances the generation performance of binders for other domains. Our work establishes a new paradigm for protein binder design that successfully bridges retrieval-based knowledge and generative AI, opening new possibilities for drug discovery.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2510.04243.pdf' target='_blank'>https://arxiv.org/pdf/2510.04243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jincan Lou, Jingkun Chen, Haoquan Li, Hang Li, Wenjian Huang, Weihua Chen, Fan Wang, Jianguo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04243">The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate liver segmentation from contrast-enhanced MRI is essential for diagnosis, treatment planning, and disease monitoring. However, it remains challenging due to limited annotated data, heterogeneous enhancement protocols, and significant domain shifts across scanners and institutions. Traditional image-to-image translation frameworks have made great progress in domain generalization, but their application is not straightforward. For example, Pix2Pix requires image registration, and cycle-GAN cannot be integrated seamlessly into segmentation pipelines. Meanwhile, these methods are originally used to deal with cross-modality scenarios, and often introduce structural distortions and suffer from unstable training, which may pose drawbacks in our single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised mean teacher scheme to exploit large amounts of unlabeled volumes. A domain adaptation module, incorporating a randomized histogram-based style appearance transfer function and a trainable contrast-aware network, enriches domain diversity and mitigates cross-center variability. Furthermore, a continual test-time adaptation strategy is employed to improve robustness during inference. Extensive experiments demonstrate that our framework consistently outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance while exhibiting strong generalization to unseen domains under low-annotation conditions.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2501.18564.pdf' target='_blank'>https://arxiv.org/pdf/2501.18564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoquan Fang, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox, Ranjay Krishna, Jiafei Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18564">SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation systems operating in diverse, dynamic environments must exhibit three critical abilities: multitask interaction, generalization to unseen scenarios, and spatial memory. While significant progress has been made in robotic manipulation, existing approaches often fall short in generalization to complex environmental variations and addressing memory-dependent tasks. To bridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based policy that leverages multi-resolution upsampling with visual representations from large-scale foundation model. SAM2Act achieves a state-of-the-art average success rate of 86.8% across 18 tasks in the RLBench benchmark, and demonstrates robust generalization on The Colosseum benchmark, with only a 4.3% performance gap under diverse environmental perturbations. Building on this foundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2, which incorporates a memory bank, an encoder, and an attention mechanism to enhance spatial memory. To address the need for evaluating memory-dependent tasks, we introduce MemoryBench, a novel benchmark designed to assess spatial memory and action recall in robotic manipulation. SAM2Act+ achieves an average success rate of 94.3% on memory-based tasks in MemoryBench, significantly outperforming existing approaches and pushing the boundaries of memory-based robotic systems. Project page: sam2act.github.io.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2412.14401.pdf' target='_blank'>https://arxiv.org/pdf/2412.14401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ainaz Eftekhar, Rose Hendrix, Luca Weihs, Jiafei Duan, Ege Caglar, Jordi Salvador, Alvaro Herrasti, Winson Han, Eli VanderBil, Aniruddha Kembhavi, Ali Farhadi, Ranjay Krishna, Kiana Ehsani, Kuo-Hao Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14401">The One RING: a Robotic Indoor Navigation Generalist</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern robots vary significantly in shape, size, and sensor configurations used to perceive and interact with their environments. However, most navigation policies are embodiment-specific--a policy trained on one robot typically fails to generalize to another, even with minor changes in body size or camera viewpoint. As custom hardware becomes increasingly common, there is a growing need for a single policy that generalizes across embodiments, eliminating the need to retrain for each specific robot. In this paper, we introduce RING (Robotic Indoor Navigation Generalist), an embodiment-agnostic policy that turns any mobile robot into an effective indoor semantic navigator. Trained entirely in simulation, RING leverages large-scale randomization over robot embodiments to enable robust generalization to many real-world platforms. To support this, we augment the AI2-THOR simulator to instantiate robots with controllable configurations, varying in body size, rotation pivot point, and camera parameters. On the visual object-goal navigation task, RING achieves strong cross-embodiment (XE) generalization--72.1% average success rate across five simulated embodiments (a 16.7% absolute improvement on the Chores-S benchmark) and 78.9% across four real-world platforms, including Stretch RE-1, LoCoBot, and Unitree Go1--matching or even surpassing embodiment-specific policies. We further deploy RING on the RB-Y1 wheeled humanoid in a real-world kitchen environment, showcasing its out-of-the-box potential for mobile manipulation platforms. (Project website: https://one-ring-policy.allen.ai)
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2601.19091.pdf' target='_blank'>https://arxiv.org/pdf/2601.19091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Wei, Chin Chun Ooi, Jian Cheng Wong, Abhishek Gupta, Pao-Hsiung Chiu, Yew-Soon Ong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19091">Out-of-Distribution Generalization for Neural Physics Solvers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural physics solvers are increasingly used in scientific discovery, given their potential for rapid in silico insights into physical, materials, or biological systems and their long-time evolution. However, poor generalization beyond their training support limits exploration of novel designs and long-time horizon predictions. We introduce NOVA, a route to generalizable neural physics solvers that can provide rapid, accurate solutions to scenarios even under distributional shifts in partial differential equation parameters, geometries and initial conditions. By learning physics-aligned representations from an initial sparse set of scenarios, NOVA consistently achieves 1-2 orders of magnitude lower out-of-distribution errors than data-driven baselines across complex, nonlinear problems including heat transfer, diffusion-reaction and fluid flow. We further showcase NOVA's dual impact on stabilizing long-time dynamical rollouts and improving generative design through application to the simulation of nonlinear Turing systems and fluidic chip optimization. Unlike neural physics solvers that are constrained to retrieval and/or emulation within an a priori space, NOVA enables reliable extrapolation beyond known regimes, a key capability given the need for exploration of novel hypothesis spaces in scientific discovery
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2511.06571.pdf' target='_blank'>https://arxiv.org/pdf/2511.06571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyan Zhao, Zirui He, Fan Yang, Ali Payani, Mengnan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06571">Rep2Text: Decoding Full Text from a Single LLM Token Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2509.15330.pdf' target='_blank'>https://arxiv.org/pdf/2509.15330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Zhang, Bo Jiang, Jie Zhou, Yimeng Liu, Xin Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15330">CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in pre-training vision-language models (VLMs), e.g., contrastive language-image pre-training (CLIP) methods, have shown great potential in learning out-of-distribution (OOD) representations. Despite showing competitive performance, the prompt-based CLIP methods still suffer from: i) inaccurate text descriptions, which leads to degraded accuracy and robustness, and poses a challenge for zero-shot CLIP methods. ii) limited vision-language embedding alignment, which significantly affects the generalization performance. To tackle the above issues, this paper proposes a novel Conditional Domain prompt Learning (CoDoL) method, which utilizes readily-available domain information to form prompts and improves the vision-language embedding alignment for improving OOD generalization. To capture both instance-specific and domain-specific information, we further propose a lightweight Domain Meta Network (DMN) to generate input-conditional tokens for images in each domain. Extensive experiments on four OOD benchmarks (PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed CoDoL in terms of improving the vision-language embedding alignment as well as the out-of-distribution generalization performance.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2508.20835.pdf' target='_blank'>https://arxiv.org/pdf/2508.20835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Qianyu Zhou, Haijia Sun, Xiangtai Li, Xuequan Lu, Lizhuang Ma, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20835">PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) has been recently explored to enhance the generalizability of Point Cloud Classification (PCC) models toward unseen domains. Prior works are based on convolutional networks, Transformer or Mamba architectures, either suffering from limited receptive fields or high computational cost, or insufficient long-range dependency modeling. RWKV, as an emerging architecture, possesses superior linear complexity, global receptive fields, and long-range dependency. In this paper, we present the first work that studies the generalizability of RWKV models in DG PCC. We find that directly applying RWKV to DG PCC encounters two significant challenges: RWKV's fixed direction token shift methods, like Q-Shift, introduce spatial distortions when applied to unstructured point clouds, weakening local geometric modeling and reducing robustness. In addition, the Bi-WKV attention in RWKV amplifies slight cross-domain differences in key distributions through exponential weighting, leading to attention shifts and degraded generalization. To this end, we propose PointDGRWKV, the first RWKV-based framework tailored for DG PCC. It introduces two key modules to enhance spatial modeling and cross-domain robustness, while maintaining RWKV's linear efficiency. In particular, we present Adaptive Geometric Token Shift to model local neighborhood structures to improve geometric context awareness. In addition, Cross-Domain key feature Distribution Alignment is designed to mitigate attention drift by aligning key feature distributions across domains. Extensive experiments on multiple benchmarks demonstrate that PointDGRWKV achieves state-of-the-art performance on DG PCC.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2501.01453.pdf' target='_blank'>https://arxiv.org/pdf/2501.01453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Rabeh, Ethan Herron, Aditya Balu, Soumik Sarkar, Chinmay Hegde, Adarsh Krishnamurthy, Baskar Ganapathysubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01453">Geometry Matters: Benchmarking Scientific ML Approaches for Flow Prediction around Complex Geometries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid and accurate simulations of fluid dynamics around complicated geometric bodies are critical in a variety of engineering and scientific applications, including aerodynamics and biomedical flows. However, while scientific machine learning (SciML) has shown considerable promise, most studies in this field are limited to simple geometries, and complex, real-world scenarios are underexplored. This paper addresses this gap by benchmarking diverse SciML models, including neural operators and vision transformer-based foundation models, for fluid flow prediction over intricate geometries. Using a high-fidelity dataset of steady-state flows across various geometries, we evaluate the impact of geometric representations -- Signed Distance Fields (SDF) and binary masks -- on model accuracy, scalability, and generalization. Central to this effort is the introduction of a novel, unified scoring framework that integrates metrics for global accuracy, boundary layer fidelity, and physical consistency to enable a robust, comparative evaluation of model performance. Our findings demonstrate that newer foundation models significantly outperform neural operators, particularly in data-limited scenarios, and that SDF representations yield superior results with sufficient training data. Despite these promises, all models struggle with out-of-distribution generalization, highlighting a critical challenge for future SciML applications. By advancing both evaluation models and modeling capabilities, our work paves the way for robust and scalable ML solutions for fluid dynamics across complex geometries.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2410.04968.pdf' target='_blank'>https://arxiv.org/pdf/2410.04968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Zhou, Yaoxin Wu, Zhiguang Cao, Wen Song, Jie Zhang, Zhiqi Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04968">Collaboration! Towards Robust Neural Methods for Routing Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite enjoying desirable efficiency and reduced reliance on domain expertise, existing neural methods for vehicle routing problems (VRPs) suffer from severe robustness issues -- their performance significantly deteriorates on clean instances with crafted perturbations. To enhance robustness, we propose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the defense of neural VRP methods, which is crucial yet underexplored in the literature. Given a neural VRP method, we adversarially train multiple models in a collaborative manner to synergistically promote robustness against attacks, while boosting standard generalization on clean instances. A neural router is designed to adeptly distribute training instances among models, enhancing overall load balancing and collaborative efficacy. Extensive experiments verify the effectiveness and versatility of CNF in defending against various attacks across different neural VRP methods. Notably, our approach also achieves impressive out-of-distribution generalization on benchmark instances.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2510.13176.pdf' target='_blank'>https://arxiv.org/pdf/2510.13176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolin Pan, Chao Zha, Jinyuan Dong, Mingjie Xing, Yanjun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13176">GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compiler pass selection and phase ordering present a significant challenge in achieving optimal program performance, particularly for objectives like code size reduction. Standard compiler heuristics offer general applicability but often yield suboptimal, program-specific results due to their one-size-fits-all nature. While iterative compilation can find tailored solutions, its prohibitive search cost limits practical use. Machine learning approaches promise faster inference but frequently struggle with generalization to unseen programs. This paper introduces GRACE, a novel framework for compiler auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE effectively curtails the search space by leveraging pass synergies and a weighted scoring method to generate initial high-quality candidate sequences and a pass pool. It then employs contrastive learning, using pass sequence-based data augmentation, to create program embeddings that facilitate similarity-aware clustering. Evolutionary search within these clusters yields a coreset of $k$ specialized pass sequences designed for robust generalization to unseen programs. At test time, GRACE efficiently selects the best coreset sequence and refines it using lightweight techniques. Experimental results on seven diverse datasets show that GRACE reduces LLVM IR instruction count by an average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz, while incurring an average tuning time of less than 1s per program, demonstrating its state-of-the-art performance and practical effectiveness.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2510.06638.pdf' target='_blank'>https://arxiv.org/pdf/2510.06638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Wen, Wenkang Wei, Yuan Fang, Xingtong Yu, Hui Zhang, Weicheng Zhu, Xin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06638">StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge-based Visual Question Answering (KVQA) requires models to ground entities in images and reason over factual knowledge. We study its implicit-knowledge variant, IK-KVQA, where a multimodal large language model (MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs lack explicit reasoning supervision and produce inconsistent justifications, and generalize poorly after standard supervised fine-tuning (SFT). We present StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises structured traces - dual symbolic relation paths plus path-grounded natural-language explanations - so that reasoning becomes transparent and verifiable. With one open-source MLLM, StaR-KVQA constructs and selects path-grounded reasoning traces to form a trace-enriched dataset, then fine-tunes via structured self-distillation to align generation with supervision; no external retrievers, verifiers, or curated knowledge bases (KBs) are used, traces are built offline, and inference is a single autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over the strongest baseline while exhibiting robust cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2510.05580.pdf' target='_blank'>https://arxiv.org/pdf/2510.05580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Li, Zhantao Yang, Han Zhang, Fangyi Chen, Chenchen Zhu, Anudeepsekhar Bolimera, Marios Savvides
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05580">MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2509.24661.pdf' target='_blank'>https://arxiv.org/pdf/2509.24661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Wu, Rolandos Alexandros Potamias, Xuyang Zhang, Zhongqun Zhang, Jiankang Deng, Shan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24661">CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-embodiment dexterous grasp synthesis refers to adaptively generating and optimizing grasps for various robotic hands with different morphologies. This capability is crucial for achieving versatile robotic manipulation in diverse environments and requires substantial amounts of reliable and diverse grasp data for effective model training and robust generalization. However, existing approaches either rely on physics-based optimization that lacks human-like kinematic understanding or require extensive manual data collection processes that are limited to anthropomorphic structures. In this paper, we propose CEDex, a novel cross-embodiment dexterous grasp synthesis method at scale that bridges human grasping kinematics and robot kinematics by aligning robot kinematic models with generated human-like contact representations. Given an object's point cloud and an arbitrary robotic hand model, CEDex first generates human-like contact representations using a Conditional Variational Auto-encoder pretrained on human contact data. It then performs kinematic human contact alignment through topological merging to consolidate multiple human hand parts into unified robot components, followed by a signed distance field-based grasp optimization with physics-aware constraints. Using CEDex, we construct the largest cross-embodiment grasp dataset to date, comprising 500K objects across four gripper types with 20M total grasps. Extensive experiments show that CEDex outperforms state-of-the-art approaches and our dataset benefits cross-embodiment grasp learning with high-quality diverse grasps.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2509.20684.pdf' target='_blank'>https://arxiv.org/pdf/2509.20684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Wang, Di Wang, Ke Li, Yifeng Wang, Chengjian Wang, Libin Sun, Zhihong Wu, Yiming Zhang, Quan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20684">Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view geo-localization (CVGL) aims to match images of the same location captured from drastically different viewpoints. Despite recent progress, existing methods still face two key challenges: (1) achieving robustness under severe appearance variations induced by diverse UAV orientations and fields of view, which hinders cross-domain generalization, and (2) establishing reliable correspondences that capture both global scene-level semantics and fine-grained local details. In this paper, we propose EGS, a novel CVGL framework designed to enhance cross-domain generalization. Specifically, we introduce an E(2)-Steerable CNN encoder to extract stable and reliable features under rotation and viewpoint shifts. Furthermore, we construct a graph with a virtual super-node that connects to all local nodes, enabling global semantics to be aggregated and redistributed to local regions, thereby enforcing global-local consistency. Extensive experiments on the University-1652 and SUES-200 benchmarks demonstrate that EGS consistently achieves substantial performance gains and establishes a new state of the art in cross-domain CVGL.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2509.01944.pdf' target='_blank'>https://arxiv.org/pdf/2509.01944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenlong Yuan, Jing Tang, Jinguo Luo, Rui Chen, Chengxuan Qian, Lei Sun, Xiangxiang Chu, Yujun Cai, Dapeng Zhang, Shuo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01944">AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models in autonomous driving systems have recently demonstrated transformative potential by integrating multimodal perception with decision-making capabilities. However, the interpretability and coherence of the decision process and the plausibility of action sequences remain largely underexplored. To address these issues, we propose AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and self-reflection capabilities of autonomous driving systems through chain-of-thought (CoT) processing and reinforcement learning (RL). Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation. Moreover, to maximize both reasoning and self-reflection during the RL stage, we further employ the Group Relative Policy Optimization (GRPO) algorithm within a physics-grounded reward framework that incorporates spatial alignment, vehicle dynamic, and temporal smoothness criteria to ensure reliable and realistic trajectory planning. Extensive evaluation results across both nuScenes and Waymo datasets demonstrates the state-of-the-art performance and robust generalization capacity of our proposed method.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2508.19705.pdf' target='_blank'>https://arxiv.org/pdf/2508.19705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Hu, Ying Zhou, Gepeng Ji, Nick Barnes, Qiang Li, Zhiwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19705">FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing video polyp segmentation (VPS) paradigms usually struggle to balance between spatiotemporal modeling and domain generalization, limiting their applicability in real clinical scenarios. To embrace this challenge, we recast the VPS task as a track-by-detect paradigm that leverages the spatial contexts captured by the image polyp segmentation (IPS) model while integrating the temporal modeling capabilities of segment anything model 2 (SAM2). However, during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error accumulation, resulting in a snowball effect that compromises segmentation stability. We mitigate this issue by repurposing SAM2 as a video polyp segmenter with two training-free modules. In particular, the intra-association filtering module eliminates spatial inaccuracies originating from the detecting stage, reducing false positives. The inter-association refinement module adaptively updates the memory bank to prevent error propagation over time, enhancing temporal coherence. Both modules work synergistically to stabilize SAM2, achieving cutting-edge performance in both in-domain and out-of-domain scenarios. Furthermore, we demonstrate the robust tracking capabilities of FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential reliable clinical analysis.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2508.11502.pdf' target='_blank'>https://arxiv.org/pdf/2508.11502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eyad Alshami, Shashank Agnihotri, Bernt Schiele, Margret Keuper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11502">AIM: Amending Inherent Interpretability via Self-Supervised Masking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has been observed that deep neural networks (DNNs) often use both genuine as well as spurious features. In this work, we propose "Amending Inherent Interpretability via Self-Supervised Masking" (AIM), a simple yet interestingly effective method that promotes the network's utilization of genuine features over spurious alternatives without requiring additional annotations. In particular, AIM uses features at multiple encoding stages to guide a self-supervised, sample-specific feature-masking process. As a result, AIM enables the training of well-performing and inherently interpretable models that faithfully summarize the decision process. We validate AIM across a diverse range of challenging datasets that test both out-of-distribution generalization and fine-grained visual understanding. These include general-purpose classification benchmarks such as ImageNet100, HardImageNet, and ImageWoof, as well as fine-grained classification datasets such as Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual benefits: interpretability improvements, as measured by the Energy Pointing Game (EPG) score, and accuracy gains over strong baselines. These consistent gains across domains and architectures provide compelling evidence that AIM promotes the use of genuine and meaningful features that directly contribute to improved generalization and human-aligned interpretability.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2507.14141.pdf' target='_blank'>https://arxiv.org/pdf/2507.14141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danny Dongyeop Han, Ahhyun Lucy Lee, Taeyang Lee, Yonghyeon Gwon, Sebin Lee, Seongjin Lee, David Keetae Park, Shinjae Yoo, Jiook Cha, Chun Kee Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14141">DIVER-0 : A Fully Channel Equivariant EEG Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography (EEG) is a non-invasive technique widely used in brain-computer interfaces and clinical applications, yet existing EEG foundation models face limitations in modeling spatio-temporal brain dynamics and lack channel permutation equivariance, preventing robust generalization across diverse electrode configurations. To address these challenges, we propose DIVER-0, a novel EEG foundation model that demonstrates how full spatio-temporal attention-rather than segregated spatial or temporal processing-achieves superior performance when properly designed with Rotary Position Embedding (RoPE) for temporal relationships and binary attention biases for channel differentiation. We also introduce Sliding Temporal Conditional Positional Encoding (STCPE), which improves upon existing conditional positional encoding approaches by maintaining both temporal translation equivariance and channel permutation equivariance, enabling robust adaptation to arbitrary electrode configurations unseen during pretraining. Experimental results demonstrate that DIVER-0 achieves competitive performance with only 10% of pretraining data while maintaining consistent results across all channel permutation conditions, validating its effectiveness for cross-dataset generalization and establishing key design principles for handling the inherent heterogeneity of neural recording setups.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2507.04048.pdf' target='_blank'>https://arxiv.org/pdf/2507.04048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Shi, Yanfu Zhang, Ye Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04048">CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech Emotion Recognition (SER) is fundamental to affective computing and human-computer interaction, yet existing models struggle to generalize across diverse acoustic conditions. While Contrastive Language-Audio Pretraining (CLAP) provides strong multimodal alignment, it lacks dedicated mechanisms for capturing emotional cues, making it suboptimal for SER. To address this, we propose CLEP-DG, a framework that enhances CLAP's robustness in emotion recognition. First, we fine-tune CLAP to obtain CLEP, adapting it on large-scale emotional speech datasets to better encode emotion-relevant features. Then, we introduce Acoustic Context Prompt Tuning (ACPT), a text-driven augmentation strategy that optimizes learnable prompt vectors to model diverse acoustic environments without additional labeled audio. Finally, leveraging cross-modal transferability, we train a classifier on text-derived embeddings and apply it to the audio encoder during inference, mitigating domain shifts between textual supervision and audio-based emotion recognition. Experiments across five benchmark datasets show that CLEP-DG outperforms prior CLAP-based approaches, achieving state-of-the-art performance in both supervised and domain generalization settings.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2504.11914.pdf' target='_blank'>https://arxiv.org/pdf/2504.11914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Chao, Jie Liu, Jie Tang, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11914">AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial Anomaly Detection (IAD) poses a formidable challenge due to the scarcity of defective samples, making it imperative to deploy models capable of robust generalization to detect unseen anomalies effectively. Traditional approaches, often constrained by hand-crafted features or domain-specific expert models, struggle to address this limitation, underscoring the need for a paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional generalization and interpretability, to revolutionize IAD. By integrating MLLM with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution that autonomously processes inputs of image and domain knowledge, reasons through analysis, and generates precise anomaly localizations and masks. Based on the latest multimodal IAD benchmark, our compact 3-billion-parameter model outperforms existing methods, establishing state-of-the-art results. As MLLM capabilities continue to advance, this study is the first to deliver an end-to-end VLM-based IAD solution that demonstrates the transformative potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking cornerstone for next-generation intelligent anomaly detection systems in industrial applications with limited defective data.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2503.13939.pdf' target='_blank'>https://arxiv.org/pdf/2503.13939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13939">Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored. Medical vision-language tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional supervised fine-tuning (SFT) and Chain-of-Thought (CoT) strategies that work well in general domains. To address these challenges, we propose Med-R1, a reinforcement learning (RL)-enhanced vision-language model designed to improve generalization and reliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts Group Relative Policy Optimization (GRPO) to encourage reward-guided learning beyond static annotations. We comprehensively evaluate Med-R1 across eight distinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in average accuracy over its base model Qwen2-VL-2B, and even outperforms Qwen2-VL-72B-a model with 36x more parameters. To assess cross-task generalization, we further evaluate Med-R1 on five question types. Med-R1 outperforms Qwen2-VL-2B by 32.06% in question-type generalization, also surpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a crucial component for the success of Deepseek-R1. Our results show that omitting intermediate rationales (No-Thinking-Med-R1) not only improves in-domain and cross-domain generalization with less training, but also challenges the assumption that more reasoning always helps. These findings suggest that in medical VQA, it is not reasoning itself, but its quality and domain alignment, that determine effectiveness. Together, these results highlight that RL improves medical reasoning and generalization, enabling efficient and reliable VLMs for real-world deployment.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2412.01115.pdf' target='_blank'>https://arxiv.org/pdf/2412.01115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wu, Zhihang Zhong, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01115">DIR: Retrieval-Augmented Image Captioning with Comprehensive Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image captioning models often suffer from performance degradation when applied to novel datasets, as they are typically trained on domain-specific data. To enhance generalization in out-of-domain scenarios, retrieval-augmented approaches have garnered increasing attention. However, current methods face two key challenges: (1) image features used for retrieval are often optimized based on ground-truth (GT) captions, which represent the image from a specific perspective and are influenced by annotator biases, and (2) they underutilize the full potential of retrieved text, typically relying on raw captions or parsed objects, which fail to capture the full semantic richness of the data. In this paper, we propose Dive Into Retrieval (DIR), a method designed to enhance both the image-to-text retrieval process and the utilization of retrieved text to achieve a more comprehensive understanding of the visual content. Our approach introduces two key innovations: (1) diffusion-guided retrieval enhancement, where a pretrained diffusion model guides image feature learning by reconstructing noisy images, allowing the model to capture more comprehensive and fine-grained visual information beyond standard annotated captions; and (2) a high-quality retrieval database, which provides comprehensive semantic information to enhance caption generation, especially in out-of-domain scenarios. Extensive experiments demonstrate that DIR not only maintains competitive in-domain performance but also significantly improves out-of-domain generalization, all without increasing inference costs.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2601.02020.pdf' target='_blank'>https://arxiv.org/pdf/2601.02020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shihan Peng, Yuyang Xiong, Hanyu Zhou, Zhiwei Shi, Haoyue Liu, Gang Chen, Luxin Yan, Yi Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02020">Adapting Depth Anything to Adverse Imaging Conditions with Events</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2601.00968.pdf' target='_blank'>https://arxiv.org/pdf/2601.00968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longwei Wang, Mohammad Navid Nayyem, Abdullah Al Rakin, KC Santosh, Chaowei Zhang, Yang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.00968">Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2511.19537.pdf' target='_blank'>https://arxiv.org/pdf/2511.19537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhao Guo, Yang Weng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19537">Cross-Domain Generalization of Multimodal LLMs for Global Photovoltaic Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid expansion of distributed photovoltaic (PV) systems poses challenges for power grid management, as many installations remain undocumented. While satellite imagery provides global coverage, traditional computer vision (CV) models such as CNNs and U-Nets require extensive labeled data and fail to generalize across regions. This study investigates the cross-domain generalization of a multimodal large language model (LLM) for global PV assessment. By leveraging structured prompts and fine-tuning, the model integrates detection, localization, and quantification within a unified schema. Cross-regional evaluation using the $Δ$F1 metric demonstrates that the proposed model achieves the smallest performance degradation across unseen regions, outperforming conventional CV and transformer baselines. These results highlight the robustness of multimodal LLMs under domain shift and their potential for scalable, transferable, and interpretable global PV mapping.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2510.06005.pdf' target='_blank'>https://arxiv.org/pdf/2510.06005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qin Dong, Yuntian Tang, Heming Jia, Yunhang Shen, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie, Shaohui Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06005">MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-Rank Adaptation (LoRA) has emerged as a dominant method in Parameter-Efficient Fine-Tuning (PEFT) for large language models, which augments the transformer layer with one down-projection $A$ and one up-projection $B$. However, LoRA's reliance on a single down-projection matrix ($A$) creates a representational bottleneck, as this solitary feature extractor is inherently insufficient for capturing the diverse signals required by complex tasks. This motivates our architectural shift to focus on enriching the feature adaptation to improve the downstream task adaptation ability. We propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is asymmetrically shared across layers to ensure parameter efficiency. In MASA, these specialized experts capture diverse features, which are then integrated by a single, layer-specific $B$-matrix. The effectiveness and versatility of our method are validated through a comprehensive suite of experiments spanning multi-domain generalization, single-domain specialization, and multi-task reasoning. For example, on the MMLU benchmark, MASA achieves an average accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative improvement of 1.84%) with comparable learnable parameters of 0.52%.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2509.22050.pdf' target='_blank'>https://arxiv.org/pdf/2509.22050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Ding, Muyun Jiang, Weibang Jiang, Shuailei Zhang, Xinliang Zhou, Chenyu Liu, Shanglin Li, Yong Li, Cuntai Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22050">BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography (EEG) is a non-invasive technique for recording brain electrical activity, widely used in brain-computer interface (BCI) and healthcare. Recent EEG foundation models trained on large-scale datasets have shown improved performance and generalizability over traditional decoding methods, yet significant challenges remain. Existing models often fail to explicitly capture channel-to-channel and region-to-region interactions, which are critical sources of information inherently encoded in EEG signals. Due to varying channel configurations across datasets, they either approximate spatial structure with self-attention or restrict training to a limited set of common channels, sacrificing flexibility and effectiveness. Moreover, although EEG datasets reflect diverse brain states such as emotion, motor, and others, current models rarely learn state-aware representations during self-supervised pre-training. To address these gaps, we propose BrainPro, a large EEG model that introduces a retrieval-based spatial learning block to flexibly capture channel- and region-level interactions across varying electrode layouts, and a brain state-decoupling block that enables state-aware representation learning through parallel encoders with decoupling and region-aware reconstruction losses. This design allows BrainPro to adapt seamlessly to diverse tasks and hardware settings. Pre-trained on an extensive EEG corpus, BrainPro achieves state-of-the-art performance and robust generalization across nine public BCI datasets. Our codes and the pre-trained weights will be released.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2509.07531.pdf' target='_blank'>https://arxiv.org/pdf/2509.07531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Dou, Deqing Wang, Fuzhen Zhuang, Jian Ren, Yanlin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07531">FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scientific document representation learning provides powerful embeddings for various tasks, while current methods face challenges across three approaches. 1) Contrastive training with citation-structural signals underutilizes citation information and still generates single-vector representations. 2) Fine-grained representation learning, which generates multiple vectors at the sentence or aspect level, requires costly integration and lacks domain generalization. 3) Task-aware learning depends on manually predefined task categorization, overlooking nuanced task distinctions and requiring extra training data for task-specific modules. To address these problems, we propose a new method that unifies the three approaches for better representations, namely FLeW. Specifically, we introduce a novel triplet sampling method that leverages citation intent and frequency to enhance citation-structural signals for training. Citation intents (background, method, result), aligned with the general structure of scientific writing, facilitate a domain-generalized facet partition for fine-grained representation learning. Then, we adopt a simple weight search to adaptively integrate three facet-level embeddings into a task-specific document embedding without task-aware fine-tuning. Experiments show the applicability and robustness of FLeW across multiple scientific tasks and fields, compared to prior models.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2505.19213.pdf' target='_blank'>https://arxiv.org/pdf/2505.19213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohao Rui, Kaitao Chen, Weijie Ma, Xiaosong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19213">Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in reinforcement learning with verifiable, rule-based rewards have greatly enhanced the reasoning capabilities and out-of-distribution generalization of VLMs/LLMs, obviating the need for manually crafted reasoning chains. Despite these promising developments in the general domain, their translation to medical imaging remains limited. Current medical reinforcement fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby restricting the model's ability to engage in world knowledge retrieval and flexible task adaptation. More critically, these methods fall short of addressing the critical clinical demand for open-ended, reasoning-intensive decision-making. To bridge this gap, we introduce \textbf{MedCCO}, the first multimodal reinforcement learning framework tailored for medical VQA that unifies close-ended and open-ended data within a curriculum-driven RFT paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of close-ended medical VQA tasks to establish domain-grounded reasoning capabilities, and is then progressively adapted to open-ended tasks to foster deeper knowledge enhancement and clinical interpretability. We validate MedCCO across eight challenging medical VQA benchmarks, spanning both close-ended and open-ended settings. Experimental results show that MedCCO consistently enhances performance and generalization, achieving a 11.4\% accuracy gain across three in-domain tasks, and a 5.7\% improvement on five out-of-domain benchmarks. These findings highlight the promise of curriculum-guided RL in advancing robust, clinically-relevant reasoning in medical multimodal language models.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2505.12684.pdf' target='_blank'>https://arxiv.org/pdf/2505.12684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinlin Zhu, Xunkai Li, Jishuo Jia, Miao Hu, Di Wu, Meikang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12684">Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2504.06572.pdf' target='_blank'>https://arxiv.org/pdf/2504.06572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaocong Long, Qianyu Zhou, Xikun Jiang, Chenhao Ying, Lizhuang Ma, Yuan Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06572">Domain Generalization via Discrete Codebook Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) strives to address distribution shifts across diverse environments to enhance model's generalizability. Current DG approaches are confined to acquiring robust representations with continuous features, specifically training at the pixel level. However, this DG paradigm may struggle to mitigate distribution gaps in dealing with a large space of continuous features, rendering it susceptible to pixel details that exhibit spurious correlations or noise. In this paper, we first theoretically demonstrate that the domain gaps in continuous representation learning can be reduced by the discretization process. Based on this inspiring finding, we introduce a novel learning paradigm for DG, termed Discrete Domain Generalization (DDG). DDG proposes to use a codebook to quantize the feature map into discrete codewords, aligning semantic-equivalent information in a shared discrete representation space that prioritizes semantic-level information over pixel-level intricacies. By learning at the semantic level, DDG diminishes the number of latent features, optimizing the utilization of the representation space and alleviating the risks associated with the wide-ranging space of continuous features. Extensive experiments across widely employed benchmarks in DG demonstrate DDG's superior performance compared to state-of-the-art approaches, underscoring its potential to reduce the distribution gaps and enhance the model's generalizability.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2503.21210.pdf' target='_blank'>https://arxiv.org/pdf/2503.21210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueying Gao, Dongliang Chang, Bingyao Yu, Haotian Qin, Muxi Diao, Lei Chen, Kongming Liang, Zhanyu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21210">Towards Generalizable Forgery Detection and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and interpretable detection of AI-generated images is essential for mitigating risks associated with AI misuse. However, the substantial domain gap among generative models makes it challenging to develop a generalizable forgery detection model. Moreover, since every pixel in an AI-generated image is synthesized, traditional saliency-based forgery explanation methods are not well suited for this task. To address these challenges, we formulate detection and explanation as a unified Forgery Detection and Reasoning task (FDR-Task), leveraging Multi-Modal Large Language Models (MLLMs) to provide accurate detection through reliable reasoning over forgery attributes. To facilitate this task, we introduce the Multi-Modal Forgery Reasoning dataset (MMFR-Dataset), a large-scale dataset containing 120K images across 10 generative models, with 378K reasoning annotations on forgery attributes, enabling comprehensive evaluation of the FDR-Task. Furthermore, we propose FakeReasoning, a forgery detection and reasoning framework with three key components: 1) a dual-branch visual encoder that integrates CLIP and DINO to capture both high-level semantics and low-level artifacts; 2) a Forgery-Aware Feature Fusion Module that leverages DINO's attention maps and cross-attention mechanisms to guide MLLMs toward forgery-related clues; 3) a Classification Probability Mapper that couples language modeling and forgery detection, enhancing overall performance. Experiments across multiple generative models demonstrate that FakeReasoning not only achieves robust generalization but also outperforms state-of-the-art methods on both detection and reasoning tasks.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2501.02464.pdf' target='_blank'>https://arxiv.org/pdf/2501.02464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Guo, Sparsh Garg, S. Mahdi H. Miangoleh, Xinyu Huang, Liu Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02464">Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent depth foundation models exhibit strong zero-shot generalization, achieving accurate metric depth across diverse camera types-particularly those with large fields of view (FoV) such as fisheye and 360-degree cameras-remains a significant challenge. This paper presents Depth Any Camera (DAC), a powerful zero-shot metric depth estimation framework that extends a perspective-trained model to effectively handle cameras with varying FoVs. The framework is designed to ensure that all existing 3D data can be leveraged, regardless of the specific camera types used in new applications. Remarkably, DAC is trained exclusively on perspective images but generalizes seamlessly to fisheye and 360-degree cameras without the need for specialized training data. DAC employs Equi-Rectangular Projection (ERP) as a unified image representation, enabling consistent processing of images with diverse FoVs. Its core components include pitch-aware Image-to-ERP conversion with efficient online augmentation to simulate distorted ERP patches from undistorted inputs, FoV alignment operations to enable effective training across a wide range of FoVs, and multi-resolution data augmentation to further address resolution disparities between training and testing. DAC achieves state-of-the-art zero-shot metric depth estimation, improving $Î´_1$ accuracy by up to 50% on multiple fisheye and 360-degree datasets compared to prior metric depth foundation models, demonstrating robust generalization across camera types.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2410.16732.pdf' target='_blank'>https://arxiv.org/pdf/2410.16732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runpu Wei, Zijin Yin, Kongming Liang, Min Min, Chengwei Pan, Gang Yu, Haonan Huang, Yan Liu, Zhanyu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16732">Polyp-E: Benchmarking the Robustness of Deep Segmentation Models via Polyp Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic polyp segmentation is helpful to assist clinical diagnosis and treatment. In daily clinical practice, clinicians exhibit robustness in identifying polyps with both location and size variations. It is uncertain if deep segmentation models can achieve comparable robustness in automated colonoscopic analysis. To benchmark the model robustness, we focus on evaluating the robustness of segmentation models on the polyps with various attributes (e.g. location and size) and healthy samples. Based on the Latent Diffusion Model, we perform attribute editing on real polyps and build a new dataset named Polyp-E. Our synthetic dataset boasts exceptional realism, to the extent that clinical experts find it challenging to discern them from real data. We evaluate several existing polyp segmentation models on the proposed benchmark. The results reveal most of the models are highly sensitive to attribute variations. As a novel data augmentation technique, the proposed editing pipeline can improve both in-distribution and out-of-distribution generalization ability. The code and datasets will be released.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2410.14974.pdf' target='_blank'>https://arxiv.org/pdf/2410.14974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangning Xia, Hongjie Fang, Cewu Lu, Hao-Shu Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14974">CAGE: Causal Attention Enables Data-Efficient Generalizable Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization in robotic manipulation remains a critical challenge, particularly when scaling to new environments with limited demonstrations. This paper introduces CAGE, a novel robotic manipulation policy designed to overcome these generalization barriers by integrating a causal attention mechanism. CAGE utilizes the powerful feature extraction capabilities of the vision foundation model DINOv2, combined with LoRA fine-tuning for robust environment understanding. The policy further employs a causal Perceiver for effective token compression and a diffusion-based action prediction head with attention mechanisms to enhance task-specific fine-grained conditioning. With as few as 50 demonstrations from a single training environment, CAGE achieves robust generalization across diverse visual changes in objects, backgrounds, and viewpoints. Extensive experiments validate that CAGE significantly outperforms existing state-of-the-art RGB/RGB-D approaches in various manipulation tasks, especially under large distribution shifts. In similar environments, CAGE offers an average of 42% increase in task completion rate. While all baselines fail to execute the task in unseen environments, CAGE manages to obtain a 43% completion rate and a 51% success rate in average, making a huge step towards practical deployment of robots in real-world settings. Project website: cage-policy.github.io.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2601.07264.pdf' target='_blank'>https://arxiv.org/pdf/2601.07264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihao Xuan, Qingcheng Zeng, Heli Qi, Yunze Xiao, Junjue Wang, Naoto Yokoya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07264">The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2512.02280.pdf' target='_blank'>https://arxiv.org/pdf/2512.02280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noorbakhsh Amiri Golilarz, Sindhuja Penchala, Shahram Rahimi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02280">Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fundamentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self-monitoring, lack of meta-cognitive awareness, fixed and non-adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust generalization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2510.20295.pdf' target='_blank'>https://arxiv.org/pdf/2510.20295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Qiu, Yixiong Zou, Jun Wang, Wei Liu, Xiangyu Fu, Ruixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20295">Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution generalization under distributional shifts remains a critical challenge for graph neural networks. Existing methods generally adopt the Invariant Risk Minimization (IRM) framework, requiring costly environment annotations or heuristically generated synthetic splits. To circumvent these limitations, in this work, we aim to develop an IRM-free method for capturing causal subgraphs. We first identify that causal subgraphs exhibit substantially smaller distributional variations than non-causal components across diverse environments, which we formalize as the Invariant Distribution Criterion and theoretically prove in this paper. Building on this criterion, we systematically uncover the quantitative relationship between distributional shift and representation norm for identifying the causal subgraph, and investigate its underlying mechanisms in depth. Finally, we propose an IRM-free method by introducing a norm-guided invariant distribution objective for causal subgraph discovery and prediction. Extensive experiments on two widely used benchmarks demonstrate that our method consistently outperforms state-of-the-art methods in graph generalization.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2509.24655.pdf' target='_blank'>https://arxiv.org/pdf/2509.24655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max van Spengler, Artem Moskalev, Tommaso Mansi, Mangal Prakash, Rui Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24655">HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models are increasingly applied to biological sequences like proteins and mRNA, yet their default Euclidean geometry may mismatch the hierarchical structures inherent to biological data. While hyperbolic geometry provides a better alternative for accommodating hierarchical data, it has yet to find a way into language modeling for mRNA sequences. In this work, we introduce HyperHELM, a framework that implements masked language model pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned representations with the biological hierarchy defined by the relationship between mRNA and amino acids. Across multiple multi-species datasets, it outperforms Euclidean baselines on 9 out of 10 tasks involving property prediction, with 10% improvement on average, and excels in out-of-distribution generalization to long and low-GC content sequences; for antibody region annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation accuracy. Our results highlight hyperbolic geometry as an effective inductive bias for hierarchical language modeling of mRNA sequences.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2509.21207.pdf' target='_blank'>https://arxiv.org/pdf/2509.21207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olga Fink, Ismail Nejjar, Vinay Sharma, Keivan Faghih Niresi, Han Sun, Hao Dong, Chenghao Xu, Amaury Wei, Arthur Bizzi, Raffael Theiler, Yuan Tian, Leandro Von Krannichfeldt, Zhan Ma, Sergei Garmaev, Zepeng Zhang, Mengjie Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21207">From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prognostics and Health Management ensures the reliability, safety, and efficiency of complex engineered systems by enabling fault detection, anticipating equipment failures, and optimizing maintenance activities throughout an asset lifecycle. However, real-world PHM presents persistent challenges: sensor data is often noisy or incomplete, available labels are limited, and degradation behaviors and system interdependencies can be highly complex and nonlinear. Physics-informed machine learning has emerged as a promising approach to address these limitations by embedding physical knowledge into data-driven models. This review examines how incorporating learning and observational biases through physics-informed modeling and data strategies can guide models toward physically consistent and reliable predictions. Learning biases embed physical constraints into model training through physics-informed loss functions and governing equations, or by incorporating properties like monotonicity. Observational biases influence data selection and synthesis to ensure models capture realistic system behavior through virtual sensing for estimating unmeasured states, physics-based simulation for data augmentation, and multi-sensor fusion strategies. The review then examines how these approaches enable the transition from passive prediction to active decision-making through reinforcement learning, which allows agents to learn maintenance policies that respect physical constraints while optimizing operational objectives. This closes the loop between model-based predictions, simulation, and actual system operation, empowering adaptive decision-making. Finally, the review addresses the critical challenge of scaling PHM solutions from individual assets to fleet-wide deployment. Fast adaptation methods including meta-learning and few-shot learning are reviewed alongside domain generalization techniques ...
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2509.18542.pdf' target='_blank'>https://arxiv.org/pdf/2509.18542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Wang, Hanyang Peng, Yue Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18542">Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2509.12728.pdf' target='_blank'>https://arxiv.org/pdf/2509.12728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongsol Kim, Chanseok Lee, Jongin You, Jong Chul Ye, Mooseok Jang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12728">Generalizable Holographic Reconstruction via Amplitude-Only Diffusion Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Phase retrieval in inline holography is a fundamental yet ill-posed inverse problem due to the nonlinear coupling between amplitude and phase in coherent imaging. We present a novel off-the-shelf solution that leverages a diffusion model trained solely on object amplitude to recover both amplitude and phase from diffraction intensities. Using a predictor-corrector sampling framework with separate likelihood gradients for amplitude and phase, our method enables complex field reconstruction without requiring ground-truth phase data for training. We validate the proposed approach through extensive simulations and experiments, demonstrating robust generalization across diverse object shapes, imaging system configurations, and modalities, including lensless setups. Notably, a diffusion prior trained on simple amplitude data (e.g., polystyrene beads) successfully reconstructs complex biological tissue structures, highlighting the method's adaptability. This framework provides a cost-effective, generalizable solution for nonlinear inverse problems in computational imaging, and establishes a foundation for broader coherent imaging applications beyond holography.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2509.09595.pdf' target='_blank'>https://arxiv.org/pdf/2509.09595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikang Ding, Jiwen Liu, Wenyuan Zhang, Zekun Wang, Wentao Hu, Liyuan Cui, Mingming Lao, Yingchao Shao, Hui Liu, Xiaohan Li, Ming Chen, Xiaoqiang Liu, Yu-Shen Liu, Pengfei Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09595">Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2509.02597.pdf' target='_blank'>https://arxiv.org/pdf/2509.02597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuting Xu, Runtong Liu, Zhixuan Chen, Junlin Hou, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02597">Solutions for Mitotic Figure Detection and Atypical Classification in MIDOG 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has driven significant advances in mitotic figure analysis within computational pathology. In this paper, we present our approach to the Mitosis Domain Generalization (MIDOG) 2025 Challenge, which consists of two distinct tasks, i.e., mitotic figure detection and atypical mitosis classification. For the mitotic figure detection task, we propose a two-stage detection-classification framework that first localizes candidate mitotic figures and subsequently refines the predictions using a dedicated classification module. For the atypical mitosis classification task, we employ an ensemble strategy that integrates predictions from multiple state-of-the-art deep learning architectures to improve robustness and accuracy. Extensive experiments demonstrate the effectiveness of our proposed methods across both tasks.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2506.23208.pdf' target='_blank'>https://arxiv.org/pdf/2506.23208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runtian Yuan, Qingqiu Li, Junlin Hou, Jilan Xu, Yuejie Zhang, Rui Feng, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23208">Multi-Source COVID-19 Detection via Variance Risk Extrapolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present our solution for the Multi-Source COVID-19 Detection Challenge, which aims to classify chest CT scans into COVID and Non-COVID categories across data collected from four distinct hospitals and medical centers. A major challenge in this task lies in the domain shift caused by variations in imaging protocols, scanners, and patient populations across institutions. To enhance the cross-domain generalization of our model, we incorporate Variance Risk Extrapolation (VREx) into the training process. VREx encourages the model to maintain consistent performance across multiple source domains by explicitly minimizing the variance of empirical risks across environments. This regularization strategy reduces overfitting to center-specific features and promotes learning of domain-invariant representations. We further apply Mixup data augmentation to improve generalization and robustness. Mixup interpolates both the inputs and labels of randomly selected pairs of training samples, encouraging the model to behave linearly between examples and enhancing its resilience to noise and limited data. Our method achieves an average macro F1 score of 0.96 across the four sources on the validation set, demonstrating strong generalization.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2506.21387.pdf' target='_blank'>https://arxiv.org/pdf/2506.21387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaris KÃ¼ken, Lennart Purucker, Frank Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21387">Early Stopping Tabular In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tabular foundation models have shown strong performance across various tabular learning tasks via in-context learning, offering robust generalization without any downstream finetuning. However, their inference-time costs remain high, particularly for larger datasets. To address this, we propose early-stopping the in-context learning process. We achieve this by dynamically evaluating whether to stop in-context learning after each Transformer encoder layer. Once stopped, we decode the embedding using a pre-trained layer-wise decoder. Experiments across 34 small classification tasks size show that early stopping in-context learning accelerates inference by up to x1.3 with negligible degradation in predictive performance. To assess scalability, we further evaluate our method on five larger classification tasks, achieving speedups of up to x2.2. Our results demonstrate the potential of early exiting as an effective and practical strategy for improving the efficiency of tabular in-context learning.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2506.18304.pdf' target='_blank'>https://arxiv.org/pdf/2506.18304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchao Fan, Xuyang Lei, Xiaolin Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18304">Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) has emerged as a promising paradigm for autonomous driving. However, despite their advanced capabilities, DRL-based policies remain highly vulnerable to adversarial attacks, posing serious safety risks in real-world deployments. Investigating such attacks is crucial for revealing policy vulnerabilities and guiding the development of more robust autonomous systems. While prior attack methods have made notable progress, they still face several challenges: 1) they often rely on high-frequency attacks, yet critical attack opportunities are typically context-dependent and temporally sparse, resulting in inefficient attack patterns; 2) restricting attack frequency can improve efficiency but often results in unstable training due to the adversary's limited exploration. To address these challenges, we propose an adaptive expert-guided adversarial attack method that enhances both the stability and efficiency of attack policy training. Our method first derives an expert policy from successful attack demonstrations using imitation learning, strengthened by an ensemble Mixture-of-Experts architecture for robust generalization across scenarios. This expert policy then guides a DRL-based adversary through a KL-divergence regularization term. Due to the diversity of scenarios, expert policies may be imperfect. To address this, we further introduce a performance-aware annealing strategy that gradually reduces reliance on the expert as the adversary improves. Extensive experiments demonstrate that our method achieves outperforms existing approaches in terms of collision rate, attack efficiency, and training stability, especially in cases where the expert policy is sub-optimal.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2505.18445.pdf' target='_blank'>https://arxiv.org/pdf/2505.18445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiren Song, Cheng Liu, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18445">OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose \textbf{OmniConsistency}, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2505.03261.pdf' target='_blank'>https://arxiv.org/pdf/2505.03261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Ting Chen, Yu-Jiet Vong, Yi-Tsung Lee, Sy-Yen Kuo, Qiang Gao, Sizhuo Ma, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03261">DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Quality Assessment (VQA) aims to evaluate video quality based on perceptual distortions and human preferences. Despite the promising performance of existing methods using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), they often struggle to align closely with human perceptions, particularly in diverse real-world scenarios. This challenge is exacerbated by the limited scale and diversity of available datasets. To address this limitation, we introduce a novel VQA framework, DiffVQA, which harnesses the robust generalization capabilities of diffusion models pre-trained on extensive datasets. Our framework adapts these models to reconstruct identical input frames through a control module. The adapted diffusion model is then used to extract semantic and distortion features from a resizing branch and a cropping branch, respectively. To enhance the model's ability to handle long-term temporal dynamics, a parallel Mamba module is introduced, which extracts temporal coherence augmented features that are merged with the diffusion features to predict the final score. Experiments across multiple datasets demonstrate DiffVQA's superior performance on intra-dataset evaluations and its exceptional generalization across datasets. These results confirm that leveraging a diffusion model as a feature extractor can offer enhanced VQA performance compared to CNN and ViT backbones.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2502.12859.pdf' target='_blank'>https://arxiv.org/pdf/2502.12859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxing Wei, Yao Shu, Mingwen Ou, Ying Tiffany He, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12859">PAFT: Prompt-Agnostic Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning large language models (LLMs) often causes overfitting to specific prompt wording, where minor phrasing variations drastically reduce performance. To address this, we propose Prompt-Agnostic Fine-Tuning (PAFT), a method that enhances robustness through dynamic prompt variation during training. PAFT first generates diverse synthetic prompts, then continuously samples from this set to construct training instances, forcing models to learn fundamental task principles rather than surface-level patterns. Across systematic evaluations using both supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT), PAFT demonstrates substantially improved prompt robustness, achieving 7% higher generalization accuracy on unseen prompts than standard methods. In addition to enhanced robustness, PAFT consistently yields superior overall performance on established benchmarks for question answering, mathematical reasoning, and tool use. Notably, models trained with PAFT attain 3.2 faster inference speeds due to reduced prompt sensitivity. Ablation studies further validate effectiveness of PAFT, while theoretical analysis reveals that PAFT can effectively enhance the cross-domain generalization ability of LLM.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2412.12448.pdf' target='_blank'>https://arxiv.org/pdf/2412.12448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Cheng, Ran Tao, Yuliang Gu, Shenlong Wang, Xiaofeng Wang, Naira Hovakimyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12448">Task-Parameter Nexus: Task-Specific Parameter Learning for Model-Based Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the Task-Parameter Nexus (TPN), a learning-based approach for online determination of the (near-)optimal control parameters of model-based controllers (MBCs) for tracking tasks. In TPN, a deep neural network is introduced to predict the control parameters for any given tracking task at runtime, especially when optimal parameters for new tasks are not immediately available. To train this network, we constructed a trajectory bank with various speeds and curvatures that represent different motion characteristics. Then, for each trajectory in the bank, we auto-tune the optimal control parameters offline and use them as the corresponding ground truth. With this dataset, the TPN is trained by supervised learning. We evaluated the TPN on the quadrotor platform. In simulation experiments, it is shown that the TPN can predict near-optimal control parameters for a spectrum of tracking tasks, demonstrating its robust generalization capabilities to unseen tasks.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2412.11542.pdf' target='_blank'>https://arxiv.org/pdf/2412.11542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Chen, Yiwen Ye, Feilong Tang, Yongsheng Pan, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11542">Meta Curvature-Aware Minimization for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to enhance the ability of models trained on source domains to generalize effectively to unseen domains. Recently, Sharpness-Aware Minimization (SAM) has shown promise in this area by reducing the sharpness of the loss landscape to obtain more generalized models. However, SAM and its variants sometimes fail to guide the model toward a flat minimum, and their training processes exhibit limitations, hindering further improvements in model generalization. In this paper, we first propose an improved model training process aimed at encouraging the model to converge to a flat minima. To achieve this, we design a curvature metric that has a minimal effect when the model is far from convergence but becomes increasingly influential in indicating the curvature of the minima as the model approaches a local minimum. Then we derive a novel algorithm from this metric, called Meta Curvature-Aware Minimization (MeCAM), to minimize the curvature around the local minima. Specifically, the optimization objective of MeCAM simultaneously minimizes the regular training loss, the surrogate gap of SAM, and the surrogate gap of meta-learning. We provide theoretical analysis on MeCAM's generalization error and convergence rate, and demonstrate its superiority over existing DG methods through extensive experiments on five benchmark DG datasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code will be available on GitHub.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2412.00744.pdf' target='_blank'>https://arxiv.org/pdf/2412.00744.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haowei Sun, Jinwu Hu, Zhirui Zhang, Haoyuan Tian, Xinze Xie, Yufeng Wang, Zhuliang Yu, Xiaohua Xie, Mingkui Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00744">A Cross-Scene Benchmark for Open-World Drone Active Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone Visual Active Tracking aims to autonomously follow a target object by controlling the motion system based on visual observations, providing a more practical solution for effective tracking in dynamic environments. However, accurate Drone Visual Active Tracking using reinforcement learning remains challenging due to the absence of a unified benchmark, the complexity of open-world environments with frequent interference, and the diverse motion behavior of dynamic targets. To address these issues, we propose a unified cross-scene cross-domain benchmark for open-world drone active tracking called DAT. The DAT benchmark provides 24 visually complex environments to assess the algorithms' cross-scene and cross-domain generalization abilities, and high-fidelity modeling of realistic robot dynamics. Additionally, we propose a reinforcement learning-based drone tracking method called R-VAT, which aims to improve the performance of drone tracking targets in complex scenarios. Specifically, inspired by curriculum learning, we introduce a Curriculum-Based Training strategy that progressively enhances the agent tracking performance in vast environments with complex interference. We design a goal-centered reward function to provide precise feedback to the drone agent, preventing targets farther from the center of view from receiving higher rewards than closer ones. This allows the drone to adapt to the diverse motion behavior of open-world targets. Experiments demonstrate that the R-VAT has about 400% improvement over the SOTA method in terms of the cumulative reward metric.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2411.11939.pdf' target='_blank'>https://arxiv.org/pdf/2411.11939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Masroor, Tahir Hassan, Yu Tian, Kevin Wells, David Rosewarne, Thanh-Toan Do, Gustavo Carneiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11939">Fair Distillation: Teaching Fairness from Biased Teachers in Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has achieved remarkable success in image classification and segmentation tasks. However, fairness concerns persist, as models often exhibit biases that disproportionately affect demographic groups defined by sensitive attributes such as race, gender, or age. Existing bias-mitigation techniques, including Subgroup Re-balancing, Adversarial Training, and Domain Generalization, aim to balance accuracy across demographic groups, but often fail to simultaneously improve overall accuracy, group-specific accuracy, and fairness due to conflicts among these interdependent objectives. We propose the Fair Distillation (FairDi) method, a novel fairness approach that decomposes these objectives by leveraging biased ``teacher'' models, each optimized for a specific demographic group. These teacher models then guide the training of a unified ``student'' model, which distills their knowledge to maximize overall and group-specific accuracies, while minimizing inter-group disparities. Experiments on medical imaging datasets show that FairDi achieves significant gains in both overall and group-specific accuracy, along with improved fairness, compared to existing methods. FairDi is adaptable to various medical tasks, such as classification and segmentation, and provides an effective solution for equitable model performance.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2411.10136.pdf' target='_blank'>https://arxiv.org/pdf/2411.10136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihang Fu, Ziyang Chen, Yiwen Ye, Xingliang Lei, Zhisong Wang, Yong Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10136">CoSAM: Self-Correcting SAM for Domain Generalization in 2D Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical images often exhibit distribution shifts due to variations in imaging protocols and scanners across different medical centers. Domain Generalization (DG) methods aim to train models on source domains that can generalize to unseen target domains. Recently, the segment anything model (SAM) has demonstrated strong generalization capabilities due to its prompt-based design, and has gained significant attention in image segmentation tasks. Existing SAM-based approaches attempt to address the need for manual prompts by introducing prompt generators that automatically generate these prompts. However, we argue that auto-generated prompts may not be sufficiently accurate under distribution shifts, potentially leading to incorrect predictions that still require manual verification and correction by clinicians. To address this challenge, we propose a method for 2D medical image segmentation called Self-Correcting SAM (CoSAM). Our approach begins by generating coarse masks using SAM in a prompt-free manner, providing prior prompts for the subsequent stages, and eliminating the need for prompt generators. To automatically refine these coarse masks, we introduce a generalized error decoder that simulates the correction process typically performed by clinicians. Furthermore, we generate diverse prompts as feedback based on the corrected masks, which are used to iteratively refine the predictions within a self-correcting loop, enhancing the generalization performance of our model. Extensive experiments on two medical image segmentation benchmarks across multiple scenarios demonstrate the superiority of CoSAM over state-of-the-art SAM-based methods.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2409.10281.pdf' target='_blank'>https://arxiv.org/pdf/2409.10281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fa-Ting Hong, Yunfei Liu, Yu Li, Changyin Zhou, Fei Yu, Dan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10281">DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-driven talking head synthesis strives to generate lifelike video portraits from provided audio. The diffusion model, recognized for its superior quality and robust generalization, has been explored for this task. However, establishing a robust correspondence between temporal audio cues and corresponding spatial facial expressions with diffusion models remains a significant challenge in talking head generation. To bridge this gap, we present DreamHead, a hierarchical diffusion framework that learns spatial-temporal correspondences in talking head synthesis without compromising the model's intrinsic quality and adaptability.~DreamHead learns to predict dense facial landmarks from audios as intermediate signals to model the spatial and temporal correspondences.~Specifically, a first hierarchy of audio-to-landmark diffusion is first designed to predict temporally smooth and accurate landmark sequences given audio sequence signals. Then, a second hierarchy of landmark-to-image diffusion is further proposed to produce spatially consistent facial portrait videos, by modeling spatial correspondences between the dense facial landmark and appearance. Extensive experiments show that proposed DreamHead can effectively learn spatial-temporal consistency with the designed hierarchical diffusion and produce high-fidelity audio-driven talking head videos for multiple identities.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2511.05894.pdf' target='_blank'>https://arxiv.org/pdf/2511.05894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Yu, Quan Deng, Shengeng Tang, Yuehua Li, Lechao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05894">Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding 3D scenes in open-world settings poses fundamental challenges for vision and robotics, particularly due to the limitations of closed-vocabulary supervision and static annotations. To address this, we propose a unified framework for Open-World 3D Scene Graph Generation with Retrieval-Augmented Reasoning, which enables generalizable and interactive 3D scene understanding. Our method integrates Vision-Language Models (VLMs) with retrieval-based reasoning to support multimodal exploration and language-guided interaction. The framework comprises two key components: (1) a dynamic scene graph generation module that detects objects and infers semantic relationships without fixed label sets, and (2) a retrieval-augmented reasoning pipeline that encodes scene graphs into a vector database to support text/image-conditioned queries. We evaluate our method on 3DSSG and Replica benchmarks across four tasks-scene question answering, visual grounding, instance retrieval, and task planning-demonstrating robust generalization and superior performance in diverse environments. Our results highlight the effectiveness of combining open-vocabulary perception with retrieval-based reasoning for scalable 3D scene understanding.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2510.20943.pdf' target='_blank'>https://arxiv.org/pdf/2510.20943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srivathsan Badrinarayanan, Yue Su, Janghoon Ock, Alan Pham, Sanya Ahuja, Amir Barati Farimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20943">Meta-Learning for Cross-Task Generalization in Protein Mutation Property Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein mutations can have profound effects on biological function, making accurate prediction of property changes critical for drug discovery, protein engineering, and precision medicine. Current approaches rely on fine-tuning protein-specific transformers for individual datasets, but struggle with cross-dataset generalization due to heterogeneous experimental conditions and limited target domain data. We introduce two key innovations: (1) the first application of Model-Agnostic Meta-Learning (MAML) to protein mutation property prediction, and (2) a novel mutation encoding strategy using separator tokens to directly incorporate mutations into sequence context. We build upon transformer architectures integrating them with MAML to enable rapid adaptation to new tasks through minimal gradient steps rather than learning dataset-specific patterns. Our mutation encoding addresses the critical limitation where standard transformers treat mutation positions as unknown tokens, significantly degrading performance. Evaluation across three diverse protein mutation datasets (functional fitness, thermal stability, and solubility) demonstrates significant advantages over traditional fine-tuning. In cross-task evaluation, our meta-learning approach achieves 29% better accuracy for functional fitness with 65% less training time, and 94% better accuracy for solubility with 55% faster training. The framework maintains consistent training efficiency regardless of dataset size, making it particularly valuable for industrial applications and early-stage protein design where experimental data is limited. This work establishes a systematic application of meta-learning to protein mutation analysis and introduces an effective mutation encoding strategy, offering transformative methodology for cross-domain generalization in protein engineering.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2509.15791.pdf' target='_blank'>https://arxiv.org/pdf/2509.15791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan Pan, Kaiyu Guo, Dongli Xu, Zhaorui Tan, Chen Jiang, Deshu Chen, Xin Guo, Brian C. Lovell, Limei Han, Yuan Cheng, Mahsa Baktashmotlagh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15791">Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization ability of deep learning has been extensively studied in supervised settings, yet it remains less explored in unsupervised scenarios. Recently, the Unsupervised Domain Generalization (UDG) task has been proposed to enhance the generalization of models trained with prevalent unsupervised learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the challenge of distinguishing semantics from variations without category labels. Although some recent methods have employed domain labels to tackle this issue, such domain labels are often unavailable in real-world contexts. In this paper, we address these limitations by formalizing UDG as the task of learning a Minimal Sufficient Semantic Representation: a representation that (i) preserves all semantic information shared across augmented views (sufficiency), and (ii) maximally removes information irrelevant to semantics (minimality). We theoretically ground these objectives from the perspective of information theory, demonstrating that optimizing representations to achieve sufficiency and minimality directly reduces out-of-distribution risk. Practically, we implement this optimization through Minimal-Sufficient UDG (MS-UDG), a learnable model by integrating (a) an InfoNCE-based objective to achieve sufficiency; (b) two complementary components to promote minimality: a novel semantic-variation disentanglement loss and a reconstruction-based mechanism for capturing adequate variation. Empirically, MS-UDG sets a new state-of-the-art on popular unsupervised domain-generalization benchmarks, consistently outperforming existing SSL and UDG methods, without category or domain labels during representation learning.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2505.12585.pdf' target='_blank'>https://arxiv.org/pdf/2505.12585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>En Yu, Jie Lu, Xiaoyu Yang, Guangquan Zhang, Zhen Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12585">Learning Robust Spectral Dynamics for Temporal Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern machine learning models struggle to maintain performance in dynamic environments where temporal distribution shifts, \emph{i.e., concept drift}, are prevalent. Temporal Domain Generalization (TDG) seeks to enable model generalization across evolving domains, yet existing approaches typically assume smooth incremental changes, struggling with complex real-world drifts involving long-term structure (incremental evolution/periodicity) and local uncertainties. To overcome these limitations, we introduce FreKoo, which tackles these challenges via a novel frequency-domain analysis of parameter trajectories. It leverages the Fourier transform to disentangle parameter evolution into distinct spectral bands. Specifically, low-frequency component with dominant dynamics are learned and extrapolated using the Koopman operator, robustly capturing diverse drift patterns including both incremental and periodicity. Simultaneously, potentially disruptive high-frequency variations are smoothed via targeted temporal regularization, preventing overfitting to transient noise and domain uncertainties. In addition, this dual spectral strategy is rigorously grounded through theoretical analysis, providing stability guarantees for the Koopman prediction, a principled Bayesian justification for the high-frequency regularization, and culminating in a multiscale generalization bound connecting spectral dynamics to improved generalization. Extensive experiments demonstrate FreKoo's significant superiority over SOTA TDG approaches, particularly excelling in real-world streaming scenarios with complex drifts and uncertainties.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2505.08464.pdf' target='_blank'>https://arxiv.org/pdf/2505.08464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lata Pangtey, Anukriti Bhatnagar, Shubhi Bansal, Shahid Shafi Dar, Nagendra Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08464">Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2502.20790.pdf' target='_blank'>https://arxiv.org/pdf/2502.20790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawei Zhu, Xiyu Wei, Guangxiang Zhao, Wenhao Wu, Haosheng Zou, Junfeng Ran, Xun Wang, Lin Sun, Xiangzheng Zhang, Sujian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20790">Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have highlighted the challenge of handling long-context tasks, where models need to reason over extensive input contexts to aggregate target information. While Chain-of-Thought (CoT) prompting has shown promise for multi-step reasoning, its effectiveness for long-context scenarios remains underexplored. Through systematic investigation across diverse tasks, we demonstrate that CoT's benefits generalize across most long-context scenarios and amplify with increasing context length. Motivated by this critical observation, we propose LongRePS, a process-supervised framework that teaches models to generate high-quality reasoning paths for enhanced long-context performance. Our framework incorporates a self-sampling mechanism to bootstrap reasoning paths and a novel quality assessment protocol specifically designed for long-context scenarios. Experimental results on various long-context benchmarks demonstrate the effectiveness of our approach, achieving significant improvements over outcome supervision baselines on both in-domain tasks (+13.6/+3.8 points for LLaMA/Qwen on MuSiQue) and cross-domain generalization (+9.3/+8.1 points on average across diverse QA tasks). Our code, data and trained models are made public to facilitate future research.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2601.07005.pdf' target='_blank'>https://arxiv.org/pdf/2601.07005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Yu, Yixuan Li, Hai Xu, Kang Xu, Junjielong Xu, Zhijing Li, Pinjia He, Wanyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07005">MicLog: Towards Accurate and Efficient LLM-based Log Parsing via Progressive Meta In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Log parsing converts semi-structured logs into structured templates, forming a critical foundation for downstream analysis. Traditional syntax and semantic-based parsers often struggle with semantic variations in evolving logs and data scarcity stemming from their limited domain coverage. Recent large language model (LLM)-based parsers leverage in-context learning (ICL) to extract semantics from examples, demonstrating superior accuracy. However, LLM-based parsers face two main challenges: 1) underutilization of ICL capabilities, particularly in dynamic example selection and cross-domain generalization, leading to inconsistent performance; 2) time-consuming and costly LLM querying. To address these challenges, we present MicLog, the first progressive meta in-context learning (ProgMeta-ICL) log parsing framework that combines meta-learning with ICL on small open-source LLMs (i.e., Qwen-2.5-3B). Specifically, MicLog: i) enhances LLMs' ICL capability through a zero-shot to k-shot ProgMeta-ICL paradigm, employing weighted DBSCAN candidate sampling and enhanced BM25 demonstration selection; ii) accelerates parsing via a multi-level pre-query cache that dynamically matches and refines recently parsed templates. Evaluated on Loghub-2.0, MicLog achieves 10.3% higher parsing accuracy than the state-of-the-art parser while reducing parsing time by 42.4%.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2511.12265.pdf' target='_blank'>https://arxiv.org/pdf/2511.12265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Wang, Zeming Wei, Xiyue Zhang, Meng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12265">Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2510.11184.pdf' target='_blank'>https://arxiv.org/pdf/2510.11184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11184">Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains underexplored. In this work, we investigate the cross-domain generalization of an LLM agent equipped with a code interpreter tool, which is exclusively trained on mathematical problem-solving tasks. Despite the restricted training domain, we evaluate the agent's performance across several distinct reasoning domains. The results reveal that RL-based tool usage learned from mathematical tasks can be effectively transferred to complex tasks in other domains, enabling great task performance and high token efficiency. To facilitate this cross-domain transfer, we propose a Tool Generalization Reinforcement Learning (TGRL) framework designed to promote domain-agnostic learning and skill migration, encompassing: (i) a standardized tool interface that abstracts domain-specific nuances through consistent formatting and explicit termination, fostering transferable invocation patterns; (ii) a dual-component reward system that decomposes rewards to incentivize generalizable behaviors like tool efficiency and reasoning abstraction, ensuring alignment and robustness across domain shifts; and (iii) an XML-based prompt template that separates thinking, tool calls, and responses to encourage modular, domain-invariant planning and coherent multi-turn interactions. Extensive experiments across diverse benchmarks validate our approach, achieving state-of-the-art performance and highlighting the cross-domain potential of Tool RL for LLM reasoning.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2509.24797.pdf' target='_blank'>https://arxiv.org/pdf/2509.24797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zizhao Tong, Di Chen, Sicheng Hu, Hongwei Fan, Liliang Chen, Guanghui Ren, Hao Tang, Hao Dong, Ling Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24797">Fidelity-Aware Data Composition for Robust Robot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as $Ï_0$ and Diffusion Policy improves OOD success rates by over 54\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2509.12747.pdf' target='_blank'>https://arxiv.org/pdf/2509.12747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Botao He, Amir Hossein Shahidzadeh, Yu Chen, Jiayi Wu, Tianrui Guan, Guofei Chen, Howie Choset, Dinesh Manocha, Glen Chou, Cornelia Fermuller, Yiannis Aloimonos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12747">NavMoE: Hybrid Model- and Learning-based Traversability Estimation for Local Navigation via Mixture of Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores traversability estimation for robot navigation. A key bottleneck in traversability estimation lies in efficiently achieving reliable and robust predictions while accurately encoding both geometric and semantic information across diverse environments. We introduce Navigation via Mixture of Experts (NAVMOE), a hierarchical and modular approach for traversability estimation and local navigation. NAVMOE combines multiple specialized models for specific terrain types, each of which can be either a classical model-based or a learning-based approach that predicts traversability for specific terrain types. NAVMOE dynamically weights the contributions of different models based on the input environment through a gating network. Overall, our approach offers three advantages: First, NAVMOE enables traversability estimation to adaptively leverage specialized approaches for different terrains, which enhances generalization across diverse and unseen environments. Second, our approach significantly improves efficiency with negligible cost of solution quality by introducing a training-free lazy gating mechanism, which is designed to minimize the number of activated experts during inference. Third, our approach uses a two-stage training strategy that enables the training for the gating networks within the hybrid MoE method that contains nondifferentiable modules. Extensive experiments show that NAVMOE delivers a better efficiency and performance balance than any individual expert or full ensemble across different domains, improving cross-domain generalization and reducing average computational cost by 81.2% via lazy gating, with less than a 2% loss in path quality.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2508.02458.pdf' target='_blank'>https://arxiv.org/pdf/2508.02458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Yichao, Haoran Luo, Lang Feng, Shuai Zhao, Anh Tuan Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02458">From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models show promise in emotion understanding, social reasoning, and empathy, yet they struggle with psychologically grounded tasks that require inferring implicit mental states in context-rich, ambiguous settings. These limitations arise from the absence of theory-aligned supervision and the difficulty of capturing nuanced mental processes in real-world narratives. To address this gap, we leverage expert-labeled, psychologically rich scenarios and propose a trajectory-aware reinforcement learning framework that explicitly imitates expert psychological thought patterns. By integrating real-world stimuli with structured reasoning guidance, our approach enables compact models to internalize social-cognitive principles, perform nuanced psychological inference, and support continual self-improvement. Comprehensive experiments across multiple benchmarks further demonstrate that our models achieve expert-level interpretive capabilities, exhibiting strong out-of-distribution generalization and robust continual learning across diverse, challenging, and psychologically grounded tasks.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2507.09884.pdf' target='_blank'>https://arxiv.org/pdf/2507.09884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuzhao Li, Xuchen Li, Shiyu Hu, Yongzhen Guo, Wentao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09884">VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) increasingly rely on reinforcement learning (RL) to enhance their reasoning capabilities through feedback. A critical challenge is verifying the consistency of model-generated responses and reference answers, since these responses are often lengthy, diverse, and nuanced. Rule-based verifiers struggle with complexity, prompting the use of model-based verifiers. However, specialized verifiers lack flexibility, while general LLM judges can be inconsistent. Existing research primarily focuses on building better verifiers, yet a systematic evaluation of different types of verifiers' performance across domains remains lacking, severely constraining the reliable development of Reinforcement Learning with Verifiable Reward (RLVR). To address this, we propose VerifyBench--a cross-domain comprehensive benchmark for systematically evaluating verifiers. We construct 4,000 expert-level questions covering mathematics, physics, chemistry, and biology. Each question is equipped with reference answers and diverse responses. The reliability of the evaluation is ensured through a rigorous annotation process conducted by a multidisciplinary expert team. We design a four-dimensional experimental framework to comprehensively compare the performance boundaries of specialized verifiers and general LLMs under combined conditions of extracted answers vs. complete responses, and short vs. long outputs. Our evaluation uncovers fundamental trade-offs in verifiers: while specialized verifiers achieve leading accuracy, they exhibit deficiencies in recall; general models show stronger inclusivity but unstable precision. More importantly, we discover verifiers' high sensitivity to input structure and inherent limitations in cross-domain generalization, providing critical insights into the bottlenecks of current verifier technology.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2505.09484.pdf' target='_blank'>https://arxiv.org/pdf/2505.09484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Ma, Xun Lin, Zitong Yu, Xin Liu, Xiaochen Yuan, Weicheng Xie, Linlin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09484">Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face Anti-Spoofing (FAS) is essential for the security of facial recognition systems in diverse scenarios such as payment processing and surveillance. Current multimodal FAS methods often struggle with effective generalization, mainly due to modality-specific biases and domain shifts. To address these challenges, we introduce the \textbf{M}ulti\textbf{m}odal \textbf{D}enoising and \textbf{A}lignment (\textbf{MMDA}) framework. By leveraging the zero-shot generalization capability of CLIP, the MMDA framework effectively suppresses noise in multimodal data through denoising and alignment mechanisms, thereby significantly enhancing the generalization performance of cross-modal alignment. The \textbf{M}odality-\textbf{D}omain Joint \textbf{D}ifferential \textbf{A}ttention (\textbf{MD2A}) module in MMDA concurrently mitigates the impacts of domain and modality noise by refining the attention mechanism based on extracted common noise features. Furthermore, the \textbf{R}epresentation \textbf{S}pace \textbf{S}oft (\textbf{RS2}) Alignment strategy utilizes the pre-trained CLIP model to align multi-domain multimodal data into a generalized representation space in a flexible manner, preserving intricate representations and enhancing the model's adaptability to various unseen conditions. We also design a \textbf{U}-shaped \textbf{D}ual \textbf{S}pace \textbf{A}daptation (\textbf{U-DSA}) module to enhance the adaptability of representations while maintaining generalization performance. These improvements not only enhance the framework's generalization capabilities but also boost its ability to represent complex representations. Our experimental results on four benchmark datasets under different evaluation protocols demonstrate that the MMDA framework outperforms existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy. The code will be released soon.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2503.16106.pdf' target='_blank'>https://arxiv.org/pdf/2503.16106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad Hassan N C, Divyam Gupta, Mainak Singha, Sai Bhargav Rongali, Ankit Jha, Muhammad Haris Khan, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16106">OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel paradigm unifying low-shot learning with open-set domain generalization (ODG). While prompt-based methods using models like CLIP have advanced DG, they falter in low-data regimes (e.g., 1-shot) and lack precision in detecting open-set samples with fine-grained semantics related to training classes. To address these challenges, we propose OSLOPROMPT, an advanced prompt-learning framework for CLIP with two core innovations. First, to manage limited supervision across source domains and improve DG, we introduce a domain-agnostic prompt-learning mechanism that integrates adaptable domain-specific cues and visually guided semantic attributes through a novel cross-attention module, besides being supported by learnable domain- and class-generic visual prompts to enhance cross-modal adaptability. Second, to improve outlier rejection during inference, we classify unfamiliar samples as "unknown" and train specialized prompts with systematically synthesized pseudo-open samples that maintain fine-grained relationships to known classes, generated through a targeted query strategy with off-the-shelf foundation models. This strategy enhances feature learning, enabling our model to detect open samples with varied granularity more effectively. Extensive evaluations across five benchmarks demonstrate that OSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly outperforming existing methods.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2411.17458.pdf' target='_blank'>https://arxiv.org/pdf/2411.17458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, Yueting Zhuang, Yiqi Huang, Luhui Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17458">Spatially Visual Perception for End-to-End Robotic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in imitation learning have shown significant promise for robotic control and embodied intelligence. However, achieving robust generalization across diverse mounted camera observations remains a critical challenge. In this paper, we introduce a video-based spatial perception framework that leverages 3D spatial representations to address environmental variability, with a focus on handling lighting changes. Our approach integrates a novel image augmentation technique, AugBlender, with a state-of-the-art monocular depth estimation model trained on internet-scale data. Together, these components form a cohesive system designed to enhance robustness and adaptability in dynamic scenarios. Our results demonstrate that our approach significantly boosts the success rate across diverse camera exposures, where previous models experience performance collapse. Our findings highlight the potential of video-based spatial perception models in advancing robustness for end-to-end robotic learning, paving the way for scalable, low-cost solutions in embodied intelligence.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2411.04847.pdf' target='_blank'>https://arxiv.org/pdf/2411.04847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04847">Prompt-Guided Internal States for Hallucination Detection of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of tasks in different domains. However, they sometimes generate responses that are logically coherent but factually incorrect or misleading, which is known as LLM hallucinations. Data-driven supervised methods train hallucination detectors by leveraging the internal states of LLMs, but detectors trained on specific domains often struggle to generalize well to other domains. In this paper, we aim to enhance the cross-domain performance of supervised detectors with only in-domain data. We propose a novel framework, prompt-guided internal states for hallucination detection of LLMs, namely PRISM. By utilizing appropriate prompts to guide changes to the structure related to text truthfulness in LLMs' internal states, we make this structure more salient and consistent across texts from different domains. We integrated our framework with existing hallucination detection methods and conducted experiments on datasets from different domains. The experimental results indicate that our framework significantly enhances the cross-domain generalization of existing hallucination detection methods.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2511.15308.pdf' target='_blank'>https://arxiv.org/pdf/2511.15308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Xia, Letian Shi, Yilin Di, Joao F. Henriques, Daniel Cremers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15308">Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2511.11065.pdf' target='_blank'>https://arxiv.org/pdf/2511.11065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muskaan Chopra, Lorenz Sparrenberg, Armin Berger, Sarthak Khanna, Jan H. Terheyden, Rafet Sifa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11065">From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2510.02787.pdf' target='_blank'>https://arxiv.org/pdf/2510.02787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Zdenek, Wataru Shimoda, Kota Yamaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02787">OTR: Synthesizing Overlay Text Dataset for Text Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text removal is a crucial task in computer vision with applications such as privacy preservation, image editing, and media reuse. While existing research has primarily focused on scene text removal in natural images, limitations in current datasets hinder out-of-domain generalization or accurate evaluation. In particular, widely used benchmarks such as SCUT-EnsText suffer from ground truth artifacts due to manual editing, overly simplistic text backgrounds, and evaluation metrics that do not capture the quality of generated results. To address these issues, we introduce an approach to synthesizing a text removal benchmark applicable to domains other than scene texts. Our dataset features text rendered on complex backgrounds using object-aware placement and vision-language model-generated content, ensuring clean ground truth and challenging text removal scenarios. The dataset is available at https://huggingface.co/datasets/cyberagent/OTR .
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2506.08516.pdf' target='_blank'>https://arxiv.org/pdf/2506.08516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mouadh Yagoubi, David Danan, Milad Leyli-Abadi, Ahmed Mazari, Jean-Patrick Brunet, Abbas Kabalan, Fabien Casenave, Yuxin Ma, Giovanni Catalani, Jean Fesquet, Jacob Helwig, Xuan Zhang, Haiyang Yu, Xavier Bertrand, Frederic Tost, Michael Baurheim, Joseph Morlier, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08516">NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of machine learning (ML) into the physical sciences is reshaping computational paradigms, offering the potential to accelerate demanding simulations such as computational fluid dynamics (CFD). Yet, persistent challenges in accuracy, generalization, and physical consistency hinder the practical deployment of ML models in scientific domains. To address these limitations and systematically benchmark progress, we organized the ML4CFD competition, centered on surrogate modeling for aerodynamic simulations over two-dimensional airfoils. The competition attracted over 240 teams, who were provided with a curated dataset generated via OpenFOAM and evaluated through a multi-criteria framework encompassing predictive accuracy, physical fidelity, computational efficiency, and out-of-distribution generalization. This retrospective analysis reviews the competition outcomes, highlighting several approaches that outperformed baselines under our global evaluation score. Notably, the top entry exceeded the performance of the original OpenFOAM solver on aggregate metrics, illustrating the promise of ML-based surrogates to outperform traditional solvers under tailored criteria. Drawing from these results, we analyze the key design principles of top submissions, assess the robustness of our evaluation framework, and offer guidance for future scientific ML challenges.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2506.00027.pdf' target='_blank'>https://arxiv.org/pdf/2506.00027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyu Chen, Yudong Wang, Teng Xiao, Ruochen Zhou, Xuesheng Yang, Wei Wang, Zhifang Sui, Jingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00027">From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in improving the reasoning capabilities of Large Language Models have underscored the efficacy of Process Reward Models (PRMs) in addressing intermediate errors through structured feedback mechanisms. This study analyzes PRMs from multiple perspectives, including training methodologies, scalability, and generalization capabilities. We investigate the interplay between pre-training and reward model training FLOPs to assess their influence on PRM efficiency and accuracy in complex reasoning tasks. Our analysis reveals a pattern of diminishing returns in performance with increasing PRM scale, highlighting the importance of balancing model size and computational cost. Furthermore, the diversity of training datasets significantly impacts PRM performance, emphasizing the importance of diverse data to enhance both accuracy and efficiency. We further examine test-time scaling strategies, identifying Monte Carlo Tree Search as the most effective method when computational resources are abundant, while Best-of-N Sampling serves as a practical alternative under resource-limited conditions. Notably, our findings indicate that PRMs trained on mathematical datasets exhibit performance comparable to those tailored for code generation, suggesting robust cross-domain generalization. Employing a gradient-based metric, we observe that PRMs exhibit a preference for selecting responses with similar underlying patterns, further informing their optimization.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2503.08180.pdf' target='_blank'>https://arxiv.org/pdf/2503.08180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyue Dai, Ke Fan, Bin Ji, Haoran Xu, Haoyu Zhao, Junting Dong, Jingbo Wang, Bo Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08180">Towards Synthesized and Editable Motion In-Betweening Through Part-Wise Phase Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Styled motion in-betweening is crucial for computer animation and gaming. However, existing methods typically encode motion styles by modeling whole-body motions, often overlooking the representation of individual body parts. This limitation reduces the flexibility of infilled motion, particularly in adjusting the motion styles of specific limbs independently. To overcome this challenge, we propose a novel framework that models motion styles at the body-part level, enhancing both the diversity and controllability of infilled motions. Our approach enables more nuanced and expressive animations by allowing precise modifications to individual limb motions while maintaining overall motion coherence. Leveraging phase-related insights, our framework employs periodic autoencoders to automatically extract the phase of each body part, capturing distinctive local style features. Additionally, we effectively decouple the motion source from synthesis control by integrating motion manifold learning and conditional generation techniques from both image and motion domains. This allows the motion source to generate high-quality motions across various styles, with extracted motion and style features readily available for controlled synthesis in subsequent tasks. Comprehensive evaluations demonstrate that our method achieves superior speed, robust generalization, and effective generation of extended motion sequences.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2502.08975.pdf' target='_blank'>https://arxiv.org/pdf/2502.08975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Li, Yida Xiong, Hongzhi Zhang, Xiantao Cai, Jia Wu, Bo Du, Wenbin Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08975">Graph-structured Small Molecule Drug Discovery Through Deep Learning: Progress, Challenges, and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to their excellent drug-like and pharmacokinetic properties, small molecule drugs are widely used to treat various diseases, making them a critical component of drug discovery. In recent years, with the rapid development of deep learning (DL) techniques, DL-based small molecule drug discovery methods have achieved excellent performance in prediction accuracy, speed, and complex molecular relationship modeling compared to traditional machine learning approaches. These advancements enhance drug screening efficiency and optimization and provide more precise and effective solutions for various drug discovery tasks. Contributing to this field's development, this paper aims to systematically summarize and generalize the recent key tasks and representative techniques in graph-structured small molecule drug discovery in recent years. Specifically, we provide an overview of the major tasks in small molecule drug discovery and their interrelationships. Next, we analyze the six core tasks, summarizing the related methods, commonly used datasets, and technological development trends. Finally, we discuss key challenges, such as interpretability and out-of-distribution generalization, and offer our insights into future research directions for small molecule drug discovery.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2502.03387.pdf' target='_blank'>https://arxiv.org/pdf/2502.03387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03387">LIMO: Less is More for Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We challenge the prevailing assumption that complex reasoning in large language models (LLMs) necessitates massive training data. We demonstrate that sophisticated mathematical reasoning can emerge with only a few examples. Specifically, through simple supervised fine-tuning, our model, LIMO, achieves 63.3\% accuracy on AIME24 and 95.6\% on MATH500, surpassing previous fine-tuned models (6.5\% on AIME24, 59.2\% on MATH500) while using only 1\% of the training data required by prior approaches. Furthermore, LIMO exhibits strong out-of-distribution generalization, achieving a 45.8\% absolute improvement across diverse benchmarks, outperforming models trained on 100x more data. Synthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning can emerge through minimal but strategically designed demonstrations of cognitive processes. This hypothesis suggests that the threshold for eliciting complex reasoning is not dictated by task complexity but rather by two key factors: (1) the completeness of the model's pre-trained knowledge base and (2) the effectiveness of post-training examples in serving as "cognitive templates" that guide reasoning.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2411.17798.pdf' target='_blank'>https://arxiv.org/pdf/2411.17798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangbin Zheng, Qianhui Xu, Ruichen Xia, Stan Z. Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17798">DapPep: Domain Adaptive Peptide-agnostic Learning for Universal T-cell Receptor-antigen Binding Affinity Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying T-cell receptors (TCRs) that interact with antigenic peptides provides the technical basis for developing vaccines and immunotherapies. The emergent deep learning methods excel at learning antigen binding patterns from known TCRs but struggle with novel or sparsely represented antigens. However, binding specificity for unseen antigens or exogenous peptides is critical. We introduce a domain-adaptive peptide-agnostic learning framework DapPep for universal TCR-antigen binding affinity prediction to address this challenge. The lightweight self-attention architecture combines a pre-trained protein language model with an inner-loop self-supervised regime to enable robust TCR-peptide representations. Extensive experiments on various benchmarks demonstrate that DapPep consistently outperforms existing tools, showcasing robust generalization capability, especially for data-scarce settings and unseen peptides. Moreover, DapPep proves effective in challenging clinical tasks such as sorting reactive T cells in tumor neoantigen therapy and identifying key positions in 3D structures.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2411.06842.pdf' target='_blank'>https://arxiv.org/pdf/2411.06842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vladyslav Zalevskyi, Thomas Sanchez, Margaux Roulet, HÃ©lÃ¨ne Lajous, Jordina Aviles Verdera, Roxane Licandro, Georg Langs, Gregor Kasprian, Jana Hutter, Hamza Kebiri, Meritxell Bach Cuadra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06842">DRIFTS: Optimizing Domain Randomization with Synthetic Data and Weight Interpolation for Fetal Brain Tissue Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fetal brain tissue segmentation in magnetic resonance imaging (MRI) is a crucial tool that supports understanding of neurodevelopment, yet it faces challenges due to the heterogeneity of data coming from different scanners and settings, as well as data scarcity. Recent approaches based on domain randomization, like SynthSeg, have shown great potential for single-source domain generalization by simulating images with randomized contrast and image resolution from the label maps. In this work, we investigate how to maximize the out-of-domain (OOD) generalization potential of SynthSegbased methods in fetal brain MRI. Specifically, we demonstrate that the simple Gaussian mixture models employed in FetalSynthSeg outperform physics-informed generation methods in terms of OOD generalization. We further show that incorporating intensity clustering significantly enhances generalization in settings with limited label classes by producing more realistic synthetic data. By combining synthetic pretraining with fine-tuning on real images and applying weight-space interpolation between the two models, we propose DRIFTS as an effective and practical solution for single-source domain generalization. DRIFTS consistently outperforms current state-of-the-art models across multiple benchmarks and is, to our knowledge, the first method to achieve accurate brain tissue segmentation on fetal T1-weighted images. We validate our approach on 308 subjects from four datasets acquired at three different sites, covering a range of scanner field strengths (0.55T to 3T) and both T1w and T2w modalities. We conclude with five practical recommendations to guide the development of SynthSeg-based methods for other organs and imaging modalities.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2512.10224.pdf' target='_blank'>https://arxiv.org/pdf/2512.10224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ragja Palakkadavath, Hung Le, Thanh Nguyen-Tang, Svetha Venkatesh, Sunil Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10224">Federated Domain Generalization with Latent Space Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2511.19912.pdf' target='_blank'>https://arxiv.org/pdf/2511.19912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dapeng Zhang, Zhenlong Yuan, Zhangquan Chen, Chih-Ting Liao, Yinda Chen, Fei Shen, Qingguo Zhou, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19912">Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2511.10352.pdf' target='_blank'>https://arxiv.org/pdf/2511.10352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengzhu Wang, Changyuan Deng, Shanshan Wang, Nan Yin, Long Lan, Liang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10352">FOUND: Fourier-based von Mises Distribution for Robust Single Domain Generalization in Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Domain Generalization (SDG) for object detection aims to train a model on a single source domain that can generalize effectively to unseen target domains. While recent methods like CLIP-based semantic augmentation have shown promise, they often overlook the underlying structure of feature distributions and frequency-domain characteristics that are critical for robustness. In this paper, we propose a novel framework that enhances SDG object detection by integrating the von Mises-Fisher (vMF) distribution and Fourier transformation into a CLIP-guided pipeline. Specifically, we model the directional features of object representations using vMF to better capture domain-invariant semantic structures in the embedding space. Additionally, we introduce a Fourier-based augmentation strategy that perturbs amplitude and phase components to simulate domain shifts in the frequency domain, further improving feature robustness. Our method not only preserves the semantic alignment benefits of CLIP but also enriches feature diversity and structural consistency across domains. Extensive experiments on the diverse weather-driving benchmark demonstrate that our approach outperforms the existing state-of-the-art method.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2511.05293.pdf' target='_blank'>https://arxiv.org/pdf/2511.05293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Yan, Yibo Li, Han Ding, Fei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05293">Cross-domain EEG-based Emotion Recognition with Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalogram (EEG)-based emotion recognition is vital for affective computing but faces challenges in feature utilization and cross-domain generalization. This work introduces EmotionCLIP, which reformulates recognition as an EEG-text matching task within the CLIP framework. A tailored backbone, SST-LegoViT, captures spatial, spectral, and temporal features using multi-scale convolution and Transformer modules. Experiments on SEED and SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%, and cross-time accuracies of 88.46% and 77.54%, outperforming existing models. Results demonstrate the effectiveness of multimodal contrastive learning for robust EEG emotion recognition.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2509.25856.pdf' target='_blank'>https://arxiv.org/pdf/2509.25856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Han Huang, Jeng-Lin Li, Po-Hsuan Huang, Ming-Ching Chang, Wei-Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25856">PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection is increasingly relying on foundation models, aiming for strong out-of-distribution generalization and rapid adaptation in real-world deployments. Notably, past studies have primarily focused on textual prompt tuning, leaving the intrinsic visual counterpart fragmented into processing steps specific to each foundation model. We aim to address this limitation by proposing a unified patch-focused framework, Patch-Exclusive Anomaly Detection (PatchEAD), enabling training-free anomaly detection that is compatible with diverse foundation models. The framework constructs visual prompting techniques, including an alignment module and foreground masking. Our experiments show superior few-shot and batch zero-shot performance compared to prior work, despite the absence of textual features. Our study further examines how backbone structure and pretrained characteristics affect patch-similarity robustness, providing actionable guidance for selecting and configuring foundation models for real-world visual inspection. These results confirm that a well-unified patch-only framework can enable quick, calibration-light deployment without the need for carefully engineered textual prompts.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2509.22854.pdf' target='_blank'>https://arxiv.org/pdf/2509.22854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqian Li, Yanshu Li, Ligong Han, Ruixiang Tang, Wenya Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22854">Towards Generalizable Implicit In-Context Learning with Attention Routing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Implicit in-context learning (ICL) has newly emerged as a promising paradigm that simulates ICL behaviors in the representation space of Large Language Models (LLMs), aiming to attain few-shot performance at zero-shot cost. However, existing approaches largely rely on injecting shift vectors into residual flows, which are typically constructed from labeled demonstrations or task-specific alignment. Such designs fall short of utilizing the structural mechanisms underlying ICL and suffer from limited generalizability. To address this, we propose In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level. It extracts reusable structural directions that emerge during ICL and employs a learnable input-conditioned router to modulate attention logits accordingly, enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world datasets spanning diverse domains and multiple LLMs. The results show that ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle. These findings position ICR to push the boundary of ICL's practical value.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2507.16795.pdf' target='_blank'>https://arxiv.org/pdf/2507.16795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16795">Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2507.08218.pdf' target='_blank'>https://arxiv.org/pdf/2507.08218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atticus Wang, Joshua Engels, Oliver Clive-Griffin, Senthooran Rajamanoharan, Neel Nanda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08218">Simple Mechanistic Explanations for Out-Of-Context Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs exhibit surprisingly deep out-of-distribution generalization. Rather than learning shallow heuristics, they implicitly internalize and act on the consequences of observations scattered throughout the fine-tuning data. In this work, we investigate this phenomenon mechanistically and find that many instances of OOCR in the literature have a simple explanation: the LoRA fine-tuning essentially adds a constant steering vector, steering the model towards a general concept. This improves performance on the fine-tuning task and in many other concept-related domains, causing the surprising generalization. Moreover, we can directly train steering vectors for these tasks from scratch, which also induces OOCR. We find that our results hold even for a task that seems like it must involve conditional behavior (model backdoors); it turns out that unconditionally adding a steering vector is sufficient. Overall, our work presents one explanation of what gets learned during fine-tuning for OOCR tasks, contributing to the key question of why LLMs can reason out of context, an advanced capability that is highly relevant to their safe and reliable deployment.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2507.04391.pdf' target='_blank'>https://arxiv.org/pdf/2507.04391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochen Zhou, Minrui Xu, Shiqi Chen, Junteng Liu, Yunqi Li, Xinxin Lin, Zhengyu Chen, Junxian He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04391">Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been a growing interest in enhancing the mathematical problem-solving (MPS) capabilities of large language models. While the majority of research efforts concentrate on creating specialized models to solve mathematical problems, it remains unknown how learning mathematical problem-solving generalizes to help develop other reasoning abilities. In this paper, we present an empirical investigation into the generalization potential of various MPS training approaches, such as continual pretraining, instruction tuning, and rule-based reinforcement learning across various data sources, including both short and long chain-of-thought (CoT) samples. Evaluation on 5 mathematical and 8 general reasoning benchmarks show that continual pretraining on math text is able to generalize to general reasoning tasks to some extent. In constrast, instruction tuning on conventional, short MPS samples provides limited benefits and, in many cases, even impairs generalization performance. Notably, training with long CoT responses for MPS samples and incorporating rule-based reinforcement learning on MPS queries exhibit distinct behavior, significantly enhancing generalization by extending the model's reasoning processes into other domains. These results suggest that traditional approaches to learning MPS with short reasoning chains largely fail to achieve robust generalization. However, the emerging paradigm of longer reasoning chains, coupled with self-reflection, offers a promising direction for improving generalized reasoning abilities through learning from specialized domains.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2506.13215.pdf' target='_blank'>https://arxiv.org/pdf/2506.13215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenlong Yuan, Dapeng Zhang, Zehao Li, Chengxuan Qian, Jianing Chen, Yinda Chen, Kehua Chen, Tianlu Mao, Zhaoxin Li, Hao Jiang, Zhaoqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13215">DVP-MVS++: Synergize Depth-Normal-Edge and Harmonized Visibility Prior for Multi-View Stereo</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, patch deformation-based methods have demonstrated significant effectiveness in multi-view stereo due to their incorporation of deformable and expandable perception for reconstructing textureless areas. However, these methods generally focus on identifying reliable pixel correlations to mitigate matching ambiguity of patch deformation, while neglecting the deformation instability caused by edge-skipping and visibility occlusions, which may cause potential estimation deviations. To address these issues, we propose DVP-MVS++, an innovative approach that synergizes both depth-normal-edge aligned and harmonized cross-view priors for robust and visibility-aware patch deformation. Specifically, to avoid edge-skipping, we first apply DepthPro, Metric3Dv2 and Roberts operator to generate coarse depth maps, normal maps and edge maps, respectively. These maps are then aligned via an erosion-dilation strategy to produce fine-grained homogeneous boundaries for facilitating robust patch deformation. Moreover, we reformulate view selection weights as visibility maps, and then implement both an enhanced cross-view depth reprojection and an area-maximization strategy to help reliably restore visible areas and effectively balance deformed patch, thus acquiring harmonized cross-view priors for visibility-aware patch deformation. Additionally, we obtain geometry consistency by adopting both aggregated normals via view selection and projection depth differences via epipolar lines, and then employ SHIQ for highlight correction to enable geometry consistency with highlight-aware perception, thus improving reconstruction quality during propagation and refinement stage. Evaluation results on ETH3D, Tanks & Temples and Strecha datasets exhibit the state-of-the-art performance and robust generalization capability of our proposed method.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2505.24597.pdf' target='_blank'>https://arxiv.org/pdf/2505.24597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Liu, Ning Cao, Yile Chen, Yue Jiang, Gao Cong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24597">Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next location prediction plays a critical role in understanding human mobility patterns. However, existing approaches face two core limitations: (1) they fall short in capturing the complex, multi-functional semantics of real-world locations; and (2) they lack the capacity to model heterogeneous behavioral dynamics across diverse user groups. To tackle these challenges, we introduce NextLocMoE, a novel framework built upon large language models (LLMs) and structured around a dual-level Mixture-of-Experts (MoE) design. Our architecture comprises two specialized modules: a Location Semantics MoE that operates at the embedding level to encode rich functional semantics of locations, and a Personalized MoE embedded within the Transformer backbone to dynamically adapt to individual user mobility patterns. In addition, we incorporate a history-aware routing mechanism that leverages long-term trajectory data to enhance expert selection and ensure prediction stability. Empirical evaluations across several real-world urban datasets show that NextLocMoE achieves superior performance in terms of predictive accuracy, cross-domain generalization, and interpretability
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2503.13721.pdf' target='_blank'>https://arxiv.org/pdf/2503.13721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenlong Yuan, Zhidong Yang, Yujun Cai, Kuangxin Wu, Mufan Liu, Dapeng Zhang, Hao Jiang, Zhaoxin Li, Zhaoqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13721">SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, patch-deformation methods have exhibited significant effectiveness in multi-view stereo owing to the deformable and expandable patches in reconstructing textureless areas. However, such methods primarily emphasize broadening the receptive field in textureless areas, while neglecting deformation instability caused by easily overlooked edge-skipping, potentially leading to matching distortions. To address this, we propose SED-MVS, which adopts panoptic segmentation and multi-trajectory diffusion strategy for segmentation-driven and edge-aligned patch deformation. Specifically, to prevent unanticipated edge-skipping, we first employ SAM2 for panoptic segmentation as depth-edge guidance to guide patch deformation, followed by multi-trajectory diffusion strategy to ensure patches are comprehensively aligned with depth edges. Moreover, to avoid potential inaccuracy of random initialization, we combine both sparse points from LoFTR and monocular depth map from DepthAnything V2 to restore reliable and realistic depth map for initialization and supervised guidance. Finally, we integrate segmentation image with monocular depth map to exploit inter-instance occlusion relationship, then further regard them as occlusion map to implement two distinct edge constraint, thereby facilitating occlusion-aware patch deformation. Extensive results on ETH3D, Tanks & Temples, BlendedMVS and Strecha datasets validate the state-of-the-art performance and robust generalization capability of our proposed method.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2503.05638.pdf' target='_blank'>https://arxiv.org/pdf/2503.05638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mark YU, Wenbo Hu, Jinbo Xing, Ying Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05638">TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2502.15809.pdf' target='_blank'>https://arxiv.org/pdf/2502.15809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15809">Black Sheep in the Herd: Playing with Spuriously Correlated Attributes for Vision-Language Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot adaptation for Vision-Language Models (VLMs) presents a dilemma: balancing in-distribution accuracy with out-of-distribution generalization. Recent research has utilized low-level concepts such as visual attributes to enhance generalization. However, this study reveals that VLMs overly rely on a small subset of attributes on decision-making, which co-occur with the category but are not inherently part of it, termed spuriously correlated attributes. This biased nature of VLMs results in poor generalization. To address this, 1) we first propose Spurious Attribute Probing (SAP), identifying and filtering out these problematic attributes to significantly enhance the generalization of existing attribute-based methods; 2) We introduce Spurious Attribute Shielding (SAS), a plug-and-play module that mitigates the influence of these attributes on prediction, seamlessly integrating into various Parameter-Efficient Fine-Tuning (PEFT) methods. In experiments, SAP and SAS significantly enhance accuracy on distribution shifts across 11 datasets and 3 generalization tasks without compromising downstream performance, establishing a new state-of-the-art benchmark.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2502.09507.pdf' target='_blank'>https://arxiv.org/pdf/2502.09507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elias Kempf, Simon Schrodi, Max Argus, Thomas Brox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09507">When and How Does CLIP Enable Domain and Compositional Generalization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable generalization performance of contrastive vision-language models like CLIP is often attributed to the diversity of their training distributions. However, key questions remain unanswered: Can CLIP generalize to an entirely unseen domain when trained on a diverse mixture of domains (domain generalization)? Can it generalize to unseen classes within partially seen domains (compositional generalization)? What factors affect such generalization? To answer these questions, we trained CLIP models on systematically constructed training distributions with controlled domain diversity and object class exposure. Our experiments show that domain diversity is essential for both domain and compositional generalization, yet compositional generalization can be surprisingly weaker than domain generalization when the training distribution contains a suboptimal subset of the test domain. Through data-centric and mechanistic analyses, we find that successful generalization requires the learning of sufficiently shared representations in intermediate layers and circuits.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2412.05208.pdf' target='_blank'>https://arxiv.org/pdf/2412.05208.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditi Singh, Akash Shetty, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05208">A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-SQL systems facilitate smooth interaction with databases by translating natural language queries into Structured Query Language (SQL), bridging the gap between non-technical users and complex database management systems. This survey provides a comprehensive overview of the evolution of AI-driven text-to-SQL systems, highlighting their foundational components, advancements in large language model (LLM) architectures, and the critical role of datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine the applications of text-to-SQL in domains like healthcare, education, and finance, emphasizing their transformative potential for improving data accessibility. Additionally, we analyze persistent challenges, including domain generalization, query optimization, support for multi-turn conversational interactions, and the limited availability of datasets tailored for NoSQL databases and dynamic real-world scenarios. To address these challenges, we outline future research directions, such as extending text-to-SQL capabilities to support NoSQL databases, designing datasets for dynamic multi-turn interactions, and optimizing systems for real-world scalability and robustness. By surveying current advancements and identifying key gaps, this paper aims to guide the next generation of research and applications in LLM-based text-to-SQL systems.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2411.17472.pdf' target='_blank'>https://arxiv.org/pdf/2411.17472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Hanchen Jiang, Yasi Zhang, Zhi Zhang, Yixin Wan, Andrew Lizarraga, Shufan Li, Ying Nian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17472">Unlocking the Potential of Text-to-Image Diffusion with PAC-Bayesian Theory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image (T2I) diffusion models have revolutionized generative modeling by producing high-fidelity, diverse, and visually realistic images from textual prompts. Despite these advances, existing models struggle with complex prompts involving multiple objects and attributes, often misaligning modifiers with their corresponding nouns or neglecting certain elements. Recent attention-based methods have improved object inclusion and linguistic binding, but still face challenges such as attribute misbinding and a lack of robust generalization guarantees. Leveraging the PAC-Bayes framework, we propose a Bayesian approach that designs custom priors over attention distributions to enforce desirable properties, including divergence between objects, alignment between modifiers and their corresponding nouns, minimal attention to irrelevant tokens, and regularization for better generalization. Our approach treats the attention mechanism as an interpretable component, enabling fine-grained control and improved attribute-object alignment. We demonstrate the effectiveness of our method on standard benchmarks, achieving state-of-the-art results across multiple metrics. By integrating custom priors into the denoising process, our method enhances image quality and addresses long-standing challenges in T2I diffusion models, paving the way for more reliable and interpretable generative models.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2411.12913.pdf' target='_blank'>https://arxiv.org/pdf/2411.12913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qin Tian, Chen Zhao, Minglai Shao, Wenjun Wang, Yujie Lin, Dong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12913">MLDGG: Meta-Learning for Domain Generalization on Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization on graphs aims to develop models with robust generalization capabilities, ensuring effective performance on the testing set despite disparities between testing and training distributions. However, existing methods often rely on static encoders directly applied to the target domain, constraining its flexible adaptability. In contrast to conventional methodologies, which concentrate on developing specific generalized models, our framework, MLDGG, endeavors to achieve adaptable generalization across diverse domains by integrating cross-multi-domain meta-learning with structure learning and semantic identification. Initially, it introduces a generalized structure learner to mitigate the adverse effects of task-unrelated edges, enhancing the comprehensiveness of representations learned by Graph Neural Networks (GNNs) while capturing shared structural information across domains. Subsequently, a representation learner is designed to disentangle domain-invariant semantic and domain-specific variation information in node embedding by leveraging causal reasoning for semantic identification, further enhancing generalization. In the context of meta-learning, meta-parameters for both learners are optimized to facilitate knowledge transfer and enable effective adaptation to graphs through fine-tuning within the target domains, where target graphs are inaccessible during training. Our empirical results demonstrate that MLDGG surpasses baseline methods, showcasing its effectiveness in three different distribution shift settings.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2410.11723.pdf' target='_blank'>https://arxiv.org/pdf/2410.11723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Celestini, Amirhossein Afsharrad, Daniele Gammelli, Tommaso Guffanti, Gioele Zardini, Sanjay Lall, Elisa Capello, Simone D'Amico, Marco Pavone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11723">Generalizable Spacecraft Trajectory Generation via Multimodal Learning with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective trajectory generation is essential for reliable on-board spacecraft autonomy. Among other approaches, learning-based warm-starting represents an appealing paradigm for solving the trajectory generation problem, effectively combining the benefits of optimization- and data-driven methods. Current approaches for learning-based trajectory generation often focus on fixed, single-scenario environments, where key scene characteristics, such as obstacle positions or final-time requirements, remain constant across problem instances. However, practical trajectory generation requires the scenario to be frequently reconfigured, making the single-scenario approach a potentially impractical solution. To address this challenge, we present a novel trajectory generation framework that generalizes across diverse problem configurations, by leveraging high-capacity transformer neural networks capable of learning from multimodal data sources. Specifically, our approach integrates transformer-based neural network models into the trajectory optimization process, encoding both scene-level information (e.g., obstacle locations, initial and goal states) and trajectory-level constraints (e.g., time bounds, fuel consumption targets) via multimodal representations. The transformer network then generates near-optimal initial guesses for non-convex optimization problems, significantly enhancing convergence speed and performance. The framework is validated through extensive simulations and real-world experiments on a free-flyer platform, achieving up to 30% cost improvement and 80% reduction in infeasible cases with respect to traditional approaches, and demonstrating robust generalization across diverse scenario variations.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2410.02512.pdf' target='_blank'>https://arxiv.org/pdf/2410.02512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mucong Ding, Bang An, Yuancheng Xu, Anirudh Satheesh, Furong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02512">SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation, a cornerstone technique in deep learning, is crucial in enhancing model performance, especially with scarce labeled data. While traditional techniques are effective, their reliance on hand-crafted methods limits their applicability across diverse data types and tasks. Although modern learnable augmentation methods offer increased adaptability, they are computationally expensive and challenging to incorporate within prevalent augmentation workflows. In this work, we present a novel, efficient method for data augmentation, effectively bridging the gap between existing augmentation strategies and emerging datasets and learning tasks. We introduce SAFLEX (Self-Adaptive Augmentation via Feature Label EXtrapolation), which learns the sample weights and soft labels of augmented samples provided by any given upstream augmentation pipeline, using a specifically designed efficient bilevel optimization algorithm. Remarkably, SAFLEX effectively reduces the noise and label errors of the upstream augmentation pipeline with a marginal computational cost. As a versatile module, SAFLEX excels across diverse datasets, including natural and medical images and tabular data, showcasing its prowess in few-shot learning and out-of-distribution generalization. SAFLEX seamlessly integrates with common augmentation strategies like RandAug, CutMix, and those from large pre-trained generative models like stable diffusion and is also compatible with frameworks such as CLIP's fine-tuning. Our findings highlight the potential to adapt existing augmentation pipelines for new data types and tasks, signaling a move towards more adaptable and resilient training frameworks.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2601.12539.pdf' target='_blank'>https://arxiv.org/pdf/2601.12539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Ezzat Shahroor, Mohamed Bayan Kmainasi, Abul Hasnat, Dimitar Dimitrov, Giovanni Da San Martino, Preslav Nakov, Firoj Alam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12539">MemeLens: Multilingual Multitask VLMs for Memes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2512.21039.pdf' target='_blank'>https://arxiv.org/pdf/2512.21039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roopa Bukke, Soumya Pandey, Suraj Kumar, Soumi Chattopadhyay, Chandranath Adak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21039">Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid proliferation of online misinformation poses significant risks to public trust, policy, and safety, necessitating reliable automated fake news detection. Existing methods often struggle with multimodal content, domain generalization, and explainability. We propose AMPEND-LS, an agentic multi-persona evidence-grounded framework with LLM-SLM synergy for multimodal fake news detection. AMPEND-LS integrates textual, visual, and contextual signals through a structured reasoning pipeline powered by LLMs, augmented with reverse image search, knowledge graph paths, and persuasion strategy analysis. To improve reliability, we introduce a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context, and a complementary SLM classifier to mitigate LLM uncertainty and hallucinations. Extensive experiments across three benchmark datasets demonstrate that AMPEND-LS consistently outperformed state-of-the-art baselines in accuracy, F1 score, and robustness. Qualitative case studies further highlight its transparent reasoning and resilience against evolving misinformation. This work advances the development of adaptive, explainable, and evidence-aware systems for safeguarding online information integrity.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2512.08820.pdf' target='_blank'>https://arxiv.org/pdf/2512.08820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhang, Chun-Wun Cheng, Junyi He, Ke Yu, Yushun Tang, Carola-Bibiane Schönlieb, Zhihai He, Angelica I. Aviles-Rivero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08820">Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2511.17927.pdf' target='_blank'>https://arxiv.org/pdf/2511.17927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Ma, Xun Lin, Yong Xu, Weicheng Xie, Zitong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17927">PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2511.09173.pdf' target='_blank'>https://arxiv.org/pdf/2511.09173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guojian Wang, Quinson Hon, Xuyang Chen, Lin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09173">Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain shifts present a significant challenge for decision transformer (DT) policies. Existing cross-domain policy adaptation methods typically rely on a single simple filtering criterion to select source trajectory fragments and stitch them together. They match either state structure or action feasibility. However, the selected fragments still have poor stitchability: state structures can misalign, the return-to-go (RTG) becomes incomparable when the reward or horizon changes, and actions may jump at trajectory junctions. As a result, RTG tokens lose continuity, which compromises DT's inference ability. To tackle these challenges, we propose Data Fusion-Enhanced Decision Transformer (DFDT), a compact pipeline that restores stitchability. Particularly, DFDT fuses scarce target data with selectively trusted source fragments via a two-level data filter, maximum mean discrepancy (MMD) mismatch for state-structure alignment, and optimal transport (OT) deviation for action feasibility. It then trains on a feasibility-weighted fusion distribution. Furthermore, DFDT replaces RTG tokens with advantage-conditioned tokens, which improves the continuity of the semantics in the token sequence. It also applies a $Q$-guided regularizer to suppress junction value and action jumps. Theoretically, we provide bounds that tie state value and policy performance gaps to the MMD-mismatch and OT-deviation measures, and show that the bounds tighten as these two measures shrink. We show that DFDT improves return and stability over strong offline RL and sequence-model baselines across gravity, kinematic, and morphology shifts on D4RL-style control tasks, and further corroborate these gains with token-stitching and sequence-semantics stability analyses.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2510.01248.pdf' target='_blank'>https://arxiv.org/pdf/2510.01248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruyue Liu, Rong Yin, Xiangzhen Bo, Xiaoshuai Hao, Yong Liu, Jinwen Zhong, Can Ma, Weiping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01248">SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large scale pretrained models have revolutionized Natural Language Processing (NLP) and Computer Vision (CV), showcasing remarkable cross domain generalization abilities. However, in graph learning, models are typically trained on individual graph datasets, limiting their capacity to transfer knowledge across different graphs and tasks. This approach also heavily relies on large volumes of annotated data, which presents a significant challenge in resource-constrained settings. Unlike NLP and CV, graph structured data presents unique challenges due to its inherent heterogeneity, including domain specific feature spaces and structural diversity across various applications. To address these challenges, we propose a novel structure aware self supervised learning method for Text Attributed Graphs (SSTAG). By leveraging text as a unified representation medium for graph learning, SSTAG bridges the gap between the semantic reasoning of Large Language Models (LLMs) and the structural modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces a dual knowledge distillation framework that co-distills both LLMs and GNNs into structure-aware multilayer perceptrons (MLPs), enhancing the scalability of large-scale TAGs. Additionally, we introduce an in-memory mechanism that stores typical graph representations, aligning them with memory anchors in an in-memory repository to integrate invariant knowledge, thereby improving the model's generalization ability. Extensive experiments demonstrate that SSTAG outperforms state-of-the-art models on cross-domain transfer learning tasks, achieves exceptional scalability, and reduces inference costs while maintaining competitive performance.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2509.26631.pdf' target='_blank'>https://arxiv.org/pdf/2509.26631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26631">Learning Generalizable Shape Completion with SIM(3) Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2509.24923.pdf' target='_blank'>https://arxiv.org/pdf/2509.24923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanxing Chen, Xiaoyin Chen, Yukun Huang, Roy Xie, Bhuwan Dhingra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24923">When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2509.22695.pdf' target='_blank'>https://arxiv.org/pdf/2509.22695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitao Wang, Yanke Wang, Jiangtao Wen, Roberto Horowitz, Yuxing Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22695">ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation in unstructured environments requires the generation of robust and long-horizon trajectory-level policy with conditions of perceptual observations and benefits from the advantages of SE(3)-equivariant diffusion models that are data-efficient. However, these models suffer from the inference time costs. Inspired by the inference efficiency of rectified flows, we introduce the rectification to the SE(3)-diffusion models and propose the ReSeFlow, i.e., Rectifying SE(3)-Equivariant Policy Learning Flows, providing fast, geodesic-consistent, least-computational policy generation. Crucially, both components employ SE(3)-equivariant networks to preserve rotational and translational symmetry, enabling robust generalization under rigid-body motions. With the verification on the simulated benchmarks, we find that the proposed ReSeFlow with only one inference step can achieve better performance with lower geodesic distance than the baseline methods, achieving up to a 48.5% error reduction on the painting task and a 21.9% reduction on the rotating triangle task compared to the baseline's 100-step inference. This method takes advantages of both SE(3) equivariance and rectified flow and puts it forward for the real-world application of generative policy learning models with the data and inference efficiency.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2508.04129.pdf' target='_blank'>https://arxiv.org/pdf/2508.04129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xun Lin, Xiaobao Guo, Taorui Wang, Yingjie Ma, Jiajian Huang, Jiayu Zhang, Junzhe Cao, Zitong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04129">SVC 2025: the First Multimodal Deception Detection Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deception detection is a critical task in real-world applications such as security screening, fraud prevention, and credibility assessment. While deep learning methods have shown promise in surpassing human-level performance, their effectiveness often depends on the availability of high-quality and diverse deception samples. Existing research predominantly focuses on single-domain scenarios, overlooking the significant performance degradation caused by domain shifts. To address this gap, we present the SVC 2025 Multimodal Deception Detection Challenge, a new benchmark designed to evaluate cross-domain generalization in audio-visual deception detection. Participants are required to develop models that not only perform well within individual domains but also generalize across multiple heterogeneous datasets. By leveraging multimodal data, including audio, video, and text, this challenge encourages the design of models capable of capturing subtle and implicit deceptive cues. Through this benchmark, we aim to foster the development of more adaptable, explainable, and practically deployable deception detection systems, advancing the broader field of multimodal learning. By the conclusion of the workshop competition, a total of 21 teams had submitted their final results. https://sites.google.com/view/svc-mm25 for more information.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2507.03304.pdf' target='_blank'>https://arxiv.org/pdf/2507.03304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Huang, Yan Xia, Sashuai Zhou, Hanting Wang, Shulei Wang, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03304">Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to enhance model robustness in unseen or distributionally shifted target domains through training exclusively on source domains. Although existing DG techniques, such as data manipulation, learning strategies, and representation learning, have shown significant progress, they predominantly address single-modal data. With the emergence of numerous multi-modal datasets and increasing demand for multi-modal tasks, a key challenge in Multi-modal Domain Generalization (MMDG) has emerged: enabling models trained on multi-modal sources to generalize to unseen target distributions within the same modality set. Due to the inherent differences between modalities, directly transferring methods from single-modal DG to MMDG typically yields sub-optimal results. These methods often exhibit randomness during generalization due to the invisibility of target domains and fail to consider inter-modal consistency. Applying these methods independently to each modality in the MMDG setting before combining them can lead to divergent generalization directions across different modalities, resulting in degraded generalization capabilities. To address these challenges, we propose a novel approach that leverages Unified Representations to map different paired modalities together, effectively adapting DG methods to MMDG by enabling synchronized multi-modal improvements within the unified space. Additionally, we introduce a supervised disentanglement framework that separates modal-general and modal-specific information, further enhancing the alignment of unified representations. Extensive experiments on benchmark datasets, including EPIC-Kitchens and Human-Animal-Cartoon, demonstrate the effectiveness and superiority of our method in enhancing multi-modal domain generalization.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2506.20743.pdf' target='_blank'>https://arxiv.org/pdf/2506.20743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh-Hao Van, Prateek Verma, Chen Zhao, Xintao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20743">A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\&A; atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2506.16160.pdf' target='_blank'>https://arxiv.org/pdf/2506.16160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiyao Wang, Xiao Yang, Hao Lu, Dengbo He, Kaishun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16160">Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-source synsemantic domain generalization (MSSDG) for multi-task remote physiological measurement seeks to enhance the generalizability of these metrics and attracts increasing attention. However, challenges like partial labeling and environmental noise may disrupt task-specific accuracy. Meanwhile, given that real-time adaptation is necessary for personalized products, the test-time personalized adaptation (TTPA) after MSSDG is also worth exploring, while the gap between previous generalization and personalization methods is significant and hard to fuse. Thus, we proposed a unified framework for MSSD\textbf{G} and TTP\textbf{A} employing \textbf{P}riors (\textbf{GAP}) in biometrics and remote photoplethysmography (rPPG). We first disentangled information from face videos into invariant semantics, individual bias, and noise. Then, multiple modules incorporating priors and our observations were applied in different stages and for different facial information. Then, based on the different principles of achieving generalization and personalization, our framework could simultaneously address MSSDG and TTPA under multi-task remote physiological estimation with minimal adjustments. We expanded the MSSDG benchmark to the TTPA protocol on six publicly available datasets and introduced a new real-world driving dataset with complete labeling. Extensive experiments that validated our approach, and the codes along with the new dataset will be released.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2506.08672.pdf' target='_blank'>https://arxiv.org/pdf/2506.08672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Jiaqi Li, Zilong Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08672">RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($Î$4.1% average points on eight ID tasks and $Î$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2505.20278.pdf' target='_blank'>https://arxiv.org/pdf/2505.20278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoyeon Chang, Jinho Park, Hanseul Cho, Sohee Yang, Miyoung Ko, Hyeonbin Hwang, Seungpil Won, Dohaeng Lee, Youbin Ahn, Minjoon Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20278">Characterizing Pattern Matching and Its Limits on Compositional Task Structures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite impressive capabilities, LLMs' successes often rely on pattern-matching behaviors, yet these are also linked to OOD generalization failures in compositional tasks. However, behavioral studies commonly employ task setups that allow multiple generalization sources (e.g., algebraic invariances, structural repetition), obscuring a precise and testable account of how well LLMs perform generalization through pattern matching and their limitations. To address this ambiguity, we first formalize pattern matching as functional equivalence, i.e., identifying pairs of subsequences of inputs that consistently lead to identical results when the rest of the input is held constant. Then, we systematically study how decoder-only Transformer and Mamba behave in controlled tasks with compositional structures that isolate this mechanism. Our formalism yields predictive and quantitative insights: (1) Instance-wise success of pattern matching is well predicted by the number of contexts witnessing the relevant functional equivalence. (2) We prove a tight sample complexity bound of learning a two-hop structure by identifying the exponent of the data scaling law for perfect in-domain generalization. Our empirical results align with the theoretical prediction, under 20x parameter scaling and across architectures. (3) Path ambiguity is a structural barrier: when a variable influences the output via multiple paths, models fail to form unified intermediate state representations, impairing accuracy and interpretability. (4) Chain-of-Thought reduces data requirements yet does not resolve path ambiguity. Hence, we provide a predictive, falsifiable boundary for pattern matching and a foundational diagnostic for disentangling mixed generalization mechanisms.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2505.18770.pdf' target='_blank'>https://arxiv.org/pdf/2505.18770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuedi Zhang, Shuanghao Bai, Wanqi Zhou, Zhirong Luan, Badong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18770">Dual-Path Stable Soft Prompt Generation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn a model using data from one or multiple related but distinct source domains that can generalize well to unseen out-of-distribution target domains. Inspired by the success of large pre-trained vision-language models (VLMs), prompt tuning has emerged as an effective generalization strategy. However, it often struggles to capture domain-specific features due to its reliance on manually or fixed prompt inputs. Recently, some prompt generation methods have addressed this limitation by dynamically generating instance-specific and domain-specific prompts for each input, enriching domain information and demonstrating potential for enhanced generalization. Through further investigation, we identify a notable issue in existing prompt generation methods: the same input often yields significantly different and suboptimal prompts across different random seeds, a phenomenon we term Prompt Variability. To address this, we introduce negative learning into the prompt generation process and propose Dual-Path Stable Soft Prompt Generation (DPSPG), a transformer-based framework designed to improve both the stability and generalization of prompts. Specifically, DPSPG incorporates a complementary prompt generator to produce negative prompts, thereby reducing the risk of introducing misleading information. Both theoretical and empirical analyses demonstrate that negative learning leads to more robust and effective prompts by increasing the effective margin and reducing the upper bound of the gradient norm. Extensive experiments on five DG benchmark datasets show that DPSPG consistently outperforms state-of-the-art methods while maintaining prompt stability.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2505.14088.pdf' target='_blank'>https://arxiv.org/pdf/2505.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Chen, Shen Yan, Juelin Zhu, Chen Chen, Yu Liu, Maojun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14088">Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Land-MoE, a novel approach for multispectral land cover classification (MLCC). Spectral shift, which emerges from disparities in sensors and geospatial conditions, poses a significant challenge in this domain. Existing methods predominantly rely on domain adaptation and generalization strategies, often utilizing small-scale models that exhibit limited performance. In contrast, Land-MoE addresses these issues by hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts, to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner. Specifically, Land-MoE comprises two key modules: the mixture of low-rank token experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages rank-differentiated tokens to generate diverse feature adjustments for individual instances within multispectral images. By dynamically combining learnable low-rank token experts of varying ranks, it enhances the robustness against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on the refined features. This process enables the model to effectively capture frequency band information that is strongly correlated with semantic essence, while simultaneously suppressing frequency noise irrelevant to the task. Comprehensive experiments on MLCC tasks involving cross-sensor and cross-geospatial setups demonstrate that Land-MoE outperforms existing methods by a large margin. Additionally, the proposed approach has also achieved state-of-the-art performance in domain generalization semantic segmentation tasks of RGB remote sensing images.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2410.14375.pdf' target='_blank'>https://arxiv.org/pdf/2410.14375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialin Yu, Yuxiang Zhou, Yulan He, Nevin L. Zhang, Junchi Yu, Philip Torr, Ricardo Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14375">Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting to latent-confounded shifts remains a core challenge in modern AI. These shifts are propagated via latent variables that induce spurious, non-transportable correlations between inputs and labels. One practical failure mode arises when fine-tuning pre-trained foundation models on confounded data (e.g., where certain text tokens or image backgrounds spuriously correlate with the label), leaving models vulnerable at deployment. We frame causal fine-tuning as an identification problem and pose an explicit causal model that decomposes inputs into low-level spurious features and high-level causal representations. Under this family of models, we formalize the assumptions required for identification. Using pre-trained language models as a case study, we show how identifying and adjusting these components during causal fine-tuning enables automatic adaptation to latent-confounded shifts at test time. Experiments on semi-synthetic benchmarks derived from real-world problems demonstrate that our method outperforms black-box domain generalization baselines, illustrating the benefits of explicitly modeling causal structure.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2409.08557.pdf' target='_blank'>https://arxiv.org/pdf/2409.08557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaowei Miao, Yawei Luo, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08557">DICS: Find Domain-Invariant and Class-Specific Features for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep neural networks have made remarkable progress in various vision tasks, their performance typically deteriorates when tested in out-of-distribution (OOD) scenarios. Many OOD methods focus on extracting domain-invariant features but neglect whether these features are unique to each class. Even if some features are domain-invariant, they cannot serve as key classification criteria if shared across different classes. In OOD tasks, both domain-related and class-shared features act as confounders that hinder generalization. In this paper, we propose a DICS model to extract Domain-Invariant and Class-Specific features, including Domain Invariance Testing (DIT) and Class Specificity Testing (CST), which mitigate the effects of spurious correlations introduced by confounders. DIT learns domain-related features of each source domain and removes them from inputs to isolate domain-invariant class-related features. DIT ensures domain invariance by aligning same-class features across different domains. Then, CST calculates soft labels for those features by comparing them with features learned in previous steps. We optimize the cross-entropy between the soft labels and their true labels, which enhances same-class similarity and different-class distinctiveness, thereby reinforcing class specificity. Extensive experiments on widely-used benchmarks demonstrate the effectiveness of our proposed algorithm. Additional visualizations further demonstrate that DICS effectively identifies the key features of each class in target domains.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2601.20397.pdf' target='_blank'>https://arxiv.org/pdf/2601.20397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaile Wang, Jiannong Cao, Yu Yang, Xiaoyin Li, Mingjin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20397">FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2512.07234.pdf' target='_blank'>https://arxiv.org/pdf/2512.07234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Chen, Lin Zuo, Mengmeng Jing, Kunbin He, Yuchen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07234">Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2511.18468.pdf' target='_blank'>https://arxiv.org/pdf/2511.18468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Akil Raihan Iftee, Mir Sazzat Hossain, Rakibul Hasan Rajib, Tariq Iqbal, Md Mofijul Islam, M Ashraful Amin, Amin Ahsan Ali, AKM Mahbubur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18468">SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2511.14017.pdf' target='_blank'>https://arxiv.org/pdf/2511.14017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erum Mushtaq, Anil Ramakrishna, Satyapriya Krishna, Sattvik Sahai, Prasoon Goyal, Kai-Wei Chang, Tao Zhang, Rahul Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14017">From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work has shown that fine-tuning on insecure code data can trigger an emergent misalignment (EMA) phenomenon, where models generate malicious responses even to prompts unrelated to the original insecure code-writing task. Such cross-domain generalization of harmful behavior underscores the need for a deeper understanding of the algorithms, tasks, and datasets that induce emergent misalignment. In this work, we extend this study by demonstrating that emergent misalignment can also arise from narrow refusal unlearning in specific domains. We perform refusal unlearning on Cybersecurity and Safety concept, and evaluate EMA by monitoring refusal scores across seven responsible AI (RAI) domains, Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, and Privacy. Our work shows that narrow domain unlearning can yield compliance responses for the targeted concept, however, it may also propagate EMA to unrelated domains. Among the two intervened concepts, Cybersecurity and Safety, we find that the safety concept can have larger EMA impact, i.e, causing lower refusal scores, across other unrelated domains such as bias. We observe this effect consistently across two model families, Mistral-7b-0.3v, and Qwen-7b-2.5. Further, we show that refusal unlearning augmented with cross-entropy loss function on a small set of retain data from the affected domains can largely, if not fully, restore alignment across the impacted domains while having lower refusal rate on the concept we perform unlearning on. To investigate the underlying causes of EMA, we analyze concept entanglements at the representation level via concept vectors. Our analysis reveals that concepts with higher representation similarity in earlier layers are more susceptible to EMA after intervention when the refusal stream is altered through targeted refusal unlearning.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2510.17270.pdf' target='_blank'>https://arxiv.org/pdf/2510.17270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Schulze, Juliano Decico Negri, Victor Barasuol, Vivian Suzano Medeiros, Marcelo Becker, Jan Peters, Oleg Arenz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17270">Floating-Base Deep Lagrangian Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grey-box methods for system identification combine deep learning with physics-informed constraints, capturing complex dependencies while improving out-of-distribution generalization. Yet, despite the growing importance of floating-base systems such as humanoids and quadrupeds, current grey-box models ignore their specific physical constraints. For instance, the inertia matrix is not only positive definite but also exhibits branch-induced sparsity and input independence. Moreover, the 6x6 composite spatial inertia of the floating base inherits properties of single-rigid-body inertia matrices. As we show, this includes the triangle inequality on the eigenvalues of the composite rotational inertia. To address the lack of physical consistency in deep learning models of floating-base systems, we introduce a parameterization of inertia matrices that satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN), we train neural networks to predict physically plausible inertia matrices that minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we collected and released a dataset on multiple quadrupeds and humanoids. In these experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly competitive performance on both simulated and real robots, while providing greater physical interpretability.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2509.23097.pdf' target='_blank'>https://arxiv.org/pdf/2509.23097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Su, Abdul Rehman Akbar, Usama Sajjad, Anil V. Parwani, Muhammad Khalid Khan Niazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23097">Streamline pathology foundation model by cross-magnification distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models (FM) have transformed computational pathology but remain computationally prohibitive for clinical deployment due to their massive parameter counts and high-magnification processing requirements. Here, we introduce XMAG, a lightweight FM developed through corss-magnification distillation that transfers knowledge from state-of-the-art 20x magnification teacher to an efficient 5x magnification student architecture. XMAG employs a compact backbone and operates entirely at 5x, requiring 11.3 times fewer patches per whole slide image (WSI) compared to existing approaches. Our Novel distillation framework incorporates dual-level knowledge transfer, aligning both global image representations and local spatial token mapping. We trained XMAG on 3.49 million images curated from publicly available datasets and evaluated performance across six clinically relevant histopathology analysis tasks spanning multiple cancer types. XMAG achieved diagnostic accuracy within 1% of substantially larger foundation models while delivering 30-fold processing acceleration, reaching 8.8 WSIs per minute processing speed. Our cross-institutional validation confirmed robust generalization. Further, we developed an end-to-end training strategy to further boost our model's performance to approach the larger FMs' performance. These results establish cross-magnification distillation as a viable approach for deploying FM capabilities in resource-constrained clinical environments, potentially enabling real-time pathology AI integration.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2509.15099.pdf' target='_blank'>https://arxiv.org/pdf/2509.15099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, Kei Sakaguchi, Walid Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15099">Digital Twin-based Cooperative Autonomous Driving in Smart Intersections: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsignalized intersections pose safety and efficiency challenges due to complex traffic flows and blind spots. In this paper, a digital twin (DT)-based cooperative driving system with roadside unit (RSU)-centric architecture is proposed for enhancing safety and efficiency at unsignalized intersections. The system leverages comprehensive bird-eye-view (BEV) perception to eliminate blind spots and employs a hybrid reinforcement learning (RL) framework combining offline pre-training with online fine-tuning. Specifically, driving policies are initially trained using conservative Q-learning (CQL) with behavior cloning (BC) on real datasets, then fine-tuned using multi-agent proximal policy optimization (MAPPO) with self-attention mechanisms to handle dynamic multi-agent coordination. The RSU implements real-time commands via vehicle-to-infrastructure (V2I) communications. Experimental results show that the proposed method yields failure rates below 0.03\% coordinating up to three connected autonomous vehicles (CAVs), significantly outperforming traditional methods. In addition, the system exhibits sub-linear computational scaling with inference times under 40 ms. Furthermore, it demonstrates robust generalization across diverse unsignalized intersection scenarios, indicating its practicality and readiness for real-world deployment.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2509.00956.pdf' target='_blank'>https://arxiv.org/pdf/2509.00956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Cescon, Andrea Martin, Giancarlo Ferrari-Trecate
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00956">On the Global Optimality of Linear Policies for Sinkhorn Distributionally Robust Linear Quadratic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Linear Quadratic Gaussian (LQG) regulator is a cornerstone of optimal control theory, yet its performance can degrade significantly when the noise distributions deviate from the assumed Gaussian model. To address this limitation, this work proposes a distributionally robust generalization of the finite-horizon LQG control problem. Specifically, we assume that the noise distributions are unknown and belong to ambiguity sets defined in terms of an entropy-regularized Wasserstein distance centered at a nominal Gaussian distribution. By deriving novel bounds on this Sinkhorn discrepancy and proving structural and topological properties of the resulting ambiguity sets, we establish global optimality of linear policies. Numerical experiments showcase improved distributional robustness of our control policy.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2508.16976.pdf' target='_blank'>https://arxiv.org/pdf/2508.16976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Pan, Shiyu Shen, Zongbin Wang, Zhenwei Shi, Xia Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16976">Preserving Domain Generalization in Fine-Tuning via Joint Parameter Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization seeks to develop models trained on a limited set of source domains that are capable of generalizing effectively to unseen target domains. While the predominant approach leverages large-scale pre-trained vision models as initialization, recent studies have highlighted that full fine-tuning can compromise the intrinsic generalization capabilities of these models. To address this limitation, parameter-efficient adaptation strategies have emerged, wherein only a subset of model parameters is selectively fine-tuned, thereby balancing task adaptation with the preservation of generalization. Motivated by this paradigm, we introduce Joint Parameter Selection (JPS), a novel method that restricts updates to a small, sparse subset of parameters, thereby retaining and harnessing the generalization strength of pre-trained models. Theoretically, we establish a generalization error bound that explicitly accounts for the sparsity of parameter updates, thereby providing a principled justification for selective fine-tuning. Practically, we design a selection mechanism employing dual operators to identify and update parameters exhibiting consistent and significant gradients across all source domains. Extensive benchmark experiments demonstrate that JPS achieves superior performance compared to state-of-the-art domain generalization methods, substantiating both the efficiency and efficacy of the proposed approach.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2507.12008.pdf' target='_blank'>https://arxiv.org/pdf/2507.12008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Wang, Yinda Chen, Xiaoyu Liu, Che Liu, Dong Liu, Jianqing Gao, Zhiwei Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12008">Dual form Complementary Masking for Domain-Adaptive Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have correlated Masked Image Modeling (MIM) with consistency regularization in Unsupervised Domain Adaptation (UDA). However, they merely treat masking as a special form of deformation on the input images and neglect the theoretical analysis, which leads to a superficial understanding of masked reconstruction and insufficient exploitation of its potential in enhancing feature extraction and representation learning. In this paper, we reframe masked reconstruction as a sparse signal reconstruction problem and theoretically prove that the dual form of complementary masks possesses superior capabilities in extracting domain-agnostic image features. Based on this compelling insight, we propose MaskTwins, a simple yet effective UDA framework that integrates masked reconstruction directly into the main training pipeline. MaskTwins uncovers intrinsic structural patterns that persist across disparate domains by enforcing consistency between predictions of images masked in complementary ways, enabling domain generalization in an end-to-end manner. Extensive experiments verify the superiority of MaskTwins over baseline methods in natural and biological image segmentation. These results demonstrate the significant advantages of MaskTwins in extracting domain-invariant features without the need for separate pre-training, offering a new paradigm for domain-adaptive segmentation.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2506.18880.pdf' target='_blank'>https://arxiv.org/pdf/2506.18880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18880">OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2506.17740.pdf' target='_blank'>https://arxiv.org/pdf/2506.17740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyu Han, Zeyi Liu, Shijin Chen, Dongliang Zou, Xiao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17740">Rethinking the Role of Operating Conditions for Learning-based Multi-condition Fault Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-condition fault diagnosis is prevalent in industrial systems and presents substantial challenges for conventional diagnostic approaches. The discrepancy in data distributions across different operating conditions degrades model performance when a model trained under one condition is applied to others. With the recent advancements in deep learning, transfer learning has been introduced to the fault diagnosis field as a paradigm for addressing multi-condition fault diagnosis. Among these methods, domain generalization approaches can handle complex scenarios by extracting condition-invariant fault features. Although many studies have considered fault diagnosis in specific multi-condition scenarios, the extent to which operating conditions affect fault information has been scarcely studied, which is crucial. However, the extent to which operating conditions affect fault information has been scarcely studied, which is crucial. When operating conditions have a significant impact on fault features, directly applying domain generalization methods may lead the model to learn condition-specific information, thereby reducing its overall generalization ability. This paper investigates the performance of existing end-to-end domain generalization methods under varying conditions, specifically in variable-speed and variable-load scenarios, using multiple experiments on a real-world gearbox. Additionally, a two-stage diagnostic framework is proposed, aiming to improve fault diagnosis performance under scenarios with significant operating condition impacts. By incorporating a domain-generalized encoder with a retraining strategy, the framework is able to extract condition-invariant fault features while simultaneously alleviating potential overfitting to the source domain. Several experiments on a real-world gearbox dataset are conducted to validate the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2504.08019.pdf' target='_blank'>https://arxiv.org/pdf/2504.08019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Bi, Jingjun Yi, Hao Zheng, Haolan Zhan, Wei Ji, Yawen Huang, Yuexiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08019">DGFamba: Learning Flow Factorized State Space for Visual Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn a representation from the source domain, which can be generalized to arbitrary unseen target domains. A fundamental challenge for visual domain generalization is the domain gap caused by the dramatic style variation whereas the image content is stable. The realm of selective state space, exemplified by VMamba, demonstrates its global receptive field in representing the content. However, the way exploiting the domain-invariant property for selective state space is rarely explored. In this paper, we propose a novel Flow Factorized State Space model, dubbed as DG-Famba, for visual domain generalization. To maintain domain consistency, we innovatively map the style-augmented and the original state embeddings by flow factorization. In this latent flow space, each state embedding from a certain style is specified by a latent probability path. By aligning these probability paths in the latent space, the state embeddings are able to represent the same content distribution regardless of the style differences. Extensive experiments conducted on various visual domain generalization settings show its state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2501.01109.pdf' target='_blank'>https://arxiv.org/pdf/2501.01109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiusheng Xu, Lei Qi, Jingyang Zhou, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01109">BatStyler: Advancing Multi-category Style Generation for Source-free Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Source-Free Domain Generalization (SFDG) aims to develop a model that performs on unseen domains without relying on any source domains. However, the implementation remains constrained due to the unavailability of training data. Research on SFDG focus on knowledge transfer of multi-modal models and style synthesis based on joint space of multiple modalities, thus eliminating the dependency on source domain images. However, existing works primarily work for multi-domain and less-category configuration, but performance on multi-domain and multi-category configuration is relatively poor. In addition, the efficiency of style synthesis also deteriorates in multi-category scenarios. How to efficiently synthesize sufficiently diverse data and apply it to multi-category configuration is a direction with greater practical value. In this paper, we propose a method called BatStyler, which is utilized to improve the capability of style synthesis in multi-category scenarios. BatStyler consists of two modules: Coarse Semantic Generation and Uniform Style Generation modules. The Coarse Semantic Generation module extracts coarse-grained semantics to prevent the compression of space for style diversity learning in multi-category configuration, while the Uniform Style Generation module provides a template of styles that are uniformly distributed in space and implements parallel training. Extensive experiments demonstrate that our method exhibits comparable performance on less-category datasets, while surpassing state-of-the-art methods on multi-category datasets.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2409.17993.pdf' target='_blank'>https://arxiv.org/pdf/2409.17993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchen Yu, Si-Yuan Cao, Runmin Zhang, Chenghao Zhang, Zhu Yu, Shujie Chen, Bailin Yang, Hui-liang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17993">SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel unsupervised cross-modal homography estimation learning framework, named Split Supervised Homography estimation Network (SSHNet). SSHNet reformulates the unsupervised cross-modal homography estimation into two supervised sub-problems, each addressed by its specialized network: a homography estimation network and a modality transfer network. To realize stable training, we introduce an effective split optimization strategy to train each network separately within its respective sub-problem. We also formulate an extra homography feature space supervision to enhance feature consistency, further boosting the estimation accuracy. Moreover, we employ a simple yet effective distillation training technique to reduce model parameters and improve cross-domain generalization ability while maintaining comparable performance. The training stability of SSHNet enables its cooperation with various homography estimation architectures. Experiments reveal that the SSHNet using IHN as homography estimation network, namely SSHNet-IHN, outperforms previous unsupervised approaches by a significant margin. Even compared to supervised approaches MHN and LocalTrans, SSHNet-IHN achieves 47.4% and 85.8% mean average corner errors (MACEs) reduction on the challenging OPT-SAR dataset.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2601.17257.pdf' target='_blank'>https://arxiv.org/pdf/2601.17257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier Porras-Valenzuela, Samar Hadou, Alejandro Ribeiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17257">A Constrained Optimization Perspective of Unrolled Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2601.10103.pdf' target='_blank'>https://arxiv.org/pdf/2601.10103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao Liang, Weifeng Chen, Xing Wang, Yuan Zhang, Mingyuan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.10103">FlowAct-R1: Towards Interactive Humanoid Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2601.03220.pdf' target='_blank'>https://arxiv.org/pdf/2601.03220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Finzi, Shikai Qiu, Yiding Jiang, Pavel Izmailov, J. Zico Kolter, Andrew Gordon Wilson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03220">From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2512.04797.pdf' target='_blank'>https://arxiv.org/pdf/2512.04797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>SIMA team, Adrian Bolton, Alexander Lerchner, Alexandra Cordell, Alexandre Moufarek, Andrew Bolt, Andrew Lampinen, Anna Mitenkova, Arne Olav Hallingstad, Bojan Vujatovic, Bonnie Li, Cong Lu, Daan Wierstra, Daniel P. Sawyer, Daniel Slater, David Reichert, Davide Vercelli, Demis Hassabis, Drew A. Hudson, Duncan Williams, Ed Hirst, Fabio Pardo, Felix Hill, Frederic Besse, Hannah Openshaw, Harris Chan, Hubert Soyer, Jane X. Wang, Jeff Clune, John Agapiou, John Reid, Joseph Marino, Junkyung Kim, Karol Gregor, Kaustubh Sridhar, Kay McKinney, Laura Kampis, Lei M. Zhang, Loic Matthey, Luyu Wang, Maria Abi Raad, Maria Loks-Thompson, Martin Engelcke, Matija Kecman, Matthew Jackson, Maxime Gazeau, Ollie Purkiss, Oscar Knagg, Peter Stys, Piermaria Mendolicchio, Raia Hadsell, Rosemary Ke, Ryan Faulkner, Sarah Chakera, Satinder Singh Baveja, Shane Legg, Sheleem Kashem, Tayfun Terzi, Thomas Keck, Tim Harley, Tim Scholtes, Tyson Roberts, Volodymyr Mnih, Yulan Liu, Zhengdong Wang, Zoubin Ghahramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04797">SIMA 2: A Generalist Embodied Agent for Virtual Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2511.09865.pdf' target='_blank'>https://arxiv.org/pdf/2511.09865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingye Zhu, Yi Liu, Zheren Fu, Quan Wang, Yongdong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09865">In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning via Self-Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training Large Language Models (LLMs) for chain-of-thought reasoning presents a significant challenge: supervised fine-tuning on a single "golden" rationale hurts generalization as it penalizes equally valid alternatives, whereas reinforcement learning with verifiable rewards struggles with credit assignment and prohibitive computational cost. To tackle these limitations, we introduce InTRO (In-Token Rationality Optimization), a new framework that enables both token-level exploration and self-feedback for accurate and concise reasoning. Instead of directly optimizing an intractable objective over all valid reasoning paths, InTRO leverages correction factors-token-wise importance weights estimated by the information discrepancy between the generative policy and its answer-conditioned counterpart, for informative next token selection. This approach allows the model to perform token-level exploration and receive self-generated feedback within a single forward pass, ultimately encouraging accurate and concise rationales. Across six math-reasoning benchmarks, InTRO consistently outperforms other baselines, raising solution accuracy by up to 20% relative to the base model. Its chains of thought are also notably more concise, exhibiting reduced verbosity. Beyond this, InTRO enables cross-domain transfer, successfully adapting to out-of-domain reasoning tasks that extend beyond the realm of mathematics, demonstrating robust generalization.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2510.08962.pdf' target='_blank'>https://arxiv.org/pdf/2510.08962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Cao, Mingwei Xu, Xin Yu, Jiangchao Yao, Wei Ye, Shengjun Huang, Minling Zhang, Ivor W. Tsang, Yew Soon Ong, James T. Kwok, Heng Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08962">Analytical Survey of Learning with Low-Resource Data: From Analysis to Investigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning with high-resource data has demonstrated substantial success in artificial intelligence (AI); however, the costs associated with data annotation and model training remain significant. A fundamental objective of AI research is to achieve robust generalization with limited-resource data. This survey employs agnostic active sampling theory within the Probably Approximately Correct (PAC) framework to analyze the generalization error and label complexity associated with learning from low-resource data in both model-agnostic supervised and unsupervised settings. Based on this analysis, we investigate a suite of optimization strategies tailored for low-resource data learning, including gradient-informed optimization, meta-iteration optimization, geometry-aware optimization, and LLMs-powered optimization. Furthermore, we provide a comprehensive overview of multiple learning paradigms that can benefit from low-resource data, including domain transfer, reinforcement feedback, and hierarchical structure modeling. Finally, we conclude our analysis and investigation by summarizing the key findings and highlighting their implications for learning with low-resource data.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2510.00978.pdf' target='_blank'>https://arxiv.org/pdf/2510.00978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00978">A Scene is Worth a Thousand Features: Feed-Forward Camera Localization from a Collection of Image Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visually localizing an image, i.e., estimating its camera pose, requires building a scene representation that serves as a visual map. The representation we choose has direct consequences towards the practicability of our system. Even when starting from mapping images with known camera poses, state-of-the-art approaches still require hours of mapping time in the worst case, and several minutes in the best. This work raises the question whether we can achieve competitive accuracy much faster. We introduce FastForward, a method that creates a map representation and relocalizes a query image on-the-fly in a single feed-forward pass. At the core, we represent multiple mapping images as a collection of features anchored in 3D space. FastForward utilizes these mapping features to predict image-to-scene correspondences for the query image, enabling the estimation of its camera pose. We couple FastForward with image retrieval and achieve state-of-the-art accuracy when compared to other approaches with minimal map preparation time. Furthermore, FastForward demonstrates robust generalization to unseen domains, including challenging large-scale outdoor environments.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2508.21104.pdf' target='_blank'>https://arxiv.org/pdf/2508.21104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenfeng Feng, Penghong Zhao, Guochao Jiang, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21104">PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts during training. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Moreover, PVPO is orthogonal to other advanced critic-free RL algorithms, making it compatible with and complementary to these methods. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2508.08570.pdf' target='_blank'>https://arxiv.org/pdf/2508.08570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenruo Liu, Hongjun Liu, Zeyu Lai, Yiqiu Shen, Chen Zhao, Qi Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08570">Superclass-Guided Representation Disentanglement for Spurious Correlation Mitigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enhance group robustness to spurious correlations, prior work often relies on auxiliary annotations for groups or spurious features and assumes identical sets of groups across source and target domains. These two requirements are both unnatural and impractical in real-world settings. To overcome these limitations, we propose a method that leverages the semantic structure inherent in class labels--specifically, superclass information--to naturally reduce reliance on spurious features. Our model employs gradient-based attention guided by a pre-trained vision-language model to disentangle superclass-relevant and irrelevant features. Then, by promoting the use of all superclass-relevant features for prediction, our approach achieves robustness to more complex spurious correlations without the need to annotate any source samples. Experiments across diverse datasets demonstrate that our method significantly outperforms baselines in domain generalization tasks, with clear improvements in both quantitative metrics and qualitative visualizations.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2507.07370.pdf' target='_blank'>https://arxiv.org/pdf/2507.07370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanhong Jiang, Dylan Shah, Hsin-Jung Yang, Soumik Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07370">Data-driven Kinematic Modeling in Soft Robots: System Identification and Uncertainty Quantification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise kinematic modeling is critical in calibration and controller design for soft robots, yet remains a challenging issue due to their highly nonlinear and complex behaviors. To tackle the issue, numerous data-driven machine learning approaches have been proposed for modeling nonlinear dynamics. However, these models suffer from prediction uncertainty that can negatively affect modeling accuracy, and uncertainty quantification for kinematic modeling in soft robots is underexplored. In this work, using limited simulation and real-world data, we first investigate multiple linear and nonlinear machine learning models commonly used for kinematic modeling of soft robots. The results reveal that nonlinear ensemble methods exhibit the most robust generalization performance. We then develop a conformal kinematic modeling framework for soft robots by utilizing split conformal prediction to quantify predictive position uncertainty, ensuring distribution-free prediction intervals with a theoretical guarantee.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2507.05677.pdf' target='_blank'>https://arxiv.org/pdf/2507.05677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Wang, Qin Xu, Bo Jiang, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05677">Integrated Structural Prompt Learning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning methods have significantly extended the transferability of pre-trained Vision-Language Models (VLMs) like CLIP for various downstream tasks. These methods adopt handcraft templates or learnable vectors to provide text or image instructions in fine-tuning VLMs. However, most existing works ignore the structural relationships between learnable prompts and tokens within and between modalities. Moreover, balancing the performance of base and new classes remains a significant challenge. In this paper, we propose an Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of information representations between the text and image branches. ISP introduces self-structural and cross-structural prompt modules to model the structural relationships between learnable prompts and frozen tokens within and across modalities. This enables efficient information transfer while preserving feature stability. Additionally, we propose a sample probing module that dynamically adjusts loss coefficients based on sample difficulty, preventing the mode from overfitting to simple samples and improving generalization ability to new classes. Extensive experiments on three widely used settings: base-to-new generalization, cross-dataset evaluation, and domain generalization demonstrate that the proposed ISP achieves competitive performance against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2507.05668.pdf' target='_blank'>https://arxiv.org/pdf/2507.05668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Wang, Qin Xu, Bo Jiang, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05668">Dynamic Rank Adaptation for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained large vision-language models (VLMs) like CLIP demonstrate impressive generalization ability. Existing prompt-based and adapter-based works have made significant progress in fine-tuning VLMs but still face the challenges of maintaining strong generalization abilities, particularly towards unseen new classes. This limitation partly arises from these methods treating all tokens of the image and text encoder equally, which can lead to overfitting on less informative features (e.g., background noise, template words) and degrade the general representations that are crucial for novel concept recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a novel adapter variant method, designed specifically to enhance new class generalization. DRA dynamically allocates adaptation ranks based on the importance of features during training to preserve general knowledge. DRA first employs token importance grouping, using sequence attention to evaluate and group tokens by their importance. Then, we adopt rank adaptation according to the importance of each token group dynamically by assigning higher feature ranks to the more important tokens. Also, we design a new channel response mechanism to prioritize the preservation and adaptation of feature channels identified as the most informative for each instance. In addition, a L1 regularization term is introduced to stabilize the training. Extensive experiments demonstrate the effectiveness and superiority of our proposed DRA over existing works, especially on enhancing the performance of new classes on various benchmarks, including base-new classes, cross-datasets evaluation and domain generalization. The source code will be published after the paper is received.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2506.15498.pdf' target='_blank'>https://arxiv.org/pdf/2506.15498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15498">SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Process or step-wise supervision has played a crucial role in advancing complex multi-step reasoning capabilities of Large Language Models (LLMs). However, efficient, high-quality automated process annotation remains a significant challenge. To address this, we introduce Single-Pass Annotation with Reference-Guided Evaluation (SPARE), a novel structured framework that enables efficient per-step annotation by jointly aligning solution steps to reference solutions and determine its accuracy with explicit reasoning in single generation. We demonstrate SPARE's effectiveness across four diverse datasets spanning mathematical reasoning (GSM8K, MATH), multi-hop question answering (MuSiQue-Ans), and spatial reasoning (SpaRP), showing consistent improvements in two applications: (1) training Process Reward Models (PRMs) for ranking and aggregating multiple generations, and (2) fine-tuning models via offline reinforcement learning for greedy decoding. On ProcessBench, SPARE demonstrates data-efficient out-of-distribution generalization, using only $\sim$16% of training samples compared to human-labeled and other synthetically trained baselines. Additionally, it achieves competitive performance with MCTS-based methods while offering 2.3$\times$ speedup in terms of total token count. Manual analysis reveals complementary precision-recall characteristics with MCTS approaches, suggesting potential for ensemble methods. These results establish SPARE as a practical and scalable solution for automatic process supervision in LLM reasoning.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2506.09446.pdf' target='_blank'>https://arxiv.org/pdf/2506.09446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhe Ding, Jian Liang, Bo Jiang, Zi Wang, Aihua Zheng, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09446">Harmonizing and Merging Source Models for CLIP-based Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CLIP-based domain generalization aims to improve model generalization to unseen domains by leveraging the powerful zero-shot classification capabilities of CLIP and multiple source datasets. Existing methods typically train a single model across multiple source domains to capture domain-shared information. However, this paradigm inherently suffers from two types of conflicts: 1) sample conflicts, arising from noisy samples and extreme domain shifts among sources; and 2) optimization conflicts, stemming from competition and trade-offs during multi-source training. Both hinder the generalization and lead to suboptimal solutions. Recent studies have shown that model merging can effectively mitigate the competition of multi-objective optimization and improve generalization performance. Inspired by these findings, we propose Harmonizing and Merging (HAM), a novel source model merging framework for CLIP-based domain generalization. During the training process of the source models, HAM enriches the source samples without conflicting samples, and harmonizes the update directions of all models. Then, a redundancy-aware historical model merging method is introduced to effectively integrate knowledge across all source models. HAM comprehensively consolidates source domain information while enabling mutual enhancement among source models, ultimately yielding a final model with optimal generalization capabilities. Extensive experiments on five widely used benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2506.08240.pdf' target='_blank'>https://arxiv.org/pdf/2506.08240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongkyu Cho, Rumi Chunara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08240">Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is a promising tool for enhancing out-of-distribution generalization, where the key is to produce diverse, challenging variations of the source domain via costly targeted augmentations that maximize its generalization effect. Conversely, random augmentation is inexpensive but is deemed suboptimal due to its limited effect. In this paper, we revisit random augmentation and explore methods to address its shortcomings. We show that the stochastic nature of random augmentation can produce a set of colliding augmentations that distorts the learned features, similar to catastrophic forgetting. We propose a simple solution that improves the generalization effect of random augmentation by addressing forgetting, which displays strong generalization performance across various single source domain generalization (sDG) benchmarks.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2505.10223.pdf' target='_blank'>https://arxiv.org/pdf/2505.10223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10223">Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation models are often trained on curated datasets, leading to performance degradation when deployed in real-world clinical settings due to mismatches between training and test distributions. While data augmentation techniques are widely used to address these challenges, traditional visually consistent augmentation strategies lack the robustness needed for diverse real-world scenarios. In this work, we systematically evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary Fourier Augmentation. These methods mitigate the effects of multiple variations without explicitly targeting specific sources of distribution shifts. We demonstrate how these techniques significantly improve out-of-distribution generalization and robustness to imaging variations across a wide range of transformations in cardiac cine MRI and prostate MRI segmentation. We quantitatively find that these augmentation methods enhance learned feature representations by promoting separability and compactness. Additionally, we highlight how their integration into nnU-Net training pipelines provides an easy-to-implement, effective solution for enhancing the reliability of medical segmentation models in real-world applications.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2505.01452.pdf' target='_blank'>https://arxiv.org/pdf/2505.01452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franco Maria Nardini, Thong Nguyen, Cosimo Rulli, Rossano Venturini, Andrew Yates
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01452">Effective Inference-Free Retrieval for Learned Sparse Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learned Sparse Retrieval (LSR) is an effective IR approach that exploits pre-trained language models for encoding text into a learned bag of words. Several efforts in the literature have shown that sparsity is key to enabling a good trade-off between the efficiency and effectiveness of the query processor. To induce the right degree of sparsity, researchers typically use regularization techniques when training LSR models. Recently, new efficient -- inverted index-based -- retrieval engines have been proposed, leading to a natural question: has the role of regularization changed in training LSR models? In this paper, we conduct an extended evaluation of regularization approaches for LSR where we discuss their effectiveness, efficiency, and out-of-domain generalization capabilities. We first show that regularization can be relaxed to produce more effective LSR encoders. We also show that query encoding is now the bottleneck limiting the overall query processor performance. To remove this bottleneck, we advance the state-of-the-art of inference-free LSR by proposing Learned Inference-free Retrieval (Li-LSR). At training time, Li-LSR learns a score for each token, casting the query encoding step into a seamless table lookup. Our approach yields state-of-the-art effectiveness for both in-domain and out-of-domain evaluation, surpassing Splade-v3-Doc by 1 point of mRR@10 on MS MARCO and 1.8 points of nDCG@10 on BEIR.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2504.06637.pdf' target='_blank'>https://arxiv.org/pdf/2504.06637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Ma, Haihong E., Junpeng Ding, Jun Zhang, Ziyan Ma, Huang Qing, Bofei Gao, Liang Chen, Meina Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06637">SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex Multimodal Reasoning in Academic Areas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate impressive problem-solving skills in many tasks and domains. However, their ability to reason with complex images in academic domains has not been systematically investigated. To bridge this gap, we present SCI-Reason, a dataset for complex multimodel reasoning in academic areas. SCI-Reason aims to test and improve the reasoning ability of large multimodal models using real complex images in academic domains. The dataset contains 12,066 images and 12,626 question-answer pairs extracted from PubMed, divided into training, validation and test splits. Each question-answer pair also contains an accurate and efficient inference chain as a guide to improving the inference properties of the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8 well-known models. The best performing model, Claude-3.7-Sonnet, only achieved an accuracy of 55.19%. Error analysis shows that more than half of the model failures are due to breakdowns in multi-step inference chains rather than errors in primary visual feature extraction. This finding underscores the inherent limitations in reasoning capabilities exhibited by current multimodal models when processing complex image analysis tasks within authentic academic contexts. Experiments on open-source models show that SCI-Reason not only enhances reasoning ability but also demonstrates cross-domain generalization in VQA tasks. We also explore future applications of model inference capabilities in this domain, highlighting its potential for future research.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2503.22172.pdf' target='_blank'>https://arxiv.org/pdf/2503.22172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minho Park, Sunghyun Park, Jungsoo Lee, Hyojin Park, Kyuwoong Hwang, Fatih Porikli, Jaegul Choo, Sungha Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22172">Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of data scarcity in semantic segmentation by generating datasets through text-to-image (T2I) generation models, reducing image acquisition and labeling costs. Segmentation dataset generation faces two key challenges: 1) aligning generated samples with the target domain and 2) producing informative samples beyond the training data. Fine-tuning T2I models can help generate samples aligned with the target domain. However, it often overfits and memorizes training data, limiting their ability to generate diverse and well-aligned samples. To overcome these issues, we propose Concept-Aware LoRA (CA-LoRA), a novel fine-tuning approach that selectively identifies and updates only the weights associated with necessary concepts (e.g., style or viewpoint) for domain alignment while preserving the pretrained knowledge of the T2I model to produce informative samples. We demonstrate its effectiveness in generating datasets for urban-scene segmentation, outperforming baseline and state-of-the-art methods in in-domain (few-shot and fully-supervised) settings, as well as in domain generalization tasks, especially under challenging conditions such as adverse weather and varying illumination, further highlighting its superiority.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2503.16852.pdf' target='_blank'>https://arxiv.org/pdf/2503.16852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxi Li, Di Lin, Hao Chen, Hongying Liu, Liang Wan, Wei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16852">Casual Inference via Style Bias Deconfounding for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) often struggle with out-of-distribution data, limiting their reliability in diverse realworld applications. To address this issue, domain generalization methods have been developed to learn domain-invariant features from single or multiple training domains, enabling generalization to unseen testing domains. However, existing approaches usually overlook the impact of style frequency within the training set. This oversight predisposes models to capture spurious visual correlations caused by style confounding factors, rather than learning truly causal representations, thereby undermining inference reliability. In this work, we introduce Style Deconfounding Causal Learning (SDCL), a novel causal inference-based framework designed to explicitly address style as a confounding factor. Our approaches begins with constructing a structural causal model (SCM) tailored to the domain generalization problem and applies a backdoor adjustment strategy to account for style influence. Building on this foundation, we design a style-guided expert module (SGEM) to adaptively clusters style distributions during training, capturing the global confounding style. Additionally, a back-door causal learning module (BDCL) performs causal interventions during feature extraction, ensuring fair integration of global confounding styles into sample predictions, effectively reducing style bias. The SDCL framework is highly versatile and can be seamlessly integrated with state-of-the-art data augmentation techniques. Extensive experiments across diverse natural and medical image recognition tasks validate its efficacy, demonstrating superior performance in both multi-domain and the more challenging single-domain generalization scenarios.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2503.15435.pdf' target='_blank'>https://arxiv.org/pdf/2503.15435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baolu Li, Zongzhe Xu, Jinlong Li, Xinyu Liu, Jianwu Fang, Xiaopeng Li, Hongkai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15435">V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has demonstrated its impact on the safety and effectiveness of autonomous driving. Since current cooperative perception algorithms are trained and tested on the same dataset, the generalization ability of cooperative perception systems remains underexplored. This paper is the first work to study the Domain Generalization problem of LiDAR-based V2X cooperative perception (V2X-DG) for 3D detection based on four widely-used open source datasets: OPV2V, V2XSet, V2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only within the source domain but also across other unseen domains, achieved solely through training on source domain. To this end, we propose Cooperative Mixup Augmentation based Generalization (CMAG) to improve the model generalization capability by simulating the unseen cooperation, which is designed compactly for the domain gaps in cooperative perception. Furthermore, we propose a constraint for the regularization of the robust generalized feature representation learning: Cooperation Feature Consistency (CFC), which aligns the intermediately fused features of the generalized cooperation by CMAG and the early fused features of the original cooperation in source domain. Extensive experiments demonstrate that our approach achieves significant performance gains when generalizing to other unseen datasets while it also maintains strong performance on the source dataset.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2503.08407.pdf' target='_blank'>https://arxiv.org/pdf/2503.08407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yansong Guo, Jie Hu, Yansong Qu, Liujuan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08407">WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in interactive 3D segmentation from 2D images have demonstrated impressive performance. However, current models typically require extensive scene-specific training to accurately reconstruct and segment objects, which limits their applicability in real-time scenarios. In this paper, we introduce WildSeg3D, an efficient approach that enables the segmentation of arbitrary 3D objects across diverse environments using a feed-forward mechanism. A key challenge of this feed-forward approach lies in the accumulation of 3D alignment errors across multiple 2D views, which can lead to inaccurate 3D segmentation results. To address this issue, we propose Dynamic Global Aligning (DGA), a technique that improves the accuracy of global multi-view alignment by focusing on difficult-to-match 3D points across images, using a dynamic adjustment function. Additionally, for real-time interactive segmentation, we introduce Multi-view Group Mapping (MGM), a method that utilizes an object mask cache to integrate multi-view segmentations and respond rapidly to user prompts. WildSeg3D demonstrates robust generalization across arbitrary scenes, thereby eliminating the need for scene-specific training. Specifically, WildSeg3D not only attains the accuracy of state-of-the-art (SOTA) methods but also achieves a $40\times$ speedup compared to existing SOTA models. Our code will be publicly available.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2503.08168.pdf' target='_blank'>https://arxiv.org/pdf/2503.08168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miao Zhang, Jun Yin, Pengyu Zeng, Yiqing Shen, Shuai Lu, Xueqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08168">TSCnet: A Text-driven Semantic-level Controllable Framework for Customized Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based image enhancement methods show significant advantages in reducing noise and improving visibility in low-light conditions. These methods are typically based on one-to-one mapping, where the model learns a direct transformation from low light to specific enhanced images. Therefore, these methods are inflexible as they do not allow highly personalized mapping, even though an individual's lighting preferences are inherently personalized. To overcome these limitations, we propose a new light enhancement task and a new framework that provides customized lighting control through prompt-driven, semantic-level, and quantitative brightness adjustments. The framework begins by leveraging a Large Language Model (LLM) to understand natural language prompts, enabling it to identify target objects for brightness adjustments. To localize these target objects, the Retinex-based Reasoning Segment (RRS) module generates precise target localization masks using reflection images. Subsequently, the Text-based Brightness Controllable (TBC) module adjusts brightness levels based on the generated illumination map. Finally, an Adaptive Contextual Compensation (ACC) module integrates multi-modal inputs and controls a conditional diffusion model to adjust the lighting, ensuring seamless and precise enhancements accurately. Experimental results on benchmark datasets demonstrate our framework's superior performance at increasing visibility, maintaining natural color balance, and amplifying fine details without creating artifacts. Furthermore, its robust generalization capabilities enable complex semantic-level lighting adjustments in diverse open-world environments through natural language interactions.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2502.20237.pdf' target='_blank'>https://arxiv.org/pdf/2502.20237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Bencomo, Max Gupta, Ioana Marinescu, R. Thomas McCoy, Thomas L. Griffiths
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20237">Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial neural networks can acquire many aspects of human knowledge from data, making them promising as models of human learning. But what those networks can learn depends upon their inductive biases -- the factors other than the data that influence the solutions they discover -- and the inductive biases of neural networks remain poorly understood, limiting our ability to draw conclusions about human learning from the performance of these systems. Cognitive scientists and machine learning researchers often focus on the architecture of a neural network as a source of inductive bias. In this paper we explore the impact of another source of inductive bias -- the initial weights of the network -- using meta-learning as a tool for finding initial weights that are adapted for specific problems. We evaluate four widely-used architectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430 different models across three tasks requiring different biases and forms of generalization. We find that meta-learning can substantially reduce or entirely eliminate performance differences across architectures and data representations, suggesting that these factors may be less important as sources of inductive bias than is typically assumed. When differences are present, architectures and data representations that perform well without meta-learning tend to meta-train more effectively. Moreover, all architectures generalize poorly on problems that are far from their meta-training experience, underscoring the need for stronger inductive biases for robust generalization.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2501.09688.pdf' target='_blank'>https://arxiv.org/pdf/2501.09688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiho Choi, Seonho Lee, Minhyun Lee, Seungho Lee, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09688">Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing fine-grained parts in unseen categories. We identify two primary challenges in OVPS: (1) the difficulty in aligning part-level image-text correspondence, and (2) the lack of structural understanding in segmenting object parts. To address these issues, we propose PartCATSeg, a novel framework that integrates object-aware part-level cost aggregation, compositional loss, and structural guidance from DINO. Our approach employs a disentangled cost aggregation strategy that handles object and part-level costs separately, enhancing the precision of part-level segmentation. We also introduce a compositional loss to better capture part-object relationships, compensating for the limited part annotations. Additionally, structural guidance from DINO features improves boundary delineation and inter-part understanding. Extensive experiments on Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that our method significantly outperforms state-of-the-art approaches, setting a new baseline for robust generalization to unseen part categories.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2411.07392.pdf' target='_blank'>https://arxiv.org/pdf/2411.07392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoliang Wang, Chen Zhao, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07392">Feature-Space Semantic Invariance: Enhanced OOD Detection for Open-Set Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-set domain generalization addresses a real-world challenge: training a model to generalize across unseen domains (domain generalization) while also detecting samples from unknown classes not encountered during training (open-set recognition). However, most existing approaches tackle these issues separately, limiting their practical applicability. To overcome this limitation, we propose a unified framework for open-set domain generalization by introducing Feature-space Semantic Invariance (FSI). FSI maintains semantic consistency across different domains within the feature space, enabling more accurate detection of OOD instances in unseen domains. Additionally, we adopt a generative model to produce synthetic data with novel domain styles or class labels, enhancing model robustness. Initial experiments show that our method improves AUROC by 9.1% to 18.9% on ColoredMNIST, while also significantly increasing in-distribution classification accuracy.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2411.02444.pdf' target='_blank'>https://arxiv.org/pdf/2411.02444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoliang Wang, Chen Zhao, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02444">MADOD: Generalizing OOD Detection to Unseen Domains via G-Invariance Meta-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world machine learning applications often face simultaneous covariate and semantic shifts, challenging traditional domain generalization and out-of-distribution (OOD) detection methods. We introduce Meta-learned Across Domain Out-of-distribution Detection (MADOD), a novel framework designed to address both shifts concurrently. MADOD leverages meta-learning and G-invariance to enhance model generalizability and OOD detection in unseen domains. Our key innovation lies in task construction: we randomly designate in-distribution classes as pseudo-OODs within each meta-learning task, simulating OOD scenarios using existing data. This approach, combined with energy-based regularization, enables the learning of robust, domain-invariant features while calibrating decision boundaries for effective OOD detection. Operating in a test domain-agnostic setting, MADOD eliminates the need for adaptation during inference, making it suitable for scenarios where test data is unavailable. Extensive experiments on real-world and synthetic datasets demonstrate MADOD's superior performance in semantic OOD detection across unseen domains, achieving an AUPR improvement of 8.48% to 20.81%, while maintaining competitive in-distribution classification accuracy, representing a significant advancement in handling both covariate and semantic shifts.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2411.01316.pdf' target='_blank'>https://arxiv.org/pdf/2411.01316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Jiang, Chen Zhao, Haoliang Wang, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01316">FEED: Fairness-Enhanced Meta-Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing to out-of-distribution data while being aware of model fairness is a significant and challenging problem in meta-learning. The goal of this problem is to find a set of fairness-aware invariant parameters of classifier that is trained using data drawn from a family of related training domains with distribution shift on non-sensitive features as well as different levels of dependence between model predictions and sensitive features so that the classifier can achieve good generalization performance on unknown but distinct test domains. To tackle this challenge, existing state-of-the-art methods either address the domain generalization problem but completely ignore learning with fairness or solely specify shifted domains with various fairness levels. This paper introduces an approach to fairness-aware meta-learning that significantly enhances domain generalization capabilities. Our framework, Fairness-Enhanced Meta-Learning for Domain Generalization (FEED), disentangles latent data representations into content, style, and sensitive vectors. This disentanglement facilitates the robust generalization of machine learning models across diverse domains while adhering to fairness constraints. Unlike traditional methods that focus primarily on domain invariance or sensitivity to shifts, our model integrates a fairness-aware invariance criterion directly into the meta-learning process. This integration ensures that the learned parameters uphold fairness consistently, even when domain characteristics vary widely. We validate our approach through extensive experiments across multiple benchmarks, demonstrating not only superior performance in maintaining high accuracy and fairness but also significant improvements over existing state-of-the-art methods in domain generalization tasks.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2601.18217.pdf' target='_blank'>https://arxiv.org/pdf/2601.18217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihan Liu, Lin Guan, Yixin Nie, Kai Zhang, Zhuoqun Hao, Lin Chen, Asli Celikyilmaz, Zhaoran Wang, Na Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.18217">Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2601.16359.pdf' target='_blank'>https://arxiv.org/pdf/2601.16359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayan Banerjee, Komandoor Srivathsan, Sandeep K. S. Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16359">Experience with Single Domain Generalization in Real World Medical Imaging Deployments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A desirable property of any deployed artificial intelligence is generalization across domains, i.e. data generation distribution under a specific acquisition condition. In medical imagining applications the most coveted property for effective deployment is Single Domain Generalization (SDG), which addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. In multi-center studies, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare class characteristics. This paper presents our experience on SDG in real life deployment for two exemplary medical imaging case studies on seizure onset zone detection using fMRI data, and stress electrocardiogram based coronary artery detection. Utilizing the commonly used application of diabetic retinopathy, we first demonstrate that state-of-the-art SDG techniques fail to achieve generalized performance across data domains. We then develop a generic expert knowledge integrated deep learning technique DL+EKE and instantiate it for the DR application and show that DL+EKE outperforms SOTA SDG methods on DR. We then deploy instances of DL+EKE technique on the two real world examples of stress ECG and resting state (rs)-fMRI and discuss issues faced with SDG techniques.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2601.02008.pdf' target='_blank'>https://arxiv.org/pdf/2601.02008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Midhat Urooj, Ayan Banerjee, Sandeep Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02008">XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2512.18177.pdf' target='_blank'>https://arxiv.org/pdf/2512.18177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Midhat Urooj, Ayan Banerjee, Sandeep Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18177">NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods. Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2512.13095.pdf' target='_blank'>https://arxiv.org/pdf/2512.13095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Zhang, Zezhong Tan, Xinhong Ma, Ziqiang Dong, Xi Leng, Jianfei Zhao, Xin Sun, Yang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13095">ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2512.10098.pdf' target='_blank'>https://arxiv.org/pdf/2512.10098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Midhat Urooj, Ayan Banerjee, Farhat Shaikh, Kuntal Thakur, Sandeep Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10098">MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly under domain shifts and rare-class conditions. Deep learning models often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Medical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician-derived expert knowledge to improve generalization, reduce rare-class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2512.08625.pdf' target='_blank'>https://arxiv.org/pdf/2512.08625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jisang Yoo, Gyeongjin Kang, Hyun-kyu Ko, Hyeonwoo Yu, Eunbyung Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08625">OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2511.20500.pdf' target='_blank'>https://arxiv.org/pdf/2511.20500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sidahmed Benabderrahmane, Talal Rahwan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20500">From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2511.17724.pdf' target='_blank'>https://arxiv.org/pdf/2511.17724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Atwany, Mojtaba Lashgari, Robin P. Choudhury, Vicente Grau, Abhirup Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17724">AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2511.16136.pdf' target='_blank'>https://arxiv.org/pdf/2511.16136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Yan, Ziqiang Li, Fan Wang, Kai Zeng, Zhangjie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16136">How Noise Benefits AI-generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2511.13108.pdf' target='_blank'>https://arxiv.org/pdf/2511.13108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhen Yan, Ziqiang Li, Fan Wang, Boyu Wang, Zhangjie Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13108">DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2511.07798.pdf' target='_blank'>https://arxiv.org/pdf/2511.07798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runmin Cong, Anpeng Wang, Bin Wan, Cong Zhang, Xiaofei Zhou, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07798">Divide-and-Conquer Decoupled Network for Cross-Domain Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain few-shot segmentation (CD-FSS) aims to tackle the dual challenge of recognizing novel classes and adapting to unseen domains with limited annotations. However, encoder features often entangle domain-relevant and category-relevant information, limiting both generalization and rapid adaptation to new domains. To address this issue, we propose a Divide-and-Conquer Decoupled Network (DCDNet). In the training stage, to tackle feature entanglement that impedes cross-domain generalization and rapid adaptation, we propose the Adversarial-Contrastive Feature Decomposition (ACFD) module. It decouples backbone features into category-relevant private and domain-relevant shared representations via contrastive learning and adversarial learning. Then, to mitigate the potential degradation caused by the disentanglement, the Matrix-Guided Dynamic Fusion (MGDF) module adaptively integrates base, shared, and private features under spatial guidance, maintaining structural coherence. In addition, in the fine-tuning stage, to enhanced model generalization, the Cross-Adaptive Modulation (CAM) module is placed before the MGDF, where shared features guide private features via modulation ensuring effective integration of domain-relevant information. Extensive experiments on four challenging datasets show that DCDNet outperforms existing CD-FSS methods, setting a new state-of-the-art for cross-domain generalization and few-shot adaptation.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2510.24884.pdf' target='_blank'>https://arxiv.org/pdf/2510.24884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olawale Salaudeen, Haoran Zhang, Kumail Alhamoud, Sara Beery, Marzyeh Ghassemi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24884">Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks for out-of-distribution (OOD) generalization frequently show a strong positive correlation between in-distribution (ID) and OOD accuracy across models, termed "accuracy-on-the-line." This pattern is often taken to imply that spurious correlations - correlations that improve ID but reduce OOD performance - are rare in practice. We find that this positive correlation is often an artifact of aggregating heterogeneous OOD examples. Using a simple gradient-based method, OODSelect, we identify semantically coherent OOD subsets where accuracy on the line does not hold. Across widely used distribution shift benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings indicate that aggregate metrics can obscure important failure modes of OOD robustness. We release code and the identified subsets to facilitate further research.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2510.23217.pdf' target='_blank'>https://arxiv.org/pdf/2510.23217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23217">Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2510.14095.pdf' target='_blank'>https://arxiv.org/pdf/2510.14095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Awni Altabaa, Siyu Chen, John Lafferty, Zhuoran Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14095">Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2509.02918.pdf' target='_blank'>https://arxiv.org/pdf/2509.02918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Midhat Urooj, Ayan Banerjee, Farhat Shaikh, Kuntal Thakur, Sandeep Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02918">Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization remains a critical challenge in medical imaging, where models trained on single sources often fail under real-world distribution shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy (DR) classification that integrates vision transformers with expert-guided symbolic reasoning to enable robust generalization across unseen domains. Our approach leverages clinical lesion ontologies through structured, rule-based features and retinal vessel segmentation, fusing them with deep visual representations via a confidence-weighted integration strategy. The framework addresses both single-domain generalization (SDG) and multi-domain generalization (MDG) by minimizing the KL divergence between domain embeddings, thereby enforcing alignment of high-level clinical semantics. Extensive experiments across four public datasets (APTOS, EyePACS, Messidor-1, Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in cross-domain settings and a 6% improvement over baseline ViT models. Notably, our symbolic-only model achieves a 63.67% average accuracy in MDG, while the complete neuro-symbolic integration achieves the highest accuracy compared to existing published baselines and benchmarks in challenging SDG scenarios. Ablation studies reveal that lesion-based features (84.65% accuracy) substantially outperform purely neural approaches, confirming that symbolic components act as effective regularizers beyond merely enhancing interpretability. Our findings establish neuro-symbolic integration as a promising paradigm for building clinically robust, and domain-invariant medical AI systems.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2509.02593.pdf' target='_blank'>https://arxiv.org/pdf/2509.02593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raphaël Bourgade, Guillaume Balezo, Thomas Walter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02593">Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2507.23326.pdf' target='_blank'>https://arxiv.org/pdf/2507.23326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingkai Wang, Yaoyao Zhu, Xiuding Cai, Yuhao Xiao, Haotian Wu, Yu Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23326">Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation plays a crucial role in clinical workflows, but domain shift often leads to performance degradation when models are applied to unseen clinical domains. This challenge arises due to variations in imaging conditions, scanner types, and acquisition protocols, limiting the practical deployment of segmentation models. Unlike natural images, medical images typically exhibit consistent anatomical structures across patients, with domain-specific variations mainly caused by imaging conditions. This unique characteristic makes medical image segmentation particularly challenging.
  To address this challenge, we propose a domain generalization framework tailored for medical image segmentation. Our approach improves robustness to domain-specific variations by introducing implicit feature perturbations guided by domain statistics. Specifically, we employ a learnable semantic direction selector and a covariance-based semantic intensity sampler to modulate domain-variant features while preserving task-relevant anatomical consistency. Furthermore, we design an adaptive consistency constraint that is selectively applied only when feature adjustment leads to degraded segmentation performance. This constraint encourages the adjusted features to align with the original predictions, thereby stabilizing feature selection and improving the reliability of the segmentation.
  Extensive experiments on two public multi-center benchmarks show that our framework consistently outperforms existing domain generalization approaches, achieving robust and generalizable segmentation performance across diverse clinical domains.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2507.09081.pdf' target='_blank'>https://arxiv.org/pdf/2507.09081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Yu, Mohd Yamani Idna Idris, Hua Wang, Pei Wang, Junyi Chen, Kun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09081">From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative remote sensing inversion aims to estimate continuous surface variables-such as biomass, vegetation indices, and evapotranspiration-from satellite observations, supporting applications in ecosystem monitoring, carbon accounting, and land management. With the evolution of remote sensing systems and artificial intelligence, traditional physics-based paradigms are giving way to data-driven and foundation model (FM)-based approaches. This paper systematically reviews the methodological evolution of inversion techniques, from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods (e.g., deep learning, multimodal fusion), and further to foundation models (e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application scenarios, and limitations of each paradigm, with emphasis on recent FM advances in self-supervised pretraining, multi-modal integration, and cross-task adaptation. We also highlight persistent challenges in physical interpretability, domain generalization, limited supervision, and uncertainty quantification. Finally, we envision the development of next-generation foundation models for remote sensing inversion, emphasizing unified modeling capacity, cross-domain generalization, and physical interpretability.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2506.21895.pdf' target='_blank'>https://arxiv.org/pdf/2506.21895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangling Jiang, Qi Li, Weining Wang, Gang Wang, Bing Liu, Zhenan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21895">Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently the emergence of novel presentation attacks has drawn increasing attention to face anti-spoofing. However, existing methods tend to memorize data patterns from the training set, resulting in poor generalization to unknown attack types across different scenarios and limited interpretability. To address these challenges, this paper presents a reinforcement fine-tuning-based face anti-spoofing method that stimulates the capabilities of multimodal large language models to think and learn how to solve the anti-spoofing task itself, rather than relying on the memorization of authenticity patterns. We design verifiable class consistent reward and reasoning consistent reward, and employ a GRPO-based optimization strategy to guide the model in exploring reasoning policies from multiple perspectives to maximize expected rewards. As a result, through iterative trial-and-error learning while retaining only high-reward trajectories, the model distills highly generalizable decision-making rules from the extensive solution space to effectively address cross-domain face anti-spoofing tasks. Extensive experimental results demonstrate that our method achieves state-of-the-art cross-domain generalization performance. It generalizes well to diverse unknown attack types in unseen target domains while providing interpretable reasoning for its authenticity decisions without requiring labor-intensive textual annotations for training.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2505.24149.pdf' target='_blank'>https://arxiv.org/pdf/2505.24149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Piaseczny, Md Kamran Chowdhury Shisher, Shiqiang Wang, Christopher G. Brinton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24149">RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring strict compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight policy based on a measurable accumulated loss threshold that provably limits update frequency and cost. Experimental results on three domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2503.13868.pdf' target='_blank'>https://arxiv.org/pdf/2503.13868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Wu, Fei Teng, Xingwang Li, Ji Zhang, Tianrui Li, Qiang Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13868">Out-of-Distribution Generalization in Time Series: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series frequently manifest distribution shifts, diverse latent features, and non-stationary learning dynamics, particularly in open and evolving environments. These characteristics pose significant challenges for out-of-distribution (OOD) generalization. While substantial progress has been made, a systematic synthesis of advancements remains lacking. To address this gap, we present the first comprehensive review of OOD generalization methodologies for time series, organized to delineate the field's evolutionary trajectory and contemporary research landscape. We organize our analysis across three foundational dimensions: data distribution, representation learning, and OOD evaluation. For each dimension, we present several popular algorithms in detail. Furthermore, we highlight key application scenarios, emphasizing their real-world impact. Finally, we identify persistent challenges and propose future research directions. A detailed summary of the methods reviewed for the generalization of OOD in time series can be accessed at https://tsood-generalization.com.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2502.05593.pdf' target='_blank'>https://arxiv.org/pdf/2502.05593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoyao Zhu, Xiuding Cai, Yingkai Wang, Yu Yao, Xu Luo, Zhongliang Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05593">Semantic Data Augmentation Enhanced Invariant Risk Minimization for Medical Image Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has achieved remarkable success in medical image classification. However, its clinical application is often hindered by data heterogeneity caused by variations in scanner vendors, imaging protocols, and operators. Approaches such as invariant risk minimization (IRM) aim to address this challenge of out-of-distribution generalization. For instance, VIRM improves upon IRM by tackling the issue of insufficient feature support overlap, demonstrating promising potential. Nonetheless, these methods face limitations in medical imaging due to the scarcity of annotated data and the inefficiency of augmentation strategies. To address these issues, we propose a novel domain-oriented direction selector to replace the random augmentation strategy used in VIRM. Our method leverages inter-domain covariance as a guider for augmentation direction, guiding data augmentation towards the target domain. This approach effectively reduces domain discrepancies and enhances generalization performance. Experiments on a multi-center diabetic retinopathy dataset demonstrate that our method outperforms state-of-the-art approaches, particularly under limited data conditions and significant domain heterogeneity.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2501.14653.pdf' target='_blank'>https://arxiv.org/pdf/2501.14653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Trong-Binh Nguyen, Minh-Duong Nguyen, Jinsun Park, Quoc-Viet Pham, Won Joo Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14653">Federated Domain Generalization with Data-free On-server Matching Gradient</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. One of the key approaches in DG is training an encoder which generates domain-invariant representations. However, this approach is not applicable in Federated Domain Generalization (FDG), where data from various domains are distributed across different clients. In this paper, we introduce a novel approach, dubbed Federated Learning via On-server Matching Gradient (FedOMG), which can \emph{efficiently leverage domain information from distributed domains}. Specifically, we utilize the local gradients as information about the distributed models to find an invariant gradient direction across all domains through gradient inner product maximization. The advantages are two-fold: 1) FedOMG can aggregate the characteristics of distributed models on the centralized server without incurring any additional communication cost, and 2) FedOMG is orthogonal to many existing FL/FDG methods, allowing for additional performance improvements by being seamlessly integrated with them. Extensive experimental evaluations on various settings to demonstrate the robustness of FedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA baselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and CIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome).
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2412.11408.pdf' target='_blank'>https://arxiv.org/pdf/2412.11408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Soltany, Farhad Pourpanah, Mahdiyar Molahasani, Michael Greenspan, Ali Etemad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11408">Federated Domain Generalization with Label Smoothing and Balanced Decentralized Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel approach, Federated Domain Generalization with Label Smoothing and Balanced Decentralized Training (FedSB), to address the challenges of data heterogeneity within a federated learning framework. FedSB utilizes label smoothing at the client level to prevent overfitting to domain-specific features, thereby enhancing generalization capabilities across diverse domains when aggregating local models into a global model. Additionally, FedSB incorporates a decentralized budgeting mechanism which balances training among clients, which is shown to improve the performance of the aggregated global model. Extensive experiments on four commonly used multi-domain datasets, PACS, VLCS, OfficeHome, and TerraInc, demonstrate that FedSB outperforms competing methods, achieving state-of-the-art results on three out of four datasets, indicating the effectiveness of FedSB in addressing data heterogeneity.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2412.09719.pdf' target='_blank'>https://arxiv.org/pdf/2412.09719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johann Schmidt, Frank Dreyer, Sayed Abid Hashimi, Sebastian Stober
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09719">TransferLight: Zero-Shot Traffic Signal Control on any Road-Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic signal control plays a crucial role in urban mobility. However, existing methods often struggle to generalize beyond their training environments to unseen scenarios with varying traffic dynamics. We present TransferLight, a novel framework designed for robust generalization across road-networks, diverse traffic conditions and intersection geometries. At its core, we propose a log-distance reward function, offering spatially-aware signal prioritization while remaining adaptable to varied lane configurations - overcoming the limitations of traditional pressure-based rewards. Our hierarchical, heterogeneous, and directed graph neural network architecture effectively captures granular traffic dynamics, enabling transferability to arbitrary intersection layouts. Using a decentralized multi-agent approach, global rewards, and novel state transition priors, we develop a single, weight-tied policy that scales zero-shot to any road network without re-training. Through domain randomization during training, we additionally enhance generalization capabilities. Experimental results validate TransferLight's superior performance in unseen scenarios, advancing practical, generalizable intelligent transportation systems to meet evolving urban traffic demands.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2412.08393.pdf' target='_blank'>https://arxiv.org/pdf/2412.08393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyuan Chen, Jin Wang, Xuejie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08393">Learning to Reason via Self-Iterative Process Feedback for Small Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Small language models (SLMs) are more efficient, cost-effective, and customizable than large language models (LLMs), though they often underperform in specific areas like reasoning. Past methods for enhancing SLMs' reasoning, such as supervised fine-tuning and distillation, often depend on costly external signals, resulting in SLMs being overly confident with limited supervision signals, thus limiting their abilities. Therefore, this study enables SLMs to learn to reason from self-iterative feedback. By combining odds ratio preference optimization (ORPO), we fine-tune and align SLMs using positive and negative signals generated by themselves. Additionally, we introduce process supervision for rewards in preference alignment by sampling-based inference simulation and process reward models. Compared to Supervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B by 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed method also demonstrated superior out-of-domain generalization capabilities on MMLU_Math and HumanEval.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2410.15449.pdf' target='_blank'>https://arxiv.org/pdf/2410.15449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Zhao, Zhengqiu Zhu, Chen Gao, En Wang, Jincai Huang, Fei-Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15449">Heterogeneous Graph Reinforcement Learning for Dependency-aware Multi-task Allocation in Spatial Crowdsourcing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial Crowdsourcing (SC) is gaining traction in both academia and industry, with tasks on SC platforms becoming increasingly complex and requiring collaboration among workers with diverse skills. Recent research works address complex tasks by dividing them into subtasks with dependencies and assigning them to suitable workers. However, the dependencies among subtasks and their heterogeneous skill requirements, as well as the need for efficient utilization of workers' limited work time in the multi-task allocation mode, pose challenges in achieving an optimal task allocation scheme. Therefore, this paper formally investigates the problem of Dependency-aware Multi-task Allocation (DMA) and presents a well-designed framework to solve it, known as Heterogeneous Graph Reinforcement Learning-based Task Allocation (HGRL-TA). To address the challenges associated with representing and embedding diverse problem instances to ensure robust generalization, we propose a multi-relation graph model and a Compound-path-based Heterogeneous Graph Attention Network (CHANet) for effectively representing and capturing intricate relations among tasks and workers, as well as providing embedding of problem state. The task allocation decision is determined sequentially by a policy network, which undergoes simultaneous training with CHANet using the proximal policy optimization algorithm. Extensive experiment results demonstrate the effectiveness and generality of the proposed HGRL-TA in solving the DMA problem, leading to average profits that is 21.78% higher than those achieved using the metaheuristic methods.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2601.12253.pdf' target='_blank'>https://arxiv.org/pdf/2601.12253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Xu, Jiaze Li, Jianzhong Ju, Zhenbo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12253">Federated Joint Learning for Domain and Class Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2601.05105.pdf' target='_blank'>https://arxiv.org/pdf/2601.05105.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Ghilotti, Samuel Brucker, Nahku Saidy, Matteo Matteucci, Mario Bijelic, Felix Heide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05105">UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2512.13665.pdf' target='_blank'>https://arxiv.org/pdf/2512.13665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Chen, Sezer Karaoglu, Theo Gevers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13665">Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2511.09443.pdf' target='_blank'>https://arxiv.org/pdf/2511.09443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongchao Shu, Roger D. Soberanis-Mukul, Jiru Xu, Hao Ding, Morgan Ringel, Mali Shen, Saif Iftekar Sayed, Hedyeh Rafii-Tari, Mathias Unberath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09443">BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2511.06805.pdf' target='_blank'>https://arxiv.org/pdf/2511.06805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhao Chen, Zhen Yang, Jianxin Shi, Tianyu Wo, Jie Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06805">MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \textbf{\method}, a \textbf{Math}ematical \textbf{S}elf-\textbf{E}volving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at \texttt{https://zheny2751\allowbreak-dotcom.github.io/\allowbreak MathSE.github.io/}.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2510.22630.pdf' target='_blank'>https://arxiv.org/pdf/2510.22630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adinath Dukre, Ankan Deria, Yutong Xie, Imran Razzak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22630">Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware Augmentation and Hybrid Loss for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Atypical mitotic figures are important biomarkers of tumor aggressiveness in histopathology, yet reliable recognition remains challenging due to severe class imbalance and variability across imaging domains. We present a DenseNet-121-based framework tailored for atypical mitosis classification in the MIDOG 2025 (Track 2) setting. Our method integrates stain-aware augmentation (Macenko), geometric and intensity transformations, and imbalance-aware learning via weighted sampling with a hybrid objective combining class-weighted binary cross-entropy and focal loss. Trained end-to-end with AdamW and evaluated across multiple independent domains, the model demonstrates strong generalization under scanner and staining shifts, achieving balanced accuracy 85.0%, AUROC 0.927, sensitivity 89.2%, and specificity 80.9% on the official test set. These results indicate that combining DenseNet-121 with stain-aware augmentation and imbalance-adaptive objectives yields a robust, domain-generalizable framework for atypical mitosis classification suitable for real-world computational pathology workflows.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2510.00478.pdf' target='_blank'>https://arxiv.org/pdf/2510.00478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wang, Wonho Bae, Jiahong Chen, Wenxu Wang, Junhyug Noh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00478">Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work on latent diffusion models (LDMs) has focused almost exclusively on generative tasks, leaving their potential for discriminative transfer largely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a novel LDM-based framework for a more practical variant of source-free domain adaptation (SFDA): the source provider may share not only a pre-trained classifier but also an auxiliary latent diffusion module, trained once on the source data and never exposing raw source samples. DVD encodes each source feature's label information into its latent vicinity by fitting a Gaussian prior over its k-nearest neighbors and training the diffusion network to drift noisy samples back to label-consistent representations. During adaptation, we sample from each target feature's latent vicinity, apply the frozen diffusion module to generate source-like cues, and use a simple InfoNCE loss to align the target encoder to these cues, explicitly transferring decision boundaries without source access. Across standard SFDA benchmarks, DVD outperforms state-of-the-art methods. We further show that the same latent diffusion module enhances the source classifier's accuracy on in-domain data and boosts performance in supervised classification and domain generalization experiments. DVD thus reinterprets LDMs as practical, privacy-preserving bridges for explicit knowledge transfer, addressing a core challenge in source-free domain adaptation that prior methods have yet to solve.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2509.26027.pdf' target='_blank'>https://arxiv.org/pdf/2509.26027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Pei, Yuguang Yang, Kexin Liu, Baochang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26027">Causally Guided Gaussian Perturbations for Out-Of-Distribution Generalization in Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization remains a central challenge in deploying deep learning models to real-world scenarios, particularly in domains such as biomedical images, where distribution shifts are both subtle and pervasive. While existing methods often pursue domain invariance through complex generative models or adversarial training, these approaches may overlook the underlying causal mechanisms of generalization.In this work, we propose Causally-Guided Gaussian Perturbations (CGP)-a lightweight framework that enhances OOD generalization by injecting spatially varying noise into input images, guided by soft causal masks derived from Vision Transformers. By applying stronger perturbations to background regions and weaker ones to foreground areas, CGP encourages the model to rely on causally relevant features rather than spurious correlations.Experimental results on the challenging WILDS benchmark Camelyon17 demonstrate consistent performance gains over state-of-the-art OOD baselines, highlighting the potential of causal perturbation as a tool for reliable and interpretable generalization.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2509.23176.pdf' target='_blank'>https://arxiv.org/pdf/2509.23176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Behraj Khan, Tahir Qasim Syed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23176">Confidence-Calibrating Regularization for Robust Brain MRI Segmentation Under Domain Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Segment Anything Model (SAM) exhibits strong zero-shot performance on natural images but suffers from domain shift and overconfidence when applied to medical volumes. We propose \textbf{CalSAM}, a lightweight adaptation framework that (i) reduces encoder sensitivity to domain shift via a \emph{Feature Fisher Information Penalty} (FIP) computed on 3D feature maps and (ii) penalizes overconfident voxel-wise errors through a \emph{Confidence Misalignment Penalty} (CMP). The combined loss, \(\mathcal{L}_{\mathrm{CalSAM}}\) fine-tunes only the mask decoder while keeping SAM's encoders frozen. On cross-center and scanner-shift evaluations, CalSAM substantially improves accuracy and calibration: e.g., on the BraTS scanner split (Siemens$\to$GE) CalSAM shows a $+7.4\%$ relative improvement in $\mathrm{DSC}$ (80.1\% vs.\ 74.6\%), a $-26.9\%$ reduction in $\mathrm{HD95}$ (4.6 mm vs.\ 6.3 mm), and a $-39.5\%$ reduction in $\mathrm{ECE}$ (5.2\% vs.\ 8.6\%). On ATLAS-C (motion corruptions), CalSAM achieves a $+5.3\%$ relative improvement in $\mathrm{DSC}$ (75.9\%) and a $-32.6\%$ reduction in $\mathrm{ECE}$ (5.8\%). Ablations show FIP and CMP contribute complementary gains ($p<0.01$), and the Fisher penalty incurs a modest $\sim$15\% training-time overhead. CalSAM therefore delivers improved domain generalization and better-calibrated uncertainty estimates for brain MRI segmentation, while retaining the computational benefits of freezing SAM's encoder.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2509.11082.pdf' target='_blank'>https://arxiv.org/pdf/2509.11082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongwu Xie, Kaijie Yun, Yang Liu, Yiming Ji, Han Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11082">Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a robust multi-modal framework for predicting traversability costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce a bird's-eye-view (BEV) terrain costmap, trained self-supervised using IMU-derived labels. Key updates include a DINOv3-based image encoder, FiLM-based sensor fusion, and an optimization loss combining Huber and smoothness terms. Experimental ablations (removing image color, occluding inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry dominates the learned cost and the model is highly robust. We attribute the small performance differences to the IMU labeling primarily reflecting terrain geometry rather than semantics and to limited data diversity. Unlike prior work claiming large gains, we emphasize our contributions: (1) a high-fidelity, reproducible simulation environment; (2) a self-supervised IMU-based labeling pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss limitations and future work such as domain generalization and dataset expansion.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2507.10961.pdf' target='_blank'>https://arxiv.org/pdf/2507.10961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joohwan Seo, Arvind Kruthiventy, Soomi Lee, Megan Teng, Xiang Zhang, Seoyeon Choi, Jongeun Choi, Roberto Horowitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10961">EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a framework for learning vision-based robotic policies for contact-rich manipulation tasks that generalize spatially across task configurations. We focus on achieving robust spatial generalization of the policy for the peg-in-hole (PiH) task trained from a small number of demonstrations. We propose EquiContact, a hierarchical policy composed of a high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF) and a novel low-level compliant visuomotor policy (Geometric Compliant ACT, G-CompACT). G-CompACT operates using only localized observations (geometrically consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB images) and produces actions defined in the end-effector frame. Through these design choices, we show that the entire EquiContact pipeline is SE(3)-equivariant, from perception to force control. We also outline three key components for spatially generalizable contact-rich policies: compliance, localized policies, and induced equivariance. Real-world experiments on PiH tasks demonstrate a near-perfect success rate and robust generalization to unseen spatial configurations, validating the proposed framework and principles. The experimental videos can be found on the project website: https://sites.google.com/berkeley.edu/equicontact
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2506.13470.pdf' target='_blank'>https://arxiv.org/pdf/2506.13470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Zhang, Jun Ma, Fuqiang Niu, Li Dong, Jinzhou Cao, Genan Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13470">Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive Inductive Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot stance detection (ZSSD) seeks to determine the stance of text toward previously unseen targets, a task critical for analyzing dynamic and polarized online discourse with limited labeled data. While large language models (LLMs) offer zero-shot capabilities, prompting-based approaches often fall short in handling complex reasoning and lack robust generalization to novel targets. Meanwhile, LLM-enhanced methods still require substantial labeled data and struggle to move beyond instance-level patterns, limiting their interpretability and adaptability. Inspired by cognitive science, we propose the Cognitive Inductive Reasoning Framework (CIRF), a schema-driven method that bridges linguistic inputs and abstract reasoning via automatic induction and application of cognitive reasoning schemas. CIRF abstracts first-order logic patterns from raw text into multi-relational schema graphs in an unsupervised manner, and leverages a schema-enhanced graph kernel model to align input structures with schema templates for robust, interpretable zero-shot inference. Extensive experiments on SemEval-2016, VAST, and COVID-19-Stance benchmarks demonstrate that CIRF not only establishes new state-of-the-art results, but also achieves comparable performance with just 30\% of the labeled data, demonstrating its strong generalization and efficiency in low-resource settings.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2506.09260.pdf' target='_blank'>https://arxiv.org/pdf/2506.09260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibin Lei, Tao Shen, Andrew Yates
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09260">ThinkQE: Query Expansion via an Evolving Thinking Process</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective query expansion for web search benefits from promoting both exploration and result diversity to capture multiple interpretations and facets of a query. While recent LLM-based methods have improved retrieval performance and demonstrate strong domain generalization without additional training, they often generate narrowly focused expansions that overlook these desiderata. We propose ThinkQE, a test-time query expansion framework addressing this limitation through two key components: a thinking-based expansion process that encourages deeper and comprehensive semantic exploration, and a corpus-interaction strategy that iteratively refines expansions using retrieval feedback from the corpus. Experiments on diverse web search benchmarks (DL19, DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches, including training-intensive dense retrievers and rerankers.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2504.03185.pdf' target='_blank'>https://arxiv.org/pdf/2504.03185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaymari Chua, Chen Wang, Lina Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03185">Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2503.10435.pdf' target='_blank'>https://arxiv.org/pdf/2503.10435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Wilkinghoff, Takuya Fujimura, Keisuke Imoto, Jonathan Le Roux, Zheng-Hua Tan, Tomoki Toda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10435">Handling Domain Shifts for Anomalous Sound Detection: A Review of DCASE-Related Work</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When detecting anomalous sounds in complex environments, one of the main difficulties is that trained models must be sensitive to subtle differences in monitored target signals, while many practical applications also require them to be insensitive to changes in acoustic domains. Examples of such domain shifts include changing the type of microphone or the location of acoustic sensors, which can have a much stronger impact on the acoustic signal than subtle anomalies themselves. Moreover, users typically aim to train a model only on source domain data, which they may have a relatively large collection of, and they hope that such a trained model will be able to generalize well to an unseen target domain by providing only a minimal number of samples to characterize the acoustic signals in that domain. In this work, we review and discuss recent publications focusing on this domain generalization problem for anomalous sound detection in the context of the DCASE challenges on acoustic machine condition monitoring.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2502.16366.pdf' target='_blank'>https://arxiv.org/pdf/2502.16366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Dobre, Mehrnaz Mofakhami, Sophie Xhonneux, Leo Schwinn, Gauthier Gidel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16366">A Generative Approach to LLM Harmfulness Mitigation with Red Flag Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many safety post-training methods for large language models (LLMs) are designed to modify the model's behaviour from producing unsafe answers to issuing refusals. However, such distribution shifts are often brittle and degrade performance on desirable tasks. To address these pitfalls, we propose augmenting the model's vocabulary with a special red flag token, and training the model to insert this token whenever harmful content is generated or imminent. This approach enables the model to explicitly learn the concept of harmfulness in its representations, with minimal impact on utility due to the marginal change in the generated distribution of natural language. Moreover, because the token is embedded in the model's vocabulary, we can naturally leverage the LLMs' generalization capabilities, such as in-context learning (ICL) and out-of-distribution generalization to languages that are not formally supported (e.g., Japanese for Llama3). In particular, we demonstrate that through ICL alone, the model can learn to initiate reflective reasoning upon generating the red flag token at inference, which steers the response away from harmful continuations or enables self-correction when the flag is raised falsely. This approach is orthogonal and complementary to existing safety technique (such as safety classifiers or standard safety training) and easier to evaluate in comparison to natural language refusals, as it does not require a human or automated judge to assess the harmlessness of the answers.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2502.09297.pdf' target='_blank'>https://arxiv.org/pdf/2502.09297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianren Zhang, Guanyu Chen, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09297">When Do Neural Networks Learn World Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions--even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2501.13084.pdf' target='_blank'>https://arxiv.org/pdf/2501.13084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13084">Attention-Driven Hierarchical Reinforcement Learning with Particle Filtering for Source Localization in Dynamic Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many real-world scenarios, such as gas leak detection or environmental pollutant tracking, solving the Inverse Source Localization and Characterization problem involves navigating complex, dynamic fields with sparse and noisy observations. Traditional methods face significant challenges, including partial observability, temporal and spatial dynamics, out-of-distribution generalization, and reward sparsity. To address these issues, we propose a hierarchical framework that integrates Bayesian inference and reinforcement learning. The framework leverages an attention-enhanced particle filtering mechanism for efficient and accurate belief updates, and incorporates two complementary execution strategies: Attention Particle Filtering Planning and Attention Particle Filtering Reinforcement Learning. These approaches optimize exploration and adaptation under uncertainty. Theoretical analysis proves the convergence of the attention-enhanced particle filter, while extensive experiments across diverse scenarios validate the framework's superior accuracy, adaptability, and computational efficiency. Our results highlight the framework's potential for broad applications in dynamic field estimation tasks.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2412.06784.pdf' target='_blank'>https://arxiv.org/pdf/2412.06784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mara Levy, Siddhant Haldar, Lerrel Pinto, Abhinav Shirivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06784">P3-PO: Prescriptive Point Priors for Visuo-Spatial Generalization of Robot Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing generalizable robot policies that can robustly handle varied environmental conditions and object instances remains a fundamental challenge in robot learning. While considerable efforts have focused on collecting large robot datasets and developing policy architectures to learn from such data, naively learning from visual inputs often results in brittle policies that fail to transfer beyond the training data. This work presents Prescriptive Point Priors for Policies or P3-PO, a novel framework that constructs a unique state representation of the environment leveraging recent advances in computer vision and robot learning to achieve improved out-of-distribution generalization for robot manipulation. This representation is obtained through two steps. First, a human annotator prescribes a set of semantically meaningful points on a single demonstration frame. These points are then propagated through the dataset using off-the-shelf vision models. The derived points serve as an input to state-of-the-art policy architectures for policy learning. Our experiments across four real-world tasks demonstrate an overall 43% absolute improvement over prior methods when evaluated in identical settings as training. Further, P3-PO exhibits 58% and 80% gains across tasks for new object instances and more cluttered environments respectively. Videos illustrating the robot's performance are best viewed at point-priors.github.io.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2412.04296.pdf' target='_blank'>https://arxiv.org/pdf/2412.04296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Bao, Zhixin Zhou, Wen Jung Li, Rui Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04296">Structure-Aware Stylized Image Synthesis for Robust Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate medical image segmentation is essential for effective diagnosis and treatment planning but is often challenged by domain shifts caused by variations in imaging devices, acquisition conditions, and patient-specific attributes. Traditional domain generalization methods typically require inclusion of parts of the test domain within the training set, which is not always feasible in clinical settings with limited diverse data. Additionally, although diffusion models have demonstrated strong capabilities in image generation and style transfer, they often fail to preserve the critical structural information necessary for precise medical analysis. To address these issues, we propose a novel medical image segmentation method that combines diffusion models and Structure-Preserving Network for structure-aware one-shot image stylization. Our approach effectively mitigates domain shifts by transforming images from various sources into a consistent style while maintaining the location, size, and shape of lesions. This ensures robust and accurate segmentation even when the target domain is absent from the training data. Experimental evaluations on colonoscopy polyp segmentation and skin lesion segmentation datasets show that our method enhances the robustness and accuracy of segmentation models, achieving superior performance metrics compared to baseline models without style transfer. This structure-aware stylization framework offers a practical solution for improving medical image segmentation across diverse domains, facilitating more reliable clinical diagnoses.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2411.01798.pdf' target='_blank'>https://arxiv.org/pdf/2411.01798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atoosa Chegini, Hamid Kazemi, Iman Mirzadeh, Dong Yin, Maxwell Horton, Moin Nabi, Mehrdad Farajtabar, Keivan Alizadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01798">SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2410.07717.pdf' target='_blank'>https://arxiv.org/pdf/2410.07717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Jarry, Ramon Dalmau, Philippe Very, Junzi Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07717">On the Generalization Properties of Deep Learning for Aircraft Fuel Flow Estimation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately estimating aircraft fuel flow is essential for evaluating new procedures, designing next-generation aircraft, and monitoring the environmental impact of current aviation practices. This paper investigates the generalization capabilities of deep learning models in predicting fuel consumption, focusing particularly on their performance for aircraft types absent from the training data. We propose a novel methodology that integrates neural network architectures with domain generalization techniques to enhance robustness and reliability across a wide range of aircraft. A comprehensive dataset containing 101 different aircraft types, separated into training and generalization sets, with each aircraft type set containing 1,000 flights. We employed the base of aircraft data (BADA) model for fuel flow estimates, introduced a pseudo-distance metric to assess aircraft type similarity, and explored various sampling strategies to optimize model performance in data-sparse regions. Our results reveal that for previously unseen aircraft types, the introduction of noise into aircraft and engine parameters improved model generalization. The model is able to generalize with acceptable mean absolute percentage error between 2\% and 10\% for aircraft close to existing aircraft, while performance is below 1\% error for known aircraft in the training set. This study highlights the potential of combining domain-specific insights with advanced machine learning techniques to develop scalable, accurate, and generalizable fuel flow estimation models.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2512.24063.pdf' target='_blank'>https://arxiv.org/pdf/2512.24063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyue Bai, Yiyou Sun, Wenjie Hu, Shi Qiu, Maggie Ziyu Huan, Peiyang Song, Robert Nowak, Dawn Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24063">How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2512.17279.pdf' target='_blank'>https://arxiv.org/pdf/2512.17279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehui Lin, Luyi Han, Xin Wang, Ying Zhou, Yanming Zhang, Tianyu Zhang, Lingyun Bao, Shandong Wu, Dong Xu, Tao Tan, the UUSIC25 Challenge Consortium
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17279">Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>IMPORTANCE: Current ultrasound AI remains fragmented into single-task tools, limiting clinical utility compared to versatile modern ultrasound systems. OBJECTIVE: To evaluate the diagnostic accuracy and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation. DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images (public/private). Evaluation used an independent, multi-center test set of 2,479 images, including data from a center completely unseen during training to assess generalization. OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory). RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models showed high capability in segmentation (e.g., fetal head DSC: 0.942) but variability in complex tasks subject to domain shift. Notably, in breast cancer molecular subtyping, the top model's performance dropped from AUC 0.571 (internal) to 0.508 (unseen external center), highlighting generalization challenges. CONCLUSIONS: General-purpose AI models achieve high accuracy and efficiency across multiple tasks using a single architecture. However, performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2512.14601.pdf' target='_blank'>https://arxiv.org/pdf/2512.14601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaolun Li, Jichang Li, Yinqi Cai, Junye Chen, Xiaonan Luo, Guanbin Li, Rushi Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14601">FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2512.10818.pdf' target='_blank'>https://arxiv.org/pdf/2512.10818.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Lu, Jindong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10818">Self-Ensemble Post Learning for Noisy Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2512.06447.pdf' target='_blank'>https://arxiv.org/pdf/2512.06447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiuyi Chen, Mingkui Tan, Haifeng Lu, Qiuna Xu, Zhihua Wang, Runhao Zeng, Xiping Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06447">Towards Stable Cross-Domain Depression Recognition under Missing Modalities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2512.04475.pdf' target='_blank'>https://arxiv.org/pdf/2512.04475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timo Stoll, Chendi Qian, Ben Finkelshtein, Ali Parviz, Darius Weber, Fabrizio Frasca, Hadar Shavit, Antoine Siraudin, Arman Mielke, Marie Anastacio, Erik Müller, Maya Bechler-Speicher, Michael Bronstein, Mikhail Galkin, Holger Hoos, Mathias Niepert, Bryan Perozzi, Jan Tönshoff, Christopher Morris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04475">GraphBench: Next-generation graph learning benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2511.09515.pdf' target='_blank'>https://arxiv.org/pdf/2511.09515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09515">WMPO: World Model-based Policy Optimization for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2511.06944.pdf' target='_blank'>https://arxiv.org/pdf/2511.06944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsheng Hong, Chao Chen, Yanhui Chen, Shanshan Lin, Zhihao Chen, Xiangwen Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06944">From Attribution to Action: Jointly ALIGNing Predictions and Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explanation-guided learning (EGL) has shown promise in aligning model predictions with interpretable reasoning, particularly in computer vision tasks. However, most approaches rely on external annotations or heuristic-based segmentation to supervise model explanations, which can be noisy, imprecise and difficult to scale. In this work, we provide both empirical and theoretical evidence that low-quality supervision signals can degrade model performance rather than improve it. In response, we propose ALIGN, a novel framework that jointly trains a classifier and a masker in an iterative manner. The masker learns to produce soft, task-relevant masks that highlight informative regions, while the classifier is optimized for both prediction accuracy and alignment between its saliency maps and the learned masks. By leveraging high-quality masks as guidance, ALIGN improves both interpretability and generalizability, showing its superiority across various settings. Experiments on the two domain generalization benchmarks, VLCS and Terra Incognita, show that ALIGN consistently outperforms six strong baselines in both in-distribution and out-of-distribution settings. Besides, ALIGN also yields superior explanation quality concerning sufficiency and comprehensiveness, highlighting its effectiveness in producing accurate and interpretable models.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2510.19330.pdf' target='_blank'>https://arxiv.org/pdf/2510.19330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juncheng Wang, Lei Shang, Ziqi Liu, Wang Lu, Xixu Hu, Zhe Hu, Jindong Wang, Shujun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19330">Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Crowd localization plays a crucial role in visual scene understanding towards predicting each pedestrian location in a crowd, thus being applicable to various downstream tasks. However, existing approaches suffer from significant performance degradation due to discrepancies in head scale distributions (scale shift) between training and testing data, a challenge known as domain generalization (DG). This paper aims to comprehend the nature of scale shift within the context of domain generalization for crowd localization models. To this end, we address four critical questions: (i) How does scale shift influence crowd localization in a DG scenario? (ii) How can we quantify this influence? (iii) What causes this influence? (iv) How to mitigate the influence? Initially, we conduct a systematic examination of how crowd localization performance varies with different levels of scale shift. Then, we establish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms to quantify the influence. Through extensive experiments, we demonstrate the limitations of existing algorithms and underscore the importance and complexity of scale shift, a topic that remains insufficiently explored. To deepen our understanding, we provide a rigorous theoretical analysis on scale shift. Building on these insights, we further propose an effective algorithm called Causal Feature Decomposition and Anisotropic Processing (Catto) to mitigate the influence of scale shift in DG settings. Later, we also provide extensive analytical experiments, revealing four significant insights for future research. Our results emphasize the importance of this novel and applicable research direction, which we term Scale Shift Domain Generalization.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2510.03259.pdf' target='_blank'>https://arxiv.org/pdf/2510.03259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoonjeon Kim, Doohyuk Jang, Eunho Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03259">Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2509.14195.pdf' target='_blank'>https://arxiv.org/pdf/2509.14195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalima Binta Manir, Tim Oates
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14195">Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mental representation, characterized by structured internal models mirroring external environments, is fundamental to advanced cognition but remains challenging to investigate empirically. Existing theory hypothesizes that second-order learning -- learning mechanisms that adapt first-order learning (i.e., learning about the task/domain) -- promotes the emergence of such environment-cognition isomorphism. In this paper, we empirically validate this hypothesis by proposing a hierarchical architecture comprising a Graph Convolutional Network (GCN) as a first-order learner and an MLP controller as a second-order learner. The GCN directly maps node-level features to predictions of optimal navigation paths, while the MLP dynamically adapts the GCN's parameters when confronting structurally novel maze environments. We demonstrate that second-order learning is particularly effective when the cognitive system develops an internal mental map structurally isomorphic to the environment. Quantitative and qualitative results highlight significant performance improvements and robust generalization on unseen maze tasks, providing empirical support for the pivotal role of structured mental representations in maximizing the effectiveness of second-order learning.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2509.08705.pdf' target='_blank'>https://arxiv.org/pdf/2509.08705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalima Binta Manir, Tim Oates
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08705">One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel Theory of Mind (ToM) framework inspired by dual-process theories from cognitive science, integrating a fast, habitual graph-based reasoning system (System 1), implemented via graph convolutional networks (GCNs), and a slower, context-sensitive meta-adaptive learning system (System 2), driven by meta-learning techniques. Our model dynamically balances intuitive and deliberative reasoning through a learned context gate mechanism. We validate our architecture on canonical false-belief tasks and systematically explore its capacity to replicate hallmark cognitive biases associated with dual-process theory, including anchoring, cognitive-load fatigue, framing effects, and priming effects. Experimental results demonstrate that our dual-process approach closely mirrors human adaptive behavior, achieves robust generalization to unseen contexts, and elucidates cognitive mechanisms underlying reasoning biases. This work bridges artificial intelligence and cognitive theory, paving the way for AI systems exhibiting nuanced, human-like social cognition and adaptive decision-making capabilities.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2508.01602.pdf' target='_blank'>https://arxiv.org/pdf/2508.01602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lubin Gan, Jing Zhang, Linhao Qu, Yijun Wang, Siying Wu, Xiaoyan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01602">Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fine-grained classification of brain tumor subtypes from histopathological whole slide images is highly challenging due to subtle morphological variations and the scarcity of annotated data. Although vision-language models have enabled promising zero-shot classification, their ability to capture fine-grained pathological features remains limited, resulting in suboptimal subtype discrimination. To address these challenges, we propose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot framework tailored for digital pathology. FG-PAN consists of two key modules: (1) a local feature refinement module that enhances patch-level visual features by modeling spatial relationships among representative patches, and (2) a fine-grained text description generation module that leverages large language models to produce pathology-aware, class-specific semantic prototypes. By aligning refined visual features with LLM-generated fine-grained descriptions, FG-PAN effectively increases class separability in both visual and semantic spaces. Extensive experiments on multiple public pathology datasets, including EBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance and robust generalization in zero-shot brain tumor subtype classification.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2508.00304.pdf' target='_blank'>https://arxiv.org/pdf/2508.00304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyin Liao, Ziwei Zhang, Yufei Sun, Chunyu Hu, Jianxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00304">Invariant Graph Transformer for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Transformers (GTs) have demonstrated great effectiveness across various graph analytical tasks. However, the existing GTs focus on training and testing graph data originated from the same distribution, but fail to generalize under distribution shifts. Graph invariant learning, aiming to capture generalizable graph structural patterns with labels under distribution shifts, is potentially a promising solution, but how to design attention mechanisms and positional and structural encodings (PSEs) based on graph invariant learning principles remains challenging. To solve these challenges, we introduce Graph Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn generalized graph representations by capturing invariant relationships between predictive graph structures and labels through jointly optimizing three modules. Specifically, we first develop a GT-based entropy-guided invariant subgraph disentangler to separate invariant and variant subgraphs while preserving the sharpness of the attention function. Next, we design an evolving subgraph positional and structural encoder to effectively and efficiently capture the encoding information of dynamically changing subgraphs during training. Finally, we propose an invariant learning module utilizing subgraph node representations and encodings to derive generalizable graph representations that can to unseen graphs. We also provide theoretical justifications for our method. Extensive experiments on benchmark datasets demonstrate the superiority of our method over state-of-the-art baselines under distribution shifts.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2506.10805.pdf' target='_blank'>https://arxiv.org/pdf/2506.10805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex McKenzie, Urja Pawar, Phil Blandfort, William Bankes, David Krueger, Ekdeep Singh Lubana, Dmitrii Krasheninnikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10805">Detecting High-Stakes Interactions with Activation Probes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring is an important aspect of safely deploying Large Language Models (LLMs). This paper examines activation probes for detecting "high-stakes" interactions -- where the text indicates that the interaction might lead to significant harm -- as a critical, yet underexplored, target for such monitoring. We evaluate several probe architectures trained on synthetic data, and find them to exhibit robust generalization to diverse, out-of-distribution, real-world data. Probes' performance is comparable to that of prompted or finetuned medium-sized LLM monitors, while offering computational savings of six orders-of-magnitude. Our experiments also highlight the potential of building resource-aware hierarchical monitoring systems, where probes serve as an efficient initial filter and flag cases for more expensive downstream analysis. We release our novel synthetic dataset and codebase to encourage further study.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2505.17692.pdf' target='_blank'>https://arxiv.org/pdf/2505.17692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziteng Yang, Jingzehua Xu, Yanshu Li, Zepeng Li, Yeqiang Wang, Xinghui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17692">ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any target domain training samples, relying solely on external auxiliary data. Existing CLIP-based methods attempt to activate the model's ZSAD potential via handcrafted or static learnable prompts. The former incur high engineering costs and limited semantic coverage, whereas the latter apply identical descriptions across diverse anomaly types, thus fail to adapt to complex variations. Furthermore, since CLIP is originally pretrained on large-scale classification tasks, its anomaly segmentation quality is highly sensitive to the exact wording of class names, severely constraining prompting strategies that depend on class labels. To address these challenges, we introduce ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local visual context to adaptively generate fine-grained textual prompts, eliminating manual templates and class-name priors. This design enables our model to focus on precise abnormal regions, making it particularly valuable when category labels are ambiguous or privacy-constrained. Extensive experiments on 15 industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves state-of-the-art performance and robust cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2504.21066.pdf' target='_blank'>https://arxiv.org/pdf/2504.21066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Karathanasis, John Violos, Ioannis Kompatsiaris, Symeon Papadopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21066">A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training and deploying deepfake detection models on edge devices offers the advantage of maintaining data privacy and confidentiality by processing it close to its source. However, this approach is constrained by the limited computational and memory resources available at the edge. To address this challenge, we explore compression techniques to reduce computational demands and inference time, alongside transfer learning methods to minimize training overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate the effectiveness of pruning, knowledge distillation (KD), quantization, fine-tuning, and adapter-based techniques. Our experimental results demonstrate that both compression and transfer learning can be effectively achieved, even with a high compression level of 90%, remaining at the same performance level when the training and validation data originate from the same DeepFake model. However, when the testing dataset is generated by DeepFake models not present in the training set, a domain generalization issue becomes evident.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2503.22309.pdf' target='_blank'>https://arxiv.org/pdf/2503.22309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zakaria Laskar, Tomas Vojir, Matej Grcic, Iaroslav Melekhov, Shankar Gangisettye, Juho Kannala, Jiri Matas, Giorgos Tolias, C. V. Jawahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22309">A Dataset for Semantic Segmentation in the Presence of Unknowns</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Before deployment in the real-world deep neural networks require thorough evaluation of how they handle both knowns, inputs represented in the training data, and unknowns (anomalies). This is especially important for scene understanding tasks with safety critical applications, such as in autonomous driving. Existing datasets allow evaluation of only knowns or unknowns - but not both, which is required to establish "in the wild" suitability of deep neural network models. To bridge this gap, we propose a novel anomaly segmentation dataset, ISSU, that features a diverse set of anomaly inputs from cluttered real-world environments. The dataset is twice larger than existing anomaly segmentation datasets, and provides a training, validation and test set for controlled in-domain evaluation. The test set consists of a static and temporal part, with the latter comprised of videos. The dataset provides annotations for both closed-set (knowns) and anomalies, enabling closed-set and open-set evaluation. The dataset covers diverse conditions, such as domain and cross-sensor shift, illumination variation and allows ablation of anomaly detection methods with respect to these variations. Evaluation results of current state-of-the-art methods confirm the need for improvements especially in domain-generalization, small and large object segmentation.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2503.04111.pdf' target='_blank'>https://arxiv.org/pdf/2503.04111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijia Yu, Yibo Miao, Yifan Zhu, Xiao-Shan Gao, Lijun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04111">Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive Ability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The primary objective of learning methods is generalization. Classic uniform generalization bounds, which rely on VC-dimension or Rademacher complexity, fail to explain the significant attribute that over-parameterized models in deep learning exhibit nice generalizability. On the other hand, algorithm-dependent generalization bounds, like stability bounds, often rely on strict assumptions. To establish generalizability under less stringent assumptions, this paper investigates the generalizability of neural networks that minimize or approximately minimize empirical risk. We establish a lower bound for population accuracy based on the expressiveness of these networks, which indicates that with an adequate large number of training samples and network sizes, these networks, including over-parameterized ones, can generalize effectively. Additionally, we provide a necessary condition for generalization, demonstrating that, for certain data distributions, the quantity of training data required to ensure generalization exceeds the network size needed to represent the corresponding data distribution. Finally, we provide theoretical insights into several phenomena in deep learning, including robust generalization, importance of over-parameterization, and effect of loss function on generalization.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2501.13967.pdf' target='_blank'>https://arxiv.org/pdf/2501.13967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Che, Yifei Wu, Haibo Jin, Yong Xia, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13967">FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization aims to train a global model from multiple source domains and ensure its generalization ability to unseen target domains. Due to the target domain being with unknown domain shifts, attempting to approximate these gaps by source domains may be the key to improving model generalization capability. Existing works mainly focus on sharing and recombining local domain-specific attributes to increase data diversity and simulate potential domain shifts. However, these methods may be insufficient since only the local attribute recombination can be hard to touch the out-of-distribution of global data. In this paper, we propose a simple-yet-efficient framework named Federated Domain Adversarial Generation (FedDAG). It aims to simulate the domain shift and improve the model generalization by adversarially generating novel domains different from local and global source domains. Specifically, it generates novel-style images by maximizing the instance-level feature discrepancy between original and generated images and trains a generalizable task model by minimizing their feature discrepancy. Further, we observed that FedDAG could cause different performance improvements for local models. It may be due to inherent data isolation and heterogeneity among clients, exacerbating the imbalance in their generalization contributions to the global model. Ignoring this imbalance can lead the global model's generalization ability to be sub-optimal, further limiting the novel domain generation procedure. Thus, to mitigate this imbalance, FedDAG hierarchically aggregates local models at the within-client and across-client levels by using the sharpness concept to evaluate client model generalization contributions. Extensive experiments across four medical benchmarks demonstrate FedDAG's ability to enhance generalization in federated medical scenarios.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2501.04942.pdf' target='_blank'>https://arxiv.org/pdf/2501.04942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Falih Gozi Febrinanto, Kristen Moore, Chandra Thapa, Jiangang Ma, Vidya Saikrishna, Feng Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04942">Vision Graph Non-Contrastive Learning for Audio Deepfake Detection with Limited Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in audio deepfake detection have leveraged graph neural networks (GNNs) to model frequency and temporal interdependencies in audio data, effectively identifying deepfake artifacts. However, the reliance of GNN-based methods on substantial labeled data for graph construction and robust performance limits their applicability in scenarios with limited labeled data. Although vast amounts of audio data exist, the process of labeling samples as genuine or fake remains labor-intensive and costly. To address this challenge, we propose SIGNL (Spatio-temporal vIsion Graph Non-contrastive Learning), a novel framework that maintains high GNN performance in low-label settings. SIGNL constructs spatio-temporal graphs by representing patches from the audio's visual spectrogram as nodes. These graph structures are modeled using vision graph convolutional (GC) encoders pre-trained through graph non-contrastive learning, a label-free that maximizes the similarity between positive pairs. The pre-trained encoders are then fine-tuned for audio deepfake detection, reducing reliance on labeled data. Experiments demonstrate that SIGNL outperforms state-of-the-art baselines across multiple audio deepfake detection datasets, achieving the lowest Equal Error Rate (EER) with as little as 5% labeled data. Additionally, SIGNL exhibits strong cross-domain generalization, achieving the lowest EER in evaluations involving diverse attack types and languages in the In-The-Wild dataset.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2501.03466.pdf' target='_blank'>https://arxiv.org/pdf/2501.03466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Liu, Yudong Zhang, Shuihua Wang, Siyue Li, Jin Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03466">DGSSA: Domain generalization with structural and stylistic augmentation for retinal vessel segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retinal vascular morphology is crucial for diagnosing diseases such as diabetes, glaucoma, and hypertension, making accurate segmentation of retinal vessels essential for early intervention. Traditional segmentation methods assume that training and testing data share similar distributions, which can lead to poor performance on unseen domains due to domain shifts caused by variations in imaging devices and patient demographics. This paper presents a novel approach, DGSSA, for retinal vessel image segmentation that enhances model generalization by combining structural and style augmentation strategies. We utilize a space colonization algorithm to generate diverse vascular-like structures that closely mimic actual retinal vessels, which are then used to generate pseudo-retinal images with an improved Pix2Pix model, allowing the segmentation model to learn a broader range of structure distributions. Additionally, we utilize PixMix to implement random photometric augmentations and introduce uncertainty perturbations, thereby enriching stylistic diversity and significantly enhancing the model's adaptability to varying imaging conditions. Our framework has been rigorously evaluated on four challenging datasets-DRIVE, CHASEDB, HRF, and STARE-demonstrating state-of-the-art performance that surpasses existing methods. This validates the effectiveness of our proposed approach, highlighting its potential for clinical application in automated retinal vessel analysis.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2410.08388.pdf' target='_blank'>https://arxiv.org/pdf/2410.08388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximus Powers, Shaina Raza, Alex Chang, Rehana Riaz, Umang Mavani, Harshitha Reddy Jonala, Ansh Tiwari, Hua Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08388">Responsible AI in NLP: GUS-Net Span-Level Bias Detection Dataset and Benchmark for Generalizations, Unfairness, and Stereotypes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Representational harms in language technologies often occur in short spans within otherwise neutral text, where phrases may simultaneously convey generalizations, unfairness, or stereotypes. Framing bias detection as sentence-level classification obscures which words carry bias and what type is present, limiting both auditability and targeted mitigation. We introduce the GUS-Net Framework, comprising the GUS dataset and a multi-label token-level detector for span-level analysis of social bias. The GUS dataset contains 3,739 unique snippets across multiple domains, with over 69,000 token-level annotations. Each token is labeled using BIO tags (Begin, Inside, Outside) for three pathways of representational harm: Generalizations, Unfairness, and Stereotypes. To ensure reliable data annotation, we employ an automated multi-agent pipeline that proposes candidate spans which are subsequently verified and corrected by human experts. We formulate bias detection as multi-label token-level classification and benchmark both encoder-based models (e.g., BERT family variants) and decoder-based large language models (LLMs). Our evaluations cover token-level identification and span-level entity recognition on our test set, and out-of-distribution generalization. Empirical results show that encoder-based models consistently outperform decoder-based baselines on nuanced and overlapping spans while being more computationally efficient. The framework delivers interpretable, fine-grained diagnostics that enable systematic auditing and mitigation of representational harms in real-world NLP systems.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2601.19245.pdf' target='_blank'>https://arxiv.org/pdf/2601.19245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongxin Deng, Zhen Fang, Yixuan Li, Ling Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19245">Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2512.14461.pdf' target='_blank'>https://arxiv.org/pdf/2512.14461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niklas Grieger, Jannik Raskob, Siamak Mehrkanoon, Stephan Bialonski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14461">AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2512.14442.pdf' target='_blank'>https://arxiv.org/pdf/2512.14442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixin Zhang, Kanghao Chen, Hanqing Wang, Hongfei Zhang, Harold Haodong Chen, Chenfei Liao, Litao Guo, Ying-Cong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14442">A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2512.12046.pdf' target='_blank'>https://arxiv.org/pdf/2512.12046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vittorio Giammarino, Ahmed H. Qureshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12046">Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2512.03300.pdf' target='_blank'>https://arxiv.org/pdf/2512.03300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Hu, Fan Ming, Xiaoxue Han, Chang Lu, Yue Ning, Dan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03300">HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2512.01196.pdf' target='_blank'>https://arxiv.org/pdf/2512.01196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shihang Li, Zhiqiang Gong, Weien Zhou, Yue Gao, Wen Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01196">Learning to Reconstruct Temperature Field from Sparse Observations with Implicit Physics Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate reconstruction of temperature field of heat-source systems (TFR-HSS) is crucial for thermal monitoring and reliability assessment in engineering applications such as electronic devices and aerospace structures. However, the high cost of measurement acquisition and the substantial distributional shifts in temperature field across varying conditions present significant challenges for developing reconstruction models with robust generalization capabilities. Existing DNNs-based methods typically formulate TFR-HSS as a one-to-one regression problem based solely on target sparse measurements, without effectively leveraging reference simulation data that implicitly encode thermal knowledge. To address this limitation, we propose IPTR, an implicit physics-guided temperature field reconstruction framework that introduces sparse monitoring-temperature field pair from reference simulations as priors to enrich physical understanding. To integrate both reference and target information, we design a dual physics embedding module consisting of two complementary branches: an implicit physics-guided branch employing cross-attention to distill latent physics from the reference data, and an auxiliary encoding branch based on Fourier layers to capture the spatial characteristics of the target observation. The fused representation is then decoded to reconstruct the full temperature field. Extensive experiments under single-condition, multi-condition, and few-shot settings demonstrate that IPTR consistently outperforms existing methods, achieving state-of-the-art reconstruction accuracy and strong generalization capability.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2510.19487.pdf' target='_blank'>https://arxiv.org/pdf/2510.19487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Li, Huiying Xu, Changxin Gao, Zeyu Wang, Yun Liu, Xinzhong Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19487">Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source Domain Generalized Object Detection (SDGOD), as a cutting-edge research topic in computer vision, aims to enhance model generalization capability in unseen target domains through single-source domain training. Current mainstream approaches attempt to mitigate domain discrepancies via data augmentation techniques. However, due to domain shift and limited domain-specific knowledge, models tend to fall into the pitfall of spurious correlations. This manifests as the model's over-reliance on simplistic classification features (e.g., color) rather than essential domain-invariant representations like object contours. To address this critical challenge, we propose the Cauvis (Causal Visual Prompts) method. First, we introduce a Cross-Attention Prompts module that mitigates bias from spurious features by integrating visual prompts with cross-attention. To address the inadequate domain knowledge coverage and spurious feature entanglement in visual prompts for single-domain generalization, we propose a dual-branch adapter that disentangles causal-spurious features while achieving domain adaptation via high-frequency feature extraction. Cauvis achieves state-of-the-art performance with 15.9-31.4% gains over existing domain generalization methods on SDGOD datasets, while exhibiting significant robustness advantages in complex interference environments.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2510.12132.pdf' target='_blank'>https://arxiv.org/pdf/2510.12132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yang, Jiyao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12132">FedHUG: Federated Heterogeneous Unsupervised Generalization for Remote Physiological Measurements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote physiological measurement gained wide attention, while it requires collecting users' privacy-sensitive information, and existing contactless measurements still rely on labeled client data. This presents challenges when we want to further update real-world deployed models with numerous user data lacking labels. To resolve these challenges, we instantiate a new protocol called Federated Unsupervised Domain Generalization (FUDG) in this work. Subsequently, the \textbf{Fed}erated \textbf{H}eterogeneous \textbf{U}nsupervised \textbf{G}eneralization (\textbf{FedHUG}) framework is proposed and consists of: (1) Minimal Bias Aggregation module dynamically adjusts aggregation weights based on prior-driven bias evaluation to cope with heterogeneous non-IID features from multiple domains. (2) The Global Distribution-aware Learning Controller parameterizes the label distribution and dynamically manipulates client-specific training strategies, thereby mitigating the server-client label distribution skew and long-tail issue. The proposal shows superior performance across state-of-the-art techniques in estimation with either RGB video or mmWave radar. The code will be released.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2509.12602.pdf' target='_blank'>https://arxiv.org/pdf/2509.12602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyu Chen, Guoqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12602">DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of Conflict-Driven Clause Learning solvers hinges on internal heuristics, yet the heterogeneity of SAT problems makes a single, universally optimal configuration unattainable. While prior automated methods can find specialized configurations for specific problem families, this dataset-specific approach lacks generalizability and requires costly re-optimization for new problem types. We introduce DaSAThco, a framework that addresses this challenge by learning a generalizable mapping from instance features to tailored heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework uses a Large Language Model, guided by systematically defined Problem Archetypes, to generate a diverse portfolio of specialized heuristic ensembles and subsequently learns an adaptive selection mechanism to form the final mapping. Experiments show that DaSAThco achieves superior performance and, most notably, demonstrates robust out-of-domain generalization where non-adaptive methods show limitations. Our work establishes a more scalable and practical path toward automated algorithm design for complex, configurable systems.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2509.02601.pdf' target='_blank'>https://arxiv.org/pdf/2509.02601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Giedziun, Jan SoÅtysik, Mateusz GÃ³rczany, Norbert Ropiak, Marcin Przymus, Piotr Krajewski, JarosÅaw KwiecieÅ, Artur Bartczak, Izabela Wasiak, Mateusz Maniewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02601">Foundation Model-Driven Classification of Atypical Mitotic Figures with Domain-Aware Training Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a solution for the MIDOG 2025 Challenge Track~2, addressing binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs). The approach leverages pathology-specific foundation model H-optimus-0, selected based on recent cross-domain generalization benchmarks and our empirical testing, with Low-Rank Adaptation (LoRA) fine-tuning and MixUp augmentation. Implementation includes soft labels based on multi-expert consensus, hard negative mining, and adaptive focal loss, metric learning and domain adaptation. The method demonstrates both the promise and challenges of applying foundation models to this complex classification task, achieving reasonable performance in the preliminary evaluation phase.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2509.02600.pdf' target='_blank'>https://arxiv.org/pdf/2509.02600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengyou Xu, Haochen Yang, Xiang 'Anthony' Chen, Hongyan Gu, Mohammad Haeri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02600">Team Westwood Solution for MIDOG 2025 Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This abstract presents our solution (Team Westwood) for mitosis detection and atypical mitosis classification in the MItosis DOmain Generalization (MIDOG) 2025 challenge. For mitosis detection, we trained an nnUNetV2 for initial mitosis candidate screening with high sensitivity, followed by a random forest classifier ensembling predictions of three convolutional neural networks (CNNs): EfficientNet-b3, EfficientNet-b5, and EfficientNetV2-s. For the atypical mitosis classification, we trained another random forest classifier ensembling the predictions of three CNNs: EfficientNet-b3, EfficientNet-b5, and InceptionV3. On the preliminary test set, our solution achieved an F1 score of 0.7450 for track 1 mitosis detection, and a balanced accuracy of 0.8722 for track 2 atypical mitosis classification.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2509.01031.pdf' target='_blank'>https://arxiv.org/pdf/2509.01031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhou Ye, Kevin I-Kai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01031">Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human Activity Recognition (HAR) using wearable sensors is crucial for healthcare, fitness tracking, and smart environments, yet cross-user variability -- stemming from diverse motion patterns, sensor placements, and physiological traits -- hampers generalization in real-world settings. Conventional supervised learning methods often overfit to user-specific patterns, leading to poor performance on unseen users. Existing domain generalization approaches, while promising, frequently overlook temporal dependencies or depend on impractical domain-specific labels. We propose Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a novel framework that redefines feature extraction as a sequential decision-making process driven by reinforcement learning. TPRL-DG leverages a Transformer-based autoregressive generator to produce temporal tokens that capture user-invariant activity dynamics, optimized via a multi-objective reward function balancing class discrimination and cross-user invariance. Key innovations include: (1) an RL-driven approach for domain generalization, (2) autoregressive tokenization to preserve temporal coherence, and (3) a label-free reward design eliminating the need for target user annotations. Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses state-of-the-art methods in cross-user generalization, achieving superior accuracy without per-user calibration. By learning robust, user-invariant temporal patterns, TPRL-DG enables scalable HAR systems, facilitating advancements in personalized healthcare, adaptive fitness tracking, and context-aware environments.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2509.00351.pdf' target='_blank'>https://arxiv.org/pdf/2509.00351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marzi Heidari, Yuhong Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00351">Target-Oriented Single Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep models trained on a single source domain often fail catastrophically under distribution shifts, a critical challenge in Single Domain Generalization (SDG). While existing methods focus on augmenting source data or learning invariant features, they neglect a readily available resource: textual descriptions of the target deployment environment. We propose Target-Oriented Single Domain Generalization (TO-SDG), a novel problem setup that leverages the textual description of the target domain, without requiring any target data, to guide model generalization. To address TO-SDG, we introduce Spectral TARget Alignment (STAR), a lightweight module that injects target semantics into source features by exploiting visual-language models (VLMs) such as CLIP. STAR uses a target-anchored subspace derived from the text embedding of the target description to recenter image features toward the deployment domain, then utilizes spectral projection to retain directions aligned with target cues while discarding source-specific noise. Moreover, we use a vision-language distillation to align backbone features with VLM's semantic geometry. STAR further employs feature-space Mixup to ensure smooth transitions between source and target-oriented representations. Experiments across various image classification and object detection benchmarks demonstrate STAR's superiority. This work establishes that minimal textual metadata, which is a practical and often overlooked resource, significantly enhances generalization under severe data constraints, opening new avenues for deploying robust models in target environments with unseen data.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2508.21769.pdf' target='_blank'>https://arxiv.org/pdf/2508.21769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21769">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2506.20841.pdf' target='_blank'>https://arxiv.org/pdf/2506.20841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha Min Son, Shahbaz Rezaei, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20841">FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised domain generalization (SSDG) aims to solve the problem of generalizing to out-of-distribution data when only a few labels are available. Due to label scarcity, applying domain generalization methods often underperform. Consequently, existing SSDG methods combine semi-supervised learning methods with various regularization terms. However, these methods do not explicitly regularize to learn domains invariant representations across all domains, which is a key goal for domain generalization. To address this, we introduce FixCLR. Inspired by success in self-supervised learning, we change two crucial components to adapt contrastive learning for explicit domain invariance regularization: utilization of class information from pseudo-labels and using only a repelling term. FixCLR can also be added on top of most existing SSDG and semi-supervised methods for complementary performance improvements. Our research includes extensive experiments that have not been previously explored in SSDG studies. These experiments include benchmarking different improvements to semi-supervised methods, evaluating the performance of pretrained versus non-pretrained models, and testing on datasets with many domains. Overall, FixCLR proves to be an effective SSDG method, especially when combined with other semi-supervised methods.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2506.11033.pdf' target='_blank'>https://arxiv.org/pdf/2506.11033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjae Kwon, Tyler Ingebrand, Ufuk Topcu, Lu Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11033">Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variations in hidden parameters, such as a robot's mass distribution or friction, pose safety risks during execution. We develop a runtime shielding mechanism for reinforcement learning, building on the formalism of constrained hidden-parameter Markov decision processes. Function encoders enable real-time inference of hidden parameters from observations, allowing the shield and the underlying policy to adapt online. The shield constrains the action space by forecasting future safety risks (such as obstacle proximity) and accounts for uncertainty via conformal prediction. We prove that the proposed mechanism satisfies probabilistic safety guarantees and yields optimal policies among the set of safety-compliant policies. Experiments across diverse environments with varying hidden parameters show that our method significantly reduces safety violations and achieves strong out-of-distribution generalization, while incurring minimal runtime overhead.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2506.06977.pdf' target='_blank'>https://arxiv.org/pdf/2506.06977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Hu, Xiaoxue Han, Fei Wang, Yue Ning
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06977">UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization has become a critical challenge in clinical prediction, where patient cohorts often exhibit shifting data distributions that degrade model performance. Typical domain generalization approaches struggle in real-world healthcare settings for two main reasons: (1) patient-specific domain labels are typically unavailable, making domain discovery especially difficult; (2) purely data-driven approaches overlook key clinical insights, leading to a gap in medical knowledge integration. To address these problems, we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to group diseases into higher-level categories and discover more flexible latent domains. In this paper, we introduce UdonCare, a hierarchy-guided framework that iteratively prunes fine-grained domains, encodes these refined domains, and applies a Siamese-type inference mechanism to separate domain-related signals from patient-level features. Experimental results on clinical datasets (MIMIC-III and MIMIC-IV) show that the proposed model achieves higher performance compared to other domain generalization baselines when substantial domain gaps presents, highlighting the untapped potential of medical knowledge for enhancing domain generalization in practical healthcare applications.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2506.02935.pdf' target='_blank'>https://arxiv.org/pdf/2506.02935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuepeng Zheng, Fu Luo, Zhenkun Wang, Yaoxin Wu, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02935">MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a promising approach to train a unified model capable of solving multiple Vehicle Routing Problem (VRP) variants. However, existing Reinforcement Learning (RL)-based multi-task methods can only train light decoder models on small-scale problems, exhibiting limited generalization ability when solving large-scale problems. To overcome this limitation, this work introduces a novel multi-task learning method driven by knowledge distillation (MTL-KD), which enables the efficient training of heavy decoder models with strong generalization ability. The proposed MTL-KD method transfers policy knowledge from multiple distinct RL-based single-task models to a single heavy decoder model, facilitating label-free training and effectively improving the model's generalization ability across diverse tasks. In addition, we introduce a flexible inference strategy termed Random Reordering Re-Construction (R3C), which is specifically adapted for diverse VRP tasks and further boosts the performance of the multi-task model. Experimental results on 6 seen and 10 unseen VRP variants with up to 1000 nodes indicate that our proposed method consistently achieves superior performance on both uniform and real-world benchmarks, demonstrating robust generalization abilities.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2506.01962.pdf' target='_blank'>https://arxiv.org/pdf/2506.01962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhou Ye, Kevin I-Kai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01962">Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-user variability poses a significant challenge in sensor-based Human Activity Recognition (HAR) systems, as traditional models struggle to generalize across users due to differences in behavior, sensor placement, and data distribution. To address this, we propose GNN-ADG (Graph Neural Network with Adversarial Domain Generalization), a novel method that leverages both the strength from both the Graph Neural Networks (GNNs) and adversarial learning to achieve robust cross-user generalization. GNN-ADG models spatial relationships between sensors on different anatomical body parts, extracting three types of Anatomical Units: (1) Interconnected Units, capturing inter-relations between neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or functionally similar body parts; and (3) Lateral Units, connecting sensors based on their position to capture region-specific coordination. These units information are fused into an unified graph structure with a cyclic training strategy, dynamically integrating spatial, functional, and lateral correlations to facilitate a holistic, user-invariant representation. Information fusion mechanism of GNN-ADG occurs by iteratively cycling through edge topologies during training, allowing the model to refine its understanding of inter-sensor relationships across diverse perspectives. By representing the spatial configuration of sensors as an unified graph and incorporating adversarial learning, Information Fusion GNN-ADG effectively learns features that generalize well to unseen users without requiring target user data during training, making it practical for real-world applications.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2505.22622.pdf' target='_blank'>https://arxiv.org/pdf/2505.22622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Ge, Amanda Wang, Shange Tang, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22622">Principled Out-of-Distribution Generalization via Simplicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern foundation models exhibit remarkable out-of-distribution (OOD) generalization, solving tasks far beyond the support of their training data. However, the theoretical principles underpinning this phenomenon remain elusive. This paper investigates this problem by examining the compositional generalization abilities of diffusion models in image generation. Our analysis reveals that while neural network architectures are expressive enough to represent a wide range of models -- including many with undesirable behavior on OOD inputs -- the true, generalizable model that aligns with human expectations typically corresponds to the simplest among those consistent with the training data.
  Motivated by this observation, we develop a theoretical framework for OOD generalization via simplicity, quantified using a predefined simplicity metric. We analyze two key regimes: (1) the constant-gap setting, where the true model is strictly simpler than all spurious alternatives by a fixed gap, and (2) the vanishing-gap setting, where the fixed gap is replaced by a smoothness condition ensuring that models close in simplicity to the true model yield similar predictions. For both regimes, we study the regularized maximum likelihood estimator and establish the first sharp sample complexity guarantees for learning the true, generalizable, simple model.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2505.21573.pdf' target='_blank'>https://arxiv.org/pdf/2505.21573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Wan, Rui Zhang, Hao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21573">Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial differential equations (PDEs) govern the spatiotemporal evolution of various physical systems. Classical numerical solvers, while accurate, require fine discretization and full knowledge of the governing PDEs, limiting their applicability when the physics is unknown or fast inference is required. Data-driven neural PDE solvers alleviate these constraints by learning from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes, restricting their ability to handle unknown or globally coupled systems. In this work, we propose the Spectral-inspired Neural Operator (SINO), a novel framework that learns PDE operators from limited trajectories (as few as 2-5), without any known PDE terms. SINO operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. To model nonlinear physical interactions, we design a nonlinear operator block that includes a $Î $-Block with low-pass filtering to prevent aliasing. Finally, we introduce an operator distillation technique to distill the trained model for efficient inference. SINO achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. To our knowledge, SINO is the first physics-aware method capable of accurately simulating globally coupled systems (e.g., the Navier-Stokes equations) from limited data without any explicit PDE terms.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2505.19547.pdf' target='_blank'>https://arxiv.org/pdf/2505.19547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhang, Wentao Zhang, Hao Miao, Xinke Jiang, Yuchen Fang, Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19547">STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful tool for modeling dynamic graph-structured data across diverse domains. However, they often fail to generalize in Spatio-Temporal Out-of-Distribution (STOOD) scenarios, where both temporal dynamics and spatial structures evolve beyond the training distribution. To address this problem, we propose an innovative Spatio-Temporal Retrieval-Augmented Pattern Learning framework,STRAP, which enhances model generalization by integrating retrieval-augmented learning into the STGNN continue learning pipeline. The core of STRAP is a compact and expressive pattern library that stores representative spatio-temporal patterns enriched with historical, structural, and semantic information, which is obtained and optimized during the training phase. During inference, STRAP retrieves relevant patterns from this library based on similarity to the current input and injects them into the model via a plug-and-play prompting mechanism. This not only strengthens spatio-temporal representations but also mitigates catastrophic forgetting. Moreover, STRAP introduces a knowledge-balancing objective to harmonize new information with retrieved knowledge. Extensive experiments across multiple real-world streaming graph datasets show that STRAP consistently outperforms state-of-the-art STGNN baselines on STOOD tasks, demonstrating its robustness, adaptability, and strong generalization capability without task-specific fine-tuning.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2505.12391.pdf' target='_blank'>https://arxiv.org/pdf/2505.12391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyang Lu, Qian Xia, Weifan Wang, Feng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12391">CLIP-aware Domain-Adaptive Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a novel framework that addresses the critical challenge of domain generalization in single image super-resolution. By leveraging the semantic capabilities of CLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented performance across diverse domains and extreme scaling factors. The proposed method integrates CLIP-guided feature alignment mechanism with a meta-learning inspired few-shot adaptation strategy, enabling efficient knowledge transfer and rapid adaptation to target domains. A custom domain-adaptive module processes CLIP features alongside super-resolution features through a multi-stage transformation process, including CLIP feature processing, spatial feature generation, and feature fusion. This intricate process ensures effective incorporation of semantic information into the super-resolution pipeline. Additionally, CDASR employs a multi-component loss function that combines pixel-wise reconstruction, perceptual similarity, and semantic consistency. Extensive experiments on benchmark datasets demonstrate CDASR's superiority, particularly in challenging scenarios. On the Urban100 dataset at $\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over existing methods, with even larger improvements of up to 0.30dB observed at $\times$16 scaling.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2505.11164.pdf' target='_blank'>https://arxiv.org/pdf/2505.11164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikita Rudin, Junzhe He, Joshua Aurand, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11164">Parkour in the Wild: Learning a General and Extensible Agile Locomotion Policy Using Multi-expert Distillation and RL Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Legged robots are well-suited for navigating terrains inaccessible to wheeled robots, making them ideal for applications in search and rescue or space exploration. However, current control methods often struggle to generalize across diverse, unstructured environments. This paper introduces a novel framework for agile locomotion of legged robots by combining multi-expert distillation with reinforcement learning (RL) fine-tuning to achieve robust generalization. Initially, terrain-specific expert policies are trained to develop specialized locomotion skills. These policies are then distilled into a unified foundation policy via the DAgger algorithm. The distilled policy is subsequently fine-tuned using RL on a broader terrain set, including real-world 3D scans. The framework allows further adaptation to new terrains through repeated fine-tuning. The proposed policy leverages depth images as exteroceptive inputs, enabling robust navigation across diverse, unstructured terrains. Experimental results demonstrate significant performance improvements over existing methods in synthesizing multi-terrain skills into a single controller. Deployment on the ANYmal D robot validates the policy's ability to navigate complex environments with agility and robustness, setting a new benchmark for legged robot locomotion.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2505.06580.pdf' target='_blank'>https://arxiv.org/pdf/2505.06580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyoon Yang, Jihu Lee, Yongdai Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06580">TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust domain adaptation against adversarial attacks is a critical research area that aims to develop models capable of maintaining consistent performance across diverse and challenging domains. In this paper, we derive a new generalization bound for robust risk on the target domain using a novel divergence measure specifically designed for robust domain adaptation. Building upon this, we propose a new algorithm named TAROT, which is designed to enhance both domain adaptability and robustness. Through extensive experiments, TAROT not only surpasses state-of-the-art methods in accuracy and robustness but also significantly enhances domain generalization and scalability by effectively learning domain-invariant features. In particular, TAROT achieves superior performance on the challenging DomainNet dataset, demonstrating its ability to learn domain-invariant representations that generalize well across different domains, including unseen ones. These results highlight the broader applicability of our approach in real-world domain adaptation scenarios.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2505.06301.pdf' target='_blank'>https://arxiv.org/pdf/2505.06301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhou Ye, Kevin I-Kai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06301">Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-user variability in Human Activity Recognition (HAR) remains a critical challenge due to differences in sensor placement, body dynamics, and behavioral patterns. Traditional methods often fail to capture biomechanical invariants that persist across users, limiting their generalization capability. We propose an Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG) framework that integrates anatomical correlation knowledge into a unified graph neural network (GNN) architecture. By modeling three biomechanically motivated relationships together-Interconnected Units, Analogous Units, and Lateral Units-our method encodes domain-invariant features while addressing user-specific variability through Variational Edge Feature Extractor. A Gradient Reversal Layer (GRL) enforces adversarial domain generalization, ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and DSADS datasets demonstrate state-of-the-art performance. Our work bridges biomechanical principles with graph-based adversarial learning by integrating information fusion techniques. This fusion of information underpins our unified and generalized model for cross-user HAR.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2504.02996.pdf' target='_blank'>https://arxiv.org/pdf/2504.02996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Wang, Aoming Liu, Bryan A. Plummer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02996">Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-source Domain Generalization (DG) aims to improve model robustness to new distributions. However, DG methods often overlook the effect of label noise, which can confuse a model during training, reducing performance. Limited prior work has analyzed DG method's noise-robustness, typically focused on an analysis of existing methods rather than new solutions. In this paper, we investigate this underexplored space, where models are evaluated under both distribution shifts and label noise, which we refer to as Noise-Aware Generalization (NAG). A natural solution to address label noise would be to combine a Learning with Noisy Labels (LNL) method with those from DG. Many LNL methods aim to detect distribution shifts in a class's samples, i.e., they assume that distribution shifts often correspond to label noise. However, in NAG distribution shifts can be due to label noise or domain shifts, breaking the assumptions used by LNL methods. A naive solution is to make a similar assumption made by many DG methods, where we presume to have domain labels during training, enabling us to isolate the two types of shifts. However, this ignores valuable cross-domain information. Specifically, our proposed DL4ND approach improves noise detection by taking advantage of the observation that noisy samples that may appear indistinguishable within a single domain often show greater variation when compared across domains. Experiments show that DL4ND significantly improves performance across four diverse datasets, offering a promising direction for tackling NAG.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2503.17211.pdf' target='_blank'>https://arxiv.org/pdf/2503.17211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Dai, Lehong Wang, Fangzhou Lin, Yidong Wang, Zhigang Li, Kazunori D Yamada, Ziming Zhang, Wang Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17211">A Language Anchor-Guided Method for Robust Noisy Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world machine learning applications often struggle with two major challenges: distribution shift and label noise. Models tend to overfit by focusing on redundant and uninformative features in the training data, which makes it hard for them to generalize to the target domain. Noisy data worsens this problem by causing further overfitting to the noise, meaning that existing methods often fail to tell the difference between true, invariant features and misleading, spurious ones. To tackle these issues, we introduce Anchor Alignment and Adaptive Weighting (A3W). This new algorithm uses sample reweighting guided by natural language processing (NLP) anchors to extract more representative features. In simple terms, A3W leverages semantic representations from natural language models as a source of domain-invariant prior knowledge. Additionally, it employs a weighted loss function that adjusts each sample's contribution based on its similarity to the corresponding NLP anchor. This adjustment makes the model more robust to noisy labels. Extensive experiments on standard benchmark datasets show that A3W consistently outperforms state-of-the-art domain generalization methods, offering significant improvements in both accuracy and robustness across different datasets and noise levels.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2503.15910.pdf' target='_blank'>https://arxiv.org/pdf/2503.15910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junsung Park, Hwijeong Lee, Inha Kang, Hyunjung Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15910">No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing domain generalization methods for LiDAR semantic segmentation under adverse weather struggle to accurately predict "things" categories compared to "stuff" categories. In typical driving scenes, "things" categories can be dynamic and associated with higher collision risks, making them crucial for safe navigation and planning. Recognizing the importance of "things" categories, we identify their performance drop as a serious bottleneck in existing approaches. We observed that adverse weather induces degradation of semantic-level features and both corruption of local features, leading to a misprediction of "things" as "stuff". To mitigate these corruptions, we suggest our method, NTN - segmeNt Things for No-accident. To address semantic-level feature corruption, we bind each point feature to its superclass, preventing the misprediction of things classes into visually dissimilar categories. Additionally, to enhance robustness against local corruption caused by adverse weather, we define each LiDAR beam as a local region and propose a regularization term that aligns the clean data with its corrupted counterpart in feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU gain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the SemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9 mIoU improvement on "things" classes, respectively, highlighting its effectiveness.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2503.08271.pdf' target='_blank'>https://arxiv.org/pdf/2503.08271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhe Niu, Zongxia Xie, Yanru Sun, Wei He, Man Xu, Chao Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08271">LangTime: A Language-Guided Unified Model for Time Series Forecasting with Proximal Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has shown an increasing interest in utilizing pre-trained large language models (LLMs) for a variety of time series applications. However, there are three main challenges when using LLMs as foundational models for time series forecasting: (1) Cross-domain generalization. (2) Cross-modality alignment. (3) Error accumulation in autoregressive frameworks. To address these challenges, we proposed LangTime, a language-guided unified model for time series forecasting that incorporates cross-domain pre-training with reinforcement learning-based fine-tuning. Specifically, LangTime constructs Temporal Comprehension Prompts (TCPs), which include dataset-wise and channel-wise instructions, to facilitate domain adaptation and condense time series into a single token, enabling LLMs to understand better and align temporal data. To improve autoregressive forecasting, we introduce TimePPO, a reinforcement learning-based fine-tuning algorithm. TimePPO mitigates error accumulation by leveraging a multidimensional rewards function tailored for time series and a repeat-based value estimation strategy. Extensive experiments demonstrate that LangTime achieves state-of-the-art cross-domain forecasting performance, while TimePPO fine-tuning effectively enhances the stability and accuracy of autoregressive forecasting.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2503.06288.pdf' target='_blank'>https://arxiv.org/pdf/2503.06288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yan, Marzi Heidari, Yuhong Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06288">Single Domain Generalization with Adversarial Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to train models that can generalize to unseen testing domains by leveraging data from multiple training domains. However, traditional DG methods rely on the availability of multiple diverse training domains, limiting their applicability in data-constrained scenarios. Single Domain Generalization (SDG) addresses the more realistic and challenging setting by restricting the training data to a single domain distribution. The main challenges in SDG stem from the limited diversity of training data and the inaccessibility of unseen testing data distributions. To tackle these challenges, we propose a single domain generalization method that leverages an adversarial memory bank to augment training features. Our memory-based feature augmentation network maps both training and testing features into an invariant subspace spanned by diverse memory features, implicitly aligning the training and testing domains in the projected space. To maintain a diverse and representative feature memory bank, we introduce an adversarial feature generation method that creates features extending beyond the training domain distribution. Experimental results demonstrate that our approach achieves state-of-the-art performance on standard single domain generalization benchmarks.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2503.03399.pdf' target='_blank'>https://arxiv.org/pdf/2503.03399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanyu Duan, Yi Yang, Ahmed Abbasi, Kar Yan Tam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03399">Predicting Practically? Domain Generalization for Predictive Analytics in Real-world Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive machine learning models are widely used in customer relationship management (CRM) to forecast customer behaviors and support decision-making. However, the dynamic nature of customer behaviors often results in significant distribution shifts between training data and serving data, leading to performance degradation in predictive models. Domain generalization, which aims to train models that can generalize to unseen environments without prior knowledge of their distributions, has become a critical area of research. In this work, we propose a novel domain generalization method tailored to handle complex distribution shifts, encompassing both covariate and concept shifts. Our method builds upon the Distributionally Robust Optimization framework, optimizing model performance over a set of hypothetical worst-case distributions rather than relying solely on the training data. Through simulation experiments, we demonstrate the working mechanism of the proposed method. We also conduct experiments on a real-world customer churn dataset, and validate its effectiveness in both temporal and spatial generalization settings. Finally, we discuss the broader implications of our method for advancing Information Systems (IS) design research, particularly in building robust predictive models for dynamic managerial environments.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2502.16064.pdf' target='_blank'>https://arxiv.org/pdf/2502.16064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marzi Heidari, Yuhong Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16064">Single Domain Generalization with Model-aware Parametric Batch-wise Mixup</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Domain Generalization (SDG) remains a formidable challenge in the field of machine learning, particularly when models are deployed in environments that differ significantly from their training domains. In this paper, we propose a novel data augmentation approach, named as Model-aware Parametric Batch-wise Mixup (MPBM), to tackle the challenge of SDG. MPBM deploys adversarial queries generated with stochastic gradient Langevin dynamics, and produces model-aware augmenting instances with a parametric batch-wise mixup generator network that is carefully designed through an innovative attention mechanism. By exploiting inter-feature correlations, the parameterized mixup generator introduces additional versatility in combining features across a batch of instances, thereby enhancing the capacity to generate highly adaptive and informative synthetic instances for specific queries. The synthetic data produced by this adaptable generator network, guided by informative queries, is expected to significantly enrich the representation space covered by the original training dataset and subsequently enhance the prediction model's generalizability across diverse and previously unseen domains. To prevent excessive deviation from the training data, we further incorporate a real-data alignment-based adversarial loss into the learning process of MPBM, regularizing any tendencies toward undesirable expansions. We conduct extensive experiments on several benchmark datasets. The empirical results demonstrate that by augmenting the training set with informative synthesis data, our proposed MPBM method achieves the state-of-the-art performance for single domain generalization.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2502.13310.pdf' target='_blank'>https://arxiv.org/pdf/2502.13310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adib Mosharrof, Moghis Fereidouni, A. B. Siddique
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13310">Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog Systems for Task Completion without Turn-level Dialog Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional task-oriented dialog (ToD) systems rely heavily on labor-intensive turn-level annotations, such as dialogue states and policy labels, for training. This work explores whether large language models (LLMs) can be fine-tuned solely on natural language dialogs to perform ToD tasks, without requiring such annotations. We evaluate their ability to generalize to unseen domains and compare their performance with models trained on fully annotated data. Through extensive experiments with three open-source LLMs of varying sizes and two diverse ToD datasets, we find that models fine-tuned without turn-level annotations generate coherent and contextually appropriate responses. However, their task completion performance - measured by accurate execution of API calls - remains suboptimal, with the best models achieving only around 53% success in unseen domains. To improve task completion, we propose ZeroToD, a framework that incorporates a schema augmentation mechanism to enhance API call accuracy and overall task completion rates, particularly in out-of-domain settings. We also compare ZeroToD with fine-tuning-free alternatives, such as prompting off-the-shelf LLMs, and find that our framework enables smaller, fine-tuned models that outperform large-scale proprietary LLMs in task completion. Additionally, a human study evaluating informativeness, fluency, and task completion confirms our empirical findings. These findings suggest the feasibility of developing cost-effective, scalable, and zero-shot generalizable ToD systems for real-world applications.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2412.14816.pdf' target='_blank'>https://arxiv.org/pdf/2412.14816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenfan Qu, Jian Liu, Haoxing Chen, Baihan Yu, Jingjing Liu, Weiqiang Wang, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14816">TextSleuth: Towards Explainable Tampered Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, tampered text detection has attracted increasing attention due to its essential role in information security. Although existing methods can detect the tampered text region, the interpretation of such detection remains unclear, making the prediction unreliable. To address this problem, we propose to explain the basis of tampered text detection with natural language via large multimodal models. To fill the data gap for this task, we propose a large-scale, comprehensive dataset, ETTD, which contains both pixel-level annotations for tampered text region and natural language annotations describing the anomaly of the tampered text. Multiple methods are employed to improve the quality of the proposed data. For example, elaborate queries are introduced to generate high-quality anomaly descriptions with GPT4o. A fused mask prompt is proposed to reduce confusion when querying GPT4o to generate anomaly descriptions. To automatically filter out low-quality annotations, we also propose to prompt GPT4o to recognize tampered texts before describing the anomaly, and to filter out the responses with low OCR accuracy. To further improve explainable tampered text detection, we propose a simple yet effective model called TextSleuth, which achieves improved fine-grained perception and cross-domain generalization by focusing on the suspected region, with a two-stage analysis paradigm and an auxiliary grounding prompt. Extensive experiments on both the ETTD dataset and the public dataset have verified the effectiveness of the proposed methods. In-depth analysis is also provided to inspire further research. Our dataset and code will be open-source.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2412.14474.pdf' target='_blank'>https://arxiv.org/pdf/2412.14474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shange Tang, Jiayun Wu, Jianqing Fan, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14474">Benign Overfitting in Out-of-Distribution Generalization of Linear Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benign overfitting refers to the phenomenon where an over-parameterized model fits the training data perfectly, including noise in the data, but still generalizes well to the unseen test data. While prior work provides some theoretical understanding of this phenomenon under the in-distribution setup, modern machine learning often operates in a more challenging Out-of-Distribution (OOD) regime, where the target (test) distribution can be rather different from the source (training) distribution. In this work, we take an initial step towards understanding benign overfitting in the OOD regime by focusing on the basic setup of over-parameterized linear models under covariate shift. We provide non-asymptotic guarantees proving that benign overfitting occurs in standard ridge regression, even under the OOD regime when the target covariance satisfies certain structural conditions. We identify several vital quantities relating to source and target covariance, which govern the performance of OOD generalization. Our result is sharp, which provably recovers prior in-distribution benign overfitting guarantee [Tsigler and Bartlett, 2023], as well as under-parameterized OOD guarantee [Ge et al., 2024] when specializing to each setup. Moreover, we also present theoretical results for a more general family of target covariance matrix, where standard ridge regression only achieves a slow statistical rate of $O(1/\sqrt{n})$ for the excess risk, while Principal Component Regression (PCR) is guaranteed to achieve the fast rate $O(1/n)$, where $n$ is the number of samples.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2412.02029.pdf' target='_blank'>https://arxiv.org/pdf/2412.02029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ihab Tabbara, Hussein Sibai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02029">Learning Ensembles of Vision-based Safety Control Filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety filters in control systems correct nominal controls that violate safety constraints. Designing such filters as functions of visual observations in uncertain and complex environments is challenging. Several deep learning-based approaches to tackle this challenge have been proposed recently. However, formally verifying that the learned filters satisfy critical properties that enable them to guarantee the safety of the system is currently beyond reach. Instead, in this work, motivated by the success of ensemble methods in reinforcement learning, we empirically investigate the efficacy of ensembles in enhancing the accuracy and the out-of-distribution generalization of such filters, as a step towards more reliable ones. We experiment with diverse pre-trained vision representation models as filter backbones, training approaches, and output aggregation techniques. We compare the performance of ensembles with different configurations against each other, their individual member models, and large single-model baselines in distinguishing between safe and unsafe states and controls in the DeepAccident dataset. Our results show that diverse ensembles have better state and control classification accuracies compared to individual models.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2412.00767.pdf' target='_blank'>https://arxiv.org/pdf/2412.00767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linhai Zhuo, Zheng Wang, Yuqian Fu, Tianwen Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00767">Prompt as Free Lunch: Enhancing Diversity in Source-Free Cross-domain Few-shot Learning through Semantic-Guided Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The source-free cross-domain few-shot learning (CD-FSL) task aims to transfer pretrained models to target domains utilizing minimal samples, eliminating the need for source domain data. Addressing this issue requires models to have robust generalization abilities and strong feature representation, aligning with the characteristics of large-scale pretrained models. However, large-scale models tend to lose representational ability in cross-domain scenarios due to limited sample diversity. \zlh{Given the abundant diversity provided by semantic modality, this paper leverages textual modality to enhance training sample diversity with CLP model}, meanwhile improving model transfer efficiency. Specifically, we propose the SeGD-VPT framework, which is divided into two phases. The first step aims to increase feature diversity by adding diversity prompts to each support sample, thereby generating varying input and enhancing sample diversity. Furthermore, we use diversity descriptions of classes to guide semantically meaningful learning of diversity prompts, proposing random combinations and selections of texts to increase textual diversity. Additionally, deep prompt tuning is introduced to enhance the model's transfer capability. After training of the first step, support samples with different diversity prompts are input into the CLIP backbone to generate enhanced features. After generation, the second phase trains classifiers using the generated features. Extensive experimental results across several benchmarks verify our method is comparable to SOTA source-utilized models and attain the best performance under the source-free CD-FSL setting.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2411.19534.pdf' target='_blank'>https://arxiv.org/pdf/2411.19534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenfang Sun, Yingjun Du, Gaowen Liu, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19534">QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of quantifying the number of objects by a generative text-to-image model. Rather than retraining such a model for each new image domain of interest, which leads to high computational costs and limited scalability, we are the first to consider this problem from a domain-agnostic perspective. We propose QUOTA, an optimization framework for text-to-image models that enables effective object quantification across unseen domains without retraining. It leverages a dual-loop meta-learning strategy to optimize a domain-invariant prompt. Further, by integrating prompt learning with learnable counting and domain tokens, our method captures stylistic variations and maintains accuracy, even for object classes not encountered during training. For evaluation, we adopt a new benchmark specifically designed for object quantification in domain generalization, enabling rigorous assessment of object quantification accuracy and adaptability across unseen domains in text-to-image generation. Extensive experiments demonstrate that QUOTA outperforms conventional models in both object quantification accuracy and semantic consistency, setting a new benchmark for efficient and scalable text-to-image generation for any domain.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2411.16833.pdf' target='_blank'>https://arxiv.org/pdf/2411.16833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Yao, Hao Gu, Xuweiyi Chen, Jiayun Wang, Zezhou Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16833">Open Vocabulary Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we pioneer the study of open-vocabulary monocular 3D object detection, a novel task that aims to detect and localize objects in 3D space from a single RGB image without limiting detection to a predefined set of categories. We formalize this problem, establish baseline methods, and introduce a class-agnostic approach that leverages open-vocabulary 2D detectors and lifts 2D bounding boxes into 3D space. Our approach decouples the recognition and localization of objects in 2D from the task of estimating 3D bounding boxes, enabling generalization across unseen categories. Additionally, we propose a target-aware evaluation protocol to address inconsistencies in existing datasets, improving the reliability of model performance assessment. Extensive experiments on the Omni3D dataset demonstrate the effectiveness of the proposed method in zero-shot 3D detection for novel object categories, validating its robust generalization capabilities. Our method and evaluation protocols contribute towards the development of open-vocabulary object detection models that can effectively operate in real-world, category-diverse environments.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2411.16788.pdf' target='_blank'>https://arxiv.org/pdf/2411.16788.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16788">TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of single-source domain generalization. Existing methods typically rely on extensive augmentations to synthetically cover diverse domains during training. However, they struggle with semantic shifts (e.g., background and viewpoint changes), as they often learn global features instead of local concepts that tend to be domain invariant. To address this gap, we propose an approach that compels models to leverage such local concepts during prediction. Given no suitable dataset with per-class concepts and localization maps exists, we first develop a novel pipeline to generate annotations by exploiting the rich features of diffusion and large-language models. Our next innovation is TIDE, a novel training scheme with a concept saliency alignment loss that ensures model focus on the right per-concept regions and a local concept contrastive loss that promotes learning domain-invariant concept representations. This not only gives a robust model but also can be visually interpreted using the predicted concept saliency maps. Given these maps at test time, our final contribution is a new correction algorithm that uses the corresponding local concept representations to iteratively refine the prediction until it aligns with prototypical concept representations that we store at the end of model training. We evaluate our approach extensively on four standard DG benchmark datasets and substantially outperform the current state-ofthe-art (12% improvement on average) while also demonstrating that our predictions can be visually interpreted
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2410.20164.pdf' target='_blank'>https://arxiv.org/pdf/2410.20164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjun Du, Gaowen Liu, Yuzhang Shang, Yuguang Yao, Ramana Kompella, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20164">Prompt Diffusion Robustifies Any-Modality Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models enable prompt-based classifiers for zero-shot and few-shot learning. Nonetheless, the conventional method of employing fixed prompts suffers from distributional shifts that negatively impact generalizability to unseen samples. This paper introduces prompt diffusion, which uses a diffusion model to gradually refine the prompts to obtain a customized prompt for each sample. Specifically, we first optimize a collection of prompts to obtain over-fitted prompts per sample. Then, we propose a prompt diffusion model within the prompt space, enabling the training of a generative transition process from a random prompt to its overfitted prompt. As we cannot access the label of a test image during inference, our model gradually generates customized prompts solely from random prompts using our trained, prompt diffusion. Our prompt diffusion is generic, flexible, and modality-agnostic, making it a simple plug-and-play module seamlessly embedded into existing prompt learning methods for textual, visual, or multi-modal prompt learning. Our diffusion model uses a fast ODE-based sampling strategy to optimize test sample prompts in just five steps, offering a good trade-off between performance improvement and computational efficiency. For all prompt learning methods tested, adding prompt diffusion yields more robust results for base-to-new generalization, cross-dataset generalization, and domain generalization in classification tasks tested over 15 diverse datasets.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2410.13787.pdf' target='_blank'>https://arxiv.org/pdf/2410.13787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13787">Looking Inward: Language Models Can Learn About Themselves by Introspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data.
  We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).
  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2410.08258.pdf' target='_blank'>https://arxiv.org/pdf/2410.08258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prasanna Mayilvahanan, Roland S. Zimmermann, ThaddÃ¤us Wiedemer, Evgenia Rusak, Attila Juhos, Matthias Bethge, Wieland Brendel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08258">In Search of Forgotten Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION -- LAION-Natural and LAION-Rendition -- that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale -- a crucial prerequisite for improving model robustness.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2601.20172.pdf' target='_blank'>https://arxiv.org/pdf/2601.20172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Amarel, Robyn Miller, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Alexei Skurikhin, Earl Lawrence, Gerd J. Kunde
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20172">Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2601.03825.pdf' target='_blank'>https://arxiv.org/pdf/2601.03825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Zhang, Huan Yan, Jinyang Huang, Bin Liu, Yuanhao Feng, Jianchun Liu, Meng Li, Fusang Zhang, Zhi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03825">Beyond Physical Labels: Redefining Domains for Robust WiFi-based Gesture Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose GesFi, a novel WiFi-based gesture recognition system that introduces WiFi latent domain mining to redefine domains directly from the data itself. GesFi first processes raw sensing data collected from WiFi receivers using CSI-ratio denoising, Short-Time Fast Fourier Transform, and visualization techniques to generate standardized input representations. It then employs class-wise adversarial learning to suppress gesture semantic and leverages unsupervised clustering to automatically uncover latent domain factors responsible for distributional shifts. These latent domains are then aligned through adversarial learning to support robust cross-domain generalization. Finally, the system is applied to the target environment for robust gesture inference. We deployed GesFi under both single-pair and multi-pair settings using commodity WiFi transceivers, and evaluated it across multiple public datasets and real-world environments. Compared to state-of-the-art baselines, GesFi achieves up to 78% and 50% performance improvements over existing adversarial methods, and consistently outperforms prior generalization approaches across most cross-domain tasks.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2512.23707.pdf' target='_blank'>https://arxiv.org/pdf/2512.23707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.23707">Training AI Co-Scientists Using Rubric Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2511.20258.pdf' target='_blank'>https://arxiv.org/pdf/2511.20258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Wang, Zhangtao Cheng, Ting Zhong, Leiting Chen, Fan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20258">Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2511.19126.pdf' target='_blank'>https://arxiv.org/pdf/2511.19126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beilin Chu, Weike You, Mengtao Li, Tingting Zheng, Kehan Zhao, Xuan Xu, Zhigao Lu, Jia Song, Moxuan Xu, Linna Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19126">When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2509.23626.pdf' target='_blank'>https://arxiv.org/pdf/2509.23626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomseok Kang, Niluthpol Chowdhury Mithun, Mikhail Sizintsev, Han-Pang Chiu, Supun Samarasekera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23626">Efficient Domain-Adaptive Multi-Task Dense Prediction with Vision Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task dense prediction, which aims to jointly solve tasks like semantic segmentation and depth estimation, is crucial for robotics applications but suffers from domain shift when deploying models in new environments. While unsupervised domain adaptation (UDA) addresses this challenge for single tasks, existing multi-task UDA methods primarily rely on adversarial learning approaches that are less effective than recent self-training techniques. In this paper, we introduce FAMDA, a simple yet effective UDA framework that bridges this gap by leveraging Vision Foundation Models (VFMs) as powerful teachers. Our approach integrates Segmentation and Depth foundation models into a self-training paradigm to generate high-quality pseudo-labels for the target domain, effectively distilling their robust generalization capabilities into a single, efficient student network. Extensive experiments show that FAMDA achieves state-of-the-art (SOTA) performance on standard synthetic-to-real UDA multi-task learning (MTL) benchmarks and a challenging new day-to-night adaptation task. Our framework enables the training of highly efficient models; a lightweight variant achieves SOTA accuracy while being more than 10$\times$ smaller than foundation models, highlighting FAMDA's suitability for creating domain-adaptive and efficient models for resource-constrained robotics applications.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2509.02473.pdf' target='_blank'>https://arxiv.org/pdf/2509.02473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziting Wang, Shize Zhang, Haitao Yuan, Jinwei Zhu, Shifu Li, Wei Dong, Gao Cong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02473">FDABench: A Benchmark for Data Agents on Analytical Queries over Heterogeneous Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for data-driven decision-making has created an urgent need for data agents that can integrate structured and unstructured data for analysis. While data agents show promise for enabling users to perform complex analytics tasks, this field still suffers from three critical limitations: first, comprehensive data agent benchmarks remain absent due to the difficulty of designing test cases that evaluate agents' abilities across multi-source analytical tasks; second, constructing reliable test cases that combine structured and unstructured data remains costly and prohibitively complex; third, existing benchmarks exhibit limited adaptability and generalizability, resulting in narrow evaluation scope. To address these challenges, we present FDABench, the first data agent benchmark specifically designed for evaluating agents in multi-source data analytical scenarios. Our contributions include: (i) we construct a standardized benchmark with 2,007 diverse tasks across different data sources, domains, difficulty levels, and task types to comprehensively evaluate data agent performance; (ii) we design an agent-expert collaboration framework ensuring reliable and efficient benchmark construction over heterogeneous data; (iii) we equip FDABench with robust generalization capabilities across diverse target systems and frameworks. We use FDABench to evaluate various data agent systems, revealing that each system exhibits distinct advantages and limitations regarding response quality, accuracy, latency, and token cost.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2508.13768.pdf' target='_blank'>https://arxiv.org/pdf/2508.13768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengchao Liu, Xiaoming Liu, Chengzhengxu Li, Zhaohan Zhang, Guoxin Ma, Yu Lan, Shuai Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13768">MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models have shown growing ability to generate fluent and coherent texts that are highly similar to the writing style of humans. Current detectors for Machine-Generated Text (MGT) perform well when they are trained and tested in the same domain but generalize poorly to unseen domains, due to domain shift between data from different sources. In this work, we propose MGT-Prism, an MGT detection method from the perspective of the frequency domain for better domain generalization. Our key insight stems from analyzing text representations in the frequency domain, where we observe consistent spectral patterns across diverse domains, while significant discrepancies in magnitude emerge between MGT and human-written texts (HWTs). The observation initiates the design of a low frequency domain filtering module for filtering out the document-level features that are sensitive to domain shift, and a dynamic spectrum alignment strategy to extract the task-specific and domain-invariant features for improving the detector's performance in domain generalization. Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test datasets across three domain-generalization scenarios.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2507.21533.pdf' target='_blank'>https://arxiv.org/pdf/2507.21533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler Han, Yanda Bao, Bhaumik Mehta, Gabriel Guo, Anubhav Vishwakarma, Emily Kang, Sanghun Jung, Rosario Scalise, Jason Zhou, Bryan Xu, Byron Boots
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21533">Model Predictive Adversarial Imitation Learning for Planning from Observation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human demonstration data is often ambiguous and incomplete, motivating imitation learning approaches that also exhibit reliable planning behavior. A common paradigm to perform planning-from-demonstration involves learning a reward function via Inverse Reinforcement Learning (IRL) then deploying this reward via Model Predictive Control (MPC). Towards unifying these methods, we derive a replacement of the policy in IRL with a planning-based agent. With connections to Adversarial Imitation Learning, this formulation enables end-to-end interactive learning of planners from observation-only demonstrations. In addition to benefits in interpretability, complexity, and safety, we study and observe significant improvements on sample efficiency, out-of-distribution generalization, and robustness. The study includes evaluations in both simulated control benchmarks and real-world navigation experiments using few-to-single observation-only demonstrations.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2507.17281.pdf' target='_blank'>https://arxiv.org/pdf/2507.17281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanli Zhuo, Leilei Ma, Haifeng Zhao, Shiwei Zhou, Dengdi Sun, Yanping Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17281">Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although SAM-based single-source domain generalization models for medical image segmentation can mitigate the impact of domain shift on the model in cross-domain scenarios, these models still face two major challenges. First, the segmentation of SAM is highly dependent on domain-specific expert-annotated prompts, which prevents SAM from achieving fully automated medical image segmentation and therefore limits its application in clinical settings. Second, providing poor prompts (such as bounding boxes that are too small or too large) to the SAM prompt encoder can mislead SAM into generating incorrect mask results. Therefore, we propose the FA-SAM, a single-source domain generalization framework for medical image segmentation that achieves fully automated SAM. FA-SAM introduces two key innovations: an Auto-prompted Generation Model (AGM) branch equipped with a Shallow Feature Uncertainty Modeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module integrated into the SAM mask decoder. Specifically, AGM models the uncertainty distribution of shallow features through the SUFM module to generate bounding box prompts for the target domain, enabling fully automated segmentation with SAM. The IPEF module integrates multiscale information from SAM image embeddings and prompt embeddings to capture global and local details of the target object, enabling SAM to mitigate the impact of poor prompts. Extensive experiments on publicly available prostate and fundus vessel datasets validate the effectiveness of FA-SAM and highlight its potential to address the above challenges.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2507.15349.pdf' target='_blank'>https://arxiv.org/pdf/2507.15349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15349">Scaling Decentralized Learning with FLock</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning the large language models (LLMs) are prevented by the deficiency of centralized control and the massive computing and communication overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative LLM fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B LLM in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2507.05823.pdf' target='_blank'>https://arxiv.org/pdf/2507.05823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tangzheng Lian, Guanyu Hu, Dimitrios Kollias, Xinyu Yang, Oya Celiktutan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05823">Fair Domain Generalization: An Information-Theoretic View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) and algorithmic fairness are two critical challenges in machine learning. However, most DG methods focus only on minimizing expected risk in the unseen target domain without considering algorithmic fairness. Conversely, fairness methods typically do not account for domain shifts, so the fairness achieved during training may not generalize to unseen test domains. In this work, we bridge these gaps by studying the problem of Fair Domain Generalization (FairDG), which aims to minimize both expected risk and fairness violations in unseen target domains. We derive novel mutual information-based upper bounds for expected risk and fairness violations in multi-class classification tasks with multi-group sensitive attributes. These bounds provide key insights for algorithm design from an information-theoretic perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal Fairness for Domain Generalization), a practical framework that solves the FairDG problem and models the utility-fairness trade-off through Pareto optimization. Experiments on real-world vision and language datasets show that PAFDG achieves superior utility-fairness trade-offs compared to existing methods.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2506.17868.pdf' target='_blank'>https://arxiv.org/pdf/2506.17868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Testa, SÃ¸ren Hauberg, Tamim Asfour, Leonel Rozo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17868">Geometric Contact Flows: Contactomorphisms for Dynamics and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately modeling and predicting complex dynamical systems, particularly those involving force exchange and dissipation, is crucial for applications ranging from fluid dynamics to robotics, but presents significant challenges due to the intricate interplay of geometric constraints and energy transfer. This paper introduces Geometric Contact Flows (GFC), a novel framework leveraging Riemannian and Contact geometry as inductive biases to learn such systems. GCF constructs a latent contact Hamiltonian model encoding desirable properties like stability or energy conservation. An ensemble of contactomorphisms then adapts this model to the target dynamics while preserving these properties. This ensemble allows for uncertainty-aware geodesics that attract the system's behavior toward the data support, enabling robust generalization and adaptation to unseen scenarios. Experiments on learning dynamics for physical systems and for controlling robots on interaction tasks demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2506.15211.pdf' target='_blank'>https://arxiv.org/pdf/2506.15211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng He, Zijun Chen, Xinnian Liang, Tingting Ma, Yunqi Qiu, Shuangzhi Wu, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15211">ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain generalization capabilities. However, the underlying mechanisms supporting such transfer remain poorly understood. We hypothesize that cross-domain generalization arises from shared abstract reasoning prototypes -- fundamental reasoning patterns that capture the essence of problems across domains. These prototypes minimize the nuances of the representation, revealing that seemingly diverse tasks are grounded in shared reasoning structures.Based on this hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning ability of LLMs by leveraging scalable and verifiable prototypical representations (Prolog for logical reasoning, PDDL for planning).ProtoReasoning features: (1) an automated prototype construction pipeline that transforms problems into corresponding prototype representations; (2) a comprehensive verification system providing reliable feedback through Prolog/PDDL interpreters; (3) the scalability to synthesize problems arbitrarily within prototype space while ensuring correctness. Extensive experiments show that ProtoReasoning achieves 4.7% improvement over baseline models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics (AIME24). Significantly, our ablation studies confirm that learning in prototype space also demonstrates enhanced generalization to structurally similar problems compared to training solely on natural language representations, validating our hypothesis that reasoning prototypes serve as the foundation for generalizable reasoning in large language models.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2506.07899.pdf' target='_blank'>https://arxiv.org/pdf/2506.07899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Wang, Yiming Qin, Nikolaos Dimitriadis, Alessandro Favero, Pascal Frossard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07899">MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably-without retraining or forgetting previous information-remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through sample-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks for LLaMA-3 and Mistral backbones demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2505.03414.pdf' target='_blank'>https://arxiv.org/pdf/2505.03414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03414">Enhancing Target-unspecific Tasks through a Features Matrix</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent developments in prompt learning of large Vision-Language Models (VLMs) have significantly improved performance in target-specific tasks. However, these prompting methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge. The general knowledge has a strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2) the FM significantly showcases its effectiveness in enhancing target-unspecific tasks (base-to-novel generalization, domain generalization, and cross-dataset generalization), achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2504.14810.pdf' target='_blank'>https://arxiv.org/pdf/2504.14810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jucheng Hu, Surong Yang, Lijun Wu, Dongzhan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14810">DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ad-hoc instruction fine-tuning of large language models (LLMs) is widely adopted for domain-specific adaptation. While domain-specific supervised fine-tuning (SFT) is effective and efficient, it often weakens cross-domain generalization and struggles with noisy training data. To address these challenges, we propose DONOD, a lightweight model-intrinsic data pruning method. Our approach evaluates data using two model-parameter-based metrics: Delta of Norm (DON), which captures the cumulative influence on model weights, and Norm of Delta (NOD), which quantifies weight instability. Moreover, by employing the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) algorithm, we effectively filter noisy, unlearnable, and generalization-harming samples without relying on auxiliary models during the SFT process. Experiments on mathematical tasks demonstrate that data selected by DONOD achieves superior fine-tuning efficiency and improved robustness against noisy data. By filtering out 70% of the whole dataset, we improve target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile, our selected data present superior cross-architecture generalization. Data pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD demonstrates comparable or superior performance while remaining dataset-agnostic, enabling broader applicability. Code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2503.06158.pdf' target='_blank'>https://arxiv.org/pdf/2503.06158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziruo Hao, Zhenhua Cui, Tao Yang, Bo Hu, Xiaofeng Wu, Hui Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06158">Invariant Federated Learning for Edge Intelligence: Mitigating Heterogeneity and Asynchrony via Exit Strategy and Invariant Penalty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides an invariant federated learning system for resource-constrained edge intelligence. This framework can mitigate the impact of heterogeneity and asynchrony via exit strategy and invariant penalty. We introduce parameter orthogonality into edge intelligence to measure the contribution or impact of heterogeneous and asynchronous clients. It is proved in this paper that the exit of abnormal edge clients can guarantee the effect of the model on most clients. Meanwhile, to ensure the models' performance on exited abnormal clients and those who lack training resources, we propose Federated Learning with Invariant Penalty for Generalization (FedIPG) by constructing the approximate orthogonality of the invariant parameters and the heterogeneous parameters. Theoretical proof shows that FedIPG reduces the Out-Of-Distribution prediction loss without increasing the communication burden. The performance of FedIPG combined with an exit strategy is tested empirically in multiple scales using four datasets. It shows our system can enhance In-Distribution performance and outperform the state-of-the-art algorithm in Out-Of-Distribution generalization while maintaining model convergence. Additionally, the results of the visual experiment prove that FedIPG contains preliminary causality in terms of ignoring confounding features.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2503.04315.pdf' target='_blank'>https://arxiv.org/pdf/2503.04315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuang Liu, Yihan Wang, Yifan Zhu, Yibo Miao, Xiao-Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04315">Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for adversarial noise via Wasserstein distance and statistical error via Kullback-Leibler divergence, called the Statistically Robust WDRO. We establish a robust generalization bound for the new optimization framework, implying that out-of-distribution adversarial performance is at least as good as the statistically robust training loss with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary, giving an optimal robust model in certain sense. Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2502.16372.pdf' target='_blank'>https://arxiv.org/pdf/2502.16372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Liu, Huihua Zhao, Chenran Li, Yuchen Deng, Joydeep Biswas, Soha Pouya, Yan Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16372">COMPASS: Cross-embodiment Mobility Policy via Residual RL and Skill Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots are increasingly deployed in diverse application domains, enabling robust mobility across different embodiments has become a critical challenge. Classical mobility stacks, though effective on specific platforms, require extensive per-robot tuning and do not scale easily to new embodiments. Learning-based approaches, such as imitation learning (IL), offer alternatives, but face significant limitations on the need for high-quality demonstrations for each embodiment. To address these challenges, we introduce COMPASS, a unified framework that enables scalable cross-embodiment mobility using expert demonstrations from only a single embodiment. We first pre-train a mobility policy on a single robot using IL, combining a world model with a policy model. We then apply residual reinforcement learning (RL) to efficiently adapt this policy to diverse embodiments through corrective refinements. Finally, we distill specialist policies into a single generalist policy conditioned on an embodiment embedding vector. This design significantly reduces the burden of collecting data while enabling robust generalization across a wide range of robot designs. Our experiments demonstrate that COMPASS scales effectively across diverse robot platforms while maintaining adaptability to various environment configurations, achieving a generalist policy with a success rate approximately 5X higher than the pre-trained IL policy on unseen embodiments, and further demonstrates zero-shot sim-to-real transfer.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2412.14140.pdf' target='_blank'>https://arxiv.org/pdf/2412.14140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Darshan Deshpande, Selvan Sunitha Ravi, Sky CH-Wang, Bartosz Mielczarek, Anand Kannappan, Rebecca Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14140">GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The LLM-as-judge paradigm is increasingly being adopted for automated evaluation of model outputs. While LLM judges have shown promise on constrained evaluation tasks, closed source LLMs display critical shortcomings when deployed in real world applications due to challenges of fine grained metrics and explainability, while task specific evaluation models lack cross-domain generalization. We introduce GLIDER, a powerful 3B evaluator LLM that can score any text input and associated context on arbitrary user defined criteria. GLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly outperforms prior evaluation models, achieving comparable performance to LLMs 17x its size. GLIDER supports fine-grained scoring, multilingual reasoning, span highlighting and was trained on 685 domains and 183 criteria. Extensive qualitative analysis shows that GLIDER scores are highly correlated with human judgments, with 91.3% human agreement. We have open-sourced GLIDER to facilitate future research.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2410.22461.pdf' target='_blank'>https://arxiv.org/pdf/2410.22461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyusam Chang, Jiwon Lee, Donghyun Kim, Jinkyu Kim, Dongwook Lee, Daehyun Ji, Sujin Jang, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22461">Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in 3D object detection leveraging multi-view cameras have demonstrated their practical and economical value in various challenging vision tasks. However, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (\ie, direct transfer) due to the inevitable geometric misalignment between the source and target domains. In practice, we also encounter constraints on resources for training models and collecting annotations for the successful deployment of 3D object detectors. In this paper, we propose Unified Domain Generalization and Adaptation (UDGA), a practical solution to mitigate those drawbacks. We first propose Multi-view Overlap Depth Constraint that leverages the strong association between multi-view, significantly alleviating geometric gaps due to perspective view changes. Then, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (\ie, 1$\%$ and 5$\%)$, while preserving well-defined source knowledge for training efficiency. Overall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations. We demonstrate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, and Waymo, where our framework outperforms the current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2410.21698.pdf' target='_blank'>https://arxiv.org/pdf/2410.21698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, Sanjiv Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21698">On the Role of Depth and Looping for In-Context Learning with Task Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intriguing in-context learning (ICL) abilities of deep Transformer models have lately garnered significant attention. By studying in-context linear regression on unimodal Gaussian data, recent empirical and theoretical works have argued that ICL emerges from Transformers' abilities to simulate learning algorithms like gradient descent. However, these works fail to capture the remarkable ability of Transformers to learn multiple tasks in context. To this end, we study in-context learning for linear regression with diverse tasks, characterized by data covariance matrices with condition numbers ranging from $[1, Îº]$, and highlight the importance of depth in this setting. More specifically, (a) we show theoretical lower bounds of $\log(Îº)$ (or $\sqrtÎº$) linear attention layers in the unrestricted (or restricted) attention setting and, (b) we show that multilayer Transformers can indeed solve such tasks with a number of layers that matches the lower bounds. However, we show that this expressivity of multilayer Transformer comes at the price of robustness. In particular, multilayer Transformers are not robust to even distributional shifts as small as $O(e^{-L})$ in Wasserstein distance, where $L$ is the depth of the network. We then demonstrate that Looped Transformers -- a special class of multilayer Transformers with weight-sharing -- not only exhibit similar expressive power but are also provably robust under mild assumptions. Besides out-of-distribution generalization, we also show that Looped Transformers are the only models that exhibit a monotonic behavior of loss with respect to depth.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2601.14456.pdf' target='_blank'>https://arxiv.org/pdf/2601.14456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valerio Belcamino, Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.14456">On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2601.06803.pdf' target='_blank'>https://arxiv.org/pdf/2601.06803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yubo Wang, Juntian Zhang, Yichen Wu, Yankai Lin, Nils Lukas, Yuhan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06803">Forest Before Trees: Latent Superposition for Efficient Visual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a "Forest-before-Trees" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2601.06352.pdf' target='_blank'>https://arxiv.org/pdf/2601.06352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Song, Jiang Wu, Weijia Zhang, Chengze Shen, Shaofan Yuan, Weitao Lu, Jian Wang, Amir Rahmani, Nikil Dutt, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06352">CARD: Cluster-level Adaptation with Reward-guided Decoding for Personalized Text Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting large language models to individual users remains challenging due to the tension between fine-grained personalization and scalable deployment. We present CARD, a hierarchical framework that achieves effective personalization through progressive refinement. CARD first clusters users according to shared stylistic patterns and learns cluster-specific LoRA adapters, enabling robust generalization and strong low-resource performance. To capture individual differences within each cluster, we propose an implicit preference learning mechanism that contrasts user-authored text with cluster-level generations, allowing the model to infer user-specific style preferences without manual annotation. At inference time, CARD injects personalization exclusively at decoding via lightweight user preference vectors and low-rank logit corrections, while keeping the base model frozen. Experiments on the LaMP and LongLaMP benchmarks show that CARD achieves competitive or superior generation quality compared to state-of-the-art baselines, while significantly improving efficiency and scalability for practical personalized text generation.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2601.03717.pdf' target='_blank'>https://arxiv.org/pdf/2601.03717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Cui, Jiaqi Guo, Jiepeng Zhou, Ruixuan Yang, Jiayi Lu, Jiajun Xu, Jiangcheng Song, Boran Zhao, Pengju Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03717">MIND: From Passive Mimicry to Active Reasoning through Capability-Aware Multi-Perspective CoT Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Large Language Models (LLMs) have emerged with remarkable capabilities in complex tasks through Chain-of-Thought reasoning, practical resource constraints have sparked interest in transferring these abilities to smaller models. However, achieving both domain performance and cross-domain generalization remains challenging. Existing approaches typically restrict students to following a single golden rationale and treat different reasoning paths independently. Due to distinct inductive biases and intrinsic preferences, alongside the student's evolving capacity and reasoning preferences during training, a teacher's "optimal" rationale could act as out-of-distribution noise. This misalignment leads to a degeneration of the student's latent reasoning distribution, causing suboptimal performance. To bridge this gap, we propose MIND, a capability-adaptive framework that transitions distillation from passive mimicry to active cognitive construction. We synthesize diverse teacher perspectives through a novel "Teaching Assistant" network. By employing a Feedback-Driven Inertia Calibration mechanism, this network utilizes inertia-filtered training loss to align supervision with the student's current adaptability, effectively enhancing performance while mitigating catastrophic forgetting. Extensive experiments demonstrate that MIND achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks, and our sophisticated latent space analysis further confirms the mechanism of reasoning ability internalization.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2601.02126.pdf' target='_blank'>https://arxiv.org/pdf/2601.02126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Bou, Elliot Vincent, Gabriele Facciolo, Rafael Grompone von Gioi, Jean-Michel Morel, Thibaud Ehret
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02126">Remote Sensing Change Detection via Weak Temporal Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2511.23224.pdf' target='_blank'>https://arxiv.org/pdf/2511.23224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincenzo Lipardi, Domenica Dibenedetto, Georgios Stamoulis, Evert van Nieuwenburg, Mark H. M. Winands
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23224">Nonstabilizerness Estimation using Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article proposes a Graph Neural Network (GNN) approach to estimate nonstabilizerness in quantum circuits, measured by the stabilizer Rényi entropy (SRE). Nonstabilizerness is a fundamental resource for quantum advantage, and efficient SRE estimations are highly beneficial in practical applications. We address the nonstabilizerness estimation problem through three supervised learning formulations starting from easier classification tasks to the more challenging regression task. Experimental results show that the proposed GNN manages to capture meaningful features from the graph-based circuit representation, resulting in robust generalization performances achieved across diverse scenarios. In classification tasks, the GNN is trained on product states and generalizes on circuits evolved under Clifford operations, entangled states, and circuits with higher number of qubits. In the regression task, the GNN significantly improves the SRE estimation on out-of-distribution circuits with higher number of qubits and gate counts compared to previous work, for both random quantum circuits and structured circuits derived from the transverse-field Ising model. Moreover, the graph representation of quantum circuits naturally integrates hardware-specific information. Simulations on noisy quantum hardware highlight the potential of the proposed GNN to predict the SRE measured on quantum devices.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2511.13739.pdf' target='_blank'>https://arxiv.org/pdf/2511.13739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byung-Kwan Ko, Soowon Kim, Seo-Hyun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13739">Subject-Independent Imagined Speech Detection via Cross-Subject Generalization and Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving robust generalization across individuals remains a major challenge in electroencephalogram based imagined speech decoding due to substantial variability in neural activity patterns. This study examined how training dynamics and lightweight subject specific adaptation influence cross subject performance in a neural decoding framework. A cyclic inter subject training approach, involving shorter per subject training segments and frequent alternation among subjects, led to modest yet consistent improvements in decoding performance across unseen target data. Furthermore, under the subject calibrated leave one subject out scheme, incorporating only 10 % of the target subjects data for calibration achieved an accuracy of 0.781 and an AUC of 0.801, demonstrating the effectiveness of few shot adaptation. These findings suggest that integrating cyclic training with minimal calibration provides a simple and effective strategy for developing scalable, user adaptive brain computer interface systems that balance generalization and personalization.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2511.04439.pdf' target='_blank'>https://arxiv.org/pdf/2511.04439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anisha Garg, Ganesh Venkatesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04439">The Peril of Preference: Why GRPO fails on Ordinal Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Group-relative Policy Optimization's (GRPO) simplicity makes it highly desirable for adapting LLMs to become experts at specific tasks. But this simplicity also makes it ill-specified as we seek to enhance RL training with richer, non-binary feedback. When using ordinal rewards to give partial credit, GRPO's simplicity starts to hurt, as its group-average baseline often assigns a positive advantage to failed trajectories and reinforces incorrect behavior. We introduce Correctness Relative Policy Optimization (CoRPO), a new formulation that solves this flaw. CoRPO uses an adaptive baseline that enforces a minimum quality threshold, ensuring failed solutions are never positively reinforced. Once the policy consistently meets this threshold, the baseline automatically transitions to a relative preference mode, pushing the model to find optimal solutions rather than just "acceptable" ones. We empirically validate CoRPO on a code verification task, where it demonstrates more stable convergence and better out-of-domain generalization. This work represents a critical step in our broader research program to enable LLMs to learn genuinely new capabilities through reinforcement learning. We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback - progressing from binary to ordinal rewards in this work, and onward to denser, per-step supervision.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2510.21427.pdf' target='_blank'>https://arxiv.org/pdf/2510.21427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Liang, Shuqing Shi, Yudi Zhang, Biwei Huang, Yali Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21427">Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale networked systems, such as traffic, power, and wireless grids, challenge reinforcement-learning agents with both scale and environment shifts. To address these challenges, we propose GSAC (Generalizable and Scalable Actor-Critic), a framework that couples causal representation learning with meta actor-critic learning to achieve both scalability and domain generalization. Each agent first learns a sparse local causal mask that provably identifies the minimal neighborhood variables influencing its dynamics, yielding exponentially tight approximately compact representations (ACRs) of state and domain factors. These ACRs bound the error of truncating value functions to $κ$-hop neighborhoods, enabling efficient learning on graphs. A meta actor-critic then trains a shared policy across multiple source domains while conditioning on the compact domain factors; at test time, a few trajectories suffice to estimate the new domain factor and deploy the adapted policy. We establish finite-sample guarantees on causal recovery, actor-critic convergence, and adaptation gap, and show that GSAC adapts rapidly and significantly outperforms learning-from-scratch and conventional adaptation baselines.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2509.24980.pdf' target='_blank'>https://arxiv.org/pdf/2509.24980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuang Liang, Jing He, Chuanmeizhi Wang, Lejun Liao, Guo Zhang, Yingcong Chen, Yuan Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24980">SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained diffusion models provide rich multi-scale latent features and are emerging as powerful vision backbones. While recent works such as Marigold~\citep{ke2024repurposing} and Lotus~\citep{he2024lotus} adapt diffusion priors for dense prediction with strong cross-domain generalization, their potential for structured outputs (e.g., human pose estimation) remains underexplored. In this paper, we propose \textbf{SDPose}, a fine-tuning framework built upon Stable Diffusion to fully exploit pre-trained diffusion priors for human pose estimation. First, rather than modifying cross-attention modules or introducing learnable embeddings, we directly predict keypoint heatmaps in the SD U-Net's image latent space to preserve the original generative priors. Second, we map these latent features into keypoint heatmaps through a lightweight convolutional pose head, which avoids disrupting the pre-trained backbone. Finally, to prevent overfitting and enhance out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction branch that preserves domain-transferable generative semantics. To evaluate robustness under domain shift, we further construct \textbf{COCO-OOD}, a style-transferred variant of COCO with preserved annotations. With just one-fifth of the training schedule used by Sapiens on COCO, SDPose attains parity with Sapiens-1B/2B on the COCO validation set and establishes a new state of the art on the cross-domain benchmarks HumanArt and COCO-OOD. Furthermore, we showcase SDPose as a zero-shot pose annotator for downstream controllable generation tasks, including ControlNet-based image synthesis and video generation, where it delivers qualitatively superior pose guidance.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2509.22913.pdf' target='_blank'>https://arxiv.org/pdf/2509.22913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jake S. Rhodes, Adam G. Rustad, Marshall S. Nielsen, Morgan Chase McClellan, Dallan Gardner, Dawson Hedges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22913">Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manifold alignment (MA) involves a set of techniques for learning shared representations across domains, yet many traditional MA methods are incapable of performing out-of-sample extension, limiting their real-world applicability. We propose a guided representation learning framework leveraging a geometry-regularized twin autoencoder (AE) architecture to enhance MA while enabling generalization to unseen data. Our method enforces structured cross-modal mappings to maintain geometric fidelity in learned embeddings. By incorporating a pre-trained alignment model and a multitask learning formulation, we improve cross-domain generalization and representation robustness while maintaining alignment fidelity. We evaluate our approach using several MA methods, showing improvements in embedding consistency, information preservation, and cross-domain transfer. Additionally, we apply our framework to Alzheimer's disease diagnosis, demonstrating its ability to integrate multi-modal patient data and enhance predictive accuracy in cases limited to a single domain by leveraging insights from the multi-modal problem.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2509.22412.pdf' target='_blank'>https://arxiv.org/pdf/2509.22412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Kashiani, Niloufar Alipour Talemi, Fatemeh Afghah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22412">FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake detectors often struggle to generalize to novel forgery types due to biases learned from limited training data. In this paper, we identify a new type of model bias in the frequency domain, termed spectral bias, where detectors overly rely on specific frequency bands, restricting their ability to generalize across unseen forgeries. To address this, we propose FreqDebias, a frequency debiasing framework that mitigates spectral bias through two complementary strategies. First, we introduce a novel Forgery Mixup (Fo-Mixup) augmentation, which dynamically diversifies frequency characteristics of training samples. Second, we incorporate a dual consistency regularization (CR), which enforces both local consistency using class activation maps (CAMs) and global consistency through a von Mises-Fisher (vMF) distribution on a hyperspherical embedding space. This dual CR mitigates over-reliance on certain frequency components by promoting consistent representation learning under both local and global supervision. Extensive experiments show that FreqDebias significantly enhances cross-domain generalization and outperforms state-of-the-art methods in both cross-domain and in-domain settings.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2509.12400.pdf' target='_blank'>https://arxiv.org/pdf/2509.12400.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongkun Zhu, Kangning Cui, Wei Tang, Rui-Feng Wang, Sarra Alqahtani, David Lutz, Fan Yang, Paul Fine, Jordan Karubian, Robert Plemmons, Jean-Michel Morel, Victor Pauca, Miles Silman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12400">From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate mapping of individual trees is essential for ecological monitoring and forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs) is widely used, but stitching artifacts and heavy preprocessing limit its suitability for field deployment. This study explores the use of raw UAV imagery for palm detection and crown-center localization in tropical forests. Two research questions are addressed: (1) how detection performance varies across orthomosaic and raw imagery, including within-domain and cross-domain transfer, and (2) to what extent crown-center annotations improve localization accuracy beyond bounding-box centroids. Using state-of-the-art detectors and keypoint models, we show that raw imagery yields superior performance in deployment-relevant scenarios, while orthomosaics retain value for robust cross-domain generalization. Incorporating crown-center annotations in training further improves localization and provides precise tree positions for downstream ecological analyses. These findings offer practical guidance for UAV-based biodiversity and conservation monitoring.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2509.08570.pdf' target='_blank'>https://arxiv.org/pdf/2509.08570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjun Yu, Yinchen Zhou, Jia-Xuan Jiang, Shubin Zeng, Yuee Li, Zhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08570">Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal models have achieved remarkable success in natural image segmentation, yet they often underperform when applied to the medical domain. Through extensive study, we attribute this performance gap to the challenges of multimodal fusion, primarily the significant semantic gap between abstract textual prompts and fine-grained medical visual features, as well as the resulting feature dispersion. To address these issues, we revisit the problem from the perspective of semantic aggregation. Specifically, we propose an Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel Decoder. The former mitigates feature dispersion by dynamically clustering features into compact semantic centers to enhance cross-modal correspondence. The latter is designed to bridge the semantic gap by leveraging domain-invariant textual knowledge to effectively guide deep visual representations. The synergy between these two mechanisms significantly improves the model's generalization ability. Extensive experiments on public cardiac and fundus datasets demonstrate that our method consistently outperforms existing SOTA approaches across multiple domain generalization benchmarks.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2508.17102.pdf' target='_blank'>https://arxiv.org/pdf/2508.17102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjie Jiang, Yunqi Zhou, Jiafeng Yan, Jing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17102">GRASP: Geospatial pixel Reasoning viA Structured Policy learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geospatial pixel reasoning is a nascent remote-sensing task that aims to generate segmentation masks directly from natural-language instructions. Prevailing MLLM-based systems co-train a language model and a mask decoder with dense pixel supervision, which is expensive and often weak on out-of-domain (OOD) data. We introduce GRASP, a structured policy-learning framework. In our design, a multimodal large language model first emits task-relevant bounding boxes and positive points from a vision-language instruction. These outputs are then passed to a pre-trained segmentation model, which consumes them as prompts to generate the final mask. Instead of supervised fine-tuning, we optimize the system purely with reinforcement learning: the model is trained solely with GRPO, guided by format rewards and accuracy rewards computed on boxes and points (no mask supervision). This leverages strong priors in foundation models, minimizes trainable parameters, and enables learning from inexpensive annotations. We additionally curate GRASP-1k, which contains reasoning-intensive queries, detailed reasoning traces, and fine-grained segmentation annotations. Evaluations on both in-domain and out-of-domain test sets show state-of-the-art results: about 4% improvement in-domain and up to 54% on OOD benchmarks. The experiment results evidence our model's robust generalization and demonstrate that complex geospatial segmentation behaviors can be learned via RL from weak spatial cues. Code and the dataset will be released open-source.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2508.14079.pdf' target='_blank'>https://arxiv.org/pdf/2508.14079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Heuillet, Rishika Bhagwatkar, Jonas NgnawÃ©, Yann Pequignot, Alexandre Larouche, Christian GagnÃ©, Irina Rish, Ola Ahmad, Audrey Durand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14079">A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models operating in the image domain are vulnerable to small input perturbations. For years, robustness to such perturbations was pursued by training models from scratch (i.e., with random initializations) using specialized loss objectives. Recently, robust fine-tuning has emerged as a more efficient alternative: instead of training from scratch, pretrained models are adapted to maximize predictive performance and robustness. To conduct robust fine-tuning, practitioners design an optimization strategy that includes the model update protocol (e.g., full or partial) and the specialized loss objective. Additional design choices include the architecture type and size, and the pretrained representation. These design choices affect robust generalization, which is the model's ability to maintain performance when exposed to new and unseen perturbations at test time. Understanding how these design choices influence generalization remains an open question with significant practical implications. In response, we present an empirical study spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3 adaptation protocols, yielding 1,440 training configurations and 7,200 robustness measurements across five perturbation types. To our knowledge, this is the most diverse and comprehensive benchmark of robust fine-tuning to date. While attention-based architectures and robust pretrained representations are increasingly popular, we find that convolutional neural networks pretrained in a supervised manner on large datasets often perform best. Our analysis both confirms and challenges prior design assumptions, highlighting promising research directions and offering practical guidance.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2508.08030.pdf' target='_blank'>https://arxiv.org/pdf/2508.08030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Peng, Yuanyuan Zhang, Steve Jiang, Robert Timmerman, John Minna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08030">Exploring Strategies for Personalized Radiation Therapy: Part III Identifying genetic determinants for Radiation Response with Meta Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radiation response in cancer is shaped by complex, patient specific biology, yet current treatment strategies often rely on uniform dose prescriptions without accounting for tumor heterogeneity. In this study, we introduce a meta learning framework for one-shot prediction of radiosensitivity measured by SF2 using cell line level gene expression data. Unlike the widely used Radiosensitivity Index RSI a rank-based linear model trained on a fixed 10-gene signature, our proposed meta-learned model allows the importance of each gene to vary by sample through fine tuning. This flexibility addresses key limitations of static models like RSI, which assume uniform gene contributions across tumor types and discard expression magnitude and gene gene interactions. Our results show that meta learning offers robust generalization to unseen samples and performs well in tumor subgroups with high radiosensitivity variability, such as adenocarcinoma and large cell carcinoma. By learning transferable structure across tasks while preserving sample specific adaptability, our approach enables rapid adaptation to individual samples, improving predictive accuracy across diverse tumor subtypes while uncovering context dependent patterns of gene influence that may inform personalized therapy.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2507.22792.pdf' target='_blank'>https://arxiv.org/pdf/2507.22792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoping Xu, Jayaram K. Udupa, Yajun Yu, Hua-Chieh Shao, Songlin Zhao, Wei Liu, You Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22792">Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Object Segmentation and Tracking (VOST) presents a complex yet critical challenge in computer vision, requiring robust integration of segmentation and tracking across temporally dynamic frames. Traditional methods have struggled with domain generalization, temporal consistency, and computational efficiency. The emergence of foundation models like the Segment Anything Model (SAM) and its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven segmentation with strong generalization capabilities. Building upon these advances, this survey provides a comprehensive review of SAM/SAM2-based methods for VOST, structured along three temporal dimensions: past, present, and future. We examine strategies for retaining and updating historical information (past), approaches for extracting and optimizing discriminative features from the current frame (present), and motion prediction and trajectory estimation mechanisms for anticipating object dynamics in subsequent frames (future). In doing so, we highlight the evolution from early memory-based architectures to the streaming memory and real-time segmentation capabilities of SAM2. We also discuss recent innovations such as motion-aware memory selection and trajectory-guided prompting, which aim to enhance both accuracy and efficiency. Finally, we identify remaining challenges including memory redundancy, error accumulation, and prompt inefficiency, and suggest promising directions for future research. This survey offers a timely and structured overview of the field, aiming to guide researchers and practitioners in advancing the state of VOST through the lens of foundation models.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2507.21530.pdf' target='_blank'>https://arxiv.org/pdf/2507.21530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming-Hui Liu, Harry Cheng, Xin Luo, Xin-Shun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21530">Suppressing Gradient Conflict for Generalizable Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust deepfake detection models must be capable of generalizing to ever-evolving manipulation techniques beyond training data. A promising strategy is to augment the training data with online synthesized fake images containing broadly generalizable artifacts. However, in the context of deepfake detection, it is surprising that jointly training on both original and online synthesized forgeries may result in degraded performance. This contradicts the common belief that incorporating more source-domain data should enhance detection accuracy. Through empirical analysis, we trace this degradation to gradient conflicts during backpropagation which force a trade-off between source domain accuracy and target domain generalization. To overcome this issue, we propose a Conflict-Suppressed Deepfake Detection (CS-DFD) framework that explicitly mitigates the gradient conflict via two synergistic modules. First, an Update Vector Search (UVS) module searches for an alternative update vector near the initial gradient vector to reconcile the disparities of the original and online synthesized forgeries. By further transforming the search process into an extremum optimization problem, UVS yields the uniquely update vector, which maximizes the simultaneous loss reductions for each data type. Second, a Conflict Gradient Reduction (CGR) module enforces a low-conflict feature embedding space through a novel Conflict Descent Loss. This loss penalizes misaligned gradient directions and guides the learning of representations with aligned, non-conflicting gradients. The synergy of UVS and CGR alleviates gradient interference in both parameter optimization and representation learning. Experiments on multiple deepfake benchmarks demonstrate that CS-DFD achieves state-of-the-art performance in both in-domain detection accuracy and cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2507.20907.pdf' target='_blank'>https://arxiv.org/pdf/2507.20907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongun Ryu, Heon Song, Seungeun Lee, Soo Ick Cho, Jiwon Shin, Kyunghyun Paeng, SÃ©rgio Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20907">SCORPION: Addressing Scanner-Induced Variability in Histopathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring reliable model performance across diverse domains is a critical challenge in computational pathology. A particular source of variability in Whole-Slide Images is introduced by differences in digital scanners, thus calling for better scanner generalization. This is critical for the real-world adoption of computational pathology, where the scanning devices may differ per institution or hospital, and the model should not be dependent on scanner-induced details, which can ultimately affect the patient's diagnosis and treatment planning. However, past efforts have primarily focused on standard domain generalization settings, evaluating on unseen scanners during training, without directly evaluating consistency across scanners for the same tissue. To overcome this limitation, we introduce SCORPION, a new dataset explicitly designed to evaluate model reliability under scanner variability. SCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding 2,400 spatially aligned patches. This scanner-paired design allows for the isolation of scanner-induced variability, enabling a rigorous evaluation of model consistency while controlling for differences in tissue composition. Furthermore, we propose SimCons, a flexible framework that combines augmentation-based domain generalization techniques with a consistency loss to explicitly address scanner generalization. We empirically show that SimCons improves model consistency on varying scanners without compromising task-specific performance. By releasing the SCORPION dataset and proposing SimCons, we provide the research community with a crucial resource for evaluating and improving model consistency across diverse scanners, setting a new standard for reliability testing.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2506.23577.pdf' target='_blank'>https://arxiv.org/pdf/2506.23577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanning Hou, Yanran Ruan, Junfa Li, Shanshan Wang, Jianfeng Qiu, Ke Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23577">StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing the alignment between text and image features in the CLIP model is a critical challenge in zero-shot industrial anomaly detection tasks. Recent studies predominantly utilize specific category prompts during pretraining, which can cause overfitting to the training categories and limit model generalization. To address this, we propose a method that transforms category names through multicategory name stacking to create stacked prompts, forming the basis of our StackCLIP model. Our approach introduces two key components. The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts by stacking semantically analogous categories, while utilizing multi-object textual feature fusion to amplify discriminative anomalies among similar objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific linear layers tailored for each stack cluster and adaptively integrates them based on the attributes of test categories. These modules work together to deliver superior training speed, stability, and convergence, significantly boosting anomaly segmentation performance. Additionally, our stacked prompt framework offers robust generalization across classification tasks. To further improve performance, we introduce the Regulating Prompt Learning (RPL) module, which leverages the generalization power of stacked prompts to refine prompt learning, elevating results in anomaly detection classification tasks. Extensive testing on seven industrial anomaly detection datasets demonstrates that our method achieves state-of-the-art performance in both zero-shot anomaly detection and segmentation tasks.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2506.16029.pdf' target='_blank'>https://arxiv.org/pdf/2506.16029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenting Qi, Fan Nie, Alexandre Alahi, James Zou, Himabindu Lakkaraju, Yilun Du, Eric Xing, Sham Kakade, Hanlin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16029">EvoLM: In Search of Lost Language Model Training Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2506.01121.pdf' target='_blank'>https://arxiv.org/pdf/2506.01121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob K. Christopher, Michael Cardei, Jinhao Liang, Ferdinando Fioretto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01121">Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable generative capabilities of diffusion models, their integration into safety-critical or scientifically rigorous applications remains hindered by the need to ensure compliance with stringent physical, structural, and operational constraints. To address this challenge, this paper introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves diffusion steps with symbolic optimization, enabling the generation of certifiably consistent samples under user-defined functional and logic constraints. This key feature is provided for both standard and discrete diffusion models, enabling, for the first time, the generation of both continuous (e.g., images and trajectories) and discrete (e.g., molecular structures and natural language) outputs that comply with constraints. This ability is demonstrated on tasks spanning three key challenges: (1) Safety, in the context of non-toxic molecular generation and collision-free trajectory optimization; (2) Data scarcity, in domains such as drug discovery and materials engineering; and (3) Out-of-domain generalization, where enforcing symbolic constraints allows adaptation beyond the training distribution.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2506.00649.pdf' target='_blank'>https://arxiv.org/pdf/2506.00649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neil De La Fuente, Oscar Sainz, Iker GarcÃ­a-Ferrero, Eneko Agirre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00649">GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2505.19373.pdf' target='_blank'>https://arxiv.org/pdf/2505.19373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niloufar Alipour Talemi, Hossein Kashiani, Hossein R. Nowdeh, Fatemeh Afghah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19373">DiSa: Directional Saliency-Aware Prompt Learning for Generalizable Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has emerged as a powerful paradigm for adapting vision-language models such as CLIP to downstream tasks. However, existing methods often overfit to seen data, leading to significant performance degradation when generalizing to novel classes or unseen domains. To address this limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning framework that integrates two complementary regularization strategies to enhance generalization. First, our Cross-Interactive Regularization (CIR) fosters cross-modal alignment by enabling cooperative learning between prompted and frozen encoders. Within CIR, a saliency-aware masking strategy guides the image encoder to prioritize semantically critical image regions, reducing reliance on less informative patches. Second, we introduce a directional regularization strategy that aligns visual embeddings with class-wise prototype features in a directional manner to prioritize consistency in feature orientation over strict proximity. This approach ensures robust generalization by leveraging stable prototype directions derived from class-mean statistics. Extensive evaluations on 11 diverse image classification benchmarks demonstrate that DiSa consistently outperforms state-of-the-art prompt learning methods across various settings, including base-to-novel generalization, cross-dataset transfer, domain generalization, and few-shot learning.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2505.18884.pdf' target='_blank'>https://arxiv.org/pdf/2505.18884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Borna Khodabandeh, Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, Sanjay Lall, Sajjad Amini, Seyed-Mohsen Moosavi-Dezfooli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18884">LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual encoders have become fundamental components in modern computer vision pipelines. However, ensuring robustness against adversarial perturbations remains a critical challenge. Recent efforts have explored both supervised and unsupervised adversarial fine-tuning strategies. We identify two key limitations in these approaches: (i) they often suffer from instability, especially during the early stages of fine-tuning, resulting in suboptimal convergence and degraded performance on clean data, and (ii) they exhibit a suboptimal trade-off between robustness and clean data accuracy, hindering the simultaneous optimization of both objectives. To overcome these challenges, we propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised adversarial fine-tuning framework. LORE utilizes constrained optimization, which offers a principled approach to balancing competing goals, such as improving robustness while preserving nominal performance. By enforcing embedding-space proximity constraints, LORE effectively maintains clean data performance throughout adversarial fine-tuning. Extensive experiments show that LORE significantly improves zero-shot adversarial robustness with minimal degradation in clean data accuracy. Furthermore, we demonstrate the effectiveness of the adversarially fine-tuned CLIP image encoder in out-of-distribution generalization and enhancing the interpretability of image embeddings.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2505.14808.pdf' target='_blank'>https://arxiv.org/pdf/2505.14808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soo Min Kwon, Alec S. Xu, Can Yaras, Laura Balzano, Qing Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14808">Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work aims to demystify the out-of-distribution (OOD) capabilities of in-context learning (ICL) by studying linear regression tasks parameterized with low-rank covariance matrices. With such a parameterization, we can model distribution shifts as a varying angle between the subspace of the training and testing covariance matrices. We prove that a single-layer linear attention model incurs a test risk with a non-negligible dependence on the angle, illustrating that ICL is not robust to such distribution shifts. However, using this framework, we also prove an interesting property of ICL: when trained on task vectors drawn from a union of low-dimensional subspaces, ICL can generalize to any subspace within their span, given sufficiently long prompt lengths. This suggests that the OOD generalization ability of Transformers may actually stem from the new task lying within the span of those encountered during training. We empirically show that our results also hold for models such as GPT-2, and conclude with (i) experiments on how our observations extend to nonlinear function classes and (ii) results on how LoRA has the ability to capture distribution shifts.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2505.07209.pdf' target='_blank'>https://arxiv.org/pdf/2505.07209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Xie, Zequn Zeng, Hao Zhang, Yucheng Ding, Yi Wang, Zhengjue Wang, Bo Chen, Hongwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07209">Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Concept Bottleneck Models (CBMs) try to make the decision-making process transparent by exploring an intermediate concept space between the input image and the output prediction. Existing CBMs just learn coarse-grained relations between the whole image and the concepts, less considering local image information, leading to two main drawbacks: i) they often produce spurious visual-concept relations, hence decreasing model reliability; and ii) though CBMs could explain the importance of every concept to the final prediction, it is still challenging to tell which visual region produces the prediction. To solve these problems, this paper proposes a Disentangled Optimal Transport CBM (DOT-CBM) framework to explore fine-grained visual-concept relations between local image patches and concepts. Specifically, we model the concept prediction process as a transportation problem between the patches and concepts, thereby achieving explicit fine-grained feature alignment. We also incorporate orthogonal projection losses within the modality to enhance local feature disentanglement. To further address the shortcut issues caused by statistical biases in the data, we utilize the visual saliency map and concept label statistics as transportation priors. Thus, DOT-CBM can visualize inversion heatmaps, provide more reliable concept predictions, and produce more accurate class predictions. Comprehensive experiments demonstrate that our proposed DOT-CBM achieves SOTA performance on several tasks, including image classification, local part detection and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2503.02988.pdf' target='_blank'>https://arxiv.org/pdf/2503.02988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Xu, Bin Shi, Zhen Peng, Huixiang Liu, Bo Dong, Chen Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02988">Out-of-Distribution Generalization on Graphs via Progressive Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development and evaluation of graph neural networks (GNNs) generally follow the independent and identically distributed (i.i.d.) assumption. Yet this assumption is often untenable in practice due to the uncontrollable data generation mechanism. In particular, when the data distribution shows a significant shift, most GNNs would fail to produce reliable predictions and may even make decisions randomly. One of the most promising solutions to improve the model generalization is to pick out causal invariant parts in the input graph. Nonetheless, we observe a significant distribution gap between the causal parts learned by existing methods and the ground truth, leading to undesirable performance. In response to the above issues, this paper presents GPro, a model that learns graph causal invariance with progressive inference. Specifically, the complicated graph causal invariant learning is decomposed into multiple intermediate inference steps from easy to hard, and the perception of GPro is continuously strengthened through a progressive inference process to extract causal features that are stable to distribution shifts. We also enlarge the training distribution by creating counterfactual samples to enhance the capability of the GPro in capturing the causal invariant parts. Extensive experiments demonstrate that our proposed GPro outperforms the state-of-the-art methods by 4.91% on average. For datasets with more severe distribution shifts, the performance improvement can be up to 6.86%.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2501.17871.pdf' target='_blank'>https://arxiv.org/pdf/2501.17871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Mishra, David Joffe, Sankara Surendra Telidevara, David S Oakley, Anqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17871">On the challenges of detecting MCI using EEG in the wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies have shown promising results in the detection of Mild Cognitive Impairment (MCI) using easily accessible Electroencephalogram (EEG) data which would help administer early and effective treatment for dementia patients. However, the reliability and practicality of such systems remains unclear. In this work, we investigate the potential limitations and challenges in developing a robust MCI detection method using two contrasting datasets: 1) CAUEEG, collected and annotated by expert neurologists in controlled settings and 2) GENEEG, a new dataset collected and annotated in general practice clinics, a setting where routine MCI diagnoses are typically made. We find that training on small datasets, as is done by most previous works, tends to produce high variance models that make overconfident predictions, and are unreliable in practice. Additionally, distribution shifts between datasets make cross-domain generalization challenging. Finally, we show that MCI detection using EEG may suffer from fundamental limitations because of the overlapping nature of feature distributions with control groups. We call for more effort in high-quality data collection in actionable settings (like general practice clinics) to make progress towards this salient goal of non-invasive MCI detection.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2501.15486.pdf' target='_blank'>https://arxiv.org/pdf/2501.15486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Gupta, Vinay Sutar, Varunav Singh, Amit Sethi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15486">FedAlign: Federated Domain Generalization with Cross-Client Feature Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) offers a decentralized paradigm for collaborative model training without direct data sharing, yet it poses unique challenges for Domain Generalization (DG), including strict privacy constraints, non-i.i.d. local data, and limited domain diversity. We introduce FedAlign, a lightweight, privacy-preserving framework designed to enhance DG in federated settings by simultaneously increasing feature diversity and promoting domain invariance. First, a cross-client feature extension module broadens local domain representations through domain-invariant feature perturbation and selective cross-client feature transfer, allowing each client to safely access a richer domain space. Second, a dual-stage alignment module refines global feature learning by aligning both feature embeddings and predictions across clients, thereby distilling robust, domain-invariant features. By integrating these modules, our method achieves superior generalization to unseen domains while maintaining data privacy and operating with minimal computational and communication overhead.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2501.14605.pdf' target='_blank'>https://arxiv.org/pdf/2501.14605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jules Sanchez, Jean-Emmanuel Deschaud, FranÃ§ois Goulette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14605">3DLabelProp: Geometric-Driven Domain Generalization for LiDAR Semantic Segmentation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to find ways for deep learning models to maintain their performance despite significant domain shifts between training and inference datasets. This is particularly important for models that need to be robust or are costly to train. LiDAR perception in autonomous driving is impacted by both of these concerns, leading to the emergence of various approaches. This work addresses the challenge by proposing a geometry-based approach, leveraging the sequential structure of LiDAR sensors, which sets it apart from the learning-based methods commonly found in the literature. The proposed method, called 3DLabelProp, is applied on the task of LiDAR Semantic Segmentation (LSS). Through extensive experimentation on seven datasets, it is demonstrated to be a state-of-the-art approach, outperforming both naive and other domain generalization methods.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2501.13273.pdf' target='_blank'>https://arxiv.org/pdf/2501.13273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaojie Jin, Sihao Wu, Jiaxu Liu, Tianjin Huang, Ronghui Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13273">Enhancing Robust Fairness via Confusional Spectral Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has highlighted a critical issue known as ``robust fairness", where robust accuracy varies significantly across different classes, undermining the reliability of deep neural networks (DNNs). A common approach to address this has been to dynamically reweight classes during training, giving more weight to those with lower empirical robust performance. However, we find there is a divergence of class-wise robust performance between training set and testing set, which limits the effectiveness of these explicit reweighting methods, indicating the need for a principled alternative. In this work, we derive a robust generalization bound for the worst-class robust error within the PAC-Bayesian framework, accounting for unknown data distributions. Our analysis shows that the worst-class robust error is influenced by two main factors: the spectral norm of the empirical robust confusion matrix and the information embedded in the model and training set. While the latter has been extensively studied, we propose a novel regularization technique targeting the spectral norm of the robust confusion matrix to improve worst-class robust accuracy and enhance robust fairness. We validate our approach through comprehensive experiments on various datasets and models, demonstrating its effectiveness in enhancing robust fairness.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2412.12793.pdf' target='_blank'>https://arxiv.org/pdf/2412.12793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shizhuo Deng, Bowen Han, Jiaqi Chen, Hao Wang, Dongyue Chen, Tong Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12793">CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Noisy labels threaten the robustness of few-shot learning (FSL) due to the inexact features in a new domain. CLIP, a large-scale vision-language model, performs well in FSL on image-text embedding similarities, but it is susceptible to misclassification caused by noisy labels. How to enhance domain generalization of CLIP on noisy data within FSL tasks is a critical challenge. In this paper, we provide a novel view to mitigate the influence of noisy labels, CLIP-based Robust Few-shot learning (CRoF). CRoF is a general plug-in module for CLIP-based models. To avoid misclassification and confused label embedding, we design the few-shot task-oriented prompt generator to give more discriminative descriptions of each category. The proposed prompt achieves larger distances of inter-class textual embedding. Furthermore, rather than fully trusting zero-shot classification by CLIP, we fine-tune CLIP on noisy few-shot data in a new domain with a weighting strategy like label-smooth. The weights for multiple potentially correct labels consider the relationship between CLIP's prior knowledge and original label information to ensure reliability. Our multiple label loss function further supports robust training under this paradigm. Comprehensive experiments show that CRoF, as a plug-in, outperforms fine-tuned and vanilla CLIP models on different noise types and noise ratios.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2411.16018.pdf' target='_blank'>https://arxiv.org/pdf/2411.16018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niloufar Alipour Talemi, Hossein Kashiani, Fatemeh Afghah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16018">Style-Pro: Style-Guided Prompt Learning for Generalizable Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained Vision-language (VL) models, such as CLIP, have shown significant generalization ability to downstream tasks, even with minimal fine-tuning. While prompt learning has emerged as an effective strategy to adapt pre-trained VL models for downstream tasks, current approaches frequently encounter severe overfitting to specific downstream data distributions. This overfitting constrains the original behavior of the VL models to generalize to new domains or unseen classes, posing a critical challenge in enhancing the adaptability and generalization of VL models. To address this limitation, we propose Style-Pro, a novel style-guided prompt learning framework that mitigates overfitting and preserves the zero-shot generalization capabilities of CLIP. Style-Pro employs learnable style bases to synthesize diverse distribution shifts, guided by two specialized loss functions that ensure style diversity and content integrity. Then, to minimize discrepancies between unseen domains and the source domain, Style-Pro maps the unseen styles into the known style representation space as a weighted combination of style bases. Moreover, to maintain consistency between the style-shifted prompted model and the original frozen CLIP, Style-Pro introduces consistency constraints to preserve alignment in the learned embeddings, minimizing deviation during adaptation to downstream tasks. Extensive experiments across 11 benchmark datasets demonstrate the effectiveness of Style-Pro, consistently surpassing state-of-the-art methods in various settings, including base-to-new generalization, cross-dataset transfer, and domain generalization.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2410.07719.pdf' target='_blank'>https://arxiv.org/pdf/2410.07719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Xu, Xiao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07719">Understanding Adversarially Robust Generalization via Weight-Curvature Index</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite extensive research on adversarial examples, the underlying mechanisms of adversarially robust generalization, a critical yet challenging task for deep learning, remain largely unknown. In this work, we propose a novel perspective to decipher adversarially robust generalization through the lens of the Weight-Curvature Index (WCI). The proposed WCI quantifies the vulnerability of models to adversarial perturbations using the Frobenius norm of weight matrices and the trace of Hessian matrices. We prove generalization bounds based on PAC-Bayesian theory and second-order loss function approximations to elucidate the interplay between robust generalization gap, model parameters, and loss landscape curvature. Our theory and experiments show that WCI effectively captures the robust generalization performance of adversarially trained models. By offering a nuanced understanding of adversarial robustness based on the scale of model parameters and the curvature of the loss landscape, our work provides crucial insights for designing more resilient deep learning models, enhancing their reliability and security.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2409.08589.pdf' target='_blank'>https://arxiv.org/pdf/2409.08589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilyass Moummad, Romain Serizel, Emmanouil Benetos, Nicolas Farrugia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08589">Domain-Invariant Representation Learning of Bird Sounds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Passive acoustic monitoring (PAM) is crucial for bioacoustic research, enabling non-invasive species tracking and biodiversity monitoring. Citizen science platforms provide large annotated datasets from focal recordings, where the target species is intentionally recorded. However, PAM requires monitoring in passive soundscapes, creating a domain shift between focal and passive recordings, challenging deep learning models trained on focal recordings. To address domain generalization, we leverage supervised contrastive learning by enforcing domain invariance across same-class examples from different domains. Additionally, we propose ProtoCLR, an alternative to SupCon loss which reduces the computational complexity by comparing examples to class prototypes instead of pairwise comparisons. We conduct few-shot classification based on BIRB, a large-scale bird sound benchmark to assess pre-trained bioacoustic models. Our findings suggest that ProtoCLR is a better alternative to SupCon.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2601.20355.pdf' target='_blank'>https://arxiv.org/pdf/2601.20355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Liang, Jiatong Du, Ziyi Yang, Yanjun Huang, Hong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20355">CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2601.17557.pdf' target='_blank'>https://arxiv.org/pdf/2601.17557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aref Farhadipour, Ming Jin, Valeriia Vyshnevetska, Xiyang Li, Elisa Pellegrino, Srikanth Madikeri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17557">Spoofing-Aware Speaker Verification via Wavelet Prompt Tuning and Multi-Model Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes the UZH-CL system submitted to the SASV section of the WildSpoof 2026 challenge. The challenge focuses on the integrated defense against generative spoofing attacks by requiring the simultaneous verification of speaker identity and audio authenticity. We proposed a cascaded Spoofing-Aware Speaker Verification framework that integrates a Wavelet Prompt-Tuned XLSR-AASIST countermeasure with a multi-model ensemble. The ASV component utilizes the ResNet34, ResNet293, and WavLM-ECAPA-TDNN architectures, with Z-score normalization followed by score averaging. Trained on VoxCeleb2 and SpoofCeleb, the system obtained a Macro a-DCF of 0.2017 and a SASV EER of 2.08%. While the system achieved a 0.16% EER in spoof detection on the in-domain data, results on unseen datasets, such as the ASVspoof5, highlight the critical challenge of cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2601.15284.pdf' target='_blank'>https://arxiv.org/pdf/2601.15284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anurag Bagchi, Zhipeng Bao, Homanga Bharadhwaj, Yu-Xiong Wang, Pavel Tokmakov, Martial Hebert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15284">Walk through Paintings: Egocentric World Models from Internet Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2601.12111.pdf' target='_blank'>https://arxiv.org/pdf/2601.12111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wyatt McCurdy, Xin Zhang, Yuqi Song, Min Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12111">RCDN: Real-Centered Detection Network for Robust Face Forgery Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image forgery has become a critical threat with the rapid proliferation of AI-based generation tools, which make it increasingly easy to synthesize realistic but fraudulent facial content. Existing detection methods achieve near-perfect performance when training and testing are conducted within the same domain, yet their effectiveness deteriorates substantially in crossdomain scenarios. This limitation is problematic, as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations. To address this challenge, we propose the Real-Centered Detection Network (RCDN), a frequency spatial convolutional neural networks(CNN) framework with an Xception backbone that anchors its representation space around authentic facial images. Instead of modeling the diverse and evolving patterns of forgeries, RCDN emphasizes the consistency of real images, leveraging a dual-branch architecture and a real centered loss design to enhance robustness under distribution shifts. Extensive experiments on the DiFF dataset, focusing on three representative forgery types (FE, I2I, T2I), demonstrate that RCDN achieves both state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Notably, RCDN reduces the generalization gap compared to leading baselines and achieves the highest cross/in-domain stability ratio, highlighting its potential as a practical solution for defending against evolving and unseen image forgery techniques.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2601.05955.pdf' target='_blank'>https://arxiv.org/pdf/2601.05955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Chen, Xi Lin, Jun Wu, Xiangrui Cai, Qiaolun Zhang, Xichun Fan, Jiapeng Xu, Xiu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05955">Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead. To address these challenges, this paper presents a new FDG framework, dubbed FaST-PT, which facilitates local feature augmentation and efficient unseen domain adaptation in a distributed manner. First, we propose a lightweight Multi-Modal Style Transfer (MST) method to transform image embedding under text supervision, which could expand the training data distribution and mitigate domain shift. We then design a dual-prompt module that decomposes the prompt into global and domain prompts. Specifically, global prompts capture general knowledge from augmented embedding across clients, while domain prompts capture domain-specific knowledge from local data. Besides, Domain-aware Prompt Generation (DPG) is introduced to adaptively generate suitable prompts for each sample, which facilitates unseen domain adaptation through knowledge fusion. Extensive experiments on four cross-domain benchmark datasets, e.g., PACS and DomainNet, demonstrate the superior performance of FaST-PT over SOTA FDG methods such as FedDG-GA and DiPrompt. Ablation studies further validate the effectiveness and efficiency of FaST-PT.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2601.02414.pdf' target='_blank'>https://arxiv.org/pdf/2601.02414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jichao Zhu, Jun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02414">MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2512.06530.pdf' target='_blank'>https://arxiv.org/pdf/2512.06530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Wattad, Tamir Shor, Alex Bronstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06530">On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2512.01510.pdf' target='_blank'>https://arxiv.org/pdf/2512.01510.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franz Thaler, Martin Urschler, Mateusz Kozinski, Matthias AF Gsell, Gernot Plank, Darko Stern
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01510">Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation. To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM. During training, our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware, where we make a step towards closing the domain gap in this even more challenging setting. Overall, our evaluation shows that SRCSM can be considered a new state-of-the-art in DG for medical image segmentation and, moreover, even achieves a segmentation performance that matches the performance of the in-domain baseline in several settings.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2512.00940.pdf' target='_blank'>https://arxiv.org/pdf/2512.00940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00940">Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds. Remarkably, they preserve knowledge of once-learned environments sans catastrophic forgetting, a phenomenon neuroscientists hypothesize, is due to a singular neural circuitry dynamically overlayed by neuromodulatory agents such as dopamine and acetylcholine. In parallel, deep learning research addresses analogous challenges via domain generalization (DG) and continual learning (CL), yet these methods remain siloed, despite the brains ability to perform them seamlessly. In particular, prior work has not explored architectures involving associative memories (AMs), which are an integral part of biological systems, to jointly address these tasks. We propose Memory-Integrated Reconfigurable Adapters (MIRA), a unified framework that integrates Hopfield-style associative memory modules atop a shared backbone. Associative memory keys are learned post-hoc to index and retrieve an affine combination of stored adapter updates for any given task or domain on a per-sample basis. By varying only the task-specific objectives, we demonstrate that MIRA seamlessly accommodates domain shifts and sequential task exposures under one roof. Empirical evaluations on standard benchmarks confirm that our AM-augmented architecture significantly enhances adaptability and retention: in DG, MIRA achieves SoTA out-of-distribution accuracy, and in incremental learning settings, it outperforms architectures explicitly designed to handle catastrophic forgetting using generic CL algorithms. By unifying adapter-based modulation with biologically inspired associative memory, MIRA delivers rapid task switching and enduring knowledge retention in a single extensible architecture, charting a path toward more versatile and memory-augmented AI systems.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2511.18537.pdf' target='_blank'>https://arxiv.org/pdf/2511.18537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuomas Varanka, Juan Luis Gonzalez, Hyeongwoo Kim, Pablo Garrido, Xu Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18537">Zero-Shot Video Deraining with Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion. Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases. In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities. By inverting an input video into the latent space of diffusion models, its reconstruction process can be intervened and pushed away from the model's concept of rain using negative prompting. At the core of our approach is an attention switching mechanism that we found is crucial for maintaining dynamic backgrounds as well as structural consistency between the input and the derained video, mitigating artifacts introduced by naive negative prompting. Our approach is validated through extensive experiments on real-world rain datasets, demonstrating substantial improvements over prior methods and showcasing robust generalization without the need for supervised training.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2511.14638.pdf' target='_blank'>https://arxiv.org/pdf/2511.14638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Yang, Dandan Huang, Yunting Lin, Pengfei Wu, Zhikun Wu, Gangyuan Ma, Yulan Lu, Xinran Dong, Dingpeng Li, Junshuang Ge, Zhiyan Zhang, Xuanzhao Huang, Wenyan Nong, Yao Zhou, Hui Tang, Hongxi Yang, Shijie Zhang, Juan Li, Xiaojun Cao, Lin Yang, Xia Gao, Kaishou Xu, Xiaoqiong Gu, Wen Zhang, Huimin Xia, Li Liu, Wenhao Zhou, Mulin Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14638">A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2511.13020.pdf' target='_blank'>https://arxiv.org/pdf/2511.13020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Wen, Yuting Zhang, Jingdan Kang, Hao Ren, Weibin Cheng, Jintai Chen, Kaishun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13020">SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2510.22964.pdf' target='_blank'>https://arxiv.org/pdf/2510.22964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liling Yang, Ning Chen, Jun Yue, Yidan Liu, Jiayi Ma, Pedram Ghamisi, Antonio Plaza, Leyuan Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22964">Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models have transformed natural language processing and computer vision, and their impact is now reshaping remote sensing image analysis. With powerful generalization and transfer learning capabilities, they align naturally with the multimodal, multi-resolution, and multi-temporal characteristics of remote sensing data. To address unique challenges in the field, multimodal geospatial foundation models (GFMs) have emerged as a dedicated research frontier. This survey delivers a comprehensive review of multimodal GFMs from a modality-driven perspective, covering five core visual and vision-language modalities. We examine how differences in imaging physics and data representation shape interaction design, and we analyze key techniques for alignment, integration, and knowledge transfer to tackle modality heterogeneity, distribution shifts, and semantic gaps. Advances in training paradigms, architectures, and task-specific adaptation strategies are systematically assessed alongside a wealth of emerging benchmarks. Representative multimodal visual and vision-language GFMs are evaluated across ten downstream tasks, with insights into their architectures, performance, and application scenarios. Real-world case studies, spanning land cover mapping, agricultural monitoring, disaster response, climate studies, and geospatial intelligence, demonstrate the practical potential of GFMs. Finally, we outline pressing challenges in domain generalization, interpretability, efficiency, and privacy, and chart promising avenues for future research.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2510.13136.pdf' target='_blank'>https://arxiv.org/pdf/2510.13136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tan Le, Van Le, Sachin Shetty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13136">Privacy-Aware Framework of Robust Malware Detection in Indoor Robots: Hybrid Quantum Computing and Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly exposed to Denial of Service (DoS) attacks that compromise localization, control and telemetry integrity. We propose a privacy-aware malware detection framework for indoor robotic systems, which leverages hybrid quantum computing and deep neural networks to counter DoS threats in CPS, while preserving privacy information. By integrating quantum-enhanced feature encoding with dropout-optimized deep learning, our architecture achieves up to 95.2% detection accuracy under privacy-constrained conditions. The system operates without handcrafted thresholds or persistent beacon data, enabling scalable deployment in adversarial environments. Benchmarking reveals robust generalization, interpretability and resilience against training instability through modular circuit design. This work advances trustworthy AI for secure, autonomous CPS operations.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2510.01165.pdf' target='_blank'>https://arxiv.org/pdf/2510.01165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oussama Gabouj, Kamel Charaf, Ivan Zakazov, Nicolas Baldwin, Robert West
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01165">GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) achieve strong performance across diverse tasks, but their effectiveness often depends on the quality of the provided context. Retrieval-Augmented Generation (RAG) enriches prompts with external information, but its reliance on static databases constrains adaptability and can result in irrelevant demonstrations. In this work, we propose a Generative Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach where an LLM model is trained to generate input-specific concise demonstrations. By tailoring demonstrations to each input, our method offers better contextual support than traditional RAG approaches. We demonstrate the superiority of GRAD under budget constraints, where we limit both the number of tokens used per demonstration and the number of tokens used for the final output. Trained solely on a math dataset, GRAD consistently outperforms strong baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM questions, highlighting GRAD's robust generalization to out-of-distribution (OOD) domains such as physics, chemistry, and computer science. Furthermore, we show that demonstrations generated by trained smaller models can effectively guide larger target models, reducing training costs while maintaining competitive accuracy. Overall, this work introduces a scalable demonstration generator model presenting the first step toward a dynamic few-shot learning paradigm in resource-constrained settings. We release the code used for the project.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2509.07381.pdf' target='_blank'>https://arxiv.org/pdf/2509.07381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sichao Wu, Jiang Wu, Xingyu Cao, Fawang Zhang, Guangyuan Yu, Junjie Zhao, Yue Qu, Fei Ma, Jingliang Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07381">TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional online Model Predictive Control (MPC) methods often suffer from excessive computational complexity, limiting their practical deployment. Explicit MPC mitigates online computational load by pre-computing control policies offline; however, existing explicit MPC methods typically rely on simplified system dynamics and cost functions, restricting their accuracy for complex systems. This paper proposes TransMPC, a novel Transformer-based explicit MPC algorithm capable of generating highly accurate control sequences in real-time for complex dynamic systems. Specifically, we formulate the MPC policy as an encoder-only Transformer leveraging bidirectional self-attention, enabling simultaneous inference of entire control sequences in a single forward pass. This design inherently accommodates variable prediction horizons while ensuring low inference latency. Furthermore, we introduce a direct policy optimization framework that alternates between sampling and learning phases. Unlike imitation-based approaches dependent on precomputed optimal trajectories, TransMPC directly optimizes the true finite-horizon cost via automatic differentiation. Random horizon sampling combined with a replay buffer provides independent and identically distributed (i.i.d.) training samples, ensuring robust generalization across varying states and horizon lengths. Extensive simulations and real-world vehicle control experiments validate the effectiveness of TransMPC in terms of solution accuracy, adaptability to varying horizons, and computational efficiency.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2509.03394.pdf' target='_blank'>https://arxiv.org/pdf/2509.03394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03394">CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cloud platforms are increasingly relied upon to host diverse, resource-intensive workloads due to their scalability, flexibility, and cost-efficiency. In multi-tenant cloud environments, virtual machines are consolidated on shared physical servers to improve resource utilization. While virtualization guarantees resource partitioning for CPU, memory, and storage, it cannot ensure performance isolation. Competition for shared resources such as last-level cache, memory bandwidth, and network interfaces often leads to severe performance degradation. Existing management techniques, including VM scheduling and resource provisioning, require accurate performance prediction to mitigate interference. However, this remains challenging in public clouds due to the black-box nature of VMs and the highly dynamic nature of workloads. To address these limitations, we propose CloudFormer, a dual-branch Transformer-based model designed to predict VM performance degradation in black-box environments. CloudFormer jointly models temporal dynamics and system-level interactions, leveraging 206 system metrics at one-second resolution across both static and dynamic scenarios. This design enables the model to capture transient interference effects and adapt to varying workload conditions without scenario-specific tuning. Complementing the methodology, we provide a fine-grained dataset that significantly expands the temporal resolution and metric diversity compared to existing benchmarks. Experimental results demonstrate that CloudFormer consistently outperforms state-of-the-art baselines across multiple evaluation metrics, achieving robust generalization across diverse and previously unseen workloads. Notably, CloudFormer attains a mean absolute error (MAE) of just 7.8%, representing a substantial improvement in predictive accuracy and outperforming existing methods at least by 28%.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2509.01642.pdf' target='_blank'>https://arxiv.org/pdf/2509.01642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian P. Oppelt, Andreas Foltyn, Nadine R. Lang-Richter, Bjoern M. Eskofier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01642">REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task load detection is essential for optimizing human performance across diverse applications, yet current models often lack generalizability beyond narrow experimental domains. While prior research has focused on individual tasks and limited modalities, there remains a gap in evaluating model robustness and transferability in real-world scenarios. This paper addresses these limitations by introducing a new multimodal dataset that extends established cognitive load detection benchmarks with a real-world gaming application, using the $n$-back test as a scientific foundation. Task load annotations are derived from objective performance, subjective NASA-TLX ratings, and task-level design, enabling a comprehensive evaluation framework. State-of-the-art end-to-end model, including xLSTM, ConvNeXt, and Transformer architectures are systematically trained and evaluated on multiple modalities and application domains to assess their predictive performance and cross-domain generalization. Results demonstrate that multimodal approaches consistently outperform unimodal baselines, with specific modalities and model architectures showing varying impact depending on the application subset. Importantly, models trained on one domain exhibit reduced performance when transferred to novel applications, underscoring remaining challenges for universal cognitive load estimation. These findings provide robust baselines and actionable insights for developing more generalizable cognitive load detection systems, advancing both research and practical implementation in human-computer interaction and adaptive systems.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2508.16514.pdf' target='_blank'>https://arxiv.org/pdf/2508.16514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parker Seegmiller, Kartik Mehta, Soumya Saha, Chenyang Tao, Shereen Oraby, Arpit Gupta, Tagyoung Chung, Mohit Bansal, Nanyun Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16514">FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2508.07539.pdf' target='_blank'>https://arxiv.org/pdf/2508.07539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Shigeyasu, Shota Harada, Akihiko Yoshizawa, Kazuhiro Terada, Naoki Nakazima, Mariyo Kurata, Hiroyuki Abe, Tetsuo Ushiku, Ryoma Bise
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07539">Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address domain shifts in pathological images by focusing on shifts within whole slide images~(WSIs), such as patient characteristics and tissue thickness, rather than shifts between hospitals. Traditional approaches rely on multi-hospital data, but data collection challenges often make this impractical. Therefore, the proposed domain generalization method captures and leverages intra-hospital domain shifts by clustering WSI-level features from non-tumor regions and treating these clusters as domains. To mitigate domain shift, we apply contrastive learning to reduce feature gaps between WSI pairs from different clusters. The proposed method introduces a two-stage contrastive learning approach WSI-level and patch-level contrastive learning to minimize these gaps effectively.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2508.05135.pdf' target='_blank'>https://arxiv.org/pdf/2508.05135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thinh Nguyen, Trung Phan, Binh T. Nguyen, Khoa D Doan, Kok-Seng Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05135">HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) is a decentralized approach where multiple clients collaboratively train a shared global model without sharing their raw data. Despite its effectiveness, conventional FL faces scalability challenges due to excessive computational and communication demands placed on a single central server as the number of participating devices grows. Hierarchical Federated Learning (HFL) addresses these issues by distributing model aggregation tasks across intermediate nodes (stations), thereby enhancing system scalability and robustness against single points of failure. However, HFL still suffers from a critical yet often overlooked limitation: domain shift, where data distributions vary significantly across different clients and stations, reducing model performance on unseen target domains. While Federated Domain Generalization (FedDG) methods have emerged to improve robustness to domain shifts, their integration into HFL frameworks remains largely unexplored. In this paper, we formally introduce Hierarchical Federated Domain Generalization (HFedDG), a novel scenario designed to investigate domain shift within hierarchical architectures. Specifically, we propose HFedATM, a hierarchical aggregation method that first aligns the convolutional filters of models from different stations through Filter-wise Optimal Transport Alignment and subsequently merges aligned models using a Shrinkage-aware Regularized Mean Aggregation. Our extensive experimental evaluations demonstrate that HFedATM significantly boosts the performance of existing FedDG baselines across multiple datasets and maintains computational and communication efficiency. Moreover, theoretical analyses indicate that HFedATM achieves tighter generalization error bounds compared to standard hierarchical averaging, resulting in faster convergence and stable training behavior.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2508.04594.pdf' target='_blank'>https://arxiv.org/pdf/2508.04594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Sun, Qi Feng, Lehao Lin, Chris Ding, Jicong Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04594">GraphProp: Training the Graph Foundation Models using Graph Properties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on training graph foundation models (GFMs) that have strong generalization ability in graph-level tasks such as graph classification. Effective GFM training requires capturing information consistent across different domains. We discover that graph structures provide more consistent cross-domain information compared to node features and graph labels. However, traditional GFMs primarily focus on transferring node features from various domains into a unified representation space but often lack structural cross-domain generalization. To address this, we introduce GraphProp, which emphasizes structural generalization. The training process of GraphProp consists of two main phases. First, we train a structural GFM by predicting graph invariants. Since graph invariants are properties of graphs that depend only on the abstract structure, not on particular labellings or drawings of the graph, this structural GFM has a strong ability to capture the abstract structural information and provide discriminative graph representations comparable across diverse domains. In the second phase, we use the representations given by the structural GFM as positional encodings to train a comprehensive GFM. This phase utilizes domain-specific node attributes and graph labels to further improve cross-domain node feature generalization. Our experiments demonstrate that GraphProp significantly outperforms the competitors in supervised learning and few-shot learning, especially in handling graphs without node attributes.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2508.04552.pdf' target='_blank'>https://arxiv.org/pdf/2508.04552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franz Thaler, Darko Stern, Gernot Plank, Martin Urschler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04552">Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the leading cause of death worldwide, cardiovascular diseases motivate the development of more sophisticated methods to analyze the heart and its substructures from medical images like Computed Tomography (CT) and Magnetic Resonance (MR). Semantic segmentations of important cardiac structures that represent the whole heart are useful to assess patient-specific cardiac morphology and pathology. Furthermore, accurate semantic segmentations can be used to generate cardiac digital twin models which allows e.g. electrophysiological simulation and personalized therapy planning. Even though deep learning-based methods for medical image segmentation achieved great advancements over the last decade, retaining good performance under domain shift -- i.e. when training and test data are sampled from different data distributions -- remains challenging. In order to perform well on domains known at training-time, we employ a (1) balanced joint training approach that utilizes CT and MR data in equal amounts from different source domains. Further, aiming to alleviate domain shift towards domains only encountered at test-time, we rely on (2) strong intensity and spatial augmentation techniques to greatly diversify the available training data. Our proposed whole heart segmentation method, a 5-fold ensemble with our contributions, achieves the best performance for MR data overall and a performance similar to the best performance for CT data when compared to a model trained solely on CT. With 93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR data, our method demonstrates great potential to efficiently obtain accurate semantic segmentations from which patient-specific cardiac twin models can be generated.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2508.01582.pdf' target='_blank'>https://arxiv.org/pdf/2508.01582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhui Li, Xinyu He, Qiming Hu, Xiaojie Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01582">Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce, for the first time, the concept of Set Pivot Learning, a paradigm shift that redefines domain generalization (DG) based on Vision Foundation Models (VFMs). Traditional DG assumes that the target domain is inaccessible during training, but the emergence of VFMs, trained on vast and diverse data, renders this assumption unclear and obsolete. Traditional DG assumes that the target domain is inaccessible during training, but the emergence of VFMs, which are trained on vast and diverse datasets, renders this assumption unclear and obsolete. To address this challenge, we propose Set Pivot Learning (SPL), a new definition of domain migration task based on VFMs, which is more suitable for current research and application requirements. Unlike conventional DG methods, SPL prioritizes adaptive refinement over rigid domain transfer, ensuring continuous alignment with evolving real-world conditions. Specifically, SPL features two key attributes: (i) Dynamic adaptation, transitioning from static domain alignment to flexible, task-driven feature optimization, enabling models to evolve with downstream scenarios; (ii) VFM-centric tuning, leveraging pretrained knowledge as a pivot to hone task-specific representations while preserving cross-domain robustness. Building on SPL, we propose a Dynamic Prompt Fine-Tuning method, which combines a Dynamic Class-aware Prompter with a Prompt-guided Feature Focuser, to elevate VFM performance in targeted scenarios. Extensive experiments on benchmark datasets show the effectiveness of our method, highlighting its superiority over state-of-the-art methods, particularly in generalized segmentation.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2507.21783.pdf' target='_blank'>https://arxiv.org/pdf/2507.21783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malte Londschien, Manuel Burger, Gunnar RÃ¤tsch, Peter BÃ¼hlmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21783">Domain Generalization and Adaptation in Intensive Care with Anchor Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of predictive models in clinical settings often degrades when deployed in new hospitals due to distribution shifts. This paper presents a large-scale study of causality-inspired domain generalization on heterogeneous multi-center intensive care unit (ICU) data. We apply anchor regression and introduce anchor boosting, a novel, tree-based nonlinear extension, to a large dataset comprising 400,000 patients from nine distinct ICU databases. The anchor regularization consistently improves out-of-distribution performance, particularly for the most dissimilar target domains. The methods appear robust to violations of theoretical assumptions, such as anchor exogeneity. Furthermore, we propose a novel conceptual framework to quantify the utility of large external data datasets. By evaluating performance as a function of available target-domain data, we identify three regimes: (i) a domain generalization regime, where only the external model should be used, (ii) a domain adaptation regime, where refitting the external model is optimal, and (iii) a data-rich regime, where external data provides no additional value.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2507.07572.pdf' target='_blank'>https://arxiv.org/pdf/2507.07572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupu Liang, Yaping Zhang, Zhiyang Zhang, Yang Zhao, Lu Xiang, Chengqing Zong, Yu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07572">Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Document Image Machine Translation (DIMT) aims to translate text within document images, facing generalization challenges due to limited training data and the complex interplay between visual and textual information. To address these challenges, we introduce M4Doc, a novel single-to-mix modality alignment framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an image-only encoder with the multimodal representations of an MLLM, pre-trained on large-scale document image datasets. This alignment enables a lightweight DIMT model to learn crucial visual-textual correlations during training. During inference, M4Doc bypasses the MLLM, maintaining computational efficiency while benefiting from its multimodal knowledge. Comprehensive experiments demonstrate substantial improvements in translation quality, especially in cross-domain generalization and challenging document image scenarios.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2506.23467.pdf' target='_blank'>https://arxiv.org/pdf/2506.23467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23467">AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2506.17761.pdf' target='_blank'>https://arxiv.org/pdf/2506.17761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiheng Liang, Ziru Yu, Zujie Xie, Yuchen Guo, Yulan Guo, Xiangyang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17761">Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the limitations of current spectral analysis methods-such as reliance on single-modality data, limited generalizability, and poor interpretability-we propose a novel multi-modal spectral analysis framework that integrates prior knowledge graphs with Large Language Models. Our method explicitly bridges physical spectral measurements and chemical structural semantics by representing them in a unified Textual Graph format, enabling flexible, interpretable, and generalizable spectral understanding. Raw spectra are first transformed into TAGs, where nodes and edges are enriched with textual attributes describing both spectral properties and chemical context. These are then merged with relevant prior knowledge-including functional groups and molecular graphs-to form a Task Graph that incorporates "Prompt Nodes" supporting LLM-based contextual reasoning. A Graph Neural Network further processes this structure to complete downstream tasks. This unified design enables seamless multi-modal integration and automated feature decoding with minimal manual annotation. Our framework achieves consistently high performance across multiple spectral analysis tasks, including node-level, edge-level, and graph-level classification. It demonstrates robust generalization in both zero-shot and few-shot settings, highlighting its effectiveness in learning from limited data and supporting in-context reasoning. This work establishes a scalable and interpretable foundation for LLM-driven spectral analysis, unifying physical and chemical modalities for scientific applications.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2505.24346.pdf' target='_blank'>https://arxiv.org/pdf/2505.24346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wang, Zhi Gao, Boxuan Yu, Zirui Dai, Yuxiang Song, Qingyuan Lu, Jin Chen, Xinxiao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24346">VUDG: A Dataset for Video Understanding Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding has made remarkable progress in recent years, largely driven by advances in deep models and the availability of large-scale annotated datasets. However, existing works typically ignore the inherent domain shifts encountered in real-world video applications, leaving domain generalization (DG) in video understanding underexplored. Hence, we propose Video Understanding Domain Generalization (VUDG), a novel dataset designed specifically for evaluating the DG performance in video understanding. VUDG contains videos from 11 distinct domains that cover three types of domain shifts, and maintains semantic similarity across different domains to ensure fair and meaningful evaluation. We propose a multi-expert progressive annotation framework to annotate each video with both multiple-choice and open-ended question-answer pairs. Extensive experiments on 9 representative large video-language models (LVLMs) and several traditional video question answering methods show that most models (including state-of-the-art LVLMs) suffer performance degradation under domain shifts. These results highlight the challenges posed by VUDG and the difference in the robustness of current models to data distribution shifts. We believe VUDG provides a valuable resource for prompting future research in domain generalization video understanding.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2505.21010.pdf' target='_blank'>https://arxiv.org/pdf/2505.21010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sabbir Ahmed, Mamshad Nayeem Rizve, Abdullah Al Arafat, Jacqueline Liu, Rahim Hossain, Mohaiminul Al Nahian, Adnan Siraj Rakin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21010">Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-Supervised Federated Learning (SSFL) is gaining popularity over conventional Federated Learning in many real-world applications. Due to the practical limitation of limited labeled data on the client side, SSFL considers that participating clients train with unlabeled data, and only the central server has the necessary resources to access limited labeled data, making it an ideal fit for real-world applications (e.g., healthcare). However, traditional SSFL assumes that the data distributions in the training phase and testing phase are the same. In practice, however, domain shifts frequently occur, making it essential for SSFL to incorporate generalization capabilities and enhance their practicality. The core challenge is improving model generalization to new, unseen domains while the client participate in SSFL. However, the decentralized setup of SSFL and unsupervised client training necessitates innovation to achieve improved generalization across domains. To achieve this, we propose a novel framework called the Unified Alignment Protocol (UAP), which consists of an alternating two-stage training process. The first stage involves training the server model to learn and align the features with a parametric distribution, which is subsequently communicated to clients without additional communication overhead. The second stage proposes a novel training algorithm that utilizes the server feature distribution to align client features accordingly. Our extensive experiments on standard domain generalization benchmark datasets across multiple model architectures reveal that proposed UAP successfully achieves SOTA generalization performance in SSFL setting.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2505.12745.pdf' target='_blank'>https://arxiv.org/pdf/2505.12745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Kyu Cho, Inwoo Hwang, Sanghack Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12745">PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data augmentation is a popular tool for single source domain generalization, which expands the source domain by generating simulated ones, improving generalization on unseen target domains. In this work, we show that the performance of such augmentation-based methods in the target domains universally fluctuates during training, posing challenges in model selection under realistic scenarios. We argue that the fluctuation stems from the inability of the model to accumulate the knowledge learned from diverse augmentations, exacerbating feature distortion during training. Based on this observation, we propose a novel generalization method, coined Parameter-Space Ensemble with Entropy Regularization (PEER), that uses a proxy model to learn the augmented data on behalf of the main model. The main model is updated by averaging its parameters with the proxy model, progressively accumulating knowledge over the training steps. Maximizing the mutual information between the output representations of the two models guides the learning process of the proxy model, mitigating feature distortion during training. Experimental results demonstrate the effectiveness of PEER in reducing the OOD performance fluctuation and enhancing generalization across various datasets, including PACS, Digits, Office-Home, and VLCS. Notably, our method with simple random augmentation achieves state-of-the-art performance, surpassing prior approaches on sDG that utilize complex data augmentation strategies.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2505.12568.pdf' target='_blank'>https://arxiv.org/pdf/2505.12568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lekang Jiang, Chengzu Li, Stephan Goetz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12568">Enriching Patent Claim Generation with European Patent Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drafting patent claims is time-intensive, costly, and requires professional skill. Therefore, researchers have investigated large language models (LLMs) to assist inventors in writing claims. However, existing work has largely relied on datasets from the United States Patent and Trademark Office (USPTO). To enlarge research scope regarding various jurisdictions, drafting conventions, and legal standards, we introduce EPD, a European patent dataset. EPD presents rich textual data and structured metadata to support multiple patent-related tasks, including claim generation. This dataset enriches the field in three critical aspects: (1) Jurisdictional diversity: Patents from different offices vary in legal and drafting conventions. EPD fills a critical gap by providing a benchmark for European patents to enable more comprehensive evaluation. (2) Quality improvement: EPD offers high-quality granted patents with finalized and legally approved texts, whereas others consist of patent applications that are unexamined or provisional. Experiments show that LLMs fine-tuned on EPD significantly outperform those trained on previous datasets and even GPT-4o in claim quality and cross-domain generalization. (3) Real-world simulation: We propose a difficult subset of EPD to better reflect real-world challenges of claim generation. Results reveal that all tested LLMs perform substantially worse on these challenging samples, which highlights the need for future research.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2504.02411.pdf' target='_blank'>https://arxiv.org/pdf/2504.02411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Misrahi, Nadezhda Chirkova, Maxime Louis, Vassilina Nikoulina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02411">Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-Augmented Generation (RAG) enhances LLM factuality, but multi-domain applications face challenges like lack of diverse benchmarks and poor out-of-domain generalization. The first contribution of this work is to introduce a diverse benchmark comprising a variety of question-answering tasks from 8 sources and covering 13 domains. Our second contribution consists in systematically testing out-of-domain generalization for typical RAG tuning strategies. While our findings reveal that standard fine-tuning fails to generalize effectively, we show that sequence-level distillation with teacher-generated labels improves out-of-domain performance by providing more coherent supervision. Our findings highlight key strategies for improving multi-domain RAG robustness.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2503.08639.pdf' target='_blank'>https://arxiv.org/pdf/2503.08639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>DuÅ¡an MaliÄ, Christian Fruhwirth-Reisinger, Samuel Schulter, Horst Possegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08639">GBlobs: Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain LiDAR-based 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D detectors need large datasets for training, yet they struggle to generalize to novel domains. Domain Generalization (DG) aims to mitigate this by training detectors that are invariant to such domain shifts. Current DG approaches exclusively rely on global geometric features (point cloud Cartesian coordinates) as input features. Over-reliance on these global geometric features can, however, cause 3D detectors to prioritize object location and absolute position, resulting in poor cross-domain performance. To mitigate this, we propose to exploit explicit local point cloud structure for DG, in particular by encoding point cloud neighborhoods with Gaussian blobs, GBlobs. Our proposed formulation is highly efficient and requires no additional parameters. Without any bells and whistles, simply by integrating GBlobs in existing detectors, we beat the current state-of-the-art in challenging single-source DG benchmarks by over 21 mAP (Waymo->KITTI), 13 mAP (KITTI->Waymo), and 12 mAP (nuScenes->KITTI), without sacrificing in-domain performance. Additionally, GBlobs demonstrate exceptional performance in multi-source DG, surpassing the current state-of-the-art by 17, 12, and 5 mAP on Waymo, KITTI, and ONCE, respectively.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2503.01157.pdf' target='_blank'>https://arxiv.org/pdf/2503.01157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobin Hong, Jiawen Zhang, Wenzhong Li, Sanglu Lu, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01157">Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time Series Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of foundation models has revolutionized natural language processing and computer vision, yet their best practices to time series forecasting remains underexplored. Existing time series foundation models often adopt methodologies from these fields without addressing the unique characteristics of time series data. In this paper, we identify two key challenges in cross-domain time series forecasting: the complexity of temporal patterns and semantic misalignment. To tackle these issues, we propose the ``Unify and Anchor" transfer paradigm, which disentangles frequency components for a unified perspective and incorporates external context as domain anchors for guided adaptation. Based on this framework, we introduce ContexTST, a Transformer-based model that employs a time series coordinator for structured representation and the Transformer blocks with a context-informed mixture-of-experts mechanism for effective cross-domain generalization. Extensive experiments demonstrate that ContexTST advances state-of-the-art forecasting performance while achieving strong zero-shot transferability across diverse domains.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2502.19173.pdf' target='_blank'>https://arxiv.org/pdf/2502.19173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gregory W. Kyro, Tianyin Qiu, Victor S. Batista
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19173">A Model-Centric Review of Deep Learning for Protein Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has transformed protein design, enabling accurate structure prediction, sequence optimization, and de novo protein generation. Advances in single-chain protein structure prediction via AlphaFold2, RoseTTAFold, ESMFold, and others have achieved near-experimental accuracy, inspiring successive work extended to biomolecular complexes via AlphaFold Multimer, RoseTTAFold All-Atom, AlphaFold 3, Chai-1, Boltz-1 and others. Generative models such as ProtGPT2, ProteinMPNN, and RFdiffusion have enabled sequence and backbone design beyond natural evolution-based limitations. More recently, joint sequence-structure co-design models, including ESM3, have integrated both modalities into a unified framework, resulting in improved designability. Despite these advances, challenges still exist pertaining to modeling sequence-structure-function relationships and ensuring robust generalization beyond the regions of protein space spanned by the training data. Future advances will likely focus on joint sequence-structure-function co-design frameworks that are able to model the fitness landscape more effectively than models that treat these modalities independently. Current capabilities, coupled with the dizzying rate of progress, suggest that the field will soon enable rapid, rational design of proteins with tailored structures and functions that transcend the limitations imposed by natural evolution. In this review, we discuss the current capabilities of deep learning methods for protein design, focusing on some of the most revolutionary and capable models with respect to their functionality and the applications that they enable, leading up to the current challenges of the field and the optimal path forward.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2502.07847.pdf' target='_blank'>https://arxiv.org/pdf/2502.07847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Behraj Khan, Rizwan Qureshi, Nouman Muhammad Durrani, Tahir Syed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07847">Confidence-calibrated covariate shift correction for few-shot classification in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the establishment of vision-language foundation models as the new mainstay in low-shot vision classification tasks, the question of domain generalization arising from insufficient target data is assuming more importance. This scarcity challenge induces sampling bias and amplifies model sensitivity to variations and shifts in data distributions. While fine-tuning on multiple domains could mitigate such domain generalization issues, it is resource-intensive and demands diverse data sources.
  In this work, we systematically analyze two critical challenges: (1) covariate shift between the pre-training distribution and the underspecified target distribution, and (2) confidence misalignment, where predictions on novel data are overconfident.
  To address both challenges simultaneously, we introduce \textbf{Confidence-Calibrated Covariate Shift Correction (CalShift)} -- a unified approach that combines a Fisher information penalty to mitigate covariate shift and a Confidence Misalignment Penalty (CMP) to reduce overconfidence in misclassified examples.
  Experimental evaluations across various vision and covariate shift benchmarks demonstrate that CalShift significantly improves model calibration, achieving up to a 5.82\% reduction in Expected Calibration Error (ECE). Furthermore, CalShift enhances robustness, improving accuracy by 3.5\% on challenging datasets impacted by covariate shifts.
  Our results highlight CalShift as a promising strategy for building robust and reliable low-shot vision-language systems for real-world applications.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2502.02717.pdf' target='_blank'>https://arxiv.org/pdf/2502.02717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristobal Donoso-Oliva, Ignacio Becker, Pavlos Protopapas, Guillermo Cabrera-Vives, Martina CÃ¡diz-Leyton, Daniel Moreno-Cartagena
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02717">Astromer 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundational models have emerged as a powerful paradigm in deep learning field, leveraging their capacity to learn robust representations from large-scale datasets and effectively to diverse downstream applications such as classification. In this paper, we present Astromer 2 a foundational model specifically designed for extracting light curve embeddings. We introduce Astromer 2 as an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model's representations. Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings. Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer's attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2501.17595.pdf' target='_blank'>https://arxiv.org/pdf/2501.17595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Behraj Khan, Tahir Syed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17595">Technical report on label-informed logit redistribution for better domain generalization in low-shot classification with foundation models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Confidence calibration is an emerging challenge in real-world decision systems based on foundations models when used for downstream vision classification tasks. Due to various reasons exposed, logit scores on the CLIP head remain large irrespective of whether the image-language pairs reconcile. It is difficult to address in data space, given the few-shot regime. We propose a penalty incorporated into loss objective that penalizes incorrect classifications whenever one is made during finetuning, by moving an amount of log-likelihood to the true class commensurate to the relative amplitudes of the two likelihoods. We refer to it as \textit{confidence misalignment penalty (CMP)}. Extensive experiments on $12$ vision datasets and $5$ domain generalization datasets supports the calibration performance of our method against stat-of-the-art. CMP outperforms the benchmarked prompt learning methods, demonstrating average improvement in Expected Calibration Error (ECE) by average $6.01$\%, $4.01$ \% at minimum and $9.72$\% at maximum.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2501.15144.pdf' target='_blank'>https://arxiv.org/pdf/2501.15144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Yadav, Lingqiao Liu, Yuankai Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15144">Exploring Primitive Visual Measurement Understanding and the Role of Output Format in Learning in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the capabilities of current vision-language models (VLMs) in visual understanding and attribute measurement of primitive shapes using a benchmark focused on controlled 2D shape configurations with variations in spatial positioning, occlusion, rotation, size, and shape attributes such as type, quadrant, center-coordinates, rotation, occlusion status, and color as shown in Figure 1 and supplementary Figures S3-S81. We fine-tune state-of-the-art VLMs (2B-8B parameters) using Low-Rank Adaptation (LoRA) and validate them on multiple out-of-domain (OD) scenarios from our proposed benchmark. Our findings reveal that coherent sentence-based outputs outperform tuple formats, particularly in OD scenarios with large domain gaps. Additionally, we demonstrate that scaling numeric tokens during loss computation enhances numerical approximation capabilities, further improving performance on spatial and measurement tasks. These results highlight the importance of output format design, loss scaling strategies, and robust generalization techniques in enhancing the training and fine-tuning of VLMs, particularly for tasks requiring precise spatial approximations and strong OD generalization.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2412.16339.pdf' target='_blank'>https://arxiv.org/pdf/2412.16339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16339">Deliberative Alignment: Reasoning Enables Safer Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align OpenAI's o-series models, and achieved highly precise adherence to OpenAI's safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2412.05269.pdf' target='_blank'>https://arxiv.org/pdf/2412.05269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krzysztof Maziarz, Guoqing Liu, Hubert Misztela, Austin Tripp, Junren Li, Aleksei Kornev, Piotr GaiÅski, Holger Hoefling, Mike Fortunato, Rishi Gupta, Marwin Segler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05269">Chemist-aligned retrosynthesis by ensembling diverse inductive bias models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chemical synthesis remains a critical bottleneck in the discovery and manufacture of functional small molecules. AI-based synthesis planning models could be a potential remedy to find effective syntheses, and have made progress in recent years. However, they still struggle with less frequent, yet critical reactions for synthetic strategy, as well as hallucinated, incorrect predictions. This hampers multi-step search algorithms that rely on models, and leads to misalignment with chemists' expectations. Here we propose RetroChimera: a frontier retrosynthesis model, built upon two newly developed components with complementary inductive biases, which we fuse together using a new framework for integrating predictions from multiple sources via a learning-based ensembling strategy. Through experiments across several orders of magnitude in data scale and splitting strategy, we show RetroChimera outperforms all major models by a large margin, demonstrating robustness outside the training data, as well as for the first time the ability to learn from even a very small number of examples per reaction class. Moreover, industrial organic chemists prefer predictions from RetroChimera over the reactions it was trained on in terms of quality, revealing high levels of alignment. Finally, we demonstrate zero-shot transfer to an internal dataset from a major pharmaceutical company, showing robust generalization under distribution shift. With the new dimension that our ensembling framework unlocks, we anticipate further acceleration in the development of even more accurate models.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2412.03897.pdf' target='_blank'>https://arxiv.org/pdf/2412.03897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhu Han, Ce Zhang, Lianru Gao, Zhiqiang Zeng, Michael K. Ng, Bing Zhang, Jocelyn Chanussot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03897">Multisource Collaborative Domain Generalization for Cross-Scene Remote Sensing Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-scene image classification aims to transfer prior knowledge of ground materials to annotate regions with different distributions and reduce hand-crafted cost in the field of remote sensing. However, existing approaches focus on single-source domain generalization to unseen target domains, and are easily confused by large real-world domain shifts due to the limited training information and insufficient diversity modeling capacity. To address this gap, we propose a novel multi-source collaborative domain generalization framework (MS-CDG) based on homogeneity and heterogeneity characteristics of multi-source remote sensing data, which considers data-aware adversarial augmentation and model-aware multi-level diversification simultaneously to enhance cross-scene generalization performance. The data-aware adversarial augmentation adopts an adversary neural network with semantic guide to generate MS samples by adaptively learning realistic channel and distribution changes across domains. In views of cross-domain and intra-domain modeling, the model-aware diversification transforms the shared spatial-channel features of MS data into the class-wise prototype and kernel mixture module, to address domain discrepancies and cluster different classes effectively. Finally, the joint classification of original and augmented MS samples is employed by introducing a distribution consistency alignment to increase model diversity and ensure better domain-invariant representation learning. Extensive experiments on three public MS remote sensing datasets demonstrate the superior performance of the proposed method when benchmarked with the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2412.03068.pdf' target='_blank'>https://arxiv.org/pdf/2412.03068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangkai Ma, Xiaobin Hong, Mingkai Lin, Han Zhang, Wenzhong Li, Sanglu Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03068">Domain Fusion Controllable Generalization for Cross-Domain Time Series Forecasting from Multi-Domain Integrated Distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional deep models have achieved unprecedented success in time series forecasting. However, facing the challenge of cross-domain generalization, existing studies utilize statistical prior as prompt engineering fails under the huge distribution shift among various domains. In this paper, a novel time series generalization diffusion model (TimeControl) that pioneers the Domain-Fusion paradigm, systematically integrating information from multiple time series domains into a unified generative process via diffusion models. Unlike the autoregressive models that capture the conditional probabilities of the prediction horizon to the historical sequence, we use the diffusion denoising process to model the mixed distribution of the cross-domain data and generate the prediction sequence for the target domain directly utilizing conditional sampling. The proposed TimeControl contains three pivotal designs: (1) The condition network captures the multi-scale fluctuation patterns from the observation sequence, which are utilized as context representations to guide the denoising network to generate the prediction sequence; (2) Adapter-based fine-tuning strategy, the multi-domain universal representation learned in the pretraining stage is utilized for downstream tasks in target domains; (3) A novel hybrid architecture is designed to align the observation and prediction spaces, enabling TimeControl to generate prediction sequences of arbitrary lengths with flexibility. We conduct extensive experiments on mainstream 49 benchmarks and 30 baselines, and the TimeControl outperforms existing baselines on all data domains, exhibiting superior zero-shot generalization ability.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2410.09069.pdf' target='_blank'>https://arxiv.org/pdf/2410.09069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Hosseini Chagahi, Niloufar Delfan, Saeed Mohammadi Dashtaki, Behzad Moshiri, Md. Jalil Piran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09069">Explainable AI for Fraud Detection: An Attention-Based Ensemble of CNNs, GNNs, and A Confidence-Driven Gating Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid expansion of e-commerce and the widespread use of credit cards in online purchases and financial transactions have significantly heightened the importance of promptly and accurately detecting credit card fraud (CCF). Not only do fraudulent activities in financial transactions lead to substantial monetary losses for banks and financial institutions, but they also undermine user trust in digital services. This study presents a new stacking-based approach for CCF detection by adding two extra layers to the usual classification process: an attention layer and a confidence-based combination layer. In the attention layer, we combine soft outputs from a convolutional neural network (CNN) and a recurrent neural network (RNN) using the dependent ordered weighted averaging (DOWA) operator, and from a graph neural network (GNN) and a long short-term memory (LSTM) network using the induced ordered weighted averaging (IOWA) operator. These weighted outputs capture different predictive signals, increasing the model's accuracy. Next, in the confidence-based layer, we select whichever aggregate (DOWA or IOWA) shows lower uncertainty to feed into a meta-learner. To make the model more explainable, we use shapley additive explanations (SHAP) to identify the top ten most important features for distinguishing between fraud and normal transactions. These features are then used in our attention-based model. Experiments on three datasets show that our method achieves high accuracy and robust generalization, making it effective for CCF detection.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2410.08165.pdf' target='_blank'>https://arxiv.org/pdf/2410.08165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryo Lotfi, Enrico Fini, Samy Bengio, Moin Nabi, Emmanuel Abbe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08165">Chain-of-Sketch: Enabling Global Visual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern vision models have achieved remarkable success in benchmarks where local features provide critical information about the target. There is now a growing interest in tackling tasks requiring more global reasoning, where local features do not provide significant information. Minsky and Papert put forward such tasks in 1969 with their connectivity study, exposing the limitations of the perceptron model. In this paper, we introduce an expanded set of global visual datasets involving graphs, strings, mazes, and image grids. We show that large vision models still struggle to learn these tasks efficiently. Similarly, state-of-the-art multi-modal LLMs perform poorly on these datasets. We explain this learning inefficiency by means of the 'globality degree' measure. To mitigate this, we propose a method called chain-of-sketch (CoS). Similar to the chain-of-thought and scratchpad techniques used in language models, CoS breaks the original task into intermediate visual steps to help learn a complex task. In addition, we show that not all CoS strategies perform equally well. Our key insight is to impose a Markovian structure on the CoS frames. This leads to the introduction of 'inductive CoS' which achieves better out-of-distribution generalization and performs well even with smaller models compared to non-inductive variants.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2410.02136.pdf' target='_blank'>https://arxiv.org/pdf/2410.02136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Liu, Lu Zhang, Tian Gao, Yue Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02136">Disentangled Representation Learning for Parametric Partial Differential Equations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural operators (NOs) have demonstrated remarkable success in learning mappings between function spaces, serving as efficient approximators for the forward solutions of complex physical systems governed by partial differential equations (PDEs). However, while effective as black-box solvers, they offer limited insight into the underlying physical mechanism, due to the lack of interpretable representations of the physical parameters that drive the system. To tackle this challenge, we propose a new paradigm for learning disentangled representations from neural operator parameters, thereby effectively solving an inverse problem. Specifically, we introduce DisentangO, a novel hyper-neural operator architecture designed to unveil and disentangle the latent physical factors of variation embedded within the black-box neural operator parameters. At the core of DisentangO is a multi-task neural operator architecture that distills the varying parameters of the governing PDE through a task-wise adaptive layer, coupled with a hierarchical variational autoencoder that disentangles these variations into identifiable latent factors. By learning these disentangled representations, our model not only enhances physical interpretability but also enables more robust generalization across diverse physical systems. Empirical evaluations across supervised, semi-supervised, and unsupervised learning contexts show that DisentangO effectively extracts meaningful and interpretable latent features, bridging the divide between predictive performance and physical understanding in neural operator frameworks.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2512.15493.pdf' target='_blank'>https://arxiv.org/pdf/2512.15493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hampus Linander, Conor Heins, Alexander Tschantz, Marco Perin, Christopher Buckley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15493">Soft Geometric Inductive Bias for Object Centric Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2512.12268.pdf' target='_blank'>https://arxiv.org/pdf/2512.12268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Lei, Yingjun Du, Yawen Huang, Xiantong Zhen, Ling Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12268">MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2512.10792.pdf' target='_blank'>https://arxiv.org/pdf/2512.10792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Botta, Piermario Vitullo, Thomas Ventimiglia, Andreas Linninger, Paolo Zunino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10792">Physics-Informed Learning of Microvascular Flow Models using Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The simulation of microcirculatory blood flow in realistic vascular architectures poses significant challenges due to the multiscale nature of the problem and the topological complexity of capillary networks. In this work, we propose a novel deep learning-based reduced-order modeling strategy, leveraging Graph Neural Networks (GNNs) trained on synthetic microvascular graphs to approximate hemodynamic quantities on anatomically realistic domains. Our method combines algorithms for synthetic vascular generation with a physics-informed training procedure that integrates graph topological information and local flow dynamics. To ensure the physical reliability of the learned surrogates, we incorporate a physics-informed loss functional derived from the governing equations, allowing enforcement of mass conservation and rheological constraints. The resulting GNN architecture demonstrates robust generalization capabilities across diverse network configurations. The GNN formulation is validated on benchmark problems with linear and nonlinear rheology, showing accurate pressure and velocity field reconstruction with substantial computational gains over full-order solvers. The methodology showcases significant generalization capabilities with respect to vascular complexity, as highlighted by tests on data from the mouse cerebral cortex. This work establishes a new class of graph-based surrogate models for microvascular flow, grounded in physical laws and equipped with inductive biases that mirror mass conservation and rheological models, opening new directions for real-time inference in vascular modeling and biomedical applications.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2512.02696.pdf' target='_blank'>https://arxiv.org/pdf/2512.02696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omid Reza Heidari, Yang Wang, Xinxin Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02696">ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2511.12498.pdf' target='_blank'>https://arxiv.org/pdf/2511.12498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jongseong Bae, Junwoo Ha, Jinnyeong Heo, Yeongin Lee, Ha Young Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12498">Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2511.07877.pdf' target='_blank'>https://arxiv.org/pdf/2511.07877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Gao, Shuguang Dou, Junzhou Li, Zhiheng Yu, Yin Li, Dongsheng Jiang, Shugong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07877">Visual Bridge: Universal Visual Perception Representations Generating</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2511.06648.pdf' target='_blank'>https://arxiv.org/pdf/2511.06648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Hui, Sanping Zhou, Ye deng, Wenli Huang, Jinjun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06648">FreqGRL: Suppressing Low-Frequency Bias and Mining High-Frequency Knowledge for Cross-Domain Few-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain few-shot learning (CD-FSL) aims to recognize novel classes with only a few labeled examples under significant domain shifts. While recent approaches leverage a limited amount of labeled target-domain data to improve performance, the severe imbalance between abundant source data and scarce target data remains a critical challenge for effective representation learning. We present the first frequency-space perspective to analyze this issue and identify two key challenges: (1) models are easily biased toward source-specific knowledge encoded in the low-frequency components of source data, and (2) the sparsity of target data hinders the learning of high-frequency, domain-generalizable features. To address these challenges, we propose \textbf{FreqGRL}, a novel CD-FSL framework that mitigates the impact of data imbalance in the frequency space. Specifically, we introduce a Low-Frequency Replacement (LFR) module that substitutes the low-frequency components of source tasks with those from the target domain to create new source tasks that better align with target characteristics, thus reducing source-specific biases and promoting generalizable representation learning. We further design a High-Frequency Enhancement (HFE) module that filters out low-frequency components and performs learning directly on high-frequency features in the frequency space to improve cross-domain generalization. Additionally, a Global Frequency Filter (GFF) is incorporated to suppress noisy or irrelevant frequencies and emphasize informative ones, mitigating overfitting risks under limited target supervision. Extensive experiments on five standard CD-FSL benchmarks demonstrate that our frequency-guided framework achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2511.02996.pdf' target='_blank'>https://arxiv.org/pdf/2511.02996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ailar Mahdizadeh, Puria Azadi Moghadam, Xiangteng He, Shahriar Mirabbasi, Panos Nasiopoulos, Leonid Sigal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02996">SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have demonstrated strong cross-modal capabilities, yet most work remains limited to 2D data and assumes binary supervision (i.e., positive vs. negative pairs), overlooking the continuous and structured dependencies present in volumetric data such as CT. Existing approaches often treat volumetric scans as independent 2D slices, compromising spatial coherence and underutilizing rich clinical semantics. We propose SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework that integrates (i) volumetric spatial semantics to preserve anatomical structure and (ii) domain-aware, knowledge-infused semantics (e.g., radiological ontologies) to guide alignment. This yields structurally consistent and semantically grounded representations under limited supervision, demonstrating strong cross-task transferability (retrieval, report generation, and classification), and cross-domain generalizability with consistent gains without further fine-tuning. In particular, compared to the previous state of the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval, improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an out-of-domain external dataset, we observe consistent gains, indicating the cross-task and cross-domain generalization ability of SCALE-VLP.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2510.17928.pdf' target='_blank'>https://arxiv.org/pdf/2510.17928.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>He Du, Bowen Li, Aijun Yang, Siyang He, Qipeng Guo, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17928">EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2510.12547.pdf' target='_blank'>https://arxiv.org/pdf/2510.12547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madi Matymov, Ba-Hien Tran, Maurizio Filippone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12547">Universal Adaptive Environment Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An open problem in Machine Learning is how to avoid models to exploit spurious correlations in the data; a famous example is the background-label shortcut in the Waterbirds dataset. A common remedy is to train a model across multiple environments; in the Waterbirds dataset, this corresponds to training by randomizing the background. However, selecting the right environments is a challenging problem, given that these are rarely known a priori. We propose Universal Adaptive Environment Discovery (UAED), a unified framework that learns a distribution over data transformations that instantiate environments, and optimizes any robust objective averaged over this learned distribution. UAED yields adaptive variants of IRM, REx, GroupDRO, and CORAL without predefined groups or manual environment design. We provide a theoretical analysis by providing PAC-Bayes bounds and by showing robustness to test environment distributions under standard conditions. Empirically, UAED discovers interpretable environment distributions and improves worst-case accuracy on standard benchmarks, while remaining competitive on mean accuracy. Our results indicate that making environments adaptive is a practical route to out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2510.03540.pdf' target='_blank'>https://arxiv.org/pdf/2510.03540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Schwonberg, Hanno Gottschalk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03540">Domain Generalization for Semantic Segmentation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization of deep neural networks to unknown domains is a major challenge despite their tremendous progress in recent years. For this reason, the dynamic area of domain generalization (DG) has emerged. In contrast to unsupervised domain adaptation, there is no access to or knowledge about the target domains, and DG methods aim to generalize across multiple different unseen target domains. Domain generalization is particularly relevant for the task semantic segmentation which is used in several areas such as biomedicine or automated driving. This survey provides a comprehensive overview of the rapidly evolving topic of domain generalized semantic segmentation. We cluster and review existing approaches and identify the paradigm shift towards foundation-model-based domain generalization. Finally, we provide an extensive performance comparison of all approaches, which highlights the significant influence of foundation models on domain generalization. This survey seeks to advance domain generalization research and inspire scientists to explore new research directions.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2509.25637.pdf' target='_blank'>https://arxiv.org/pdf/2509.25637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kotaro Yoshida, Atsushi Nitanda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25637">How Does Preconditioning Guide Feature Learning in Deep Neural Networks?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preconditioning is widely used in machine learning to accelerate convergence on the empirical risk, yet its role on the expected risk remains underexplored. In this work, we investigate how preconditioning affects feature learning and generalization performance. We first show that the input information available to the model is conveyed solely through the Gram matrix defined by the preconditioner's metric, thereby inducing a controllable spectral bias on feature learning. Concretely, instantiating the preconditioner as the $p$-th power of the input covariance matrix and within a single-index teacher model, we prove that in generalization, the exponent $p$ and the alignment between the teacher and the input spectrum are crucial factors. We further investigate how the interplay between these factors influences feature learning from three complementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution generalization, and (iii) Forward knowledge transfer. Our results indicate that the learned feature representations closely mirror the spectral bias introduced by the preconditioner -- favoring components that are emphasized and exhibiting reduced sensitivity to those that are suppressed. Crucially, we demonstrate that generalization is significantly enhanced when this spectral bias is aligned with that of the teacher.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2509.23688.pdf' target='_blank'>https://arxiv.org/pdf/2509.23688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soroosh Safari Loaliyan, Jose-Luis Ambite, Paul M. Thompson, Neda Jahanshad, Greg Ver Steeg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23688">FedDAPL: Toward Client-Private Generalization in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) trains models locally at each research center or clinic and aggregates only model updates, making it a natural fit for medical imaging, where strict privacy laws forbid raw data sharing. A major obstacle is scanner-induced domain shift: non-biological variations in hardware or acquisition protocols can cause models to fail on external sites. Most harmonization methods correct this shift by directly comparing data across sites, conflicting with FL's privacy constraints. Domain Generalization (DG) offers a privacy-friendly alternative - learning site-invariant representations without sharing raw data - but standard DG pipelines still assume centralized access to multi-site data, again violating FL's guarantees. This paper meets these difficulties with a straightforward integration of a Domain-Adversarial Neural Network (DANN) within the FL process. After demonstrating that a naive federated DANN fails to converge, we propose a proximal regularization method that stabilizes adversarial training among clients. Experiments on T1-weighted 3-D brain MRIs from the OpenBHB dataset, performing brain-age prediction on participants aged 6-64 y (mean 22+/-6 y; 45 percent male) in training and 6-79 y (mean 19+/-13 y; 55 percent male) in validation, show that training on 15 sites and testing on 19 unseen sites yields superior cross-site generalization over FedAvg and ERM while preserving data privacy.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2509.21234.pdf' target='_blank'>https://arxiv.org/pdf/2509.21234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abi Aryan, Zac Liu, Aaron Childress
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21234">AbideGym: Turning Static RL Worlds into Adaptive Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agents trained with reinforcement learning often develop brittle policies that fail when dynamics shift, a problem amplified by static benchmarks. AbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and scalable complexity to enforce intra-episode adaptation. By exposing weaknesses in static policies and promoting resilience, AbideGym provides a modular, reproducible evaluation framework for advancing research in curriculum learning, continual learning, and robust generalization.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2509.16897.pdf' target='_blank'>https://arxiv.org/pdf/2509.16897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuewan He, Jielei Wang, Zihan Cheng, Yuchen Su, Shiyue Huang, Guoming Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16897">PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access to the real in-distribution (ID) data. While existing methods perform well on small-scale images, they suffer from mode collapse when synthesizing large-scale images, resulting in limited knowledge transfer. Recently, leveraging advanced generative models to synthesize photorealistic images has emerged as a promising alternative. Nevertheless, directly using off-the-shelf diffusion to generate datasets faces the precision-recall challenges: 1) ensuring synthetic data aligns with the real distribution, and 2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a precision-recall informed synthesis method. Specifically, we introduce Energy-guided Distribution Alignment to avoid the generation of out-of-distribution samples, and design the Diversified Prompt Engineering to enhance coverage of the real ID manifold. Extensive experiments on various large-scale image datasets demonstrate the superiority of PRISM. Moreover, we demonstrate that models trained with PRISM exhibit strong domain generalization.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2509.03614.pdf' target='_blank'>https://arxiv.org/pdf/2509.03614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungho Choe, Xiaoli Qin, Abubakr Shafique, Amanda Dy, Susan Done, Dimitrios Androutsos, April Khademi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03614">Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Counting mitotic figures is time-intensive for pathologists and leads to inter-observer variability. Artificial intelligence (AI) promises a solution by automatically detecting mitotic figures while maintaining decision consistency. However, AI tools are susceptible to domain shift, where a significant drop in performance can occur due to differences in the training and testing sets, including morphological diversity between organs, species, and variations in staining protocols. Furthermore, the number of mitoses is much less than the count of normal nuclei, which introduces severely imbalanced data for the detection task. In this work, we formulate mitosis detection as a pixel-level segmentation and propose a teacher-student model that simultaneously addresses mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our method is based on a UNet segmentation backbone that integrates domain generalization modules, namely contrastive representation learning and domain-adversarial training. A teacher-student strategy is employed to generate pixel-level pseudo-masks not only for annotated mitoses and hard negatives but also for normal nuclei, thereby enhancing feature discrimination and improving robustness against domain shift. For the classification task, we introduce a multi-scale CNN classifier that leverages feature maps from the segmentation model within a multi-task learning paradigm. On the preliminary test set, the algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of 0.8414 in Track 2, demonstrating the effectiveness of integrating segmentation-based detection and classification into a unified framework for robust mitosis analysis.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2509.02837.pdf' target='_blank'>https://arxiv.org/pdf/2509.02837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Payel Santra, Madhusudan Ghosh, Debasis Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02837">HF-RAG: Hierarchical Fusion-based RAG with Multiple Sources and Rankers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging both labeled (input-output associations) and unlabeled data (wider contextual grounding) may provide complementary benefits in retrieval augmented generation (RAG). However, effectively combining evidence from these heterogeneous sources is challenging as the respective similarity scores are not inter-comparable. Additionally, aggregating beliefs from the outputs of multiple rankers can improve the effectiveness of RAG. Our proposed method first aggregates the top-documents from a number of IR models using a standard rank fusion technique for each source (labeled and unlabeled). Next, we standardize the retrieval score distributions within each source by applying z-score transformation before merging the top-retrieved documents from the two sources. We evaluate our approach on the fact verification task, demonstrating that it consistently improves over the best-performing individual ranker or source and also shows better out-of-domain generalization.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2509.01554.pdf' target='_blank'>https://arxiv.org/pdf/2509.01554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Chih Lee, Zelong Liu, Hamza Ahmed, Spencer Kim, Sean Huver, Vishwesh Nath, Zahi A. Fayad, Timothy Deyer, Xueyan Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01554">Unified Supervision For Vision-Language Modeling in 3D Computed Tomography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purpose vision-language models (VLMs) have emerged as promising tools in radiology, offering zero-shot capabilities that mitigate the need for large labeled datasets. However, in high-stakes domains like diagnostic radiology, these models often lack the discriminative precision required for reliable clinical use. This challenge is compounded by the scarcity and heterogeneity of publicly available volumetric CT datasets, which vary widely in annotation formats and granularity. To address these limitations, we introduce Uniferum, a volumetric VLM that unifies diverse supervision signals, encoded in classification labels and segmentation masks, into a single training framework. By harmonizing three public 3D CT datasets with distinct annotations, Uniferum achieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark by 7% compared to CLIP-based and conventional multi-label convolutional models. The model demonstrates robust out-of-distribution generalization, with observed evidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT datasets. Our results highlight the effectiveness of integrating heterogeneous annotations and body segmentation to enhance model performance, setting a new direction for clinically reliable, data-efficient VLMs in 3D medical imaging.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2508.21035.pdf' target='_blank'>https://arxiv.org/pdf/2508.21035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gennaro Percannella, Mattia Sarno, Francesco Tortorella, Mario Vento
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21035">A multi-task neural network for atypical mitosis recognition under domain shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing atypical mitotic figures in histopathology images allows physicians to correctly assess tumor aggressiveness. Although machine learning models could be exploited for automatically performing such a task, under domain shift these models suffer from significative performance drops. In this work, an approach based on multi-task learning is proposed for addressing this problem. By exploiting auxiliary tasks, correlated to the main classification task, the proposed approach, submitted to the track 2 of the MItosis DOmain Generalization (MIDOG) challenge, aims to aid the model to focus only on the object to classify, ignoring the domain varying background of the image. The proposed approach shows promising performance in a preliminary evaluation conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25 challenge.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2508.21033.pdf' target='_blank'>https://arxiv.org/pdf/2508.21033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gennaro Percannella, Mattia Sarno, Francesco Tortorella, Mario Vento
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21033">Mitosis detection in domain shift scenarios: a Mamba-based approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitosis detection in histopathology images plays a key role in tumor assessment. Although machine learning algorithms could be exploited for aiding physicians in accurately performing such a task, these algorithms suffer from significative performance drop when evaluated on images coming from domains that are different from the training ones. In this work, we propose a Mamba-based approach for mitosis detection under domain shift, inspired by the promising performance demonstrated by Mamba in medical imaging segmentation tasks. Specifically, our approach exploits a VM-UNet architecture for carrying out the addressed task, as well as stain augmentation operations for further improving model robustness against domain shift. Our approach has been submitted to the track 1 of the MItosis DOmain Generalization (MIDOG) challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show large room for improvement for the proposed method.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2508.15716.pdf' target='_blank'>https://arxiv.org/pdf/2508.15716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongqi Li, Yitong Chen, Yujuan Wang, Weihang Ni, Haodong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15716">Foundation Models for Cross-Domain EEG Analysis Application: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography (EEG) analysis stands at the forefront of neuroscience and artificial intelligence research, where foundation models are reshaping the traditional EEG analysis paradigm by leveraging their powerful representational capacity and cross-modal generalization. However, the rapid proliferation of these techniques has led to a fragmented research landscape, characterized by diverse model roles, inconsistent architectures, and a lack of systematic categorization. To bridge this gap, this study presents the first comprehensive modality-oriented taxonomy for foundation models in EEG analysis, systematically organizing research advances based on output modalities of the native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal frameworks. We rigorously analyze each category's research ideas, theoretical foundations, and architectural innovations, while highlighting open challenges such as model interpretability, cross-domain generalization, and real-world applicability in EEG-based systems. By unifying this dispersed field, our work not only provides a reference framework for future methodology development but accelerates the translation of EEG foundation models into scalable, interpretable, and online actionable solutions.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2508.14094.pdf' target='_blank'>https://arxiv.org/pdf/2508.14094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Pikus, Pratyush Ranjan Tiwari, Burton Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14094">Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collecting high-quality training examples for language model fine-tuning is expensive, with practical budgets limiting the amount of data that can be procured. We investigate whether example difficulty affects GRPO training effectiveness by comparing selection strategies (easy, medium, hard, random) across multiple models and reasoning tasks. Training on the hardest 10\% of examples (those where the base model fails most often) yields dramatic performance gains up to 47\%, while easy examples produce minimal improvements of 3-15\%. This occurs because GRPO requires outcome variance to generate learning signals; hard examples maintain mixed success/failure outcomes throughout training while easy examples quickly converge to consistent success, eliminating learning opportunities. Moreover, models trained on hard examples show superior out-of-distribution generalization, with only hard-trained models achieving meaningful gains on the AIME2025 benchmark. Our findings provide clear guidance: when budget-constrained, prioritize collecting and annotating examples where your base model struggles, as these drive nearly all learning value in GRPO fine-tuning
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2508.08549.pdf' target='_blank'>https://arxiv.org/pdf/2508.08549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Li, Pengcheng Zhou, Linye Ma, Wenyi Zhao, Huihua Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08549">Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Both limited annotation and domain shift are significant challenges frequently encountered in medical image segmentation, leading to derivative scenarios like semi-supervised medical (SSMIS), semi-supervised medical domain generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA). Conventional methods are generally tailored to specific tasks in isolation, the error accumulation hinders the effective utilization of unlabeled data and limits further improvements, resulting in suboptimal performance when these issues occur. In this paper, we aim to develop a generic framework that masters all three tasks. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data and increasing the diversity of the model. To tackle this issue, we employ a Diverse Teaching and Label Propagation Network (DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation. Our DTLP-Net involves a single student model and two diverse teacher models, which can generate reliable pseudo-labels for the student model. The first teacher model decouple the training process with labeled and unlabeled data, The second teacher is momentum-updated periodically, thus generating reliable yet divers pseudo-labels. To fully utilize the information within the data, we adopt inter-sample and intra-sample data augmentation to learn the global and local knowledge. In addition, to further capture the voxel-level correlations, we propose label propagation to enhance the model robust. We evaluate our proposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG tasks. The results showcase notable improvements compared to state-of-the-art methods across all five settings, indicating the potential of our framework to tackle more challenging SSL scenarios.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2508.06248.pdf' target='_blank'>https://arxiv.org/pdf/2508.06248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrii Yermakov, Jan Cech, Jiri Matas, Mario Fritz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06248">Deepfake Detection that Generalizes Across Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generalization of deepfake detectors to unseen manipulation techniques remains a challenge for practical deployment. Although many approaches adapt foundation models by introducing significant architectural complexity, this work demonstrates that robust generalization is achievable through a parameter-efficient adaptation of a pre-trained CLIP vision encoder. The proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters (0.03% of the total) and enhances generalization by enforcing a hyperspherical feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from 2019 to 2025. The proposed method achieves state-of-the-art performance, outperforming more complex, recent approaches in average cross-dataset AUROC. Our analysis yields two primary findings for the field: 1) training on paired real-fake data from the same source video is essential for mitigating shortcut learning and improving generalization, and 2) detection difficulty on academic datasets has not strictly increased over time, with models trained on older, diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method, proving that state-of-the-art generalization is attainable by making targeted, minimal changes to a pre-trained CLIP model. The code will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2507.21367.pdf' target='_blank'>https://arxiv.org/pdf/2507.21367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>I-Hsiang Chen, Hua-En Chang, Wei-Ting Chen, Jenq-Neng Hwang, Sy-Yen Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21367">Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging task, as domain shifts in unseen environments can severely compromise model performance. While recent studies enhance feature alignment by projecting features into the source domain, they often neglect intrinsic latent domain priors, leading to suboptimal results. In this paper, we introduce PDAF, a Probabilistic Diffusion Alignment Framework that enhances the generalization of existing segmentation networks through probabilistic diffusion modeling. PDAF introduces a Latent Domain Prior (LDP) to capture domain shifts and uses this prior as a conditioning factor to align both source and unseen target domains. To achieve this, PDAF integrates into a pre-trained segmentation model and utilizes paired source and pseudo-target images to simulate latent domain shifts, enabling LDP modeling. The framework comprises three modules: the Latent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the Domain Compensation Module (DCM) adjusts feature representations to mitigate domain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion process to estimate the LDP without requiring paired samples. This design enables PDAF to iteratively model domain shifts, progressively refining feature representations to enhance generalization under complex target conditions. Extensive experiments validate the effectiveness of PDAF across diverse and challenging urban scenes.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2507.14227.pdf' target='_blank'>https://arxiv.org/pdf/2507.14227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khoi Do, Duong Nguyen, Nam-Khanh Le, Quoc-Viet Pham, Binh-Son Hua, Won-Joo Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14227">Domain Generalization via Pareto Optimal Gradient Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we address the gradient-based domain generalization problem, where predictors aim for consistent gradient directions across different domains. Existing methods have two main challenges. First, minimization of gradient empirical distance or gradient inner products (GIP) leads to gradient fluctuations among domains, thereby hindering straightforward learning. Second, the direct application of gradient learning to the joint loss function can incur high computation overheads due to second-order derivative approximation. To tackle these challenges, we propose a new Pareto Optimality Gradient Matching (POGM) method. In contrast to existing methods that add gradient matching as regularization, we leverage gradient trajectories as collected data and apply independent training at the meta-learner. In the meta-update, we maximize GIP while limiting the learned gradient from deviating too far from the empirical risk minimization gradient trajectory. By doing so, the aggregate gradient can incorporate knowledge from all domains without suffering gradient fluctuation towards any particular domain. Experimental evaluations on datasets from DomainBed demonstrate competitive results yielded by POGM against other baselines while achieving computational efficiency.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2507.13966.pdf' target='_blank'>https://arxiv.org/pdf/2507.13966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhishma Dedhia, Yuval Kansal, Niraj K. Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13966">Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. We present a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. We fine-tune language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, we validate our approach in medicine, where reliable KGs exist. Using a medical KG, we curate 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. We fine-tune the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Our experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, we envision a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2506.23088.pdf' target='_blank'>https://arxiv.org/pdf/2506.23088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Zhou, Jiayu Tang, Xiaoyan Xiao, Yueyao Lin, Linkai Liu, Zipeng Guo, Hao Fei, Xiaobo Xia, Chao Gou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23088">Where, What, Why: Towards Explainable Driver Attention Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modeling task-driven attention in driving is a fundamental challenge for both autonomous vehicles and cognitive science. Existing methods primarily predict where drivers look by generating spatial heatmaps, but fail to capture the cognitive motivations behind attention allocation in specific contexts, which limits deeper understanding of attention mechanisms. To bridge this gap, we introduce Explainable Driver Attention Prediction, a novel task paradigm that jointly predicts spatial attention regions (where), parses attended semantics (what), and provides cognitive reasoning for attention allocation (why). To support this, we present W3DA, the first large-scale explainable driver attention dataset. It enriches existing benchmarks with detailed semantic and causal annotations across diverse driving scenarios, including normal conditions, safety-critical situations, and traffic accidents. We further propose LLada, a Large Language model-driven framework for driver attention prediction, which unifies pixel modeling, semantic parsing, and cognitive reasoning within an end-to-end architecture. Extensive experiments demonstrate the effectiveness of LLada, exhibiting robust generalization across datasets and driving conditions. This work serves as a key step toward a deeper understanding of driver attention mechanisms, with significant implications for autonomous driving, intelligent driver training, and human-computer interaction.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2506.05292.pdf' target='_blank'>https://arxiv.org/pdf/2506.05292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Declan A. Norton, Yuanzhao Zhang, Michelle Girvan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05292">Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning techniques offer an effective approach to modeling dynamical systems solely from observed data. However, without explicit structural priors -- built-in assumptions about the underlying dynamics -- these techniques typically struggle to generalize to aspects of the dynamics that are poorly represented in the training data. Here, we demonstrate that reservoir computing -- a simple, efficient, and versatile machine learning framework often used for data-driven modeling of dynamical systems -- can generalize to unexplored regions of state space without explicit structural priors. First, we describe a multiple-trajectory training scheme for reservoir computers that supports training across a collection of disjoint time series, enabling effective use of available training data. Then, applying this training scheme to multistable dynamical systems, we show that RCs trained on trajectories from a single basin of attraction can achieve out-of-domain generalization by capturing system behavior in entirely unobserved basins.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2506.03972.pdf' target='_blank'>https://arxiv.org/pdf/2506.03972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guohua Wu, Shengqi Chen, Pengchao Deng, Wenting Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03972">MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complete blood cell detection holds significant value in clinical diagnostics. Conventional manual microscopy methods suffer from time inefficiency and diagnostic inaccuracies. Existing automated detection approaches remain constrained by high deployment costs and suboptimal accuracy. While deep learning has introduced powerful paradigms to this field, persistent challenges in detecting overlapping cells and multi-scale objects hinder practical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a blood cell detection model based on the YOLOv11 framework, incorporating three key architectural innovations to enhance detection performance. Specifically, the multi-scale dilated residual module (MS-DRM) replaces the original C3K2 modules to improve multi-scale discriminability; the dynamic cross-path feature enhancement module (DCFEM) enables the fusion of hierarchical features from the backbone with aggregated features from the neck to enhance feature representations; and the light adaptive-weight downsampling module (LADS) improves feature downsampling through adaptive spatial weighting while reducing computational complexity. Experimental results on the CBC benchmark demonstrate that MS-YOLO achieves precise detection of overlapping cells and multi-scale objects, particularly small targets such as platelets, achieving an mAP@50 of 97.4% that outperforms existing models. Further validation on the supplementary WBCDD dataset confirms its robust generalization capability. Additionally, with a lightweight architecture and real-time inference efficiency, MS-YOLO meets clinical deployment requirements, providing reliable technical support for standardized blood pathology assessment.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2506.03709.pdf' target='_blank'>https://arxiv.org/pdf/2506.03709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aniruddh Sikdar, Aditya Gandhamal, Suresh Sundaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03709">AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary semantic segmentation (OVSS) involves assigning labels to each pixel in an image based on textual descriptions, leveraging world models like CLIP. However, they encounter significant challenges in cross-domain generalization, hindering their practical efficacy in real-world applications. Embodied AI systems are transforming autonomous navigation for ground vehicles and drones by enhancing their perception abilities, and in this study, we present AetherVision-Bench, a benchmark for multi-angle segmentation across aerial, and ground perspectives, which facilitates an extensive evaluation of performance across different viewing angles and sensor modalities. We assess state-of-the-art OVSS models on the proposed benchmark and investigate the key factors that impact the performance of zero-shot transfer models. Our work pioneers the creation of a robustness benchmark, offering valuable insights and establishing a foundation for future research.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2505.18035.pdf' target='_blank'>https://arxiv.org/pdf/2505.18035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naseem Khan, Tuan Nguyen, Amine Bermak, Issa Khalil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18035">CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of sophisticated AI-generated deepfakes poses critical challenges for digital media authentication and societal security. While existing detection methods perform well within specific generative domains, they exhibit significant performance degradation when applied to manipulations produced by unseen architectures--a fundamental limitation as generative technologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal Embeddings), a framework that dynamically integrates visual, textual, and frequency-domain features through a multi-head cross-attention mechanism to establish robust cross-domain generalization. Extensive experiments demonstrate CAMME's superiority over state-of-the-art methods, yielding improvements of 12.56% on natural scenes and 13.25% on facial deepfakes. The framework demonstrates exceptional resilience, maintaining (over 91%) accuracy under natural image perturbations and achieving 89.01% and 96.14% accuracy against PGD and FGSM adversarial attacks, respectively. Our findings validate that integrating complementary modalities through cross-attention enables more effective decision boundary realignment for reliable deepfake detection across heterogeneous generative architectures.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2505.16778.pdf' target='_blank'>https://arxiv.org/pdf/2505.16778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianing Chen, Si Huo, Borui Jiang, Hailin Hu, Xinghao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16778">Single Domain Generalization for Few-Shot Counting via Universal Representation Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot counting estimates the number of target objects in an image using only a few annotated exemplars. However, domain shift severely hinders existing methods to generalize to unseen scenarios. This falls into the realm of single domain generalization that remains unexplored in few-shot counting. To solve this problem, we begin by analyzing the main limitations of current methods, which typically follow a standard pipeline that extract the object prototypes from exemplars and then match them with image feature to construct the correlation map. We argue that existing methods overlook the significance of learning highly generalized prototypes. Building on this insight, we propose the first single domain generalization few-shot counting model, Universal Representation Matching, termed URM. Our primary contribution is the discovery that incorporating universal vision-language representations distilled from a large scale pretrained vision-language model into the correlation construction process substantially improves robustness to domain shifts without compromising in domain performance. As a result, URM achieves state-of-the-art performance on both in domain and the newly introduced domain generalization setting.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2504.20568.pdf' target='_blank'>https://arxiv.org/pdf/2504.20568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danilo Avola, Federica Bruni, Gian Luca Foresti, Daniele Pannone, Amedeo Ranaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20568">Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze environments, enabling tasks such as tracking people, detecting intrusions, and recognizing gestures. The rise of this technology is driven by the IEEE 802.11bf standard and growing demand for tools that can ensure privacy and operate through obstacles. However, the performance of Wi-Fi sensing is heavily influenced by environmental conditions, especially when extracting spatial and temporal features from the surrounding scene. A key challenge is achieving robust generalization across domains, ensuring stable performance even when the sensing environment changes significantly. This paper introduces a novel deep learning model for cross-domain adaptation of Wi-Fi signals, inspired by physical signal shielding. The model uses a Relativistic average Generative Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM) architectures for both the generator and discriminator. To simulate physical shielding, an acrylic box lined with electromagnetic shielding fabric was constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from various materials both inside (domain-free) and outside (domain-dependent) the box to train the model. A multi-class Support Vector Machine (SVM) was trained on domain-free spectra and tested on signals denoised by the RaGAN. The system achieved 96% accuracy and demonstrated strong material discrimination capabilities, offering potential for use in security applications to identify concealed objects based on their composition.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2504.19882.pdf' target='_blank'>https://arxiv.org/pdf/2504.19882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runhui Zhang, Sijin Zhou, Zhuang Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19882">Federated Out-of-Distribution Generalization: A Causal Augmentation View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning aims to collaboratively model by integrating multi-source information to obtain a model that can generalize across all client data. Existing methods often leverage knowledge distillation or data augmentation to mitigate the negative impact of data bias across clients. However, the limited performance of teacher models on out-of-distribution samples and the inherent quality gap between augmented and original data hinder their effectiveness and they typically fail to leverage the advantages of incorporating rich contextual information. To address these limitations, this paper proposes a Federated Causal Augmentation method, termed FedCAug, which employs causality-inspired data augmentation to break the spurious correlation between attributes and categories. Specifically, it designs a causal region localization module to accurately identify and decouple the background and objects in the image, providing rich contextual information for causal data augmentation. Additionally, it designs a causality-inspired data augmentation module that integrates causal features and within-client context to generate counterfactual samples. This significantly enhances data diversity, and the entire process does not require any information sharing between clients, thereby contributing to the protection of data privacy. Extensive experiments conducted on three datasets reveal that FedCAug markedly reduces the model's reliance on background to predict sample labels, achieving superior performance compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2504.08020.pdf' target='_blank'>https://arxiv.org/pdf/2504.08020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Bi, Jingjun Yi, Haolan Zhan, Wei Ji, Gui-Song Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08020">Learning Fine-grained Domain Generalization via Hyperbolic State Space Hallucination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained domain generalization (FGDG) aims to learn a fine-grained representation that can be well generalized to unseen target domains when only trained on the source domain data. Compared with generic domain generalization, FGDG is particularly challenging in that the fine-grained category can be only discerned by some subtle and tiny patterns. Such patterns are particularly fragile under the cross-domain style shifts caused by illumination, color and etc. To push this frontier, this paper presents a novel Hyperbolic State Space Hallucination (HSSH) method. It consists of two key components, namely, state space hallucination (SSH) and hyperbolic manifold consistency (HMC). SSH enriches the style diversity for the state embeddings by firstly extrapolating and then hallucinating the source images. Then, the pre- and post- style hallucinate state embeddings are projected into the hyperbolic manifold. The hyperbolic state space models the high-order statistics, and allows a better discernment of the fine-grained patterns. Finally, the hyperbolic distance is minimized, so that the impact of style variation on fine-grained patterns can be eliminated. Experiments on three FGDG benchmarks demonstrate its state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2504.04981.pdf' target='_blank'>https://arxiv.org/pdf/2504.04981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sohyun Lee, Nayeong Kim, Juwon Kang, Seong Joon Oh, Suha Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04981">TestDG: Test-time Domain Generalization for Continual Test-time Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies continual test-time adaptation (CTTA), the task of adapting a model to constantly changing unseen domains in testing while preserving previously learned knowledge. Existing CTTA methods mostly focus on adaptation to the current test domain only, overlooking generalization to arbitrary test domains a model may face in the future. To tackle this limitation, we present a novel online test-time domain generalization framework for CTTA, dubbed TestDG. TestDG aims to learn features invariant to both current and previous test domains on the fly during testing, improving the potential for effective generalization to future domains. To this end, we propose a new model architecture and a test-time adaptation strategy dedicated to learning domain-invariant features, along with a new data structure and optimization algorithm for effectively managing information from previous test domains. TestDG achieved state of the art on four public CTTA benchmarks. Moreover, it showed superior generalization to unseen test domains.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2504.00839.pdf' target='_blank'>https://arxiv.org/pdf/2504.00839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00839">Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2504.00186.pdf' target='_blank'>https://arxiv.org/pdf/2504.00186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olawale Salaudeen, Nicole Chiou, Shiny Weng, Sanmi Koyejo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00186">Are Domain Generalization Benchmarks with Accuracy on the Line Misspecified?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spurious correlations, unstable statistical shortcuts a model can exploit, are expected to degrade performance out-of-distribution (OOD). However, across many popular OOD generalization benchmarks, vanilla empirical risk minimization (ERM) often achieves the highest OOD accuracy. Moreover, gains in in-distribution accuracy generally improve OOD accuracy, a phenomenon termed accuracy on the line, which contradicts the expected harm of spurious correlations. We show that these observations are an artifact of misspecified OOD datasets that do not include shifts in spurious correlations that harm OOD generalization, the setting they are meant to evaluate. Consequently, current practice evaluates "robustness" without truly stressing the spurious signals we seek to eliminate; our work pinpoints when that happens and how to fix it. Contributions. (i) We derive necessary and sufficient conditions for a distribution shift to reveal a model's reliance on spurious features; when these conditions hold, "accuracy on the line" disappears. (ii) We audit leading OOD datasets and find that most still display accuracy on the line, suggesting they are misspecified for evaluating robustness to spurious correlations. (iii) We catalog the few well-specified datasets and summarize generalizable design principles, such as identifying datasets of natural interventions (e.g., a pandemic), to guide future well-specified benchmarks.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2503.23430.pdf' target='_blank'>https://arxiv.org/pdf/2503.23430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjun Song, Youngsik Hwang, Jonghun Lee, Heechang Lee, Dong-Young Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23430">DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) aims to learn models that perform well on unseen target domains by training on multiple source domains. Sharpness-Aware Minimization (SAM), known for finding flat minima that improve generalization, has therefore been widely adopted in DG. However, our analysis reveals that SAM in DG may converge to \textit{fake flat minima}, where the total loss surface appears flat in terms of global sharpness but remains sharp with respect to individual source domains. To understand this phenomenon more precisely, we formalize the average worst-case domain risk as the maximum loss under domain distribution shifts within a bounded divergence, and derive a generalization bound that reveals the limitations of global sharpness-aware minimization. In contrast, we show that individual sharpness provides a valid upper bound on this risk, making it a more suitable proxy for robust domain generalization. Motivated by these insights, we shift the DG paradigm toward minimizing individual sharpness across source domains. We propose \textit{Decreased-overhead Gradual SAM (DGSAM)}, which applies gradual domain-wise perturbations in a computationally efficient manner to consistently reduce individual sharpness. Extensive experiments demonstrate that DGSAM not only improves average accuracy but also reduces performance variance across domains, while incurring less computational overhead than SAM.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2503.20839.pdf' target='_blank'>https://arxiv.org/pdf/2503.20839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amr Mousa, Neil Karavis, Michele Caprio, Wei Pan, Richard Allmendinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20839">TAR: Teacher-Aligned Representations via Contrastive Learning for Quadrupedal Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed using the teacher-student paradigm, where a privileged teacher guides a proprioceptive student policy. However, key challenges such as representation misalignment between privileged teacher and proprioceptive-only student, covariate shift due to behavioral cloning, and lack of deployable adaptation; lead to poor generalization in real-world scenarios. We propose Teacher-Aligned Representations via Contrastive Learning (TAR), a framework that leverages privileged information with self-supervised contrastive learning to bridge this gap. By aligning representations to a privileged teacher in simulation via contrastive objectives, our student policy learns structured latent spaces and exhibits robust generalization to Out-of-Distribution (OOD) scenarios, surpassing the fully privileged "Teacher". Results showed accelerated training by 2x compared to state-of-the-art baselines to achieve peak performance. OOD scenarios showed better generalization by 40% on average compared to existing methods. Moreover, TAR transitions seamlessly into learning during deployment without requiring privileged states, setting a new benchmark in sample-efficient, adaptive locomotion and enabling continual fine-tuning in real-world scenarios. Open-source code and videos are available at https://amrmousa.com/TARLoco/.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2503.10177.pdf' target='_blank'>https://arxiv.org/pdf/2503.10177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yirong Sun, Yanjun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10177">PRISM: Preference Refinement via Implicit Scene Modeling for 3D Vision-Language Preference-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose PRISM, a novel framework designed to overcome the limitations of 2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point cloud modeling and future-aware preference refinement. At its core, PRISM adopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and viewpoint biases, ensuring more stable and spatially consistent preference signals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to incorporate long-horizon considerations, thereby preventing the short-sighted feedback often seen in static preference comparisons. In contrast to conventional PBRL techniques, this integration of 3D perception and future-oriented reasoning leads to significant gains in preference agreement rates, faster policy convergence, and robust generalization across unseen robotic environments. Our empirical results, spanning tasks such as robotic manipulation and autonomous navigation, highlight PRISM's potential for real-world applications where precise spatial understanding and reliable long-term decision-making are critical. By bridging 3D geometric awareness with CoT-driven preference modeling, PRISM establishes a comprehensive foundation for scalable, human-aligned reinforcement learning.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2503.00470.pdf' target='_blank'>https://arxiv.org/pdf/2503.00470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junqi He, Yujie Zhang, Jialu Wang, Tao Wang, Pan Zhang, Chengjie Cai, Jinxing Yang, Xiao Lin, Xiaohui Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00470">Rapid morphology characterization of two-dimensional TMDs and lateral heterostructures based on deep learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two-dimensional (2D) materials and heterostructures exhibit unique physical properties, necessitating efficient and accurate characterization methods. Leveraging advancements in artificial intelligence, we introduce a deep learning-based method for efficiently characterizing heterostructures and 2D materials, specifically MoS2-MoSe2 lateral heterostructures and MoS2 flakes with varying shapes and thicknesses. By utilizing YOLO models, we achieve an accuracy rate of over 94.67% in identifying these materials. Additionally, we explore the application of transfer learning across different materials, which further enhances model performance. This model exhibits robust generalization and anti-interference ability, ensuring reliable results in diverse scenarios. To facilitate practical use, we have developed an application that enables real-time analysis directly from optical microscope images, making the process significantly faster and more cost-effective than traditional methods. This deep learning-driven approach represents a promising tool for the rapid and accurate characterization of 2D materials, opening new avenues for research and development in material science.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2502.19712.pdf' target='_blank'>https://arxiv.org/pdf/2502.19712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manveer Singh Tamber, Suleman Kazi, Vivek Sourabh, Jimmy Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19712">Teaching Dense Retrieval Models to Specialize with Listwise Distillation and LLM Data Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the current state-of-the-art dense retrieval models exhibit strong out-of-domain generalization, they might fail to capture nuanced domain-specific knowledge. In principle, fine-tuning these models for specialized retrieval tasks should yield higher effectiveness than relying on a one-size-fits-all model, but in practice, results can disappoint. We show that standard fine-tuning methods using an InfoNCE loss can unexpectedly degrade effectiveness rather than improve it, even for domain-specific scenarios. This holds true even when applying widely adopted techniques such as hard-negative mining and negative de-noising. To address this, we explore a training strategy that uses listwise distillation from a teacher cross-encoder, leveraging rich relevance signals to fine-tune the retriever. We further explore synthetic query generation using large language models. Through listwise distillation and training with a diverse set of queries ranging from natural user searches and factual claims to keyword-based queries, we achieve consistent effectiveness gains across multiple datasets. Our results also reveal that synthetic queries can rival human-written queries in training utility. However, we also identify limitations, particularly in the effectiveness of cross-encoder teachers as a bottleneck. We release our code and scripts to encourage further research.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2502.19671.pdf' target='_blank'>https://arxiv.org/pdf/2502.19671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ju-Hyeon Nam, Sang-Chul Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19671">Test-Time Modality Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable medical image segmentation is essential for ensuring consistent performance across diverse unseen clinical settings. However, existing methods often overlook the capability to generalize effectively across arbitrary unseen modalities. In this paper, we introduce a novel Test-Time Modality Generalization (TTMG) framework, which comprises two core components: Modality-Aware Style Projection (MASP) and Modality-Sensitive Instance Whitening (MSIW), designed to enhance generalization in arbitrary unseen modality datasets. The MASP estimates the likelihood of a test instance belonging to each seen modality and maps it onto a distribution using modality-specific style bases, guiding its projection effectively. Furthermore, as high feature covariance hinders generalization to unseen modalities, the MSIW is applied during training to selectively suppress modality-sensitive information while retaining modality-invariant features. By integrating MASP and MSIW, the TTMG framework demonstrates robust generalization capabilities for medical image segmentation in unseen modalities a challenge that current methods have largely neglected. We evaluated TTMG alongside other domain generalization techniques across eleven datasets spanning four modalities (colonoscopy, ultrasound, dermoscopy, and radiology), consistently achieving superior segmentation performance across various modality combinations.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2502.09660.pdf' target='_blank'>https://arxiv.org/pdf/2502.09660.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Yao, Qiushi Yang, Miaomiao Cui, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09660">Towards Fine-grained Interactive Segmentation in Images and Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent Segment Anything Models (SAMs) have emerged as foundational visual models for general interactive segmentation. Despite demonstrating robust generalization abilities, they still suffer performance degradations in scenarios demanding accurate masks. Existing methods for high-precision interactive segmentation face a trade-off between the ability to perceive intricate local details and maintaining stable prompting capability, which hinders the applicability and effectiveness of foundational segmentation models. To this end, we present an SAM2Refiner framework built upon the SAM2 backbone. This architecture allows SAM2 to generate fine-grained segmentation masks for both images and videos while preserving its inherent strengths. Specifically, we design a localization augment module, which incorporates local contextual cues to enhance global features via a cross-attention mechanism, thereby exploiting potential detailed patterns and maintaining semantic information. Moreover, to strengthen the prompting ability toward the enhanced object embedding, we introduce a prompt retargeting module to renew the embedding with spatially aligned prompt features. In addition, to obtain accurate high resolution segmentation masks, a mask refinement module is devised by employing a multi-scale cascaded structure to fuse mask features with hierarchical representations from the encoder. Extensive experiments demonstrate the effectiveness of our approach, revealing that the proposed method can produce highly precise masks for both images and videos, surpassing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2502.07281.pdf' target='_blank'>https://arxiv.org/pdf/2502.07281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taro Makino, Ji Won Park, Natasa Tagasovska, Takamasa Kudo, Paula Coelho, Jan-Christian Huetter, Heming Yao, Burkhard Hoeckendorf, Ana Carolina Leote, Stephen Ra, David Richmond, Kyunghyun Cho, Aviv Regev, Romain Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07281">Supervised Contrastive Block Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world datasets often combine data collected under different experimental conditions. This yields larger datasets, but also introduces spurious correlations that make it difficult to model the phenomena of interest. We address this by learning two embeddings to independently represent the phenomena of interest and the spurious correlations. The embedding representing the phenomena of interest is correlated with the target variable $y$, and is invariant to the environment variable $e$. In contrast, the embedding representing the spurious correlations is correlated with $e$. The invariance to $e$ is difficult to achieve on real-world datasets. Our primary contribution is an algorithm called Supervised Contrastive Block Disentanglement (SCBD) that effectively enforces this invariance. It is based purely on Supervised Contrastive Learning, and applies to real-world data better than existing approaches. We empirically validate SCBD on two challenging problems. The first problem is domain generalization, where we achieve strong performance on a synthetic dataset, as well as on Camelyon17-WILDS. We introduce a single hyperparameter $Î±$ to control the degree of invariance to $e$. When we increase $Î±$ to strengthen the degree of invariance, out-of-distribution performance improves at the expense of in-distribution performance. The second problem is batch correction, in which we apply SCBD to preserve biological signal and remove inter-well batch effects when modeling single-cell perturbations from 26 million Optical Pooled Screening images.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2411.16142.pdf' target='_blank'>https://arxiv.org/pdf/2411.16142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaobin Mo, Qingyuan Liu, Baohua Yan, Longxiang Zhang, Xuan Di
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16142">Causal Adjacency Learning for Spatiotemporal Prediction Over Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatiotemporal prediction over graphs (STPG) is crucial for transportation systems. In existing STPG models, an adjacency matrix is an important component that captures the relations among nodes over graphs. However, most studies calculate the adjacency matrix by directly memorizing the data, such as distance- and correlation-based matrices. These adjacency matrices do not consider potential pattern shift for the test data, and may result in suboptimal performance if the test data has a different distribution from the training one. This issue is known as the Out-of-Distribution generalization problem. To address this issue, in this paper we propose a Causal Adjacency Learning (CAL) method to discover causal relations over graphs. The learned causal adjacency matrix is evaluated on a downstream spatiotemporal prediction task using real-world graph data. Results demonstrate that our proposed adjacency matrix can capture the causal relations, and using our learned adjacency matrix can enhance prediction performance on the OOD test data, even though causal learning is not conducted in the downstream task.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2411.08606.pdf' target='_blank'>https://arxiv.org/pdf/2411.08606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengwei Yin, Jingjing Wang, Guanzhong Zeng, Di Xie, Jiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08606">LG-Gaze: Learning Geometry-aware Continuous Prompts for Language-Guided Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability of gaze estimation models to generalize is often significantly hindered by various factors unrelated to gaze, especially when the training dataset is limited. Current strategies aim to address this challenge through different domain generalization techniques, yet they have had limited success due to the risk of overfitting when solely relying on value labels for regression. Recent progress in pre-trained vision-language models has motivated us to capitalize on the abundant semantic information available. We propose a novel approach in this paper, reframing the gaze estimation task as a vision-language alignment issue. Our proposed framework, named Language-Guided Gaze Estimation (LG-Gaze), learns continuous and geometry-sensitive features for gaze estimation benefit from the rich prior knowledges of vision-language models. Specifically, LG-Gaze aligns gaze features with continuous linguistic features through our proposed multimodal contrastive regression loss, which customizes adaptive weights for different negative samples. Furthermore, to better adapt to the labels for gaze estimation task, we propose a geometry-aware interpolation method to obtain more precise gaze embeddings. Through extensive experiments, we validate the efficacy of our framework in four different cross-domain evaluation tasks.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2410.12953.pdf' target='_blank'>https://arxiv.org/pdf/2410.12953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Agrawal, Aniruddh Sikdar, Rajini Makam, Suresh Sundaram, Suresh Kumar Besai, Mahesh Gopi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12953">Syn2Real Domain Generalization for Underwater Mine-like Object Detection Using Side-Scan Sonar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater mine detection with deep learning suffers from limitations due to the scarcity of real-world data.
  This scarcity leads to overfitting, where models perform well on training data but poorly on unseen data. This paper proposes a Syn2Real (Synthetic to Real) domain generalization approach using diffusion models to address this challenge. We demonstrate that synthetic data generated with noise by DDPM and DDIM models, even if not perfectly realistic, can effectively augment real-world samples for training. The residual noise in the final sampled images improves the model's ability to generalize to real-world data with inherent noise and high variation. The baseline Mask-RCNN model when trained on a combination of synthetic and original training datasets, exhibited approximately a 60% increase in Average Precision (AP) compared to being trained solely on the original training data. This significant improvement highlights the potential of Syn2Real domain generalization for underwater mine detection tasks.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2410.07432.pdf' target='_blank'>https://arxiv.org/pdf/2410.07432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leyan Pan, Vijay Ganesh, Jacob Abernethy, Chris Esposo, Wenke Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07432">Can Transformers Reason Logically? A Study in SAT Solving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We formally study the logical reasoning capabilities of decoder-only Transformers in the context of the boolean satisfiability (SAT) problem. First, we prove by construction that decoder-only Transformers can decide 3-SAT, in a non-uniform model of computation, using backtracking and deduction via Chain-of-Thought (CoT). %We prove its correctness by showing trace equivalence to the well-known DPLL SAT-solving algorithm. Second, we implement our construction as a PyTorch model with a tool (PARAT) that we designed to empirically demonstrate its correctness and investigate its properties. Third, rather than \textit{programming} a transformer to reason, we evaluate empirically whether it can be \textit{trained} to do so by learning directly from algorithmic traces (``reasoning paths'') from our theoretical construction. The trained models demonstrate strong out-of-distribution generalization on problem sizes seen during training but has limited length generalization, which is consistent with the implications of our theoretical result
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2409.17087.pdf' target='_blank'>https://arxiv.org/pdf/2409.17087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luigi Russo, Francesco Mauro, Alessandro Sebastianelli, Paolo Gamba, Silvia Liberata Ullo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17087">SEN12-WATER: A New Dataset for Hydrological Applications and its Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate change and increasing droughts pose significant challenges to water resource management around the world. These problems lead to severe water shortages that threaten ecosystems, agriculture, and human communities. To advance the fight against these challenges, we present a new dataset, SEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL) framework for proactive drought-related analysis. The dataset, identified as a spatiotemporal datacube, integrates SAR polarization, elevation, slope, and multispectral optical bands. Our DL framework enables the analysis and estimation of water losses over time in reservoirs of interest, revealing significant insights into water dynamics for drought analysis by examining temporal changes in physical quantities such as water volume. Our methodology takes advantage of the multitemporal and multimodal characteristics of the proposed dataset, enabling robust generalization and advancing understanding of drought, contributing to climate change resilience and sustainable water resource management. The proposed framework involves, among the several components, speckle noise removal from SAR data, a water body segmentation through a U-Net architecture, the time series analysis, and the predictive capability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Results are validated through ground truth data acquired on-ground via dedicated sensors and (tailored) metrics, such as Precision, Recall, Intersection over Union, Mean Squared Error, Structural Similarity Index Measure and Peak Signal-to-Noise Ratio.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2409.05790.pdf' target='_blank'>https://arxiv.org/pdf/2409.05790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farah Alsafadi, Aidan Furlong, Xu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05790">Predicting Critical Heat Flux with Uncertainty Quantification and Domain Generalization Using Conditional Variational Autoencoders and Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep generative models (DGMs) can generate synthetic data samples that closely resemble the original dataset, addressing data scarcity. In this work, we developed a conditional variational autoencoder (CVAE) to augment critical heat flux (CHF) data used for the 2006 Groeneveld lookup table. To compare with traditional methods, a fine-tuned deep neural network (DNN) regression model was evaluated on the same dataset. Both models achieved small mean absolute relative errors, with the CVAE showing more favorable results. Uncertainty quantification (UQ) was performed using repeated CVAE sampling and DNN ensembling. The DNN ensemble improved performance over the baseline, while the CVAE maintained consistent results with less variability and higher confidence. Both models achieved small errors inside and outside the training domain, with slightly larger errors outside. Overall, the CVAE performed better than the DNN in predicting CHF and exhibited better uncertainty behavior.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2601.11827.pdf' target='_blank'>https://arxiv.org/pdf/2601.11827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Rubbi, Amir Akbarnejad, Mohammad Vali Sanian, Aryan Yazdan Parast, Hesam Asadollahzadeh, Arian Amani, Naveed Akhtar, Sarah Cooper, Andrew Bassett, Pietro Liò, Lassi Paavolainen, Sattar Vakili, Mo Lotfollahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.11827">MixFlow: Mixture-Conditioned Flow Matching for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving robust generalization under distribution shift remains a central challenge in conditional generative modeling, as existing conditional flow-based methods often struggle to extrapolate beyond the training conditions. We introduce MixFlow, a conditional flow-matching framework for descriptor-controlled generation that directly targets this limitation by jointly learning a descriptor-conditioned base distribution and a descriptor-conditioned flow field via shortest-path flow matching. By modeling the base distribution as a learnable, descriptor-dependent mixture, MixFlow enables smooth interpolation and extrapolation to unseen conditions, leading to substantially improved out-of-distribution generalization. We provide analytical insights into the behavior of the proposed framework and empirically demonstrate its effectiveness across multiple domains, including prediction of responses to unseen perturbations in single-cell transcriptomic data and high-content microscopy-based drug screening tasks. Across these diverse settings, MixFlow consistently outperforms standard conditional flow-matching baselines. Overall, MixFlow offers a simple yet powerful approach for achieving robust, generalizable, and controllable generative modeling across heterogeneous domains.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2601.08511.pdf' target='_blank'>https://arxiv.org/pdf/2601.08511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seong-Gyu Park, Sohee Park, Jisu Lee, Hyunsik Na, Daeseon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08511">STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coherent paths, they effectively evade conventional detection. To address this, we propose STAR (State-Transition Amplification Ratio), a framework that detects backdoors by analyzing output probability shifts. STAR exploits the statistical discrepancy where a malicious input-induced path exhibits high posterior probability despite a low prior probability in the model's general knowledge. We quantify this state-transition amplification and employ the CUSUM algorithm to detect persistent anomalies. Experiments across diverse models (8B-70B) and five benchmark datasets demonstrate that STAR exhibits robust generalization capabilities, consistently achieving near-perfect performance (AUROC $\approx$ 1.0) with approximately $42\times$ greater efficiency than existing baselines. Furthermore, the framework proves robust against adaptive attacks attempting to bypass detection.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2512.18210.pdf' target='_blank'>https://arxiv.org/pdf/2512.18210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Huang, Yuchen Mao, Yanmin Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18210">A Data-Centric Approach to Generalizable Speech Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving robust generalization in speech deepfake detection (SDD) remains a primary challenge, as models often fail to detect unseen forgery methods. While research has focused on model-centric and algorithm-centric solutions, the impact of data composition is often underexplored. This paper proposes a data-centric approach, analyzing the SDD data landscape from two practical perspectives: constructing a single dataset and aggregating multiple datasets. To address the first perspective, we conduct a large-scale empirical study to characterize the data scaling laws for SDD, quantifying the impact of source and generator diversity. To address the second, we propose the Diversity-Optimized Sampling Strategy (DOSS), a principled framework for mixing heterogeneous data with two implementations: DOSS-Select (pruning) and DOSS-Weight (re-weighting). Our experiments show that DOSS-Select outperforms the naive aggregation baseline while using only 3% of the total available data. Furthermore, our final model, trained on a 12k-hour curated data pool using the optimal DOSS-Weight strategy, achieves state-of-the-art performance, outperforming large-scale baselines with greater data and model efficiency on both public benchmarks and a new challenge set of various commercial APIs.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2512.13376.pdf' target='_blank'>https://arxiv.org/pdf/2512.13376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carla Monteiro, Valentina Corbetta, Regina Beets-Tan, Luís F. Teixeira, Wilson Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13376">Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2512.02421.pdf' target='_blank'>https://arxiv.org/pdf/2512.02421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyao Li, Yinjie Min, Hongbo Chen, Zhekai Du, Fengling Li, Jingjing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02421">Generalizing Vision-Language Models with Dedicated Prompt Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2511.22288.pdf' target='_blank'>https://arxiv.org/pdf/2511.22288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaorui Meng, Lu Yin, Yangqing Hou, Anjun Chen, Shihui Guo, Yipeng Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22288">Improving Sparse IMU-based Motion Capture with Motion Label Smoothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse Inertial Measurement Units (IMUs) based human motion capture has gained significant momentum, driven by the adaptation of fundamental AI tools such as recurrent neural networks (RNNs) and transformers that are tailored for temporal and spatial modeling. Despite these achievements, current research predominantly focuses on pipeline and architectural designs, with comparatively little attention given to regularization methods, highlighting a critical gap in developing a comprehensive AI toolkit for this task. To bridge this gap, we propose motion label smoothing, a novel method that adapts the classic label smoothing strategy from classification to the sparse IMU-based motion capture task. Specifically, we first demonstrate that a naive adaptation of label smoothing, including simply blending a uniform vector or a ``uniform'' motion representation (e.g., dataset-average motion or a canonical T-pose), is suboptimal; and argue that a proper adaptation requires increasing the entropy of the smoothed labels. Second, we conduct a thorough analysis of human motion labels, identifying three critical properties: 1) Temporal Smoothness, 2) Joint Correlation, and 3) Low-Frequency Dominance, and show that conventional approaches to entropy enhancement (e.g., blending Gaussian noise) are ineffective as they disrupt these properties. Finally, we propose the blend of a novel skeleton-based Perlin noise for motion label smoothing, designed to raise label entropy while satisfying motion properties. Extensive experiments applying our motion label smoothing to three state-of-the-art methods across four real-world IMU datasets demonstrate its effectiveness and robust generalization (plug-and-play) capability.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2510.27047.pdf' target='_blank'>https://arxiv.org/pdf/2510.27047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mario Camarena, Het Patel, Fatemeh Nazari, Evangelos Papalexakis, Mohamadhossein Noruzoliaee, Jia Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27047">AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the Autonomous Driving Segment Anything Model (AD-SAM), a fine-tuned vision foundation model for semantic segmentation in autonomous driving (AD). AD-SAM extends the Segment Anything Model (SAM) with a dual-encoder and deformable decoder tailored to spatial and geometric complexity of road scenes. The dual-encoder produces multi-scale fused representations by combining global semantic context from SAM's pretrained Vision Transformer (ViT-H) with local spatial detail from a trainable convolutional deep learning backbone (i.e., ResNet-50). A deformable fusion module aligns heterogeneous features across scales and object geometries. The decoder performs progressive multi-stage refinement using deformable attention. Training is guided by a hybrid loss that integrates Focal, Dice, Lovasz-Softmax, and Surface losses, improving semantic class balance, boundary precision, and optimization stability. Experiments on the Cityscapes and Berkeley DeepDrive 100K (BDD100K) benchmarks show that AD-SAM surpasses SAM, Generalized SAM (G-SAM), and a deep learning baseline (DeepLabV3) in segmentation accuracy. It achieves 68.1 mean Intersection over Union (mIoU) on Cityscapes and 59.5 mIoU on BDD100K, outperforming SAM, G-SAM, and DeepLabV3 by margins of up to +22.9 and +19.2 mIoU in structured and diverse road scenes, respectively. AD-SAM demonstrates strong cross-domain generalization with a 0.87 retention score (vs. 0.76 for SAM), and faster, more stable learning dynamics, converging within 30-40 epochs, enjoying double the learning speed of benchmark models. It maintains 0.607 mIoU with only 1000 samples, suggesting data efficiency critical for reducing annotation costs. These results confirm that targeted architectural and optimization enhancements to foundation models enable reliable and scalable AD perception.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2510.19520.pdf' target='_blank'>https://arxiv.org/pdf/2510.19520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Li, Haojie Yang, Kaimiao Hu, Runzhi Wu, Liangliang Liu, Ran Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19520">CDI-DTI: A Strong Cross-domain Interpretable Drug-Target Interaction Prediction Framework Based on Multi-Strategy Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of drug-target interactions (DTI) is pivotal for drug discovery, yet existing methods often fail to address challenges like cross-domain generalization, cold-start prediction, and interpretability. In this work, we propose CDI-DTI, a novel cross-domain interpretable framework for DTI prediction, designed to overcome these limitations. By integrating multi-modal features-textual, structural, and functional-through a multi-strategy fusion approach, CDI-DTI ensures robust performance across different domains and in cold-start scenarios. A multi-source cross-attention mechanism is introduced to align and fuse features early, while a bidirectional cross-attention layer captures fine-grained intra-modal drug-target interactions. To enhance model interpretability, we incorporate Gram Loss for feature alignment and a deep orthogonal fusion module to eliminate redundancy. Experimental results on several benchmark datasets demonstrate that CDI-DTI significantly outperforms existing methods, particularly in cross-domain and cold-start tasks, while maintaining high interpretability for practical applications in drug-target interaction prediction.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2510.13921.pdf' target='_blank'>https://arxiv.org/pdf/2510.13921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Levy Chaves, Eduardo Valle, Sandra Avila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13921">Weight Weaving: Parameter Pooling for Data-Free Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model merging provides a cost-effective and data-efficient combination of specialized deep neural networks through parameter integration. This technique leverages expert models across downstream tasks without requiring retraining. Most model merging approaches critically depend on scaling hyper-parameters $λ$, which weight each model's contribution globally or individually. Principled approaches for setting scaling factors without accessing any data (data-free) are scarce, often leading researchers to tune $λ$ using privileged data from the evaluation set, which is obviously unfeasible in practice. To address this limitation, we introduce Weight Weaving, a plug-and-play technique that pools model weights across $λ$ values search space using user-defined pooling functions, such as averaging, random selection, or even existing model merging methods. Our method demonstrates high modularity, imposing minimal constraints on the search space. It operates orthogonally to existing model merging methods and eliminates evaluation data requirements. We validate Weight Weaving across three ViT variants in three experimental setups: vision multi-task learning, vision continual learning, and domain generalization. Our method consistently improves the performance of several model merging methods, achieving average accuracy gains of up to 15.9 percentage points in a data-free setting.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2510.12159.pdf' target='_blank'>https://arxiv.org/pdf/2510.12159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyuan Gao, Philippe Morel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12159">DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One-shot medical image segmentation faces fundamental challenges in prototype representation due to limited annotated data and significant anatomical variability across patients. Traditional prototype-based methods rely on deterministic averaging of support features, creating brittle representations that fail to capture intra-class diversity essential for robust generalization. This work introduces Diffusion Prototype Learning (DPL), a novel framework that reformulates prototype construction through diffusion-based feature space exploration. DPL models one-shot prototypes as learnable probability distributions, enabling controlled generation of diverse yet semantically coherent prototype variants from minimal labeled data. The framework operates through three core innovations: (1) a diffusion-based prototype enhancement module that transforms single support prototypes into diverse variant sets via forward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism that leverages geometric properties derived from prototype feature statistics, and (3) a conservative fusion strategy that preserves prototype fidelity while maximizing representational diversity. DPL ensures training-inference consistency by using the same diffusion enhancement and fusion pipeline in both phases. This process generates enhanced prototypes that serve as the final representations for similarity calculations, while the diffusion process itself acts as a regularizer. Extensive experiments on abdominal MRI and CT datasets demonstrate significant improvements respectively, establishing new state-of-the-art performance in one-shot medical image segmentation.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2510.04441.pdf' target='_blank'>https://arxiv.org/pdf/2510.04441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Zhu, Naihao Deng, Naichen Shi, Aditya Gangrade, Clayton Scott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04441">Domain Generalization: A Tale of Two ERMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) is the problem of generalizing from several distributions (or domains), for which labeled training data are available, to a new test domain for which no labeled data is available. A common finding in the DG literature is that it is difficult to outperform empirical risk minimization (ERM) on the pooled training data. In this work, we argue that this finding has primarily been reported for datasets satisfying a \emph{covariate shift} assumption. When the dataset satisfies a \emph{posterior drift} assumption instead, we show that ``domain-informed ERM,'' wherein feature vectors are augmented with domain-specific information, outperforms pooling ERM. These claims are supported by a theoretical framework and experiments on language and vision tasks.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2509.05975.pdf' target='_blank'>https://arxiv.org/pdf/2509.05975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nam Duong Tran, Nam Nguyen Phuong, Hieu H. Pham, Phi Le Nguyen, My T. Thai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05975">ConstStyle: Robust Domain Generalization with Unified Style Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks often suffer performance drops when test data distribution differs from training data. Domain Generalization (DG) aims to address this by focusing on domain-invariant features or augmenting data for greater diversity. However, these methods often struggle with limited training domains or significant gaps between seen (training) and unseen (test) domains. To enhance DG robustness, we hypothesize that it is essential for the model to be trained on data from domains that closely resemble unseen test domains-an inherently difficult task due to the absence of prior knowledge about the unseen domains. Accordingly, we propose ConstStyle, a novel approach that leverages a unified domain to capture domain-invariant features and bridge the domain gap with theoretical analysis. During training, all samples are mapped onto this unified domain, optimized for seen domains. During testing, unseen domain samples are projected similarly before predictions. By aligning both training and testing data within this unified domain, ConstStyle effectively reduces the impact of domain shifts, even with large domain gaps or few seen domains. Extensive experiments demonstrate that ConstStyle consistently outperforms existing methods across diverse scenarios. Notably, when only a limited number of seen domains are available, ConstStyle can boost accuracy up to 19.82\% compared to the next best approach.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2508.20534.pdf' target='_blank'>https://arxiv.org/pdf/2508.20534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frederik Rajiv Manichand, Robin Deuber, Robert Jakob, Steve Swerling, Jamie Rosen, Elgar Fleisch, Patrick Langer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20534">Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating Body Mass Index (BMI) from camera images with machine learning models enables rapid weight assessment when traditional methods are unavailable or impractical, such as in telehealth or emergency scenarios. Existing computer vision approaches have been limited to datasets of up to 14,500 images. In this study, we present a deep learning-based BMI estimation method trained on our WayBED dataset, a large proprietary collection of 84,963 smartphone images from 25,353 individuals. We introduce an automatic filtering method that uses posture clustering and person detection to curate the dataset by removing low-quality images, such as those with atypical postures or incomplete views. This process retained 71,322 high-quality images suitable for training. We achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test set (WayBED data) using full-body images, the lowest value in the published literature to the best of our knowledge. Further, we achieve a MAPE of 13% on the completely unseen~(during training) VisualBodyToBMI dataset, comparable with state-of-the-art approaches trained on it, demonstrating robust generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the full pipeline, including image filtering and BMI estimation, on Android devices using the CLAID framework. We release our complete code for model training, filtering, and the CLAID package for mobile deployment as open-source contributions.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2508.07950.pdf' target='_blank'>https://arxiv.org/pdf/2508.07950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Shen, Wanqing Zhang, Kehan Li, Erwen Huang, Haitao Bi, Aiying Fan, Yiwen Shen, Hongmei Dong, Ji Zhang, Yuming Shao, Zengjia Liu, Xinshe Liu, Tao Li, Chunxia Yan, Shuanliang Fan, Di Wu, Jianhua Ma, Bin Cong, Zhenyuan Wang, Chunfeng Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07950">FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory & Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2507.16238.pdf' target='_blank'>https://arxiv.org/pdf/2507.16238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Xu, Chaoyue Ren, Wei Liu, Wenke Huang, Bin Yang, Zhixi Yu, Kui Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16238">Positive Style Accumulation: A Style Screening and Continuous Utilization Framework for Federated DG-ReID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Federated Domain Generalization for Person re-identification (FedDG-ReID) aims to learn a global server model that can be effectively generalized to source and target domains through distributed source domain data. Existing methods mainly improve the diversity of samples through style transformation, which to some extent enhances the generalization performance of the model. However, we discover that not all styles contribute to the generalization performance. Therefore, we define styles that are beneficial or harmful to the model's generalization performance as positive or negative styles. Based on this, new issues arise: How to effectively screen and continuously utilize the positive styles. To solve these problems, we propose a Style Screening and Continuous Utilization (SSCU) framework. Firstly, we design a Generalization Gain-guided Dynamic Style Memory (GGDSM) for each client model to screen and accumulate generated positive styles. Meanwhile, we propose a style memory recognition loss to fully leverage the positive styles memorized by Memory. Furthermore, we propose a Collaborative Style Training (CST) strategy to make full use of positive styles. Unlike traditional learning strategies, our approach leverages both newly generated styles and the accumulated positive styles stored in memory to train client models on two distinct branches. This training strategy is designed to effectively promote the rapid acquisition of new styles by the client models, and guarantees the continuous and thorough utilization of positive styles, which is highly beneficial for the model's generalization performance. Extensive experimental results demonstrate that our method outperforms existing methods in both the source domain and the target domain.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2507.13089.pdf' target='_blank'>https://arxiv.org/pdf/2507.13089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Peng, Pengfei Wang, Jianzhuang Liu, Shifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13089">GLAD: Generalizable Tuning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained vision-language models, such as CLIP, show impressive zero-shot recognition ability and can be easily transferred to specific downstream tasks via prompt tuning, even with limited training data. However, existing prompt tuning methods face two main challenges: (1) In few-shot scenarios, data scarcity often leads to overfitting, making the model sensitive to changes in the input domain. (2) To mitigate overfitting, these methods typically rely on complex task-specific model architectures and sensitive hyperparameter tuning, severely restricting their general applicability. To address these issues, we propose a simpler and more general framework called GLAD (Generalizable LoRA tuning with RegulArized GraDient). We show that merely applying LoRA achieves performance in downstream tasks comparable to current state-of-the-art prompt-based methods. While LoRA is effective and easy to use, it remains susceptible to overfitting in few-shot learning scenarios. To mitigate this risk, we introduce a gradient-based regularization technique. This technique effectively steers the optimization trajectory, encouraging the model to find a more stable parameter region that is robust to variations in data distribution. Through extensive experiments conducted on 15 benchmark datasets, we demonstrate that GLAD outperforms previous tuning approaches in terms of base-to-novel class generalization, image domain generalization, and cross-dataset generalization. The code will be publicly available.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2507.07313.pdf' target='_blank'>https://arxiv.org/pdf/2507.07313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alan Malek, Jiawei Ge, Nevena Lazic, Chi Jin, AndrÃ¡s GyÃ¶rgy, Csaba SzepesvÃ¡ri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07313">Frontier LLMs Still Struggle with Simple Reasoning Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While state-of-the-art large language models (LLMs) demonstrate advanced reasoning capabilities-achieving remarkable performance on challenging competitive math and coding benchmarks-they also frequently fail on tasks that are easy for humans. This work studies the performance of frontier LLMs on a broad set of such "easy" reasoning problems. By extending previous work in the literature, we create a suite of procedurally generated simple reasoning tasks, including counting, first-order logic, proof trees, and travel planning, with changeable parameters (such as document length. or the number of variables in a math problem) that can arbitrarily increase the amount of computation required to produce the answer while preserving the fundamental difficulty. While previous work showed that traditional, non-thinking models can be made to fail on such problems, we demonstrate that even state-of-the-art thinking models consistently fail on such problems and for similar reasons (e.g. statistical shortcuts, errors in intermediate steps, and difficulties in processing long contexts). To further understand the behavior of the models, we introduce the unpuzzles dataset, a different "easy" benchmark consisting of trivialized versions of well-known math and logic puzzles. Interestingly, while modern LLMs excel at solving the original puzzles, they tend to fail on the trivialized versions, exhibiting several systematic failure patterns related to memorizing the originals. We show that this happens even if the models are otherwise able to solve problems with different descriptions but requiring the same logic. Our results highlight that out-of-distribution generalization is still problematic for frontier language models and the new generation of thinking models, even for simple reasoning tasks, and making tasks easier does not necessarily imply improved performance.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2507.03898.pdf' target='_blank'>https://arxiv.org/pdf/2507.03898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Xiong, Lei Zhang, Shuoyuan Wang, Dongzhou Cheng, Wenbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03898">Deconfounding Causal Inference through Two-Branch Framework with Early-Forking for Sensor-Based Cross-Domain Activity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, domain generalization (DG) has emerged as a promising solution to mitigate distribution-shift issue in sensor-based human activity recognition (HAR) scenario. However, most existing DG-based works have merely focused on modeling statistical dependence between sensor data and activity labels, neglecting the importance of intrinsic casual mechanism. Intuitively, every sensor input can be viewed as a mixture of causal (category-aware) and non-causal factors (domain-specific), where only the former affects activity classification judgment. In this paper, by casting such DG-based HAR as a casual inference problem, we propose a causality-inspired representation learning algorithm for cross-domain activity recognition. To this end, an early-forking two-branch framework is designed, where two separate branches are respectively responsible for learning casual and non-causal features, while an independence-based Hilbert-Schmidt Information Criterion is employed to implicitly disentangling them. Additionally, an inhomogeneous domain sampling strategy is designed to enhance disentanglement, while a category-aware domain perturbation layer is performed to prevent representation collapse. Extensive experiments on several public HAR benchmarks demonstrate that our causality-inspired approach significantly outperforms eleven related state-of-the-art baselines under cross-person, cross-dataset, and cross-position settings. Detailed ablation and visualizations analyses reveal underlying casual mechanism, indicating its effectiveness, efficiency, and universality in cross-domain activity recognition scenario.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2506.12009.pdf' target='_blank'>https://arxiv.org/pdf/2506.12009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junha Lee, Eunha Park, Chunghyun Park, Dahyun Kang, Minsu Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12009">Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Affordance grounding-localizing object regions based on natural language descriptions of interactions-is a critical challenge for enabling intelligent agents to understand and interact with their environments. However, this task remains challenging due to the need for fine-grained part-level localization, the ambiguity arising from multiple valid interaction regions, and the scarcity of large-scale datasets. In this work, we introduce Affogato, a large-scale benchmark comprising 150K instances, annotated with open-vocabulary text descriptions and corresponding 3D affordance heatmaps across a diverse set of objects and interactions. Building on this benchmark, we develop simple yet effective vision-language models that leverage pretrained part-aware vision backbones and a text-conditional heatmap decoder. Our models trained with the Affogato dataset achieve promising performance on the existing 2D and 3D benchmarks, and notably, exhibit effectiveness in open-vocabulary cross-domain generalization. The Affogato dataset is shared in public: https://huggingface.co/datasets/project-affogato/affogato
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2506.10245.pdf' target='_blank'>https://arxiv.org/pdf/2506.10245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iago Alves Brito, Julia Soares Dollis, Fernanda Bufon FÃ¤rber, Diogo Fernandes Costa Silva, Arlindo Rodrigues GalvÃ£o Filho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10245">ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ToxSyn-PT, the first large-scale Portuguese corpus that enables fine-grained hate-speech classification across nine legally protected minority groups. The dataset contains 53,274 synthetic sentences equally distributed between minorities groups and toxicity labels. ToxSyn-PT is created through a novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and (4) enrichment, plus additional neutral texts to curb overfitting to group-specific cues. The resulting corpus is class-balanced, stylistically diverse, and free from the social-media domain that dominate existing Portuguese datasets. Despite domain differences with traditional benchmarks, experiments on both binary and multi-label classification on the corpus yields strong results across five public Portuguese hate-speech datasets, demonstrating robust generalization even across domain boundaries. The dataset is publicly released to advance research on synthetic data and hate-speech detection in low-resource settings.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2506.10146.pdf' target='_blank'>https://arxiv.org/pdf/2506.10146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tejaswi Kasarla, Max van Spengler, Pascal Mettes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10146">Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution recognition forms an important and well-studied problem in deep learning, with the goal to filter out samples that do not belong to the distribution on which a network has been trained. The conclusion of this paper is simple: a good hierarchical hyperbolic embedding is preferred for discriminating in- and out-of-distribution samples. We introduce Balanced Hyperbolic Learning. We outline a hyperbolic class embedding algorithm that jointly optimizes for hierarchical distortion and balancing between shallow and wide subhierarchies. We then use the class embeddings as hyperbolic prototypes for classification on in-distribution data. We outline how to generalize existing out-of-distribution scoring functions to operate with hyperbolic prototypes. Empirical evaluations across 13 datasets and 13 scoring functions show that our hyperbolic embeddings outperform existing out-of-distribution approaches when trained on the same data with the same backbones. We also show that our hyperbolic embeddings outperform other hyperbolic approaches, beat state-of-the-art contrastive methods, and natively enable hierarchical out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2506.02256.pdf' target='_blank'>https://arxiv.org/pdf/2506.02256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Xiao, Harshit Sharma, Sawinder Kaur, Dessa Bergen-Cico, Asif Salekin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02256">Human Heterogeneity Invariant Stress Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stress affects physical and mental health, and wearable devices have been widely used to detect daily stress through physiological signals. However, these signals vary due to factors such as individual differences and health conditions, making generalizing machine learning models difficult. To address these challenges, we present Human Heterogeneity Invariant Stress Sensing (HHISS), a domain generalization approach designed to find consistent patterns in stress signals by removing person-specific differences. This helps the model perform more accurately across new people, environments, and stress types not seen during training. Its novelty lies in proposing a novel technique called person-wise sub-network pruning intersection to focus on shared features across individuals, alongside preventing overfitting by leveraging continuous labels while training. The study focuses especially on people with opioid use disorder (OUD)-a group where stress responses can change dramatically depending on their time of daily medication taking. Since stress often triggers cravings, a model that can adapt well to these changes could support better OUD rehabilitation and recovery. We tested HHISS on seven different stress datasets-four of which we collected ourselves and three public ones. Four are from lab setups, one from a controlled real-world setting, driving, and two are from real-world in-the-wild field datasets without any constraints. This is the first study to evaluate how well a stress detection model works across such a wide range of data. Results show HHISS consistently outperformed state-of-the-art baseline methods, proving both effective and practical for real-world use. Ablation studies, empirical justifications, and runtime evaluations confirm HHISS's feasibility and scalability for mobile stress sensing in sensitive real-world applications.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2505.20235.pdf' target='_blank'>https://arxiv.org/pdf/2505.20235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Wenger, Beau Coker, Juraj Marusic, John P. Cunningham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20235">Variational Deep Learning via Implicit Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern deep learning models generalize remarkably well in-distribution, despite being overparametrized and trained with little to no explicit regularization. Instead, current theory credits implicit regularization imposed by the choice of architecture, hyperparameters and optimization procedure. However, deep neural networks can be surprisingly non-robust, resulting in overconfident predictions and poor out-of-distribution generalization. Bayesian deep learning addresses this via model averaging, but typically requires significant computational resources as well as carefully elicited priors to avoid overriding the benefits of implicit regularization. Instead, in this work, we propose to regularize variational neural networks solely by relying on the implicit bias of (stochastic) gradient descent. We theoretically characterize this inductive bias in overparametrized linear models as generalized variational inference and demonstrate the importance of the choice of parametrization. Empirically, our approach demonstrates strong in- and out-of-distribution performance without additional hyperparameter tuning and with minimal computational overhead.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2505.06690.pdf' target='_blank'>https://arxiv.org/pdf/2505.06690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianxin Zhang, Lianzi Jiang, Xinyu Han, Xiangrong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06690">A Causality- and Frequency-Aware Deep Learning Framework for Wave Elevation Prediction Behind Floating Breakwaters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting the elevations of nonlinear wave fields behind floating breakwaters (FBs) is crucial for optimizing coastal engineering structures, enhancing safety, and improving design efficiency. Existing deep learning approaches exhibit limited generalization capability under unseen operating conditions. To address this challenge, this study proposes the Exogenous-to-Endogenous Frequency-Aware Network (E2E-FANet), a novel end-to-end neural network designed to model relationships between waves and structures. First, the Dual-Basis Frequency Mapping (DBFM) module leverages orthogonal cosine and sine bases to generate an adaptive time-frequency representation, enabling the model to effectively disentangle the evolving spectral components of wave signals. Second, the Exogenous-to-Endogenous Cross-Attention (E2ECA) module employs cross attention to explicitly model the unidirectional causal influence of floating breakwater motion on wave elevations. Additionally, a Temporal-wise Attention (TA) mechanism is incorporated that adaptively captures complex dependencies in endogenous variables. Extensive experiments, including generalization tests across diverse wave conditions and adaptability tests under varying relative water density (RW) conditions, demonstrate that E2E-FANet achieves superior predictive accuracy and robust generalization compared to mainstream models. This work emphasizes the importance of integrating causality and frequency-aware modeling in deep learning architectures for modeling nonlinear dynamics systems.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2504.15900.pdf' target='_blank'>https://arxiv.org/pdf/2504.15900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15900">SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to "think before answering." Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2504.14151.pdf' target='_blank'>https://arxiv.org/pdf/2504.14151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio Arnaud, Paul McVay, Ada Martin, Arjun Majumdar, Krishna Murthy Jatavallabhula, Phillip Thomas, Ruslan Partsey, Daniel Dugas, Abha Gejji, Alexander Sax, Vincent-Pierre Berges, Mikael Henaff, Ayush Jain, Ang Cao, Ishita Prasad, Mrinal Kalakrishnan, Michael Rabbat, Nicolas Ballas, Mido Assran, Oleksandr Maksymets, Aravind Rajeswaran, Franziska Meier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14151">Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LOCATE 3D, a model for localizing objects in 3D scenes from referring expressions like "the small coffee table between the sofa and the lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2503.22936.pdf' target='_blank'>https://arxiv.org/pdf/2503.22936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei-Kai Huanga, Jun-Xiong Chong, Ming-Tsung Hsu, Fang-Yu Hsu, Chiou-Ting Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22936">Enhancing Learnable Descriptive Convolutional Vision Transformer for Face Anti-Spoofing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face anti-spoofing (FAS) heavily relies on identifying live/spoof discriminative features to counter face presentation attacks. Recently, we proposed LDCformer to successfully incorporate the Learnable Descriptive Convolution (LDC) into ViT, to model long-range dependency of locally descriptive features for FAS. In this paper, we propose three novel training strategies to effectively enhance the training of LDCformer to largely boost its feature characterization capability. The first strategy, dual-attention supervision, is developed to learn fine-grained liveness features guided by regional live/spoof attentions. The second strategy, self-challenging supervision, is designed to enhance the discriminability of the features by generating challenging training data. In addition, we propose a third training strategy, transitional triplet mining strategy, through narrowing the cross-domain gap while maintaining the transitional relationship between live and spoof features, to enlarge the domain-generalization capability of LDCformer. Extensive experiments show that LDCformer under joint supervision of the three novel training strategies outperforms previous methods.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2503.14897.pdf' target='_blank'>https://arxiv.org/pdf/2503.14897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaibhav Rathore, Shubhranil B, Saikat Dutta, Sarthak Mehrotra, Zsolt Kira, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14897">When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalized Class Discovery (GCD) clusters base and novel classes in a target domain using supervision from a source domain with only base classes. Current methods often falter with distribution shifts and typically require access to target data during training, which can sometimes be impractical. To address this issue, we introduce the novel paradigm of Domain Generalization in GCD (DG-GCD), where only source data is available for training, while the target domain, with a distinct data distribution, remains unseen until inference. To this end, our solution, DG2CD-Net, aims to construct a domain-independent, discriminative embedding space for GCD. The core innovation is an episodic training strategy that enhances cross-domain generalization by adapting a base model on tasks derived from source and synthetic domains generated by a foundation model. Each episode focuses on a cross-domain GCD task, diversifying task setups over episodes and combining open-set domain adaptation with a novel margin loss and representation learning for optimizing the feature space progressively. To capture the effects of fine-tuning on the base model, we extend task arithmetic by adaptively weighting the local task vectors concerning the fine-tuned models based on their GCD performance on a validation distribution. This episodic update mechanism boosts the adaptability of the base model to unseen targets. Experiments across three datasets confirm that DG2CD-Net outperforms existing GCD methods customized for DG-GCD.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2503.12406.pdf' target='_blank'>https://arxiv.org/pdf/2503.12406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binggwong Leung, Worasuchad Haomachai, Joachim Winther Pedersen, Sebastian Risi, Poramate Manoonpong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12406">Bio-Inspired Plastic Neural Networks for Zero-Shot Out-of-Distribution Generalization in Complex Animal-Inspired Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial neural networks can be used to solve a variety of robotic tasks. However, they risk failing catastrophically when faced with out-of-distribution (OOD) situations. Several approaches have employed a type of synaptic plasticity known as Hebbian learning that can dynamically adjust weights based on local neural activities. Research has shown that synaptic plasticity can make policies more robust and help them adapt to unforeseen changes in the environment. However, networks augmented with Hebbian learning can lead to weight divergence, resulting in network instability. Furthermore, such Hebbian networks have not yet been applied to solve legged locomotion in complex real robots with many degrees of freedom. In this work, we improve the Hebbian network with a weight normalization mechanism for preventing weight divergence, analyze the principal components of the Hebbian's weights, and perform a thorough evaluation of network performance in locomotion control for real 18-DOF dung beetle-like and 16-DOF gecko-like robots. We find that the Hebbian-based plastic network can execute zero-shot sim-to-real adaptation locomotion and generalize to unseen conditions, such as uneven terrain and morphological damage.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2503.06472.pdf' target='_blank'>https://arxiv.org/pdf/2503.06472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Luo, Jiaqi Tang, Chenyi Huang, Feiyang Hao, Zhouhui Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06472">CalliReader: Contextualizing Chinese Calligraphy via an Embedding-Aligned Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chinese calligraphy, a UNESCO Heritage, remains computationally challenging due to visual ambiguity and cultural complexity. Existing AI systems fail to contextualize their intricate scripts, because of limited annotated data and poor visual-semantic alignment. We propose CalliReader, a vision-language model (VLM) that solves the Chinese Calligraphy Contextualization (CC$^2$) problem through three innovations: (1) character-wise slicing for precise character extraction and sorting, (2) CalliAlign for visual-text token compression and alignment, (3) embedding instruction tuning (e-IT) for improving alignment and addressing data scarcity. We also build CalliBench, the first benchmark for full-page calligraphic contextualization, addressing three critical issues in previous OCR and VQA approaches: fragmented context, shallow reasoning, and hallucination. Extensive experiments including user studies have been conducted to verify our CalliReader's \textbf{superiority to other state-of-the-art methods and even human professionals in page-level calligraphy recognition and interpretation}, achieving higher accuracy while reducing hallucination. Comparisons with reasoning models highlight the importance of accurate recognition as a prerequisite for reliable comprehension. Quantitative analyses validate CalliReader's efficiency; evaluations on document and real-world benchmarks confirm its robust generalization ability.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2502.20249.pdf' target='_blank'>https://arxiv.org/pdf/2502.20249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Vuillecard, Jean-Marc Odobez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20249">Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D gaze estimation in unconstrained real-world environments remains a significant challenge due to variations in appearance, head pose, occlusion, and the limited availability of in-the-wild 3D gaze datasets. To address these challenges, we introduce a novel Self-Training Weakly-Supervised Gaze Estimation framework (ST-WSGE). This two-stage learning framework leverages diverse 2D gaze datasets, such as gaze-following data, which offer rich variations in appearances, natural scenes, and gaze distributions, and proposes an approach to generate 3D pseudo-labels and enhance model generalization. Furthermore, traditional modality-specific models, designed separately for images or videos, limit the effective use of available training data. To overcome this, we propose the Gaze Transformer (GaT), a modality-agnostic architecture capable of simultaneously learning static and dynamic gaze information from both image and video datasets. By combining 3D video datasets with 2D gaze target labels from gaze following tasks, our approach achieves the following key contributions: (i) Significant state-of-the-art improvements in within-domain and cross-domain generalization on unconstrained benchmarks like Gaze360 and GFIE, with notable cross-modal gains in video gaze estimation; (ii) Superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360 compared to frontal face methods. Code and pre-trained models will be released to the community.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2502.20162.pdf' target='_blank'>https://arxiv.org/pdf/2502.20162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aristotelis Ballas, Christos Diou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20162">Gradient-Guided Annealing for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) research has gained considerable traction as of late, since the ability to generalize to unseen data distributions is a requirement that eludes even state-of-the-art training algorithms. In this paper we observe that the initial iterations of model training play a key role in domain generalization effectiveness, since the loss landscape may be significantly different across the training and test distributions, contrary to the case of i.i.d. data. Conflicts between gradients of the loss components of each domain lead the optimization procedure to undesirable local minima that do not capture the domain-invariant features of the target classes. We propose alleviating domain conflicts in model optimization, by iteratively annealing the parameters of a model in the early stages of training and searching for points where gradients align between domains. By discovering a set of parameter values where gradients are updated towards the same direction for each data distribution present in the training set, the proposed Gradient-Guided Annealing (GGA) algorithm encourages models to seek out minima that exhibit improved robustness against domain shifts. The efficacy of GGA is evaluated on five widely accepted and challenging image classification domain generalization benchmarks, where its use alone is able to establish highly competitive or even state-of-the-art performance. Moreover, when combined with previously proposed domain-generalization algorithms it is able to consistently improve their effectiveness by significant margins.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2502.12195.pdf' target='_blank'>https://arxiv.org/pdf/2502.12195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sameer Ambekar, Zehao Xiao, Xiantong Zhen, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12195">GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time Distribution Shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of test-time domain generalization, where a model is trained on several source domains and adjusted on target domains never seen during training. Different from the common methods that fine-tune the model or adjust the classifier parameters online, we propose to generate multiple layer parameters on the fly during inference by a lightweight meta-learned transformer, which we call \textit{GeneralizeFormer}. The layer-wise parameters are generated per target batch without fine-tuning or online adjustment. By doing so, our method is more effective in dynamic scenarios with multiple target distributions and also avoids forgetting valuable source distribution characteristics. Moreover, by considering layer-wise gradients, the proposed method adapts itself to various distribution shifts. To reduce the computational and time cost, we fix the convolutional parameters while only generating parameters of the Batch Normalization layers and the linear classifier. Experiments on six widely used domain generalization datasets demonstrate the benefits and abilities of the proposed method to efficiently handle various distribution shifts, generalize in dynamic scenarios, and avoid forgetting.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2502.00197.pdf' target='_blank'>https://arxiv.org/pdf/2502.00197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingshan Chang, Yonatan Bisk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00197">Learning Model Successors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The notion of generalization has moved away from the classical one defined in statistical learning theory towards an emphasis on out-of-domain generalization (OODG). There has been a growing focus on generalization from easy to hard, where a progression of difficulty implicitly governs the direction of domain shifts. This emerging regime has appeared in the literature under different names, such as length/logical/algorithmic extrapolation, but a formal definition is lacking. We argue that the unifying theme is induction -- based on finite samples observed in training, a learner should infer an inductive principle that applies in an unbounded manner. This work formalizes the notion of inductive generalization along a difficulty progression and argues that our path ahead lies in transforming the learning paradigm. We attempt to make inroads by proposing a novel learning paradigm, Inductive Learning, which involves a central concept called model successors. We outline practical steps to adapt well-established techniques towards learning model successors. This work calls for restructuring of the research discussion around induction and generalization from fragmented task-centric communities to a more unified effort, focused on universal properties of learning and computation.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2411.03687.pdf' target='_blank'>https://arxiv.org/pdf/2411.03687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehao Xiao, Cees G. M. Snoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03687">Beyond Model Adaptation at Test Time: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning algorithms have achieved remarkable success across various disciplines, use cases and applications, under the prevailing assumption that training and test samples are drawn from the same distribution. Consequently, these algorithms struggle and become brittle even when samples in the test distribution start to deviate from the ones observed during training. Domain adaptation and domain generalization have been studied extensively as approaches to address distribution shifts across test and train domains, but each has its limitations. Test-time adaptation, a recently emerging learning paradigm, combines the benefits of domain adaptation and domain generalization by training models only on source data and adapting them to target data during test-time inference. In this survey, we provide a comprehensive and systematic review on test-time adaptation, covering more than 400 recent papers. We structure our review by categorizing existing methods into five distinct categories based on what component of the method is adjusted for test-time adaptation: the model, the inference, the normalization, the sample, or the prompt, providing detailed analysis of each. We further discuss the various preparation and adaptation settings for methods within these categories, offering deeper insights into the effective deployment for the evaluation of distribution shifts and their real-world application in understanding images, video and 3D, as well as modalities beyond vision. We close the survey with an outlook on emerging research opportunities for test-time adaptation.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2410.15687.pdf' target='_blank'>https://arxiv.org/pdf/2410.15687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haohan Yuan, Haopeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15687">DomainSum: A Hierarchical Benchmark for Fine-Grained Domain Shift in Abstractive Text Summarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most research on abstractive summarization focuses on single-domain applications, often neglecting how domain shifts between documents affect performance and the generalization ability of summarization models. To address this issue, we introduce DomainSum, a hierarchical benchmark designed to capture fine-grained domain shifts in abstractive summarization. We categorize these shifts into three levels: genre, style, and topic, and demonstrate through comprehensive benchmark analysis that they follow a hierarchical structure. Furthermore, we evaluate the domain generalization capabilities of commonly used pre-trained language models (PLMs) and large language models (LLMs) in in-domain and cross-domain settings.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2410.06128.pdf' target='_blank'>https://arxiv.org/pdf/2410.06128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Divyat Mahajan, Jannes Gladrow, Agrin Hilmkil, Cheng Zhang, Meyer Scetbon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06128">Amortized Inference of Causal Models via Conditional Fixed-Point Iterations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Structural Causal Models (SCMs) offer a principled framework to reason about interventions and support out-of-distribution generalization, which are key goals in scientific discovery. However, the task of learning SCMs from observed data poses formidable challenges, and often requires training a separate model for each dataset. In this work, we propose amortized inference of SCMs by training a single model on multiple datasets sampled from different SCMs. We first use a transformer-based architecture for amortized learning of dataset embeddings, and then extend the Fixed-Point Approach (FiP) (Scetbon et al.) to infer SCMs conditionally on their dataset embeddings. As a byproduct, our method can generate observational and interventional data from novel SCMs at inference time, without updating parameters. Empirical results show that our amortized procedure performs on par with baselines trained specifically for each dataset on both in and out-of-distribution problems, and also outperforms them in scare data regimes.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2409.17063.pdf' target='_blank'>https://arxiv.org/pdf/2409.17063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neda Zamanitajeddin, Mostafa Jahanifar, Kesi Xu, Fouzia Siraj, Nasir Rajpoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17063">Benchmarking Domain Generalization Algorithms in Computational Pathology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have shown immense promise in computational pathology (CPath) tasks, but their performance often suffers when applied to unseen data due to domain shifts. Addressing this requires domain generalization (DG) algorithms. However, a systematic evaluation of DG algorithms in the CPath context is lacking. This study aims to benchmark the effectiveness of 30 DG algorithms on 3 CPath tasks of varying difficulty through 7,560 cross-validation runs. We evaluate these algorithms using a unified and robust platform, incorporating modality-specific techniques and recent advances like pretrained foundation models. Our extensive cross-validation experiments provide insights into the relative performance of various DG strategies. We observe that self-supervised learning and stain augmentation consistently outperform other methods, highlighting the potential of pretrained models and data augmentation. Furthermore, we introduce a new pan-cancer tumor detection dataset (HISTOPANTUM) as a benchmark for future research. This study offers valuable guidance to researchers in selecting appropriate DG approaches for CPath tasks.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2601.15038.pdf' target='_blank'>https://arxiv.org/pdf/2601.15038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mertcan Daysalilar, Fuat Uyguroglu, Gabriel Nicolosi, Adam Meyers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.15038">A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2601.14678.pdf' target='_blank'>https://arxiv.org/pdf/2601.14678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Cheung, Samuel Savine, Calvin Nguyen, Lin Lu, Alhassan S. Yasin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.14678">Transfer Learning from One Cancer to Another via Deep Learning Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised deep learning models often achieve excellent performance within their training distribution but struggle to generalize beyond it. In cancer histopathology, for example, a convolutional neural network (CNN) may classify cancer severity accurately for cancer types represented in its training data, yet fail on related but unseen types. Although adenocarcinomas from different organs share morphological features that might support limited cross-domain generalization, addressing domain shift directly is necessary for robust performance. Domain adaptation offers a way to transfer knowledge from labeled data in one cancer type to unlabeled data in another, helping mitigate the scarcity of annotated medical images. This work evaluates cross-domain classification performance among lung, colon, breast, and kidney adenocarcinomas. A ResNet50 trained on any single adenocarcinoma achieves over 98% accuracy on its own domain but shows minimal generalization to others. Ensembling multiple supervised models does not resolve this limitation. In contrast, converting the ResNet50 into a domain adversarial neural network (DANN) substantially improves performance on unlabeled target domains. A DANN trained on labeled breast and colon data and adapted to unlabeled lung data reaches 95.56% accuracy. We also examine the impact of stain normalization on domain adaptation. Its effects vary by target domain: for lung, accuracy drops from 95.56% to 66.60%, while for breast and colon targets, stain normalization boosts accuracy from 49.22% to 81.29% and from 78.48% to 83.36%, respectively. Finally, using Integrated Gradients reveals that DANNs consistently attribute importance to biologically meaningful regions such as densely packed nuclei, indicating that the model learns clinically relevant features and can apply them to unlabeled cancer types.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2601.12260.pdf' target='_blank'>https://arxiv.org/pdf/2601.12260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Ding, Qiang Sun, Puzhen Wu, Sirui Li, Siwen Luo, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12260">Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2512.17730.pdf' target='_blank'>https://arxiv.org/pdf/2512.17730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichen Jiang, Mohammed Talha Alam, Sohail Ahmed Khan, Duc-Tien Dang-Nguyen, Fakhri Karray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17730">AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vision-language models, specifically CLIP, to identify synthetic content across diverse generative techniques. First, we introduce Diff-Gen, a large-scale benchmark dataset comprising 100k diffusion-generated fakes that capture broad spectral artifacts unlike traditional GAN datasets. Models trained on Diff-Gen demonstrate stronger cross-domain generalization, particularly on previously unseen image generators. Second, we propose AdaptPrompt, a parameter-efficient transfer learning framework that jointly learns task-specific textual prompts and visual adapters while keeping the CLIP backbone frozen. We further show via layer ablation that pruning the final transformer block of the vision encoder enhances the retention of high-frequency generative artifacts, significantly boosting detection accuracy. Our evaluation spans 25 challenging test sets, covering synthetic content generated by GANs, diffusion models, and commercial tools, establishing a new state-of-the-art in both standard and cross-domain scenarios. We further demonstrate the framework's versatility through few-shot generalization (using as few as 320 images) and source attribution, enabling the precise identification of generator architectures in closed-set settings.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2512.02715.pdf' target='_blank'>https://arxiv.org/pdf/2512.02715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peirong Zhang, Yidan Zhang, Luxiao Xu, Jinliang Lin, Zonghao Guo, Fengxiang Wang, Xue Yang, Kaiwen Wei, Lei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02715">GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2511.17339.pdf' target='_blank'>https://arxiv.org/pdf/2511.17339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yassir Bendou, Omar Ezzahir, Eduardo Fernandes Montesuma, Gabriel Mahuas, Victoria Shevchenko, Mike Gartrell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17339">ReBaPL: Repulsive Bayesian Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2510.20868.pdf' target='_blank'>https://arxiv.org/pdf/2510.20868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zan Li, Rui Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20868">Crisis-Resilient Portfolio Management via Graph-based Spatio-Temporal Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Financial time series forecasting faces a fundamental challenge: predicting optimal asset allocations requires understanding regime-dependent correlation structures that transform during crisis periods. Existing graph-based spatio-temporal learning approaches rely on predetermined graph topologies--correlation thresholds, sector classifications--that fail to adapt when market dynamics shift across different crisis mechanisms: credit contagion, pandemic shocks, or inflation-driven selloffs. We present CRISP (Crisis-Resilient Investment through Spatio-temporal Patterns), a graph-based spatio-temporal learning framework that encodes spatial relationships via Graph Convolutional Networks and temporal dynamics via BiLSTM with self-attention, then learns sparse structures through multi-head Graph Attention Networks. Unlike fixed-topology methods, CRISP discovers which asset relationships matter through attention mechanisms, filtering 92.5% of connections as noise while preserving crisis-relevant dependencies for accurate regime-specific predictions. Trained on 2005--2021 data encompassing credit and pandemic crises, CRISP demonstrates robust generalization to 2022--2024 inflation-driven markets--a fundamentally different regime--by accurately forecasting regime-appropriate correlation structures. This enables adaptive portfolio allocation that maintains profitability during downturns, achieving Sharpe ratio 3.76: 707% improvement over equal-weight baselines and 94% improvement over static graph methods. Learned attention weights provide interpretable regime detection, with defensive cluster attention strengthening 49% during crises versus 31% market-wide--emergent behavior from learning to forecast rather than imposing assumptions.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2510.18383.pdf' target='_blank'>https://arxiv.org/pdf/2510.18383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ChangSu Choi, Hoyun Song, Dongyeon Kim, WooHyeon Jung, Minkyung Cho, Sunjin Park, NohHyeob Bae, Seona Yu, KyungTae Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18383">MENTOR: A Reinforcement Learning Framework for Enabling Tool Use in Small Models via Teacher-Optimized Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2510.16914.pdf' target='_blank'>https://arxiv.org/pdf/2510.16914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongwei Yan, Guanglong Sun, Zhiqi Kang, Yi Zhong, Liyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16914">Domain Generalizable Continual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To adapt effectively to dynamic real-world environments, intelligent systems must continually acquire new skills while generalizing them to diverse, unseen scenarios. Here, we introduce a novel and realistic setting named domain generalizable continual learning (DGCL): a model learns sequential tasks with each involving a single domain, aiming to perform well across all encountered tasks and domains. This setting poses unique challenges in acquiring, retaining, and leveraging both semantic- and domain-relevant information for robust generalization. Although state-of-the-art continual learning (CL) methods have employed pre-trained models (PTMs) to enhance task-specific generalization, they typically assume identical training and testing domains for each task and therefore perform poorly in DGCL. To this end, we propose adaptive Domain Transformation (DoT), an innovative PTMs-based approach tailored to DGCL. Inspired by the distributed-plus-hub theory of the human brain, DoT disentangles semantic- and domain-relevant information in representation learning, and adaptively transforms task representations across various domains for output alignment, ensuring balanced and generalized predictions. DoT serves as a plug-in strategy that greatly facilitates state-of-the-art CL baselines under both full parameter tuning and parameter-efficient tuning paradigms in DGCL, validated by extensive experiments. Also, DoT is shown to accumulate domain-generalizable knowledge from DGCL, and ensure resource efficiency with a lightweight implementation.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2510.04688.pdf' target='_blank'>https://arxiv.org/pdf/2510.04688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joann Ching, Gerhard Widmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04688">A Study on the Data Distribution Gap in Music Emotion Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Music Emotion Recognition (MER) is a task deeply connected to human perception, relying heavily on subjective annotations collected from contributors. Prior studies tend to focus on specific musical styles rather than incorporating a diverse range of genres, such as rock and classical, within a single framework. In this paper, we address the task of recognizing emotion from audio content by investigating five datasets with dimensional emotion annotations -- EmoMusic, DEAM, PMEmo, WTC, and WCMED -- which span various musical styles. We demonstrate the problem of out-of-distribution generalization in a systematic experiment. By closely looking at multiple data and feature sets, we provide insight into genre-emotion relationships in existing data and examine potential genre dominance and dataset biases in certain feature representations. Based on these experiments, we arrive at a simple yet effective framework that combines embeddings extracted from the Jukebox model with chroma features and demonstrate how, alongside a combination of several diverse training sets, this permits us to train models with substantially improved cross-dataset generalization capabilities.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2509.18901.pdf' target='_blank'>https://arxiv.org/pdf/2509.18901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas PopoviÄ, Michael FÃ¤rber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18901">Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works in Natural Language Inference (NLI) and related tasks, such as automated fact-checking, employ atomic fact decomposition to enhance interpretability and robustness. For this, existing methods rely on resource-intensive generative large language models (LLMs) to perform decomposition. We propose JEDI, an encoder-only architecture that jointly performs extractive atomic fact decomposition and interpretable inference without requiring generative models during inference. To facilitate training, we produce a large corpus of synthetic rationales covering multiple NLI benchmarks. Experimental results demonstrate that JEDI achieves competitive accuracy in distribution and significantly improves robustness out of distribution and in adversarial settings over models based solely on extractive rationale supervision. Our findings show that interpretability and robust generalization in NLI can be realized using encoder-only architectures and synthetic rationales. Code and data available at https://jedi.nicpopovic.com
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2509.16935.pdf' target='_blank'>https://arxiv.org/pdf/2509.16935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lavish Ramchandani, Gunjan Deotale, Dev Kumar Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16935">Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated with tumor aggressiveness and poor prognosis. Their detection remains a significant challenge due to subtle morphological cues, class imbalance, and inter-observer variability among pathologists. The MIDOG 2025 challenge introduced a dedicated track for atypical mitosis classification, enabling systematic evaluation of deep learning methods. In this study, we investigated the use of large vision foundation models, including Virchow, Virchow2, and UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We conducted extensive experiments with different LoRA ranks, as well as random and group-based data splits, to analyze robustness under varied conditions. Our best approach, Virchow with LoRA rank 8 and ensemble of three-fold cross-validation, achieved a balanced accuracy of 88.37% on the preliminary test set, ranking joint 9th in the challenge leaderboard. These results highlight the promise of foundation models with efficient adaptation strategies for the classification of atypical mitosis, while underscoring the need for improvements in specificity and domain generalization.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2509.13442.pdf' target='_blank'>https://arxiv.org/pdf/2509.13442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnab Kumar Roy, Hemant Kumar Kathania, Paban Sapkota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13442">Enhancing Speaker-Independent Dysarthric Speech Severity Classification with DSSCNet and Cross-Corpus Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dysarthric speech severity classification is crucial for objective clinical assessment and progress monitoring in individuals with motor speech disorders. Although prior methods have addressed this task, achieving robust generalization in speaker-independent (SID) scenarios remains challenging. This work introduces DSSCNet, a novel deep neural architecture that combines Convolutional, Squeeze-Excitation (SE), and Residual network, helping it extract discriminative representations of dysarthric speech from mel spectrograms. The addition of SE block selectively focuses on the important features of the dysarthric speech, thereby minimizing loss and enhancing overall model performance. We also propose a cross-corpus fine-tuning framework for severity classification, adapted from detection-based transfer learning approaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora: TORGO and UA-Speech under speaker-independent evaluation protocols: One-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols. DSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and 64.18% under LOSO setting on TORGO and UA-Speech respectively outperforming existing state-of-the-art methods. Upon fine-tuning, the performance improves substantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25% on UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. These results demonstrate the effectiveness and generalizability of DSSCNet for fine-grained severity classification across diverse dysarthric speech datasets.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2509.09262.pdf' target='_blank'>https://arxiv.org/pdf/2509.09262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seung Gyu Jeong, Seong Eun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09262">Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this technical report, we describe our submission for Task 1, Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025 Challenge. Our work tackles the dual challenges of strict complexity constraints and robust generalization to both seen and unseen devices, while also leveraging the new rule allowing the use of device labels at test time. Our proposed system is based on a knowledge distillation framework where an efficient CP-MobileNet student learns from a compact, specialized two-teacher ensemble. This ensemble combines a baseline PaSST teacher, trained with standard cross-entropy, and a 'generalization expert' teacher. This expert is trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted from prior work, which explicitly structures the feature space for device robustness. To capitalize on the availability of test-time device labels, the distilled student model then undergoes a final device-specific fine-tuning stage. Our proposed system achieves a final accuracy of 57.93\% on the development set, demonstrating a significant improvement over the official baseline, particularly on unseen devices.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2509.02983.pdf' target='_blank'>https://arxiv.org/pdf/2509.02983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02983">DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous underwater navigation remains a challenging problem due to limited sensing capabilities and the difficulty of constructing accurate maps in underwater environments. In this paper, we propose a Diffusion-based Underwater Visual Navigation policy via knowledge-transferred depth features, named DUViN, which enables vision-based end-to-end 4-DoF motion control for underwater vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles and maintain a safe and perception awareness altitude relative to the terrain without relying on pre-built maps. To address the difficulty of collecting large-scale underwater navigation datasets, we propose a method that ensures robust generalization under domain shifts from in-air to underwater environments by leveraging depth features and introducing a novel model transfer strategy. Specifically, our training framework consists of two phases: we first train the diffusion-based visual navigation policy on in-air datasets using a pre-trained depth feature extractor. Secondly, we retrain the extractor on an underwater depth estimation task and integrate the adapted extractor into the trained navigation policy from the first step. Experiments in both simulated and real-world underwater environments demonstrate the effectiveness and generalization of our approach. The experimental videos are available at https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2508.09418.pdf' target='_blank'>https://arxiv.org/pdf/2508.09418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Usman Anjum, Chris Stockman, Cat Luong, Justin Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09418">Domain-Generalization to Improve Learning in Meta-Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Domain Generalization Sharpness-Aware Minimization Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm designed to generalize across tasks with limited training data. DGS-MAML combines gradient matching with sharpness-aware minimization in a bi-level optimization framework to enhance model adaptability and robustness. We support our method with theoretical analysis using PAC-Bayes and convergence guarantees. Experimental results on benchmark datasets show that DGS-MAML outperforms existing approaches in terms of accuracy and generalization. The proposed method is particularly useful for scenarios requiring few-shot learning and quick adaptation, and the source code is publicly available at GitHub.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2508.02995.pdf' target='_blank'>https://arxiv.org/pdf/2508.02995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Zhang Xinyu, Timothy Putra Prasetio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02995">VCNet: Recreating High-Level Visual Cortex Principles for Robust Artificial Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their success in image classification, modern convolutional neural networks (CNNs) exhibit fundamental limitations, including data inefficiency, poor out-of-distribution generalization, and vulnerability to adversarial perturbations. The primate visual system, in contrast, demonstrates superior efficiency and robustness, suggesting that its architectural principles may offer a blueprint for more capable artificial vision systems. This paper introduces Visual Cortex Network (VCNet), a novel neural network architecture whose design is informed by the macro-scale organization of the primate visual cortex. VCNet emulates key biological mechanisms, including hierarchical processing across distinct cortical areas, dual-stream information segregation, and top-down predictive feedback. We evaluate VCNet on two specialized benchmarks: the Spots-10 animal pattern dataset and a light field image classification task. Our results show that VCNet achieves a classification accuracy of 92.1\% on Spots-10 and 74.4\% on the light field dataset, surpassing contemporary models of comparable size. This work demonstrates that integrating neuroscientific principles into network design can lead to more efficient and robust models, providing a promising direction for addressing long-standing challenges in machine learning.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2507.22092.pdf' target='_blank'>https://arxiv.org/pdf/2507.22092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Carloni, Biagio Brattoli, Seongho Keum, Jongchan Park, Taebum Lee, Chang Ho Ahn, Sergio Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22092">Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computational pathology (CPath) has shown great potential in mining actionable insights from Whole Slide Images (WSIs). Deep Learning (DL) has been at the center of modern CPath, and while it delivers unprecedented performance, it is also known that DL may be affected by irrelevant details, such as those introduced during scanning by different commercially available scanners. This may lead to scanner bias, where the model outputs for the same tissue acquired by different scanners may vary. In turn, it hinders the trust of clinicians in CPath-based tools and their deployment in real-world clinical practices. Recent pathology Foundation Models (FMs) promise to provide better domain generalization capabilities. In this paper, we benchmark FMs using a multi-scanner dataset and show that FMs still suffer from scanner bias. Following this observation, we propose ScanGen, a contrastive loss function applied during task-specific fine-tuning that mitigates scanner bias, thereby enhancing the models' robustness to scanner variations. Our approach is applied to the Multiple Instance Learning task of Epidermal Growth Factor Receptor (EGFR) mutation prediction from H\&E-stained WSIs in lung cancer. We observe that ScanGen notably enhances the ability to generalize across scanners, while retaining or improving the performance of EGFR mutation prediction.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2507.12630.pdf' target='_blank'>https://arxiv.org/pdf/2507.12630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianxin Luan, John Thompson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12630">Achieving Robust Channel Estimation Neural Networks by Designed Training Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Channel estimation is crucial in wireless communications. However, in many papers neural networks are frequently tested by training and testing on one example channel or similar channels. This is because data-driven methods often degrade on new data which they are not trained on, as they cannot extrapolate their training knowledge. This is despite the fact physical channels are often assumed to be time-variant. However, due to the low latency requirements and limited computing resources, neural networks may not have enough time and computing resources to execute online training to fine-tune the parameters. This motivates us to design offline-trained neural networks that can perform robustly over wireless channels, but without any actual channel information being known at design time. In this paper, we propose design criteria to generate synthetic training datasets for neural networks, which guarantee that after training the resulting networks achieve a certain mean squared error (MSE) on new and previously unseen channels. Therefore, trained neural networks require no prior channel information or parameters update for real-world implementations. Based on the proposed design criteria, we further propose a benchmark design which ensures intelligent operation for different channel profiles. To demonstrate general applicability, we use neural networks with different levels of complexity to show that the generalization achieved appears to be independent of neural network architecture. From simulations, neural networks achieve robust generalization to wireless channels with both fixed channel profiles and variable delay spreads.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2507.11955.pdf' target='_blank'>https://arxiv.org/pdf/2507.11955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Zhengyu Zhang, Muxin Liao, Shishun Tian, Wenbin Zou, Lu Zhang, Chen Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11955">Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizable semantic segmentation aims to perform well on unseen target domains, a critical challenge due to real-world applications requiring high generalizability. Class-wise prototypes, representing class centroids, serve as domain-invariant cues that benefit generalization due to their stability and semantic consistency. However, this approach faces three challenges. First, existing methods often adopt coarse prototypical alignment strategies, which may hinder performance. Second, naive prototypes computed by averaging source batch features are prone to overfitting and may be negatively affected by unrelated source data. Third, most methods treat all source samples equally, ignoring the fact that different features have varying adaptation difficulties. To address these limitations, we propose a novel framework for generalizable semantic segmentation: Prototypical Progressive Alignment and Reweighting (PPAR), leveraging the strong generalization ability of the CLIP model. Specifically, we define two prototypes: the Original Text Prototype (OTP) and Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for alignment. We then introduce a progressive alignment strategy that aligns features in an easy-to-difficult manner, reducing domain gaps gradually. Furthermore, we propose a prototypical reweighting mechanism that estimates the reliability of source data and adjusts its contribution, mitigating the effect of irrelevant or harmful features (i.e., reducing negative transfer). We also provide a theoretical analysis showing the alignment between our method and domain generalization theory. Extensive experiments across multiple benchmarks demonstrate that PPAR achieves state-of-the-art performance, validating its effectiveness.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2507.06190.pdf' target='_blank'>https://arxiv.org/pdf/2507.06190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kwanghyuk Park, Jiaxi Gu, Jae-Hun Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06190">Conservative approximation-based feedforward neural network for WENO schemes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present the feedforward neural network based on the conservative approximation to the derivative from point values, for the weighted essentially non-oscillatory (WENO) schemes in solving hyperbolic conservation laws. The feedforward neural network, whose inputs are point values from the three-point stencil and outputs are two nonlinear weights, takes the place of the classical WENO weighting procedure. For the training phase, we employ the supervised learning and create a new labeled dataset for one-dimensional conservative approximation, where we construct a numerical flux function from the given point values such that the flux difference approximates the derivative to high-order accuracy. The symmetric-balancing term is introduced for the loss function so that it propels the neural network to match the conservative approximation to the derivative and satisfy the symmetric property that WENO3-JS and WENO3-Z have in common. The consequent WENO schemes, WENO3-CADNNs, demonstrate robust generalization across various benchmark scenarios and resolutions, where they outperform WENO3-Z and achieve accuracy comparable to WENO5-JS.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2505.15191.pdf' target='_blank'>https://arxiv.org/pdf/2505.15191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hana Satou, Alan Mitkiy, F Monkey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15191">Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning under domain shift remains a fundamental challenge due to the divergence between source and target data manifolds. In this paper, we propose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework that decomposes adversarial perturbations into on-manifold and off-manifold components to simultaneously capture semantic variation and model brittleness. We theoretically demonstrate that enforcing on-manifold consistency reduces hypothesis complexity and improves generalization, while off-manifold regularization smooths decision boundaries in low-density regions. Moreover, we introduce a geometry-aware alignment loss that minimizes geodesic discrepancy between source and target manifolds. Experiments on DomainNet, VisDA, and Office-Home show that MAADA consistently outperforms existing adversarial and adaptation methods in both unsupervised and few-shot settings, demonstrating superior structural robustness and cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2505.12681.pdf' target='_blank'>https://arxiv.org/pdf/2505.12681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hana Satou, Alan Mitkiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12681">On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning across domains with distribution shift remains a fundamental challenge in building robust and adaptable machine learning systems. While adversarial perturbations are traditionally viewed as threats that expose model vulnerabilities, recent studies suggest that they can also serve as constructive tools for data augmentation. In this work, we systematically investigate the role of adversarial data augmentation (ADA) in enhancing both robustness and adaptivity in transfer learning settings. We analyze how adversarial examples, when used strategically during training, improve domain generalization by enriching decision boundaries and reducing overfitting to source-domain-specific features. We further propose a unified framework that integrates ADA with consistency regularization and domain-invariant representation learning. Extensive experiments across multiple benchmark datasets -- including VisDA, DomainNet, and Office-Home -- demonstrate that our method consistently improves target-domain performance under both unsupervised and few-shot domain adaptation settings. Our results highlight a constructive perspective of adversarial learning, transforming perturbation from a destructive attack into a regularizing force for cross-domain transferability.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2505.11654.pdf' target='_blank'>https://arxiv.org/pdf/2505.11654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Liu, Yingxue Zhang, Xin Zhang, Ling Tian, Yanhua Li, Jun Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11654">UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting urban dynamics is crucial for managing transportation systems, optimizing urban planning, and enhancing public services. While neural network-based approaches have achieved success, they often rely on task-specific architectures and large volumes of data, limiting their ability to generalize across diverse urban scenarios. Meanwhile, Large Language Models (LLMs) offer strong reasoning and generalization capabilities, yet their application to spatial-temporal urban dynamics remains underexplored. Existing LLM-based methods struggle to effectively integrate multifaceted spatial-temporal data and fail to address distributional shifts between training and testing data, limiting their predictive reliability in real-world applications. To bridge this gap, we propose UrbanMind, a novel spatial-temporal LLM framework for multifaceted urban dynamics prediction that ensures both accurate forecasting and robust generalization. At its core, UrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with specialized masking strategies that capture intricate spatial-temporal dependencies and intercorrelations among multifaceted urban dynamics. Additionally, we design a semantic-aware prompting and fine-tuning strategy that encodes spatial-temporal contextual details into prompts, enhancing LLMs' ability to reason over spatial-temporal patterns. To further improve generalization, we introduce a test time adaptation mechanism with a test data reconstructor, enabling UrbanMind to dynamically adjust to unseen test data by reconstructing LLM-generated embeddings. Extensive experiments on real-world urban datasets across multiple cities demonstrate that UrbanMind consistently outperforms state-of-the-art baselines, achieving high accuracy and robust generalization, even in zero-shot settings.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2505.07050.pdf' target='_blank'>https://arxiv.org/pdf/2505.07050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binbin Wei, Yuhang Zhang, Shishun Tian, Muxin Liao, Wei Li, Wenbin Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07050">Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised Domain Adaptation (UDA) aims to align source and target domain distributions to close the domain gap, but still struggles with obtaining the target data. Fortunately, Domain Generalization (DG) excels without the need for any target data. Recent works expose that depth maps contribute to improved generalized performance in the UDA tasks, but they ignore the noise and holes in depth maps due to device and environmental factors, failing to sufficiently and effectively learn domain-invariant representation. Although high-sensitivity region suppression has shown promising results in learning domain-invariant features, existing methods cannot be directly applicable to depth maps due to their unique characteristics. Hence, we propose a novel framework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal stylization flow (DSSS), focusing on learning domain-invariant features from depth maps for the DG semantic segmentation. Specifically, we propose the RGB-D inter-modal stylization flow to generate stylized depth maps for sensitivity detection, cleverly utilizing RGB information as the stylization source. Then, a class-wise soft spatial sensitivity suppression is designed to identify and emphasize non-sensitive depth features that contain more domain-invariant information. Furthermore, an RGB-D soft alignment loss is proposed to ensure that the stylized depth maps only align part of the RGB features while still retaining the unique depth information. To our best knowledge, our DSSS framework is the first work to integrate RGB and Depth information in the multi-class DG semantic segmentation task. Extensive experiments over multiple backbone networks show that our framework achieves remarkable performance improvement.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2505.05291.pdf' target='_blank'>https://arxiv.org/pdf/2505.05291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05291">Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to learn robust representations from large-scale natural image datasets, enhancing their generalization across domains. In retinal imaging, foundation models pretrained on either natural or ophthalmic data have shown promise, but the benefits of in-domain pretraining remain uncertain. To investigate this, we benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets totaling 70,000 expert-annotated images for the task of moderate-to-late age-related macular degeneration (AMD) identification. Our results show that iBOT pretrained on natural images achieves the highest out-of-distribution generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models, which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining, which achieved AUROCs of 0.68-0.91. These findings highlight the value of foundation models in improving AMD identification and challenge the assumption that in-domain pretraining is necessary. Furthermore, we release BRAMD, an open-access dataset (n=587) of DFIs with AMD labels from Brazil.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2504.21385.pdf' target='_blank'>https://arxiv.org/pdf/2504.21385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijun Zhou, Baojie Fan, Jiandong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21385">Physics-Guided Image Dehazing Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the domain gap between real-world and synthetic hazy images, current data-driven dehazing algorithms trained on synthetic datasets perform well on synthetic data but struggle to generalize to real-world scenarios. To address this challenge, we propose \textbf{I}mage \textbf{D}ehazing \textbf{D}iffusion \textbf{M}odels (IDDM), a novel diffusion process that incorporates the atmospheric scattering model into noise diffusion. IDDM aims to use the gradual haze formation process to help the denoising Unet robustly learn the distribution of clear images from the conditional input hazy images. We design a specialized training strategy centered around IDDM. Diffusion models are leveraged to bridge the domain gap from synthetic to real-world, while the atmospheric scattering model provides physical guidance for haze formation. During the forward process, IDDM simultaneously introduces haze and noise into clear images, and then robustly separates them during the sampling process. By training with physics-guided information, IDDM shows the ability of domain generalization, and effectively restores the real-world hazy images despite being trained on synthetic datasets. Extensive experiments demonstrate the effectiveness of our method through both quantitative and qualitative comparisons with state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2504.14664.pdf' target='_blank'>https://arxiv.org/pdf/2504.14664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jixiang Sun, Fei Lei, Jiawei Zhang, Wenxiu Sun, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14664">Frequency-domain Learning with Kernel Prior for Blind Image Deblurring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While achieving excellent results on various datasets, many deep learning methods for image deblurring suffer from limited generalization capabilities with out-of-domain data. This limitation is likely caused by their dependence on certain domain-specific datasets. To address this challenge, we argue that it is necessary to introduce the kernel prior into deep learning methods, as the kernel prior remains independent of the image context. For effective fusion of kernel prior information, we adopt a rational implementation method inspired by traditional deblurring algorithms that perform deconvolution in the frequency domain. We propose a module called Frequency Integration Module (FIM) for fusing the kernel prior and combine it with a frequency-based deblurring Transfomer network. Experimental results demonstrate that our method outperforms state-of-the-art methods on multiple blind image deblurring tasks, showcasing robust generalization abilities. Source code will be available soon.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2504.13562.pdf' target='_blank'>https://arxiv.org/pdf/2504.13562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Li, Han Jiang, Zhihua Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13562">DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DETAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize the user's core intention, minimizing interference from attack tokens. Our experimental results demonstrate that DETAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, in evaluating the model's utility, we incorporated over-defense datasets, which further validate the superior performance of our approach. The code will be released immediately upon acceptance.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2504.12456.pdf' target='_blank'>https://arxiv.org/pdf/2504.12456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huantao Ren, Minmin Yang, Senem Velipasalar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12456">DG-MVP: 3D Domain Generalization via Multiple Views of Point Clouds for Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks have achieved significant success in 3D point cloud classification while relying on large-scale, annotated point cloud datasets, which are labor-intensive to build. Compared to capturing data with LiDAR sensors and then performing annotation, it is relatively easier to sample point clouds from CAD models. Yet, data sampled from CAD models is regular, and does not suffer from occlusion and missing points, which are very common for LiDAR data, creating a large domain shift. Therefore, it is critical to develop methods that can generalize well across different point cloud domains. %In this paper, we focus on the 3D point cloud domain generalization problem. Existing 3D domain generalization methods employ point-based backbones to extract point cloud features. Yet, by analyzing point utilization of point-based methods and observing the geometry of point clouds from different domains, we have found that a large number of point features are discarded by point-based methods through the max-pooling operation. This is a significant waste especially considering the fact that domain generalization is more challenging than supervised learning, and point clouds are already affected by missing points and occlusion to begin with. To address these issues, we propose a novel method for 3D point cloud domain generalization, which can generalize to unseen domains of point clouds. Our proposed method employs multiple 2D projections of a 3D point cloud to alleviate the issue of missing points and involves a simple yet effective convolution-based model to extract features. The experiments, performed on the PointDA-10 and Sim-to-Real benchmarks, demonstrate the effectiveness of our proposed method, which outperforms different baselines, and can transfer well from synthetic domain to real-world domain.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2503.24111.pdf' target='_blank'>https://arxiv.org/pdf/2503.24111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur M. Faria, Ignacio F. GraÃ±a, Savvas Varsamopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24111">Inductive Graph Representation Learning with Quantum Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum Graph Neural Networks (QGNNs) present a promising approach for combining quantum computing with graph-structured data processing. While classical Graph Neural Networks (GNNs) are renowned for their scalability and robustness, existing QGNNs often lack flexibility due to graph-specific quantum circuit designs, limiting their applicability to a narrower range of graph-structured problems, falling short of real-world scenarios. To address these limitations, we propose a versatile QGNN framework inspired by the classical GraphSAGE approach, utilizing quantum models as aggregators. In this work, we integrate established techniques for inductive representation learning on graphs with parametrized quantum convolutional and pooling layers, effectively bridging classical and quantum paradigms. The convolutional layer is flexible, enabling tailored designs for specific problems. Benchmarked on a node regression task with the QM9 dataset, we demonstrate that our framework successfully models a non-trivial molecular dataset, achieving performance comparable to classical GNNs. In particular, we show that our quantum approach exhibits robust generalization across molecules with varying numbers of atoms without requiring circuit modifications, slightly outperforming classical GNNs. Furthermore, we numerically investigate the scalability of the QGNN framework. Specifically, we demonstrate the absence of barren plateaus in our architecture as the number of qubits increases, suggesting that the proposed quantum model can be extended to handle larger and more complex graph-based problems effectively.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2503.18695.pdf' target='_blank'>https://arxiv.org/pdf/2503.18695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Zeyu Zhang, Yue Huang, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18695">OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although foundation models (FMs) claim to be powerful, their generalization ability significantly decreases when faced with distribution shifts, weak supervision, or malicious attacks in the open world. On the other hand, most domain generalization or adversarial fine-tuning methods are task-related or model-specific, ignoring the universality in practical applications and the transferability between FMs. This paper delves into the problem of generalizing FMs to the out-of-domain data. We propose a novel framework, the Object-Concept-Relation Triad (OCRT), that enables FMs to extract sparse, high-level concepts and intricate relational structures from raw visual inputs. The key idea is to bind objects in visual scenes and a set of object-centric representations through unsupervised decoupling and iterative refinement. To be specific, we project the object-centric representations onto a semantic concept space that the model can readily interpret and estimate their importance to filter out irrelevant elements. Then, a concept-based graph, which has a flexible degree, is constructed to incorporate the set of concepts and their corresponding importance, enabling the extraction of high-order factors from informative concepts and facilitating relational reasoning among these concepts. Extensive experiments demonstrate that OCRT can substantially boost the generalizability and robustness of SAM and CLIP across multiple downstream tasks.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2503.16271.pdf' target='_blank'>https://arxiv.org/pdf/2503.16271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Borges S. Carvalho, Alessandro Torcinovich, Victor Jimenez Rodriguez, Antonio E. CinÃ, Carlos Cotrini, Lea SchÃ¶nherr, Joachim M. Buhmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16271">Rethinking Robustness in Machine Learning: A Posterior Agreement Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The robustness of algorithms against covariate shifts is a fundamental problem with critical implications for the deployment of machine learning algorithms in the real world. Current evaluation methods predominantly match the robustness definition to that of standard generalization, relying on standard metrics like accuracy-based scores, which, while designed for performance assessment, lack a theoretical foundation encompassing their application in estimating robustness to distribution shifts. In this work, we set the desiderata for a robustness metric, and we propose a novel principled framework for the robustness assessment problem that directly follows the Posterior Agreement (PA) theory of model validation. Specifically, we extend the PA framework to the covariate shift setting by proposing a PA metric for robustness evaluation in supervised classification tasks. We assess the soundness of our metric in controlled environments and through an empirical robustness analysis in two different covariate shift scenarios: adversarial learning and domain generalization. We illustrate the suitability of PA by evaluating several models under different nature and magnitudes of shift, and proportion of affected observations. The results show that the PA metric provides a sensible and consistent analysis of the vulnerabilities in learning algorithms, even in the presence of few perturbed observations.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2503.15149.pdf' target='_blank'>https://arxiv.org/pdf/2503.15149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxiang Shen, RaÃºl I. Sosa, Jakub Lengiewicz, Alexandre Tkatchenko, StÃ©phane P. A. Bordas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15149">Machine learning surrogate models of many-body dispersion interactions in polymer melts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of many-body dispersion (MBD) interactions is essential for understanding the van der Waals forces that govern the behavior of many complex molecular systems. However, the high computational cost of MBD calculations limits their direct application in large-scale simulations. In this work, we introduce a machine learning surrogate model specifically designed to predict MBD forces in polymer melts, a system that demands accurate MBD description and offers structural advantages for machine learning approaches. Our model is based on a trimmed SchNet architecture that selectively retains the most relevant atomic connections and incorporates trainable radial basis functions for geometric encoding. We validate our surrogate model on datasets from polyethylene, polypropylene, and polyvinyl chloride melts, demonstrating high predictive accuracy and robust generalization across diverse polymer systems. In addition, the model captures key physical features, such as the characteristic decay behavior of MBD interactions, providing valuable insights for optimizing cutoff strategies. Characterized by high computational efficiency, our surrogate model enables practical incorporation of MBD effects into large-scale molecular simulations.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2503.06759.pdf' target='_blank'>https://arxiv.org/pdf/2503.06759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Q. Vo, Samira Zare, Son T. Ly, Lin Wang, Chika F. Ezeana, Xiaohui Yu, Kelvin K. Wong, Stephen T. C. Wong, Hien V. Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06759">Revisiting Invariant Learning for Out-of-Domain Generalization on Multi-Site Mammogram Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in robust deep learning techniques for mammogram breast cancer classification, their reliability in real-world clinical development settings remains uncertain. The translation of these models to clinical practice faces challenges due to variations in medical centers, imaging protocols, and patient populations. To enhance their robustness, invariant learning methods have been proposed, prioritizing causal factors over misleading features. However, their effectiveness in clinical development and impact on mammogram classification require investigation. This paper reassesses the application of invariant learning for breast cancer risk estimation based on mammograms. Utilizing diverse multi-site public datasets, it represents the first study in this area. The objective is to evaluate invariant learning's benefits in developing robust models. Invariant learning methods, including Invariant Risk Minimization and Variance Risk Extrapolation, are compared quantitatively against Empirical Risk Minimization. Evaluation metrics include accuracy, average precision, and area under the curve. Additionally, interpretability is examined through class activation maps and visualization of learned representations. This research examines the advantages, limitations, and challenges of invariant learning for mammogram classification, guiding future studies to develop generalized methods for breast cancer prediction on whole mammograms in out-of-domain scenarios.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2502.14917.pdf' target='_blank'>https://arxiv.org/pdf/2502.14917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhao, Qirui Yuan, Jinyu Li, Haofeng Hu, Yun Li, Chengyuan Zheng, Fei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14917">Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2502.10838.pdf' target='_blank'>https://arxiv.org/pdf/2502.10838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janne Laakkonen, Ivan Kukanov, Ville HautamÃ¤ki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10838">Generalizable speech deepfake detection via meta-learned LoRA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable detection of speech deepfakes (spoofs) must remain effective when the distribution of spoofing attacks shifts. We frame the task as domain generalization and show that inserting Low-Rank Adaptation (LoRA) adapters into every attention head of a self-supervised (SSL) backbone, then training only those adapters with Meta-Learning Domain Generalization (MLDG), yields strong zero-shot performance. The resulting model updates about 3.6 million parameters, roughly 1.1% of the 318 million updated in full fine-tuning, yet surpasses a fully fine-tuned counterpart on five of six evaluation corpora. A first-order MLDG loop encourages the adapters to focus on cues that persist across attack types, lowering the average EER from 8.84% for the fully fine-tuned model to 5.30% with our best MLDG-LoRA configuration. Our findings show that combining meta-learning with parameter-efficient adaptation offers an effective method for zero-shot, distribution-shift-aware speech deepfake detection.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2502.05726.pdf' target='_blank'>https://arxiv.org/pdf/2502.05726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jayden Teoh, Wenjun Li, Pradeep Varakantham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05726">Improving Environment Novelty Quantification for Effective Unsupervised Environment Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised Environment Design (UED) formalizes the problem of autocurricula through interactive training between a teacher agent and a student agent. The teacher generates new training environments with high learning potential, curating an adaptive curriculum that strengthens the student's ability to handle unseen scenarios. Existing UED methods mainly rely on regret, a metric that measures the difference between the agent's optimal and actual performance, to guide curriculum design. Regret-driven methods generate curricula that progressively increase environment complexity for the student but overlook environment novelty -- a critical element for enhancing an agent's generalizability. Measuring environment novelty is especially challenging due to the underspecified nature of environment parameters in UED, and existing approaches face significant limitations. To address this, this paper introduces the Coverage-based Evaluation of Novelty In Environment (CENIE) framework. CENIE proposes a scalable, domain-agnostic, and curriculum-aware approach to quantifying environment novelty by leveraging the student's state-action space coverage from previous curriculum experiences. We then propose an implementation of CENIE that models this coverage and measures environment novelty using Gaussian Mixture Models. By integrating both regret and novelty as complementary objectives for curriculum design, CENIE facilitates effective exploration across the state-action space while progressively increasing curriculum complexity. Empirical evaluations demonstrate that augmenting existing regret-based UED algorithms with CENIE achieves state-of-the-art performance across multiple benchmarks, underscoring the effectiveness of novelty-driven autocurricula for robust generalization.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2502.04289.pdf' target='_blank'>https://arxiv.org/pdf/2502.04289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thorben Prein, Elton Pan, Sami Haddouti, Marco Lorenz, Janik Jehkul, Tymoteusz Wilk, Cansu Moran, Menelaos Panagiotis Fotiadis, Artur P. Toshev, Elsa Olivetti, Jennifer L. M. Rupp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04289">Retro-Rank-In: A Ranking-Based Approach for Inorganic Materials Synthesis Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrosynthesis strategically plans the synthesis of a chemical target compound from simpler, readily available precursor compounds. This process is critical for synthesizing novel inorganic materials, yet traditional methods in inorganic chemistry continue to rely on trial-and-error experimentation. Emerging machine-learning approaches struggle to generalize to entirely new reactions due to their reliance on known precursors, as they frame retrosynthesis as a multi-label classification task. To address these limitations, we propose Retro-Rank-In, a novel framework that reformulates the retrosynthesis problem by embedding target and precursor materials into a shared latent space and learning a pairwise ranker on a bipartite graph of inorganic compounds. We evaluate Retro-Rank-In's generalizability on challenging retrosynthesis dataset splits designed to mitigate data duplicates and overlaps. For instance, for Cr2AlB2, it correctly predicts the verified precursor pair CrB + Al despite never seeing them in training, a capability absent in prior work. Extensive experiments show that Retro-Rank-In sets a new state-of-the-art, particularly in out-of-distribution generalization and candidate set ranking, offering a powerful tool for accelerating inorganic material synthesis.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2502.02017.pdf' target='_blank'>https://arxiv.org/pdf/2502.02017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Wang, Bokui Wang, Zhixiang Shen, Boyan Deng, Zhao Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02017">Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in CV and NLP have inspired researchers to develop general-purpose graph foundation models through pre-training across diverse domains. However, a fundamental challenge arises from the substantial differences in graph topologies across domains. Additionally, real-world graphs are often sparse and prone to noisy connections and adversarial attacks. To address these issues, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework that aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. MDGFM bridges different domains by adaptively balancing features and topology while refining original graphs to eliminate noise and align topological structures. To further enhance knowledge transfer, we introduce an efficient prompt-tuning approach. By aligning topologies, MDGFM not only improves multi-domain pre-training but also enables robust knowledge transfer to unseen domains. Theoretical analyses provide guarantees of MDGFM's effectiveness and domain generalization capabilities. Extensive experiments on both homophilic and heterophilic graph datasets validate the robustness and efficacy of our method.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2502.00545.pdf' target='_blank'>https://arxiv.org/pdf/2502.00545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotong Tu, Chenyu Ma, Qingyao Wu, Yinhao Liu, Hongyang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00545">Integrating Frequency Guidance into Multi-source Domain Generalization for Bearing Fault Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent generalizable fault diagnosis researches have effectively tackled the distributional shift between unseen working conditions. Most of them mainly focus on learning domain-invariant representation through feature-level methods. However, the increasing numbers of unseen domains may lead to domain-invariant features contain instance-level spurious correlations, which impact the previous models' generalizable ability. To address the limitations, we propose the Fourier-based Augmentation Reconstruction Network, namely FARNet.The methods are motivated by the observation that the Fourier phase component and amplitude component preserve different semantic information of the signals, which can be employed in domain augmentation techniques. The network comprises an amplitude spectrum sub-network and a phase spectrum sub-network, sequentially reducing the discrepancy between the source and target domains. To construct a more robust generalized model, we employ a multi-source domain data augmentation strategy in the frequency domain. Specifically, a Frequency-Spatial Interaction Module (FSIM) is introduced to handle global information and local spatial features, promoting representation learning between the two sub-networks. To refine the decision boundary of our model output compared to conventional triplet loss, we propose a manifold triplet loss to contribute to generalization. Through extensive experiments on the CWRU and SJTU datasets, FARNet demonstrates effective performance and achieves superior results compared to current cross-domain approaches on the benchmarks.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2501.13726.pdf' target='_blank'>https://arxiv.org/pdf/2501.13726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shi-Qi Yan, Zhen-Hua Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13726">RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing external knowledge, its generation process heavily depends on the quality and accuracy of the retrieved context. Large language models (LLMs) struggle to evaluate the correctness of non-parametric knowledge retrieved externally when it differs from internal memorization, leading to knowledge conflicts during response generation. To this end, we introduce the Retrieval Preference Optimization (RPO), a lightweight and effective alignment method to adaptively leverage multi-source knowledge based on retrieval relevance. An implicit representation of retrieval relevance is derived and incorporated into the reward model to integrate retrieval evaluation and response generation into a single model, solving the problem that previous methods necessitate the additional procedure to assess the retrieval quality. Notably, RPO is the only RAG-dedicated alignment approach that quantifies the awareness of retrieval relevance in training, overcoming mathematical obstacles. Experiments on four datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any extra component, exhibiting its robust generalization.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2501.03782.pdf' target='_blank'>https://arxiv.org/pdf/2501.03782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sy-Tuyen Ho, Tuan Van Vo, Somayeh Ebrahimkhani, Ngai-Man Cheung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03782">Vision Transformer Neural Architecture Search for Out-of-Distribution Generalization: Benchmark and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While ViTs have achieved across machine learning tasks, deploying them in real-world scenarios faces a critical challenge: generalizing under OoD shifts. A crucial research gap exists in understanding how to design ViT architectures, both manually and automatically, for better OoD generalization. To this end, we introduce OoD-ViT-NAS, the first systematic benchmark for ViTs NAS focused on OoD generalization. This benchmark includes 3000 ViT architectures of varying computational budgets evaluated on 8 common OoD datasets. Using this benchmark, we analyze factors contributing to OoD generalization. Our findings reveal key insights. First, ViT architecture designs significantly affect OoD generalization. Second, ID accuracy is often a poor indicator of OoD accuracy, highlighting the risk of optimizing ViT architectures solely for ID performance. Third, we perform the first study of NAS for ViTs OoD robustness, analyzing 9 Training-free NAS methods. We find that existing Training-free NAS methods are largely ineffective in predicting OoD accuracy despite excelling at ID accuracy. Simple proxies like Param or Flop surprisingly outperform complex Training-free NAS methods in predicting OoD accuracy. Finally, we study how ViT architectural attributes impact OoD generalization and discover that increasing embedding dimensions generally enhances performance. Our benchmark shows that ViT architectures exhibit a wide range of OoD accuracy, with up to 11.85% improvement for some OoD shifts. This underscores the importance of studying ViT architecture design for OoD. We believe OoD-ViT-NAS can catalyze further research into how ViT designs influence OoD generalization.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2412.10089.pdf' target='_blank'>https://arxiv.org/pdf/2412.10089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Cao, Songcan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10089">Guidance Not Obstruction: A Conjugate Consistent Enhanced Strategy for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization addresses domain shift in real-world applications. Most approaches adopt a domain angle, seeking invariant representation across domains by aligning their marginal distributions, irrespective of individual classes, naturally leading to insufficient exploration of discriminative information. Switching to a class angle, we find that multiple domain-related peaks or clusters within the same individual classes must emerge due to distribution shift. In other words, marginal alignment does not guarantee conditional alignment, leading to suboptimal generalization. Therefore, we argue that acquiring discriminative generalization between classes within domains is crucial. In contrast to seeking distribution alignment, we endeavor to safeguard domain-related between-class discrimination. To this end, we devise a novel Conjugate Consistent Enhanced Module, namely Con2EM, based on a distribution over domains, i.e., a meta-distribution. Specifically, we employ a novel distribution-level Universum strategy to generate supplementary diverse domain-related class-conditional distributions, thereby enhancing generalization. This allows us to resample from these generated distributions to provide feedback to the primordial instance-level classifier, further improving its adaptability to the target-agnostic. To ensure generation accuracy, we establish an additional distribution-level classifier to regularize these conditional distributions. Extensive experiments have been conducted to demonstrate its effectiveness and low computational cost compared to SOTAs.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2412.08946.pdf' target='_blank'>https://arxiv.org/pdf/2412.08946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08946">MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, LoRA has emerged as a crucial technique for fine-tuning large pre-trained models, yet its performance in multi-task learning scenarios often falls short. In contrast, the MoE architecture presents a natural solution to this issue. However, it introduces challenges such as mutual interference of data across multiple domains and knowledge forgetting of various tasks. Additionally, MoE significantly increases the number of parameters, posing a computational cost challenge. Therefore, in this paper, we propose MoSLD, a mixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these challenges by sharing the upper projection matrix in LoRA among different experts, encouraging the model to learn general knowledge across tasks, while still allowing the lower projection matrix to focus on the unique features of each task. The application of dropout alleviates the imbalanced update of parameter matrix and mitigates parameter overfitting in LoRA. Extensive experiments demonstrate that our model exhibits excellent performance in both single-task and multi-task scenarios, with robust out-of-domain generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2410.23641.pdf' target='_blank'>https://arxiv.org/pdf/2410.23641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanchao Liu, Yujiang Li, Tai-Jiang Mu, Shi-Min Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23641">Recovering Complete Actions for Cross-dataset Skeleton Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite huge progress in skeleton-based action recognition, its generalizability to different domains remains a challenging issue. In this paper, to solve the skeleton action generalization problem, we present a recover-and-resample augmentation framework based on a novel complete action prior. We observe that human daily actions are confronted with temporal mismatch across different datasets, as they are usually partial observations of their complete action sequences. By recovering complete actions and resampling from these full sequences, we can generate strong augmentations for unseen domains. At the same time, we discover the nature of general action completeness within large datasets, indicated by the per-frame diversity over time. This allows us to exploit two assets of transferable knowledge that can be shared across action samples and be helpful for action completion: boundary poses for determining the action start, and linear temporal transforms for capturing global action patterns. Therefore, we formulate the recovering stage as a two-step stochastic action completion with boundary pose-conditioned extrapolation followed by smooth linear transforms. Both the boundary poses and linear transforms can be efficiently learned from the whole dataset via clustering. We validate our approach on a cross-dataset setting with three skeleton action datasets, outperforming other domain generalization approaches by a considerable margin.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2409.04768.pdf' target='_blank'>https://arxiv.org/pdf/2409.04768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Qiao, Wenyu Wang, Meixia Qu, Kun Su, Bin Jiang, Qiang Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04768">Medical Image Segmentation via Single-Source Domain Generalization with Random Amplitude Spectrum Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of medical image segmentation is challenged by domain generalization (DG) due to domain shifts in clinical datasets. The DG challenge is exacerbated by the scarcity of medical data and privacy concerns. Traditional single-source domain generalization (SSDG) methods primarily rely on stacking data augmentation techniques to minimize domain discrepancies. In this paper, we propose Random Amplitude Spectrum Synthesis (RASS) as a training augmentation for medical images. RASS enhances model generalization by simulating distribution changes from a frequency perspective. This strategy introduces variability by applying amplitude-dependent perturbations to ensure broad coverage of potential domain variations. Furthermore, we propose random mask shuffle and reconstruction components, which can enhance the ability of the backbone to process structural information and increase resilience intra- and cross-domain changes. The proposed Random Amplitude Spectrum Synthesis for Single-Source Domain Generalization (RAS^4DG) is validated on 3D fetal brain images and 2D fundus photography, and achieves an improved DG segmentation performance compared to other SSDG models.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2601.18527.pdf' target='_blank'>https://arxiv.org/pdf/2601.18527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Maria Molfese, Momchil Hardalov, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.18527">Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2601.13060.pdf' target='_blank'>https://arxiv.org/pdf/2601.13060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zecheng Li, Zhihui Cao, Wenke Huang, Yudong Zhang, Keying Qi, Rui Wang, Zeyu Zheng, Jian Zhao, Hao Zhu, Hengxin Wu, Yuran Wang, Guitao Fan, Guokun Wu, Yicong Liu, Zhilin Gao, Haikun Xu, He Yang, Minqi Xiang, Xingyu Liu, Zuojian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.13060">MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2512.21348.pdf' target='_blank'>https://arxiv.org/pdf/2512.21348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Xiao, Shangwen Wang, Sicen Liu, Dingyuan Xue, Xian Zhan, Yepang Liu, Jie M. Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21348">Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional software fairness research typically emphasizes ethical and social imperatives, neglecting that fairness fundamentally represents a core software quality issue arising directly from performance disparities across sensitive user groups. Recognizing fairness explicitly as a software quality dimension yields practical benefits beyond ethical considerations, notably improved predictive performance for unprivileged groups, enhanced out-of-distribution generalization, and increased geographic transferability in real-world deployments. Nevertheless, existing bias mitigation methods face a critical dilemma: while pre-processing methods offer broad applicability across model types, they generally fall short in effectiveness compared to post-processing techniques. To overcome this challenge, we propose Correlation Tuning (CoT), a novel pre-processing approach designed to mitigate bias by adjusting data correlations. Specifically, CoT introduces the Phi-coefficient, an intuitive correlation measure, to systematically quantify correlation between sensitive attributes and labels, and employs multi-objective optimization to address the proxy biases. Extensive evaluations demonstrate that CoT increases the true positive rate of unprivileged groups by an average of 17.5% and reduces three key bias metrics, including statistical parity difference (SPD), average odds difference (AOD), and equal opportunity difference (EOD), by more than 50% on average. CoT outperforms state-of-the-art methods by three and ten percentage points in single attribute and multiple attributes scenarios, respectively. We will publicly release our experimental results and source code to facilitate future research.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2512.20760.pdf' target='_blank'>https://arxiv.org/pdf/2512.20760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20760">Generalization of RLVR Using Causal Reasoning as a Testbed</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2512.16948.pdf' target='_blank'>https://arxiv.org/pdf/2512.16948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Xu, Shuai Gong, Xuming Ran, Haihua Luo, Yangfan Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16948">AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep learning models have shown strong performance in simulating neural responses, they often fail to clearly separate stable visual encoding from condition-specific adaptation, which limits their ability to generalize across stimuli and individuals. We introduce the Adaptive Visual Model (AVM), a structure-preserving framework that enables condition-aware adaptation through modular subnetworks, without modifying the core representation. AVM keeps a Vision Transformer-based encoder frozen to capture consistent visual features, while independently trained modulation paths account for neural response variations driven by stimulus content and subject identity. We evaluate AVM in three experimental settings, including stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all of which involve structured changes in inputs and individuals. Across two large-scale mouse V1 datasets, AVM outperforms the state-of-the-art V1T model by approximately 2% in predictive correlation, demonstrating robust generalization, interpretable condition-wise modulation, and high architectural efficiency. Specifically, AVM achieves a 9.1% improvement in explained variance (FEVE) under the cross-dataset adaptation setting. These results suggest that AVM provides a unified framework for adaptive neural modeling across biological and experimental conditions, offering a scalable solution under structural constraints. Its design may inform future approaches to cortical modeling in both neuroscience and biologically inspired AI systems.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2512.04748.pdf' target='_blank'>https://arxiv.org/pdf/2512.04748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyue Kang, Diwei Shi, Li Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04748">Model Whisper: Steering Vectors Unlock Large Language Models' Potential in Test-time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is a critical challenge to efficiently unlock the powerful reasoning potential of Large Language Models (LLMs) for specific tasks or new distributions. Existing test-time adaptation methods often require tuning model parameters, which is not only computationally expensive but also risks degrading the model's pre-existing abilities.To address this, we introduce a lightweight component, Test-Time Steering Vectors (TTSV), which is prepended to the input while keeping the LLM's parameters entirely frozen. By optimizing the TTSV on test data to minimize the model's output entropy, we steer the model towards an internal state of higher confidence, activating its inherent abilities most relevant to the current task. TTSV is both lightweight and highly efficient to optimize, making it a true plug-and-play enhancement. Extensive experiments validate our approach's effectiveness on both base models and reasoning-enhanced models. For instance, on the MATH500 task, TTSV achieves a 45.88% relative performance gain on the Qwen2.5-Math-7B model and a 16.22% relative gain on the Qwen3-4B model. Furthermore, our approach exhibits robust generalization, with its steering vectors proving highly transferable across diverse tasks.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2512.03838.pdf' target='_blank'>https://arxiv.org/pdf/2512.03838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Staniek, Artem Sokolov, Stefan Riezler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03838">Training and Evaluation of Guideline-Based Medical Reasoning in LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2511.18792.pdf' target='_blank'>https://arxiv.org/pdf/2511.18792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Jiang, Yihe Yan, Yanxiang Wang, Chun Tung Chou, Wen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18792">Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical "domain shift" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2510.22070.pdf' target='_blank'>https://arxiv.org/pdf/2510.22070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Caldera, Giacomo Bottacini, Lara Cavinato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22070">MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative modeling has emerged as a powerful paradigm for representation learning, but its direct applicability to challenging fields like medical imaging remains limited: mere generation, without task alignment, fails to provide a robust foundation for clinical use. We propose MAGIC-Flow, a conditional multiscale normalizing flow architecture that performs generation and classification within a single modular framework. The model is built as a hierarchy of invertible and differentiable bijections, where the Jacobian determinant factorizes across sub-transformations. We show how this ensures exact likelihood computation and stable optimization, while invertibility enables explicit visualization of sample likelihoods, providing an interpretable lens into the model's reasoning. By conditioning on class labels, MAGIC-Flow supports controllable sample synthesis and principled class-probability estimation, effectively aiding both generative and discriminative objectives. We evaluate MAGIC-Flow against top baselines using metrics for similarity, fidelity, and diversity. Across multiple datasets, it addresses generation and classification under scanner noise, and modality-specific synthesis and identification. Results show MAGIC-Flow creates realistic, diverse samples and improves classification. MAGIC-Flow is an effective strategy for generation and classification in data-limited domains, with direct benefits for privacy-preserving augmentation, robust generalization, and trustworthy medical AI.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2510.17188.pdf' target='_blank'>https://arxiv.org/pdf/2510.17188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaibhav Rathore, Divyam Gupta, Biplab Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17188">HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalized Category Discovery (GCD) aims to classify test-time samples into either seen categories** -- available during training -- or novel ones, without relying on label supervision. Most existing GCD methods assume simultaneous access to labeled and unlabeled data during training and arising from the same domain, limiting applicability in open-world scenarios involving distribution shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by requiring models to generalize to unseen domains containing novel categories, without accessing targetdomain data during training. The only prior DG-GCD method, DG2CD-Net, relies on episodic training with multiple synthetic domains and task vector aggregation, incurring high computational cost and error accumulation. We propose HIDISC, a hyperbolic representation learning framework that achieves domain and category-level generalization without episodic simulation. To expose the model to minimal but diverse domain variations, we augment the source domain using GPT-guided diffusion, avoiding overfitting while maintaining efficiency. To structure the representation space, we introduce Tangent CutMix, a curvature-aware interpolation that synthesizes pseudo-novel samples in tangent space, preserving manifold consistency. A unified loss -- combining penalized Busemann alignment, hybrid hyperbolic contrastive regularization, and adaptive outlier repulsion -- **facilitates compact, semantically structured embeddings. A learnable curvature parameter further adapts the geometry to dataset complexity. HIDISC achieves state-of-the-art results on PACS , Office-Home , and DomainNet, consistently outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2510.03305.pdf' target='_blank'>https://arxiv.org/pdf/2510.03305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian Zheng, Subashree Venkatasubramanian, Shuolin Li, Amy Braverman, Xinyi Ke, Zhewen Hou, Peter Jin, Samarth Sanjay Agrawal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03305">Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning has been increasingly applied in climate modeling on system emulation acceleration, data-driven parameter inference, forecasting, and knowledge discovery, addressing challenges such as physical consistency, multi-scale coupling, data sparsity, robust generalization, and integration with scientific workflows. This paper analyzes a series of case studies from applied machine learning research in climate modeling, with a focus on design choices and workflow structure. Rather than reviewing technical details, we aim to synthesize workflow design patterns across diverse projects in ML-enabled climate modeling: from surrogate modeling, ML parameterization, probabilistic programming, to simulation-based inference, and physics-informed transfer learning. We unpack how these workflows are grounded in physical knowledge, informed by simulation data, and designed to integrate observations. We aim to offer a framework for ensuring rigor in scientific machine learning through more transparent model development, critical evaluation, informed adaptation, and reproducibility, and to contribute to lowering the barrier for interdisciplinary collaboration at the interface of data science and climate modeling.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2510.00329.pdf' target='_blank'>https://arxiv.org/pdf/2510.00329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarmad Mehrdad, Maxime Sabbah, Vincent Bonnet, Ludovic Righetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00329">Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the application of Minimal Observation Inverse Reinforcement Learning (MO-IRL) to model and predict human arm-reaching movements with time-varying cost weights. Using a planar two-link biomechanical model and high-resolution motion-capture data from subjects performing a pointing task, we segment each trajectory into multiple phases and learn phase-specific combinations of seven candidate cost functions. MO-IRL iteratively refines cost weights by scaling observed and generated trajectories in the maximum entropy IRL formulation, greatly reducing the number of required demonstrations and convergence time compared to classical IRL approaches. Training on ten trials per posture yields average joint-angle Root Mean Squared Errors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight divisions, respectively, versus 10.4 deg using a single static weight. Cross-validation on remaining trials and, for the first time, inter-subject validation on an unseen subject's 20 trials, demonstrates comparable predictive accuracy, around 8 deg RMSE, indicating robust generalization. Learned weights emphasize joint acceleration minimization during movement onset and termination, aligning with smoothness principles observed in biological motion. These results suggest that MO-IRL can efficiently uncover dynamic, subject-independent cost structures underlying human motor control, with potential applications for humanoid robots.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2509.25672.pdf' target='_blank'>https://arxiv.org/pdf/2509.25672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hasan Alp CaferoÄlu, Mehmet Serhat Ãelik, ÃzgÃ¼r Ulusoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25672">SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating natural language questions into SQL has become a core challenge in enabling non-technical users to query databases. While recent work has explored large-scale synthetic data generation to improve model performance through post-training, most efforts emphasize cross-domain generalization. This leaves a gap for real-world enterprise scenarios, where models need to specialize to a single database schema and organizations require to be able to evaluate their Text-to-SQL systems on their own databases. To address this, we introduce SING-SQL, a fully automated two-stage framework for generating high-quality, high-coverage synthetic Text-to-SQL data for any target database, without relying on SQL logs or manual annotations. Our approach hierarchically partitions a database schema into sub-schemas, synthesizes SQL queries across multiple complexity levels, and applies a quality-aware pipeline that includes LLM-as-a-judge validation, executability checks, automatic repair, and column balancing. We further release SingSQL-LM, a family of compact language models fine-tuned on the synthetic data, achieving strong in-domain generalization. On the subset of the BIRD benchmark, SingSQL-LM-3B-R64 reaches 82.87% Soft F1 and 73.03% EX upper bound with 32 candidates, outperforming the best 3B-scale baseline by +16.21 in Soft F1 and +12.36 in EX. At the 1.5B scale, SingSQL-LM-1.5B-R64 improves over prior systems by +9.30 in Soft F1 and +4.49 in EX. On synthetic evaluation sets, SingSQL-LMs exceed prior systems by wide margins, establishing state-of-the-art performance among open models at comparable scales. Our study of context management strategies reveals that schema-free fine-tuning combined with schema-only inference provides the most robust results. These findings establish SING-SQL as a scalable, database-agnostic paradigm for producing and evaluating enterprise-grade Text-to-SQL systems.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2509.25158.pdf' target='_blank'>https://arxiv.org/pdf/2509.25158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehimare Okoyomon, Arbel Yaniv, Christoph Goebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25158">Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Voltage prediction in distribution grids is a critical yet difficult task for maintaining power system stability. Machine learning approaches, particularly Graph Neural Networks (GNNs), offer significant speedups but suffer from poor generalization when trained on limited or incomplete data. In this work, we systematically investigate the role of inductive biases in improving a model's ability to reliably learn power flow. Specifically, we evaluate three physics-informed strategies: (i) power-flow-constrained loss functions, (ii) complex-valued neural networks, and (iii) residual-based task reformulation. Using the ENGAGE dataset, which spans multiple low- and medium-voltage grid configurations, we conduct controlled experiments to isolate the effect of each inductive bias and assess both standard predictive performance and out-of-distribution generalization. Our study provides practical insights into which model assumptions most effectively guide learning for reliable and efficient voltage prediction in modern distribution networks.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2509.19233.pdf' target='_blank'>https://arxiv.org/pdf/2509.19233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Leyli-abadi, Antoine Marot, JÃ©rÃ´me Picault
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19233">Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of the energy transition, with increasing integration of renewable sources and cross-border electricity exchanges, power grids are encountering greater uncertainty and operational risk. Maintaining grid stability under varying conditions is a complex task, and power flow simulators are commonly used to support operators by evaluating potential actions before implementation. However, traditional physical solvers, while accurate, are often too slow for near real-time use. Machine learning models have emerged as fast surrogates, and to improve their adherence to physical laws (e.g., Kirchhoff's laws), they are often trained with embedded constraints which are also known as physics-informed or hybrid models. This paper presents an ablation study to demystify hybridization strategies, ranging from incorporating physical constraints as regularization terms or unsupervised losses, and exploring model architectures from simple multilayer perceptrons to advanced graph-based networks enabling the direct optimization of physics equations. Using our custom benchmarking pipeline for hybrid models called LIPS, we evaluate these models across four dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization. The results highlight how integrating physical knowledge impacts performance across these criteria. All the implementations are reproducible and provided in the corresponding Github page.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2509.17845.pdf' target='_blank'>https://arxiv.org/pdf/2509.17845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhang, Siming Sun, Zhengyu Fan, Qinmin Yang, Xuejun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17845">Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series analysis faces significant challenges in handling variable-length data and achieving robust generalization. While Transformer-based models have advanced time series tasks, they often struggle with feature redundancy and limited generalization capabilities. Drawing inspiration from classical CNN architectures' pyramidal structure, we propose a Multi-Scale Representation Learning Framework based on a Conv-like ScaleFusion Transformer. Our approach introduces a temporal convolution-like structure that combines patching operations with multi-head attention, enabling progressive temporal dimension compression and feature channel expansion. We further develop a novel cross-scale attention mechanism for effective feature fusion across different temporal scales, along with a log-space normalization method for variable-length sequences. Extensive experiments demonstrate that our framework achieves superior feature independence, reduced redundancy, and better performance in forecasting and classification tasks compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2509.16531.pdf' target='_blank'>https://arxiv.org/pdf/2509.16531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junghwan Kim, Haotian Zhang, David Jurgens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16531">Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Authorship representation (AR) learning, which models an author's unique writing style, has demonstrated strong performance in authorship attribution tasks. However, prior research has primarily focused on monolingual settings-mostly in English-leaving the potential benefits of multilingual AR models underexplored. We introduce a novel method for multilingual AR learning that incorporates two key innovations: probabilistic content masking, which encourages the model to focus on stylistically indicative words rather than content-specific words, and language-aware batching, which improves contrastive learning by reducing cross-lingual interference. Our model is trained on over 4.5 million authors across 36 languages and 13 domains. It consistently outperforms monolingual baselines in 21 out of 22 non-English languages, achieving an average Recall@8 improvement of 4.85%, with a maximum gain of 15.91% in a single language. Furthermore, it exhibits stronger cross-lingual and cross-domain generalization compared to a monolingual model trained solely on English. Our analysis confirms the effectiveness of both proposed techniques, highlighting their critical roles in the model's improved performance.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2509.06464.pdf' target='_blank'>https://arxiv.org/pdf/2509.06464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erez Posner, Ore Shtalrid, Oded Erell, Daniel Noy, Moshe Bouhnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06464">A Statistical 3D Stomach Shape Model for Anatomical Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic and parameterized 3D models of human anatomy have become invaluable in research, diagnostics, and surgical planning. However, the development of detailed models for internal organs, such as the stomach, has been limited by data availability and methodological challenges. In this paper, we propose a novel pipeline for the generation of synthetic 3D stomach models, enabling the creation of anatomically diverse morphologies informed by established studies on stomach shape variability. Using this pipeline, we construct a dataset of synthetic stomachs. Building on this dataset, we develop a 3D statistical shape model of the stomach, trained to capture natural anatomical variability in a low-dimensional shape space. The model is further refined using CT meshes derived from publicly available datasets through a semi-supervised alignment process, enhancing its ability to generalize to unseen anatomical variations. We evaluated the model on a held-out test set of real stomach CT scans, demonstrating robust generalization and fit accuracy. We make the statistical shape model along with the synthetic dataset publicly available on GitLab: https://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research. This work introduces the first statistical 3D shape model of the stomach, with applications ranging from surgical simulation and pre-operative planning to medical education and computational modeling. By combining synthetic data generation, parametric modeling, and real-world validation, our approach represents a significant advancement in organ modeling and opens new possibilities for personalized healthcare solutions.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2509.06011.pdf' target='_blank'>https://arxiv.org/pdf/2509.06011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhai Weng, Xinjie Li, Can Wu, Weijie He, Jianfeng Lv, Dong Zhou, Zhongliang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06011">Light-Weight Cross-Modal Enhancement Method with Benchmark Construction for UAV-based Open-Vocabulary Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Object Detection (OVD) faces severe performance degradation when applied to UAV imagery due to the domain gap from ground-level datasets. To address this challenge, we propose a complete UAV-oriented solution that combines both dataset construction and model innovation. First, we design a refined UAV-Label Engine, which efficiently resolves annotation redundancy, inconsistency, and ambiguity, enabling the generation of largescale UAV datasets. Based on this engine, we construct two new benchmarks: UAVDE-2M, with over 2.4M instances across 1,800+ categories, and UAVCAP-15K, providing rich image-text pairs for vision-language pretraining. Second, we introduce the Cross-Attention Gated Enhancement (CAGE) module, a lightweight dual-path fusion design that integrates cross-attention, adaptive gating, and global FiLM modulation for robust textvision alignment. By embedding CAGE into the YOLO-World-v2 framework, our method achieves significant gains in both accuracy and efficiency, notably improving zero-shot detection on VisDrone by +5.3 mAP while reducing parameters and GFLOPs, and demonstrating strong cross-domain generalization on SIMD. Extensive experiments and real-world UAV deployment confirm the effectiveness and practicality of our proposed solution for UAV-based OVD
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2509.02585.pdf' target='_blank'>https://arxiv.org/pdf/2509.02585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoyan Shen, Esther BÃ¤r, Maria Hawkins, Konstantin BrÃ¤utigam, Charles-Antoine Collins-Fekete
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02585">Pan-Cancer mitotic figures detection and domain generalization: MIDOG 2025 Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report details our submission to the Mitotic Domain Generalization (MIDOG) 2025 challenge, which addresses the critical task of mitotic figure detection in histopathology for cancer prognostication. Following the "Bitter Lesson"\cite{sutton2019bitterlesson} principle that emphasizes data scale over algorithmic novelty, we have publicly released two new datasets to bolster training data for both conventional \cite{Shen2024framework} and atypical mitoses \cite{shen_2025_16780587}. Besides, we implement up-to-date training methodologies for both track and reach a Track-1 F1-Score of 0.8407 on our test set, as well as a Track-2 balanced accuracy of 0.9107 for atypical mitotic cell classification.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2509.01299.pdf' target='_blank'>https://arxiv.org/pdf/2509.01299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Ni, Qingshan Liu, Xiaonan Niu, Danfeng Hong, Lingli Zhao, Haiyan Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01299">Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations over Time Intervals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-domain few-shot segmentation (CD-FSS) not only enables the segmentation of unseen categories with very limited samples, but also improves cross-domain generalization ability within the few-shot segmentation framework. Currently, existing CD-FSS studies typically design multiple independent modules to enhance the cross-domain generalization ability of feature representations. However, the independence among these modules hinders the effective flow of knowledge, making it difficult to fully leverage their collective potential. In contrast, this paper proposes an all-in-one module based on ordinary differential equations and Fourier transform, resulting in a structurally concise method--Few-Shot Segmentation over Time Intervals (FSS-TIs). FSS-TIs assumes the existence of an ODE relationship between the spectra (including amplitude and phase spectra) of domain-specific features and domain-agnostic features. This ODE formulation yields an iterative transformation process along a sequence of time intervals, while simultaneously applying affine transformations with randomized perturbations to the spectra. In doing so, the exploration of domain-agnostic feature representation spaces and the simulation of diverse potential target-domain distributions are reformulated as an optimization process over the intrinsic parameters of the ODE. Moreover, we strictly constrain the support-sample selection during target-domain fine-tuning so that it is consistent with the requirements of real-world few-shot segmentation tasks. For evaluation, we introduce five datasets from substantially different domains and define two sets of cross-domain few-shot segmentation tasks to comprehensively analyze the performance of FSS-TIs. Experimental results demonstrate the superiority of FSS-TIs over existing CD-FSS methods, and in-depth ablation studies further validate the cross-domain adaptability of FSS-TIs.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2508.21730.pdf' target='_blank'>https://arxiv.org/pdf/2508.21730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrizio Fagiolo, Nicolo' Vescera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21730">Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we present a variational algorithm for the Traveling Salesman Problem (TSP) that combines (i) a compact encoding of permutations, which reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy: where the circuit topology (``Ansatz'') is first optimized on a training instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel instances, limited to a rapid re-optimization of only the circuit parameters. This pipeline eliminates costly structural research in testing, making the procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$ cities, the resulting Ansatz achieves an average optimal trip sampling probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for 6 city cases. With 7 cities the success rate drops markedly to an average of $\sim 20\%$, revealing the onset of scalability limitations of the proposed method.
  The results show robust generalization ability for moderate problem sizes and indicate how freezing the Ansatz can dramatically reduce time-to-solution without degrading solution quality. The paper also discusses scalability limitations, the impact of ``warm-start'' initialization of parameters, and prospects for extension to more complex problems, such as Vehicle Routing and Job-Shop Scheduling.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2508.18859.pdf' target='_blank'>https://arxiv.org/pdf/2508.18859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Kashif Ali, Eun Woo Im, Dongjin Kim, Tae Hyun Kim, Vivek Gupta, Haonan Luo, Tianrui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18859">Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video stabilization remains a fundamental problem in computer vision, particularly pixel-level synthesis solutions for video stabilization, which synthesize full-frame outputs, add to the complexity of this task. These methods aim to enhance stability while synthesizing full-frame videos, but the inherent diversity in motion profiles and visual content present in each video sequence makes robust generalization with fixed parameters difficult. To address this, we present a novel method that improves pixel-level synthesis video stabilization methods by rapidly adapting models to each input video at test time. The proposed approach takes advantage of low-level visual cues available during inference to improve both the stability and visual quality of the output. Notably, the proposed rapid adaptation achieves significant performance gains even with a single adaptation pass. We further propose a jerk localization module and a targeted adaptation strategy, which focuses the adaptation on high-jerk segments for maximizing stability with fewer adaptation steps. The proposed methodology enables modern stabilizers to overcome the longstanding SOTA approaches while maintaining the full frame nature of the modern methods, while offering users with control mechanisms akin to classical approaches. Extensive experiments on diverse real-world datasets demonstrate the versatility of the proposed method. Our approach consistently improves the performance of various full-frame synthesis models in both qualitative and quantitative terms, including results on downstream applications.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2508.18749.pdf' target='_blank'>https://arxiv.org/pdf/2508.18749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunlong Wu, Zhibo Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18749">Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in prompt optimization, exemplified by methods such as TextGrad, enable automatic, gradient-like refinement of textual prompts to enhance the performance of large language models (LLMs) on specific downstream tasks. However, current approaches are typically stateless and operate independently across optimization runs, lacking mechanisms to preserve and leverage historical optimization experience. Furthermore, they are susceptible to overfitting, often yielding prompt updates that generalize poorly beyond the immediate task context.
  To address these limitations, we propose Reflection-Enhanced Meta-Optimization (REMO), a novel framework that integrates (1) a memory-augmented Reflection Retrieval-Augmented Generation (RAG) module - structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer, implemented via an LLM-driven meta-controller that synthesizes epoch-level reflective insights to iteratively improve system-level prompting strategies. This architecture enables not only local, fine-grained prompt tuning akin to TextGrad, but also the systematic accumulation and reuse of cross-run optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode - without explicit chain-of-thought prompting - and evaluate its efficacy on the GSM8K benchmark for mathematical reasoning. Experimental results demonstrate that, compared to a TextGrad baseline, REMO achieves more stable and robust generalization, albeit at the cost of increased computational overhead. We provide a detailed exposition of the algorithmic design, conduct a qualitative and quantitative analysis of optimization dynamics, and present a comprehensive ablation study to elucidate the contributions of each component.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2508.10351.pdf' target='_blank'>https://arxiv.org/pdf/2508.10351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhentai Zhang, Danyi Weng, Guibin Zhang, Xiang Chen, Kaixing Long, Jian Geng, Yanmeng Lu, Lei Zhang, Zhitao Zhou, Lei Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10351">Glo-UMF: A Unified Multi-model Framework for Automated Morphometry of Glomerular Ultrastructural Characterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background and Objective: To address the inability of single-model architectures to perform simultaneous analysis of complex glomerular ultrastructures, we developed Glo-UMF, a unified multi-model framework integrating segmentation, classification, and detection to systematically quantify key ultrastructural features. Methods: Glo-UMF decouples quantification tasks by constructing three dedicated deep models: an ultrastructure segmentation model, a glomerular filtration barrier (GFB) region classification model, and an electron-dense deposits (EDD) detection model. Their outputs are integrated through a post-processing workflow with adaptive GFB cropping and measurement location screening, enhancing measurement reliability and providing comprehensive quantitative results that overcome the limitations of traditional grading. Results: Trained on 372 electron microscopy images, Glo-UMF enables simultaneous quantification of glomerular basement membrane (GBM) thickness, the degree of foot process effacement (FPE), and EDD location. In 115 test cases spanning 9 renal pathological types, the automated quantification results showed strong agreement with pathological reports, with an average processing time of 4.23$\pm$0.48 seconds per case on a CPU environment. Conclusions: The modular design of Glo-UMF allows for flexible extensibility, supporting the joint quantification of multiple features. This framework ensures robust generalization and clinical applicability, demonstrating significant potential as an efficient auxiliary tool in glomerular pathological analysis.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2507.13207.pdf' target='_blank'>https://arxiv.org/pdf/2507.13207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Etienne Le Naour, Tahar Nabil, Ghislain Agoua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13207">MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a growing interest for time series foundation models, with a strong emphasis on the forecasting task. Yet, the crucial task of out-of-domain imputation of missing values remains largely underexplored. We propose a first step to fill this gap by leveraging implicit neural representations (INRs). INRs model time series as continuous functions and naturally handle various missing data scenarios and sampling rates. While they have shown strong performance within specific distributions, they struggle under distribution shifts. To address this, we introduce MoTM (Mixture of Timeflow Models), a step toward a foundation model for time series imputation. Building on the idea that a new time series is a mixture of previously seen patterns, MoTM combines a basis of INRs, each trained independently on a distinct family of time series, with a ridge regressor that adapts to the observed context at inference. We demonstrate robust in-domain and out-of-domain generalization across diverse imputation scenarios (e.g., block and pointwise missingness, variable sampling rates), paving the way for adaptable foundation imputation models.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2505.24067.pdf' target='_blank'>https://arxiv.org/pdf/2505.24067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu He, Ellen Vitercik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24067">Primal-Dual Neural Algorithmic Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Algorithmic Reasoning (NAR) trains neural networks to simulate classical algorithms, enabling structured and interpretable reasoning over complex data. While prior research has predominantly focused on learning exact algorithms for polynomial-time-solvable problems, extending NAR to harder problems remains an open challenge. In this work, we introduce a general NAR framework grounded in the primal-dual paradigm, a classical method for designing efficient approximation algorithms. By leveraging a bipartite representation between primal and dual variables, we establish an alignment between primal-dual algorithms and Graph Neural Networks. Furthermore, we incorporate optimal solutions from small instances to greatly enhance the model's reasoning capabilities. Our empirical results demonstrate that our model not only simulates but also outperforms approximation algorithms for multiple tasks, exhibiting robust generalization to larger and out-of-distribution graphs. Moreover, we highlight the framework's practical utility by integrating it with commercial solvers and applying it to real-world datasets.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2505.22434.pdf' target='_blank'>https://arxiv.org/pdf/2505.22434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zobia Batool, Huseyin Ozkan, Erchan Aptoula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22434">Distance Transform Guided Mixup for Alzheimer's Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Alzheimer's detection efforts aim to develop accurate models for early disease diagnosis. Significant advances have been achieved with convolutional neural networks and vision transformer based approaches. However, medical datasets suffer heavily from class imbalance, variations in imaging protocols, and limited dataset diversity, which hinder model generalization. To overcome these challenges, this study focuses on single-domain generalization by extending the well-known mixup method. The key idea is to compute the distance transform of MRI scans, separate them spatially into multiple layers and then combine layers stemming from distinct samples to produce augmented images. The proposed approach generates diverse data while preserving the brain's structure. Experimental results show generalization performance improvement across both ADNI and AIBL datasets.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2505.21581.pdf' target='_blank'>https://arxiv.org/pdf/2505.21581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhennan Wang, Jianing Teng, Canqun Xiang, Kangliang Chen, Xing Pan, Lu Deng, Weihao Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21581">CogAD: Cognitive-Hierarchy Guided End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While end-to-end autonomous driving has advanced significantly, prevailing methods remain fundamentally misaligned with human cognitive principles in both perception and planning. In this paper, we propose CogAD, a novel end-to-end autonomous driving model that emulates the hierarchical cognition mechanisms of human drivers. CogAD implements dual hierarchical mechanisms: global-to-local context processing for human-like perception and intent-conditioned multi-mode trajectory generation for cognitively-inspired planning. The proposed method demonstrates three principal advantages: comprehensive environmental understanding through hierarchical perception, robust planning exploration enabled by multi-level planning, and diverse yet reasonable multi-modal trajectory generation facilitated by dual-level uncertainty modeling. Extensive experiments on nuScenes and Bench2Drive demonstrate that CogAD achieves state-of-the-art performance in end-to-end planning, exhibiting particular superiority in long-tail scenarios and robust generalization to complex real-world driving conditions.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2505.21156.pdf' target='_blank'>https://arxiv.org/pdf/2505.21156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saisamarth Rajesh Phaye, Milos Cernak, Andrew Harper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21156">Model as Loss: A Self-Consistent Training Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional methods for speech enhancement rely on handcrafted loss functions (e.g., time or frequency domain losses) or deep feature losses (e.g., using WavLM or wav2vec), which often fail to capture subtle signal properties essential for optimal performance. To address this, we propose Model as Loss, a novel training paradigm that utilizes the encoder from the same model as a loss function to guide the training.
  The Model as Loss paradigm leverages the encoder's task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal. By using the encoder's learned features as a loss function, this framework enforces self-consistency between the clean reference speech and the enhanced model output. Our approach outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2505.16253.pdf' target='_blank'>https://arxiv.org/pdf/2505.16253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Preeti Mehta, Aman Sagar, Suchi Kumari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16253">Swin Transformer for Robust CGI Images Detection: Intra- and Inter-Dataset Analysis across Multiple Color Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study aims to address the growing challenge of distinguishing computer-generated imagery (CGI) from authentic digital images across three different color spaces; RGB, YCbCr, and HSV. Given the limitations of existing classification methods in handling the complexity and variability of CGI, this research proposes a Swin Transformer based model for accurate differentiation between natural and synthetic images. The proposed model leverages the Swin Transformer's hierarchical architecture to capture local and global features for distinguishing CGI from natural images. Its performance was assessed through intra- and inter-dataset testing across three datasets: CiFAKE, JSSSTU, and Columbia. The model was evaluated individually on each dataset (D1, D2, D3) and on the combined datasets (D1+D2+D3) to test its robustness and domain generalization. To address dataset imbalance, data augmentation techniques were applied. Additionally, t-SNE visualization was used to demonstrate the feature separability achieved by the Swin Transformer across the selected color spaces. The model's performance was tested across all color schemes, with the RGB color scheme yielding the highest accuracy for each dataset. As a result, RGB was selected for domain generalization analysis and compared with other CNN-based models, VGG-19 and ResNet-50. The comparative results demonstrate the proposed model's effectiveness in detecting CGI, highlighting its robustness and reliability in both intra-dataset and inter-dataset evaluations. The findings of this study highlight the Swin Transformer model's potential as an advanced tool for digital image forensics, particularly in distinguishing CGI from natural images. The model's strong performance indicates its capability for domain generalization, making it a valuable asset in scenarios requiring precise and reliable image classification.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2505.15422.pdf' target='_blank'>https://arxiv.org/pdf/2505.15422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nudrat Habib, Tosin Adewumi, Marcus Liwicki, Elisa Barney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15422">Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Authorship analysis plays an important role in diverse domains, including forensic linguistics, academia, cybersecurity, and digital content authentication. This paper presents a systematic literature review on two key sub-tasks of authorship analysis; Author Attribution and Author Verification. The review explores SOTA methodologies, ranging from traditional ML approaches to DL models and LLMs, highlighting their evolution, strengths, and limitations, based on studies conducted from 2015 to 2024. Key contributions include a comprehensive analysis of methods, techniques, their corresponding feature extraction techniques, datasets used, and emerging challenges in authorship analysis. The study highlights critical research gaps, particularly in low-resource language processing, multilingual adaptation, cross-domain generalization, and AI-generated text detection. This review aims to help researchers by giving an overview of the latest trends and challenges in authorship analysis. It also points out possible areas for future study. The goal is to support the development of better, more reliable, and accurate authorship analysis system in diverse textual domain.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2505.08159.pdf' target='_blank'>https://arxiv.org/pdf/2505.08159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxiang Li, Junwei Feng, Jie Luo, Bowen Jiang, Xiangyu Zheng, Qigang Song, Jian Lv, Keith Butler, Hanyu Liu, Congwei Xie, Yu Xie, Yanming Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08159">Self-Optimizing Machine Learning Potential Assisted Automated Workflow for Highly Efficient Complex Systems Material Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning interatomic potentials have revolutionized complex materials design by enabling rapid exploration of material configurational spaces via crystal structure prediction with ab initio accuracy. However, critical challenges persist in ensuring robust generalization to unknown structures and minimizing the requirement for substantial expert knowledge and time-consuming manual interventions. Here, we propose an automated crystal structure prediction framework built upon the attention-coupled neural networks potential to address these limitations. The generalizability of the potential is achieved by sampling regions across the local minima of the potential energy surface, where the self-evolving pipeline autonomously refines the potential iteratively while minimizing human intervention. The workflow is validated on Mg-Ca-H ternary and Be-P-N-O quaternary systems by exploring nearly 10 million configurations, demonstrating substantial speedup compared to first-principles calculations. These results underscore the effectiveness of our approach in accelerating the exploration and discovery of complex multi-component functional materials.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2504.20498.pdf' target='_blank'>https://arxiv.org/pdf/2504.20498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhong Han, Yupei Wang, Liang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20498">Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-source domain generalization (SDG) in object detection aims to develop a detector using only source domain data that generalizes well to unseen target domains. Existing methods are primarily CNN-based and improve robustness through data augmentation combined with feature alignment. However, these methods are limited, as augmentation is only effective when the synthetic distribution approximates that of unseen domains, thus failing to ensure generalization across diverse scenarios.
  While DEtection TRansformer (DETR) has shown strong generalization in domain adaptation due to global context modeling, its potential for SDG remains underexplored. To this end, we propose Style-Adaptive DEtection TRansformer (SA-DETR), a DETR-based detector tailored for SDG. SA-DETR introduces an online domain style adapter that projects the style representation of unseen domains into the source domain via a dynamic memory bank. This bank self-organizes into diverse style prototypes and is continuously updated under a test-time adaptation framework, enabling effective style rectification.
  Additionally, we design an object-aware contrastive learning module to promote extraction of domain-invariant features. By applying gating masks that constrain contrastive learning in both spatial and semantic dimensions, this module facilitates instance-level cross-domain contrast and enhances generalization.
  Extensive experiments across five distinct weather scenarios demonstrate that SA-DETR consistently outperforms existing methods in both detection accuracy and domain generalization capability.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2504.16972.pdf' target='_blank'>https://arxiv.org/pdf/2504.16972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Ahmadi, Sajjad Emdadi Mahdimahalleh, Arman Farahat, Banafsheh Saffari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16972">Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of unlabeled time-series data in domains such as wireless communications, radar, biomedical engineering, and the Internet of Things (IoT) has driven advancements in unsupervised learning. This review synthesizes recent progress in applying autoencoders and vision transformers for unsupervised signal analysis, focusing on their architectures, applications, and emerging trends. We explore how these models enable feature extraction, anomaly detection, and classification across diverse signal types, including electrocardiograms, radar waveforms, and IoT sensor data. The review highlights the strengths of hybrid architectures and self-supervised learning, while identifying challenges in interpretability, scalability, and domain generalization. By bridging methodological innovations and practical applications, this work offers a roadmap for developing robust, adaptive models for signal intelligence.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2504.14707.pdf' target='_blank'>https://arxiv.org/pdf/2504.14707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ratna Kandala, Katie Hoemann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14707">Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores BERTopic's potential for modeling open-ended Belgian Dutch daily narratives, contrasting its performance with Latent Dirichlet Allocation (LDA) and KMeans. Although LDA scores well on certain automated metrics, human evaluations reveal semantically irrelevant co-occurrences, highlighting the limitations of purely statistic-based methods. In contrast, BERTopic's reliance on contextual embeddings yields culturally resonant themes, underscoring the importance of hybrid evaluation frameworks that account for morphologically rich languages. KMeans performed less coherently than prior research suggested, pointing to the unique challenges posed by personal narratives. Our findings emphasize the need for robust generalization in NLP models, especially in underrepresented linguistic contexts.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2504.14373.pdf' target='_blank'>https://arxiv.org/pdf/2504.14373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Guo, Zhuo Su, Jian Wang, Shuang Li, Xu Chang, Zhaohu Li, Yang Zhao, Guidong Wang, Ruqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14373">SEGA: Drivable 3D Gaussian Head Avatar from a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2504.10001.pdf' target='_blank'>https://arxiv.org/pdf/2504.10001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlin Hao, Peiheng Wang, Haoyang Wang, Xinggong Zhang, Zongming Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10001">GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2504.03064.pdf' target='_blank'>https://arxiv.org/pdf/2504.03064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yan, Yuhong Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03064">Context-Aware Self-Adaptation for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims at developing suitable learning algorithms in source training domains such that the model learned can generalize well on a different unseen testing domain. We present a novel two-stage approach called Context-Aware Self-Adaptation (CASA) for domain generalization. CASA simulates an approximate meta-generalization scenario and incorporates a self-adaptation module to adjust pre-trained meta source models to the meta-target domains while maintaining their predictive capability on the meta-source domains. The core concept of self-adaptation involves leveraging contextual information, such as the mean of mini-batch features, as domain knowledge to automatically adapt a model trained in the first stage to new contexts in the second stage. Lastly, we utilize an ensemble of multiple meta-source models to perform inference on the testing domain. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on standard benchmarks.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2503.23612.pdf' target='_blank'>https://arxiv.org/pdf/2503.23612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Belkadi, Steve Hong, Marian Chen, Miruna Cretu, Charles Harris, Pietro Lio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23612">Diffusion-Free Graph Generation with Next-Scale Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive models excel in efficiency and plug directly into the transformer ecosystem, delivering robust generalization, predictable scalability, and seamless workflows such as fine-tuning and parallelized training. However, they require an explicit sequence order, which contradicts the unordered nature of graphs. In contrast, diffusion models maintain permutation invariance and enable one-shot generation but require up to thousands of denoising steps and additional features for expressivity, leading to high computational costs. Inspired by recent breakthroughs in image generation, especially the success of visual autoregressive methods, we propose MAG, a novel diffusion-free graph generation framework based on next-scale prediction. By leveraging a hierarchy of latent representations, the model progressively generates scales of the entire graph without the need for explicit node ordering. Experiments on both generic and molecular graph datasets demonstrated the potential of this method, achieving inference speedups of up to three orders of magnitude over state-of-the-art methods, while preserving high-quality generation.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2503.22862.pdf' target='_blank'>https://arxiv.org/pdf/2503.22862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumitri Chattopadhyay, Basar Demir, Marc Niethammer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22862">Zero-shot Domain Generalization of Foundational Models for 3D Medical Image Segmentation: An Experimental Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain shift, caused by variations in imaging modalities and acquisition protocols, limits model generalization in medical image segmentation. While foundation models (FMs) trained on diverse large-scale data hold promise for zero-shot generalization, their application to volumetric medical data remains underexplored. In this study, we examine their ability towards domain generalization (DG), by conducting a comprehensive experimental study encompassing 6 medical segmentation FMs and 12 public datasets spanning multiple modalities and anatomies. Our findings reveal the potential of promptable FMs in bridging the domain gap via smart prompting techniques. Additionally, by probing into multiple facets of zero-shot DG, we offer valuable insights into the viability of FMs for DG and identify promising avenues for future research.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2503.18567.pdf' target='_blank'>https://arxiv.org/pdf/2503.18567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biwen Meng, Xi Long, Wanrong Yang, Ruochen Liu, Yi Tian, Yalin Zheng, Jingxin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18567">Advancing Cross-Organ Domain Generalization with Test-Time Style Transfer and Diversity Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has made significant progress in addressing challenges in various fields including computational pathology (CPath). However, due to the complexity of the domain shift problem, the performance of existing models will degrade, especially when it comes to multi-domain or cross-domain tasks. In this paper, we propose a Test-time style transfer (T3s) that uses a bidirectional mapping mechanism to project the features of the source and target domains into a unified feature space, enhancing the generalization ability of the model. To further increase the style expression space, we introduce a Cross-domain style diversification module (CSDM) to ensure the orthogonality between style bases. In addition, data augmentation and low-rank adaptation techniques are used to improve feature alignment and sensitivity, enabling the model to adapt to multi-domain inputs effectively. Our method has demonstrated effectiveness on three unseen datasets.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2503.11774.pdf' target='_blank'>https://arxiv.org/pdf/2503.11774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Lian, Shangyu Li, Qixuan Huang, Zijian Huang, Haifei Liu, Jianan Qiu, Puyu Yang, Laifa Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11774">UBMF: Uncertainty-Aware Bayesian Meta-Learning Framework for Fault Diagnosis with Imbalanced Industrial Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fault diagnosis of mechanical equipment involves data collection, feature extraction, and pattern recognition but is often hindered by the imbalanced nature of industrial data, introducing significant uncertainty and reducing diagnostic reliability. To address these challenges, this study proposes the Uncertainty-Aware Bayesian Meta-Learning Framework (UBMF), which integrates four key modules: data perturbation injection for enhancing feature robustness, cross-task self-supervised feature extraction for improving transferability, uncertainty-based sample filtering for robust out-of-domain generalization, and Bayesian meta-knowledge integration for fine-grained classification. Experimental results on ten open-source datasets under various imbalanced conditions, including cross-task, small-sample, and unseen-sample scenarios, demonstrate the superiority of UBMF, achieving an average improvement of 42.22% across ten Any-way 1-5-shot diagnostic tasks. This integrated framework effectively enhances diagnostic accuracy, generalization, and adaptability, providing a reliable solution for complex industrial fault diagnosis.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2503.07906.pdf' target='_blank'>https://arxiv.org/pdf/2503.07906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinghao Ye, Xianhan Zeng, Fu Li, Chunyuan Li, Haoqi Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07906">Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated evaluation metrics and coarse annotations. In this paper, we introduce DeCapBench along with a novel metric, DCScore, specifically designed for detailed captioning tasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by deconstructing responses into the smallest self-sufficient units, termed primitive information units, and assessing them individually. Our evaluation shows that DCScore aligns more closely with human judgment than other rule-based or model-based metrics. Concurrently, DeCapBench exhibits a high correlation with VLM arena results on descriptive tasks, surpassing existing benchmarks for vision-language models. Additionally, we present an automatic fine-grained feedback collection method, FeedQuill, for preference optimization based on our advanced metric, showing robust generalization capabilities across auto-generated preference data. Extensive experiments on multiple VLMs demonstrate that our method not only significantly reduces hallucinations but also enhances performance across various benchmarks, achieving superior detail captioning performance while surpassing GPT-4o.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2503.06698.pdf' target='_blank'>https://arxiv.org/pdf/2503.06698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Thomas, Deepti Ghadiyaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06698">What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2503.04363.pdf' target='_blank'>https://arxiv.org/pdf/2503.04363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni De Felice, Arianna Casanova Flores, Francesco De Santis, Silvia Santini, Johannes Schneider, Pietro Barbiero, Alberto Termine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04363">Causally Reliable Concept Bottleneck Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Concept-based models are an emerging paradigm in deep learning that constrains the inference process to operate through human-interpretable variables, facilitating explainability and human interaction. However, these architectures, on par with popular opaque neural models, fail to account for the true causal mechanisms underlying the target phenomena represented in the data. This hampers their ability to support causal reasoning tasks, limits out-of-distribution generalization, and hinders the implementation of fairness constraints. To overcome these issues, we propose Causally reliable Concept Bottleneck Models (C$^2$BMs), a class of concept-based architectures that enforce reasoning through a bottleneck of concepts structured according to a model of the real-world causal mechanisms. We also introduce a pipeline to automatically learn this structure from observational data and unstructured background knowledge (e.g., scientific literature). Experimental evidence suggests that C$^2$BMs are more interpretable, causally reliable, and improve responsiveness to interventions w.r.t. standard opaque and concept-based models, while maintaining their accuracy.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2502.07951.pdf' target='_blank'>https://arxiv.org/pdf/2502.07951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Tan, Jiacheng Wang, Liansheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07951">Federated Self-supervised Domain Generalization for Label-efficient Polyp Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Employing self-supervised learning (SSL) methodologies assumes par-amount significance in handling unlabeled polyp datasets when building deep learning-based automatic polyp segmentation models. However, the intricate privacy dynamics surrounding medical data often preclude seamless data sharing among disparate medical centers. Federated learning (FL) emerges as a formidable solution to this privacy conundrum, yet within the realm of FL, optimizing model generalization stands as a pressing imperative. Robust generalization capabilities are imperative to ensure the model's efficacy across diverse geographical domains post-training on localized client datasets. In this paper, a Federated self-supervised Domain Generalization method is proposed to enhance the generalization capacity of federated and Label-efficient intestinal polyp segmentation, named LFDG. Based on a classical SSL method, DropPos, LFDG proposes an adversarial learning-based data augmentation method (SSADA) to enhance the data diversity. LFDG further proposes a relaxation module based on Source-reconstruction and Augmentation-masking (SRAM) to maintain stability in feature learning. We have validated LFDG on polyp images from six medical centers. The performance of our method achieves 3.80% and 3.92% better than the baseline and other recent FL methods and SSL methods, respectively.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2502.04034.pdf' target='_blank'>https://arxiv.org/pdf/2502.04034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Song, Yinpu Bai, Hui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04034">Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer Drug Response Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed FourierDrug, to address this challenge. Given the extracted feature from expression profile, we performed Fourier transforms and then introduced an asymmetric attention constraint that would cluster drug-sensitive samples into a compact group while drives resistant samples dispersed in the frequency domain. Our empirical experiments demonstrate that our model effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type. When evaluated on single-cell and patient-level drug response prediction tasks, FourierDrug--trained solely on in vitro cell line data without access to target-domain data--consistently outperforms or, at least, matched the performance of current state-of-the-art methods. These findings underscore the potential of our method for real-world clinical applications.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2501.17888.pdf' target='_blank'>https://arxiv.org/pdf/2501.17888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Chen, Yong Zu, Zhixi Feng, Shuyuan Yang, Mengchang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17888">RadioLLM: Introducing Large Language Model into Cognitive Radio via Hybrid Prompt and Token Reprogrammings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing scarcity of spectrum resources and rapid proliferation of wireless devices make efficient radio network management critical. While deep learning-enhanced Cognitive Radio Technology (CRT) provides promising solutions for tasks such as radio signal classification (RSC), denoising, and spectrum allocation, existing DL-based CRT frameworks are typically task-specific and lack scalability in diverse real-world applications. This limitation naturally leads to the exploration of Large Language Models (LLMs), whose exceptional cross-domain generalization capabilities offer new potential for advancing CRT. To bridge this gap, we propose RadioLLM, a novel framework that integrates Hybrid Prompt and Token Reprogramming (HPTR) for combining radio signal features with expert knowledge, and a Frequency-Attuned Fusion (FAF) module for enhanced high-frequency feature modeling. Extensive evaluations on multiple benchmark datasets demonstrate that RadioLLM achieves superior performance compared to existing baselines in the majority of testing scenarios.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2501.07114.pdf' target='_blank'>https://arxiv.org/pdf/2501.07114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhong Peng, Yishi Xu, Gerong Wang, Wenchao Chen, Bo Chen, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07114">Duplex: Dual Prototype Learning for Compositional Zero-Shot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compositional Zero-Shot Learning (CZSL) aims to enable models to recognize novel compositions of visual states and objects that were absent during training. Existing methods predominantly focus on learning semantic representations of seen compositions but often fail to disentangle the independent features of states and objects in images, thereby limiting their ability to generalize to unseen compositions. To address this challenge, we propose Duplex, a novel dual-prototype learning method that integrates semantic and visual prototypes through a carefully designed dual-branch architecture, enabling effective representation learning for compositional tasks. Duplex utilizes a Graph Neural Network (GNN) to adaptively update visual prototypes, capturing complex interactions between states and objects. Additionally, it leverages the strong visual-semantic alignment of pre-trained Vision-Language Models (VLMs) and employs a multi-path architecture combined with prompt engineering to align image and text representations, ensuring robust generalization. Extensive experiments on three benchmark datasets demonstrate that Duplex outperforms state-of-the-art methods in both closed-world and open-world settings.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2412.12469.pdf' target='_blank'>https://arxiv.org/pdf/2412.12469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingquan Feng, Zhijie Chen, Yixin Huang, Yizhou Liu, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12469">Optimal Control Operator Perspective and a Neural Adaptive Spectral Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optimal control problems (OCPs) involve finding a control function for a dynamical system such that a cost functional is optimized. It is central to physical systems in both academia and industry. In this paper, we propose a novel instance-solution control operator perspective, which solves OCPs in a one-shot manner without direct dependence on the explicit expression of dynamics or iterative optimization processes. The control operator is implemented by a new neural operator architecture named Neural Adaptive Spectral Method (NASM), a generalization of classical spectral methods. We theoretically validate the perspective and architecture by presenting the approximation error bounds of NASM for the control operator. Experiments on synthetic environments and a real-world dataset verify the effectiveness and efficiency of our approach, including substantial speedup in running time, and high-quality in- and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2412.10031.pdf' target='_blank'>https://arxiv.org/pdf/2412.10031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jizhihui Liu, Qixun Teng, Qing Ma, Junjun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10031">FM2S: Towards Spatially-Correlated Noise Modeling in Zero-Shot Fluorescence Microscopy Image Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fluorescence microscopy image (FMI) denoising faces critical challenges due to the compound mixed Poisson-Gaussian noise with strong spatial correlation and the impracticality of acquiring paired noisy/clean data in dynamic biomedical scenarios. While supervised methods trained on synthetic noise (e.g., Gaussian/Poisson) suffer from out-of-distribution generalization issues, existing self-supervised approaches degrade under real FMI noise due to oversimplified noise assumptions and computationally intensive deep architectures. In this paper, we propose Fluorescence Micrograph to Self (FM2S), a zero-shot denoiser that achieves efficient FMI denoising through three key innovations: 1) A noise injection module that ensures training data sufficiency through adaptive Poisson-Gaussian synthesis while preserving spatial correlation and global statistics of FMI noise for robust model generalization; 2) A two-stage progressive learning strategy that first recovers structural priors via pre-denoised targets then refines high-frequency details through noise distribution alignment; 3) An ultra-lightweight network (3.5k parameters) enabling rapid convergence with 270$\times$ faster training and inference than SOTAs. Extensive experiments across FMI datasets demonstrate FM2S's superiority: It outperforms CVF-SID by 1.4dB PSNR on average while requiring 0.1% parameters of AP-BSN. Notably, FM2S maintains stable performance across varying noise levels, proving its practicality for microscopy platforms with diverse sensor characteristics. Code and datasets will be released.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2412.02980.pdf' target='_blank'>https://arxiv.org/pdf/2412.02980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Havrilla, Andrew Dai, Laura O'Mahony, Koen Oostermeijer, Vera Zisler, Alon Albalak, Fabrizio Milo, Sharath Chandra Raparthy, Kanishk Gandhi, Baber Abbasi, Duy Phung, Maia Iyer, Dakota Mahan, Chase Blagden, Srishti Gureja, Mohammed Hamdy, Wen-Ding Li, Giovanni Paolini, Pawan Sasanka Ammanamanchi, Elliot Meyerson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02980">Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2410.15766.pdf' target='_blank'>https://arxiv.org/pdf/2410.15766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Ulmer, Leonard KlÃ¼pfel, Maximilian Durner, Rudolph Triebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15766">How Important are Data Augmentations to Close the Domain Gap for Object Detection in Orbit?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the efficacy of data augmentations to close the domain gap in spaceborne computer vision, crucial for autonomous operations like on-orbit servicing. As the use of computer vision in space increases, challenges such as hostile illumination and low signal-to-noise ratios significantly hinder performance. While learning-based algorithms show promising results, their adoption is limited by the need for extensive annotated training data and the domain gap that arises from differences between synthesized and real-world imagery. This study explores domain generalization in terms of data augmentations -- classical color and geometric transformations, corruptions, and noise -- to enhance model performance across the domain gap. To this end, we conduct an large scale experiment using a hyperparameter optimization pipeline that samples hundreds of different configurations and searches for the best set to bridge the domain gap. As a reference task, we use 2D object detection and evaluate on the SPEED+ dataset that contains real hardware-in-the-loop satellite images in its test set. Moreover, we evaluate four popular object detectors, including Mask R-CNN, Faster R-CNN, YOLO-v7, and the open set detector GroundingDINO, and highlight their trade-offs between performance, inference speed, and training time. Our results underscore the vital role of data augmentations in bridging the domain gap, improving model performance, robustness, and reliability for critical space applications. As a result, we propose two novel data augmentations specifically developed to emulate the visual effects observed in orbital imagery. We conclude by recommending the most effective augmentations for advancing computer vision in challenging orbital environments. Code for training detectors and hyperparameter search will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2410.05980.pdf' target='_blank'>https://arxiv.org/pdf/2410.05980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Loukas, Karolis Martinkus, Ed Wagstaff, Kyunghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05980">Generalizing to any diverse distribution: uniformity, gentle finetuning and rebalancing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As training datasets grow larger, we aspire to develop models that generalize well to any diverse test distribution, even if the latter deviates significantly from the training data. Various approaches like domain adaptation, domain generalization, and robust optimization attempt to address the out-of-distribution challenge by posing assumptions about the relation between training and test distribution. Differently, we adopt a more conservative perspective by accounting for the worst-case error across all sufficiently diverse test distributions within a known domain. Our first finding is that training on a uniform distribution over this domain is optimal. We also interrogate practical remedies when uniform samples are unavailable by considering methods for mitigating non-uniformity through finetuning and rebalancing. Our theory provides a mathematical grounding for previous observations on the role of entropy and rebalancing for o.o.d. generalization and foundation model training. We also provide new empirical evidence across tasks involving o.o.d. shifts which illustrate the broad applicability of our perspective.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2409.18592.pdf' target='_blank'>https://arxiv.org/pdf/2409.18592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Uecker, J. Marius ZÃ¶llner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18592">From One to the Power of Many: Invariance to Multi-LiDAR Perception from Single-Sensor Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, LiDAR segmentation methods for autonomous vehicles, powered by deep neural networks, have experienced steep growth in performance on classic benchmarks, such as nuScenes and SemanticKITTI. However, there are still large gaps in performance when deploying models trained on such single-sensor setups to modern vehicles with multiple high-resolution LiDAR sensors. In this work, we introduce a new metric for feature-level invariance which can serve as a proxy to measure cross-domain generalization without requiring labeled data. Additionally, we propose two application-specific data augmentations, which facilitate better transfer to multi-sensor LiDAR setups, when trained on single-sensor datasets. We provide experimental evidence on both simulated and real data, that our proposed augmentations improve invariance across LiDAR setups, leading to improved generalization.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2409.18529.pdf' target='_blank'>https://arxiv.org/pdf/2409.18529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Rackow, Nikolay Koldunov, Christian Lessig, Irina Sandu, Mihai Alexe, Matthew Chantry, Mariana Clare, Jesper Dramsch, Florian Pappenberger, Xabier Pedruzo-Bagazgoitia, Steffen Tietsche, Thomas Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18529">Robustness of AI-based weather forecasts in a changing climate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven machine learning models for weather forecasting have made transformational progress in the last 1-2 years, with state-of-the-art ones now outperforming the best physics-based models for a wide range of skill scores. Given the strong links between weather and climate modelling, this raises the question whether machine learning models could also revolutionize climate science, for example by informing mitigation and adaptation to climate change or to generate larger ensembles for more robust uncertainty estimates. Here, we show that current state-of-the-art machine learning models trained for weather forecasting in present-day climate produce skillful forecasts across different climate states corresponding to pre-industrial, present-day, and future 2.9K warmer climates. This indicates that the dynamics shaping the weather on short timescales may not differ fundamentally in a changing climate. It also demonstrates out-of-distribution generalization capabilities of the machine learning models that are a critical prerequisite for climate applications. Nonetheless, two of the models show a global-mean cold bias in the forecasts for the future warmer climate state, i.e. they drift towards the colder present-day climate they have been trained for. A similar result is obtained for the pre-industrial case where two out of three models show a warming. We discuss possible remedies for these biases and analyze their spatial distribution, revealing complex warming and cooling patterns that are partly related to missing ocean-sea ice and land surface information in the training data. Despite these current limitations, our results suggest that data-driven machine learning models will provide powerful tools for climate science and transform established approaches by complementing conventional physics-based models.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2409.18446.pdf' target='_blank'>https://arxiv.org/pdf/2409.18446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saptarshi Sengupta, Wenpeng Yin, Preslav Nakov, Shreya Ghosh, Suhang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18446">Exploring Language Model Generalization in Low-Resource Extractive QA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate Extractive Question Answering (EQA) with Large Language Models (LLMs) under domain drift, i.e., can LLMs generalize to domains that require specific knowledge such as medicine and law in a zero-shot fashion without additional in-domain training? To this end, we devise a series of experiments to explain the performance gap empirically. Our findings suggest that: (a) LLMs struggle with dataset demands of closed domains such as retrieving long answer spans; (b) Certain LLMs, despite showing strong overall performance, display weaknesses in meeting basic requirements as discriminating between domain-specific senses of words which we link to pre-processing decisions; (c) Scaling model parameters is not always effective for cross domain generalization; and (d) Closed-domain datasets are quantitatively much different than open-domain EQA datasets and current LLMs struggle to deal with them. Our findings point out important directions for improving existing LLMs.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2409.05817.pdf' target='_blank'>https://arxiv.org/pdf/2409.05817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad-Javad Darvishi-Bayazi, Md Rifat Arefin, Jocelyn Faubert, Irina Rish
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05817">VFA: Vision Frequency Analysis of Foundation Models and Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models often struggle with distribution shifts in real-world scenarios, whereas humans exhibit robust adaptation. Models that better align with human perception may achieve higher out-of-distribution generalization. In this study, we investigate how various characteristics of large-scale computer vision models influence their alignment with human capabilities and robustness. Our findings indicate that increasing model and data size and incorporating rich semantic information and multiple modalities enhance models' alignment with human perception and their overall robustness. Our empirical analysis demonstrates a strong correlation between out-of-distribution accuracy and human alignment.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2409.04734.pdf' target='_blank'>https://arxiv.org/pdf/2409.04734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Preetu Mehta, Aman Sagar, Suchi Kumari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04734">Swin Transformer for Robust Differentiation of Real and Synthetic Images: Intra- and Inter-Dataset Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>\textbf{Purpose} This study aims to address the growing challenge of distinguishing computer-generated imagery (CGI) from authentic digital images in the RGB color space. Given the limitations of existing classification methods in handling the complexity and variability of CGI, this research proposes a Swin Transformer-based model for accurate differentiation between natural and synthetic images.
  \textbf{Methods} The proposed model leverages the Swin Transformer's hierarchical architecture to capture local and global features crucial for distinguishing CGI from natural images. The model's performance was evaluated through intra-dataset and inter-dataset testing across three distinct datasets: CiFAKE, JSSSTU, and Columbia. The datasets were tested individually (D1, D2, D3) and in combination (D1+D2+D3) to assess the model's robustness and domain generalization capabilities.
  \textbf{Results} The Swin Transformer-based model demonstrated high accuracy, consistently achieving a range of 97-99\% across all datasets and testing scenarios. These results confirm the model's effectiveness in detecting CGI, showcasing its robustness and reliability in both intra-dataset and inter-dataset evaluations.
  \textbf{Conclusion} The findings of this study highlight the Swin Transformer model's potential as an advanced tool for digital image forensics, particularly in distinguishing CGI from natural images. The model's strong performance across multiple datasets indicates its capability for domain generalization, making it a valuable asset in scenarios requiring precise and reliable image classification.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2409.01374.pdf' target='_blank'>https://arxiv.org/pdf/2409.01374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Solim LeGris, Wai Keen Vong, Brenden M. Lake, Todd M. Gureckis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01374">H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Abstraction and Reasoning Corpus (ARC) is a visual program synthesis benchmark designed to test challenging out-of-distribution generalization in humans and machines. Since 2019, limited progress has been observed on the challenge using existing artificial intelligence methods. Comparing human and machine performance is important for the validity of the benchmark. While previous work explored how well humans can solve tasks from the ARC benchmark, they either did so using only a subset of tasks from the original dataset, or from variants of ARC, and therefore only provided a tentative estimate of human performance. In this work, we obtain a more robust estimate of human performance by evaluating 1729 humans on the full set of 400 training and 400 evaluation tasks from the original ARC problem set. We estimate that average human performance lies between 73.3% and 77.2% correct with a reported empirical average of 76.2% on the training set, and between 55.9% and 68.9% correct with a reported empirical average of 64.2% on the public evaluation set. However, we also find that 790 out of the 800 tasks were solvable by at least one person in three attempts, suggesting that the vast majority of the publicly available ARC tasks are in principle solvable by typical crowd-workers recruited over the internet. Notably, while these numbers are slightly lower than earlier estimates, human performance still greatly exceeds current state-of-the-art approaches for solving ARC. To facilitate research on ARC, we publicly release our dataset, called H-ARC (human-ARC), which includes all of the submissions and action traces from human participants.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2601.20847.pdf' target='_blank'>https://arxiv.org/pdf/2601.20847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Willams de Lima Costa, Thifany Ketuli Silva de Souza, Jonas Ferreira Silva, Carlos Gabriel Bezerra Pereira, Bruno Reis Vila Nova, Leonardo Silvino Brito, Rafael Raider Leoni, Juliano Silva Filho, Valter Ferreira, Sibele Miguel Soares Neto, Samantha Uehara, Daniel Giacometti Amaral, João Marcelo Teixeira, Veronica Teichrieb, Cristiano Coelho de Araújo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20847">A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2601.17228.pdf' target='_blank'>https://arxiv.org/pdf/2601.17228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengyue Zhang, Ruiwen Ding, Luoting Zhuang, Yuxiao Wu, Erika F. Rodriguez, William Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17228">Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2601.08868.pdf' target='_blank'>https://arxiv.org/pdf/2601.08868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Wang, Yinfeng Yu, Bin Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08868">Residual Cross-Modal Fusion Networks for Audio-Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2601.08122.pdf' target='_blank'>https://arxiv.org/pdf/2601.08122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atefeh Termehchi, Ekram Hossain, Isaac Woungang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08122">Generalization Analysis and Method for Domain Generalization for a Family of Recurrent Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning (DL) has driven broad advances across scientific and engineering domains. Despite its success, DL models often exhibit limited interpretability and generalization, which can undermine trust, especially in safety-critical deployments. As a result, there is growing interest in (i) analyzing interpretability and generalization and (ii) developing models that perform robustly under data distributions different from those seen during training (i.e. domain generalization). However, the theoretical analysis of DL remains incomplete. For example, many generalization analyses assume independent samples, which is violated in sequential data with temporal correlations. Motivated by these limitations, this paper proposes a method to analyze interpretability and out-of-domain (OOD) generalization for a family of recurrent neural networks (RNNs). Specifically, the evolution of a trained RNN's states is modeled as an unknown, discrete-time, nonlinear closed-loop feedback system. Using Koopman operator theory, these nonlinear dynamics are approximated with a linear operator, enabling interpretability. Spectral analysis is then used to quantify the worst-case impact of domain shifts on the generalization error. Building on this analysis, a domain generalization method is proposed that reduces the OOD generalization error and improves the robustness to distribution shifts. Finally, the proposed analysis and domain generalization approach are validated on practical temporal pattern-learning tasks.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2601.07748.pdf' target='_blank'>https://arxiv.org/pdf/2601.07748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robert Lewis, Katie Matton, Rosalind W. Picard, John Guttag
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07748">Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised pre-training with contrastive learning is a powerful method for learning from sparsely labeled data. However, performance can drop considerably when there is a shift in the distribution of data from training to test time. We study this phenomenon in a setting in which the training data come from multiple domains, and the test data come from a domain not seen at training that is subject to significant covariate shift. We present a new method for contrastive learning that incorporates domain labels to increase the domain invariance of learned representations, leading to improved out-of-distribution generalization. Our method adjusts the temperature parameter in the InfoNCE loss -- which controls the relative weighting of negative pairs -- using the probability that a negative sample comes from the same domain as the anchor. This upweights pairs from more similar domains, encouraging the model to discriminate samples based on domain-invariant attributes. Through experiments on a variant of the MNIST dataset, we demonstrate that our method yields better out-of-distribution performance than domain generalization baselines. Furthermore, our method maintains strong in-distribution task performance, substantially outperforming baselines on this measure.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2511.23264.pdf' target='_blank'>https://arxiv.org/pdf/2511.23264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ariful Islam, Md Rifat Hossen, Tanvir Mahmud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23264">BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2511.22503.pdf' target='_blank'>https://arxiv.org/pdf/2511.22503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katia Vendrame, Bolaji Yusuf, Santosh Kesiraju, Šimon Sedláček, Oldřich Plchot, Jan Černocký
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22503">Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end spoken dialogue state tracking (DST) is made difficult by the tandem of having to handle speech input and data scarcity. Combining speech foundation encoders and large language models has been proposed in recent work as to alleviate some of this difficulty. Although this approach has been shown to result in strong spoken DST models, achieving state-of-the-art performance in realistic multi-turn DST, it struggles to generalize across domains and requires annotated spoken DST training data for each domain of interest. However, collecting such data for every target domain is both costly and difficult. Noting that textual DST data is more easily obtained for various domains, in this work, we propose jointly training on available spoken DST data and written textual data from other domains as a way to achieve cross-domain generalization. We conduct experiments which show the efficacy of our proposed method for getting good cross-domain DST performance without relying on spoken training data from the target domains.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2511.00993.pdf' target='_blank'>https://arxiv.org/pdf/2511.00993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianming Liu, Jirong Yang, Yafeng Yin, Manzi Li, Linghao Wang, Zheng Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00993">Aligning LLM agents with human learning and adjustment behavior: a dual agent approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2511.00067.pdf' target='_blank'>https://arxiv.org/pdf/2511.00067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixing Li, Arsham Gholamzadeh Khoee, Yinan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00067">Latent Domain Prompt Learning for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The objective of domain generalization (DG) is to enable models to be robust against domain shift. DG is crucial for deploying vision-language models (VLMs) in real-world applications, yet most existing methods rely on domain labels that may not be available and often ambiguous. We instead study the DG setting where models must generalize well without access to explicit domain labels. Our key idea is to represent an unseen target domain as a combination of latent domains automatically discovered from training data, enabling the model to adaptively transfer knowledge across domains. To realize this, we perform latent domain clustering on image features and fuse domain-specific text features based on the similarity between the input image and each latent domain. Experiments on four benchmarks show that this strategy yields consistent gains over VLM-based baselines and provides new insights into improving robustness under domain shift.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2510.26635.pdf' target='_blank'>https://arxiv.org/pdf/2510.26635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Wang, Wei Dai, Thuy Thanh Dao, Steffen Bollmann, Hongfu Sun, Craig Engstrom, Shekhar S. Chandra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26635">SAMRI: Segment Anything Model for MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate magnetic resonance imaging (MRI) segmentation is crucial for clinical decision-making, but remains labor-intensive when performed manually. Convolutional neural network (CNN)-based methods can be accurate and efficient, but often generalize poorly to MRI's variable contrast, intensity inhomogeneity, and protocols. Although the transformer-based Segment Anything Model (SAM) has demonstrated remarkable generalizability in natural images, existing adaptations often treat MRI as another imaging modality, overlooking these modality-specific challenges. We present SAMRI, an MRI-specialized SAM trained and validated on 1.1 million labeled MR slices spanning whole-body organs and pathologies. We demonstrate that SAM can be effectively adapted to MRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing training time by 94% and trainable parameters by 96% versus full-model retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice of 0.87, delivering state-of-the-art accuracy across anatomical regions and robust generalization on unseen structures, particularly small and clinically important structures.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2510.06257.pdf' target='_blank'>https://arxiv.org/pdf/2510.06257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangjun Mi, Frank Mueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06257">Toward Uncertainty-Aware and Generalizable Neural Decoding for Quantum LDPC Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum error correction (QEC) is essential for scalable quantum computing, yet decoding errors via conventional algorithms result in limited accuracy (i.e., suppression of logical errors) and high overheads, both of which can be alleviated by inference-based decoders. To date, such machine-learning (ML) decoders lack two key properties crucial for practical fault tolerance: reliable uncertainty quantification and robust generalization to previously unseen codes. To address this gap, we propose \textbf{QuBA}, a Bayesian graph neural decoder that integrates attention to both dot-product and multi-head, enabling expressive error-pattern recognition alongside calibrated uncertainty estimates. Building on QuBA, we further develop \textbf{SAGU }\textbf{(Sequential Aggregate Generalization under Uncertainty)}, a multi-code training framework with enhanced cross-domain robustness enabling decoding beyond the training set. Experiments on bivariate bicycle (BB) codes and their coprime variants demonstrate that (i) both QuBA and SAGU consistently outperform the classical baseline belief propagation (BP), achieving a reduction of on average \emph{one order of magnitude} in logical error rate (LER), and up to \emph{two orders of magnitude} under confident-decision bounds on the coprime BB code $[[154, 6, 16]]$; (ii) QuBA also surpasses state-of-the-art neural decoders, providing an advantage of roughly \emph{one order of magnitude} (e.g., for the larger BB code $[[756, 16, \leq34]]$) even when considering conservative (safe) decision bounds; (iii) SAGU achieves decoding performance comparable to or even outperforming QuBA's domain-specific training approach.
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2510.00347.pdf' target='_blank'>https://arxiv.org/pdf/2510.00347.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huitao Yang, Guanting Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00347">In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) continue to grow in capability, there is increasing interest in incorporating them into decision-making tasks. A common pipeline for this is Decision-Pretrained Transformers (DPTs). However, existing training methods for DPTs often struggle to generalize beyond their pretraining data distribution. To explore mitigation of this limitation, we propose in-context curiosity -- a lightweight, exploration-inspired regularizer for offline pretraining -- and introduce the Prediction-Powered Transformer (PPT) framework. PPT augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal to encourage broader exploration during training. In proof-of-concept experiments on Gaussian multi-armed bandits, PPT shows improved robustness: it moderates the performance degradation observed in DPT when test environments exhibit higher variance in reward, particularly when pretraining data has limited diversity. While the quality of offline data remain fundamental, our preliminary results suggest that curiosity-driven pretraining offers a promising direction for enhancing out-of-distribution generalization in in-context RL agents.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2510.00346.pdf' target='_blank'>https://arxiv.org/pdf/2510.00346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanbo Hou, Zhaoyi Liu, Xin Shen, Stephen Roberts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00346">Learning Domain-Robust Bioacoustic Representations for Mosquito Species Classification with Contrastive Learning and Distribution Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mosquito Species Classification (MSC) is crucial for vector surveillance and disease control. The collection of mosquito bioacoustic data is often limited by mosquito activity seasons and fieldwork. Mosquito recordings across regions, habitats, and laboratories often show non-biological variations from the recording environment, which we refer to as domain features. This study finds that models directly trained on audio recordings with domain features tend to rely on domain information rather than the species' acoustic cues for identification, resulting in illusory good performance while actually performing poor cross-domain generalization. To this end, we propose a Domain-Robust Bioacoustic Learning (DR-BioL) framework that combines contrastive learning with distribution alignment. Contrastive learning aims to promote cohesion within the same species and mitigate inter-domain discrepancies, and species-conditional distribution alignment further enhances cross-domain species representation. Experiments on a multi-domain mosquito bioacoustic dataset from diverse environments show that the DR-BioL improves the accuracy and robustness of baselines, highlighting its potential for reliable cross-domain MSC in the real world.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2509.23631.pdf' target='_blank'>https://arxiv.org/pdf/2509.23631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Yang, Changhao Zhao, Chen Wang, Jiansheng Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23631">DRIK: Distribution-Robust Inductive Kriging without Information Leakage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inductive kriging supports high-resolution spatio-temporal estimation with sparse sensor networks, but conventional training-evaluation setups often suffer from information leakage and poor out-of-distribution (OOD) generalization. We find that the common 2x2 spatio-temporal split allows test data to influence model selection through early stopping, obscuring the true OOD characteristics of inductive kriging. To address this issue, we propose a 3x3 partition that cleanly separates training, validation, and test sets, eliminating leakage and better reflecting real-world applications. Building on this redefined setting, we introduce DRIK, a Distribution-Robust Inductive Kriging approach designed with the intrinsic properties of inductive kriging in mind to explicitly enhance OOD generalization, employing a three-tier strategy at the node, edge, and subgraph levels. DRIK perturbs node coordinates to capture continuous spatial relationships, drops edges to reduce ambiguity in information flow and increase topological diversity, and adds pseudo-labeled subgraphs to strengthen domain generalization. Experiments on six diverse spatio-temporal datasets show that DRIK consistently outperforms existing methods, achieving up to 12.48% lower MAE while maintaining strong scalability.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2509.20785.pdf' target='_blank'>https://arxiv.org/pdf/2509.20785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jincai Song, Haipeng Chen, Jun Qin, Na Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20785">Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised domain generalization (SSDG) in medical image segmentation offers a promising solution for generalizing to unseen domains during testing, addressing domain shift challenges and minimizing annotation costs. However, conventional SSDG methods assume labeled and unlabeled data are available for each source domain in the training set, a condition that is not always met in practice. The coexistence of limited annotation and domain shift in the training set is a prevalent issue. Thus, this paper explores a more practical and challenging scenario, cross-domain semi-supervised domain generalization (CD-SSDG), where domain shifts occur between labeled and unlabeled training data, in addition to shifts between training and testing sets. Existing SSDG methods exhibit sub-optimal performance under such domain shifts because of inaccurate pseudolabels. To address this issue, we propose a novel dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG. Building upon the co-training paradigm with two sub-models offering cross pseudo supervision, our DAC framework integrates extra feature-level supervision and asymmetric auxiliary tasks for each sub-model. This feature-level supervision serves to address inaccurate pseudo supervision caused by domain shifts between labeled and unlabeled data, utilizing complementary supervision from the rich feature space. Additionally, two distinct auxiliary self-supervised tasks are integrated into each sub-model to enhance domain-invariant discriminative feature learning and prevent model collapse. Extensive experiments on real-world medical image segmentation datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust generalizability of the proposed DAC framework.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2509.20489.pdf' target='_blank'>https://arxiv.org/pdf/2509.20489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>D. Darankoum, C. Habermacher, J. Volle, S. Grudinin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20489">CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electroencephalography signals (EEGs) contain rich multi-scale information crucial for understanding brain states, with potential applications in diagnosing and advancing the drug development landscape. However, extracting meaningful features from raw EEG signals while handling noise and channel variability remains a major challenge. This work proposes a novel end-to-end deep-learning framework that addresses these issues through several key innovations. First, we designed an encoder capable of explicitly capturing multi-scale frequency oscillations covering a wide range of features for different EEG-related tasks. Secondly, to model complex dependencies and handle the high temporal resolution of EEGs, we introduced an attention-based encoder that simultaneously learns interactions across EEG channels and within localized {\em patches} of individual channels. We integrated a dedicated gating network on top of the attention encoder to dynamically filter out noisy and non-informative channels, enhancing the reliability of EEG data. The entire encoding process is guided by a novel loss function, which leverages supervised and contrastive learning, significantly improving model generalization. We validated our approach in multiple applications, ranging from the classification of effects across multiple Central Nervous System (CNS) disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease. Our results demonstrate that the proposed learning paradigm can extract biologically meaningful patterns from raw EEG signals across different species, autonomously select high-quality channels, and achieve robust generalization through innovative architectural and loss design.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2509.19372.pdf' target='_blank'>https://arxiv.org/pdf/2509.19372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuzanna Dubanowska, Maciej Å»elaszczyk, MichaÅ Brzozowski, Paolo Mandica, MichaÅ Karpowicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19372">Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We critically assess the efficacy of the current SOTA in hallucination detection and find that its performance on the RAGTruth dataset is largely driven by a spurious correlation with data. Controlling for this effect, state-of-the-art performs no better than supervised linear probes, while requiring extensive hyperparameter tuning across datasets. Out-of-distribution generalization is currently out of reach, with all of the analyzed methods performing close to random. We propose a set of guidelines for hallucination detection and its evaluation.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2509.16686.pdf' target='_blank'>https://arxiv.org/pdf/2509.16686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengge Cai, Haowen Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16686">EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it incurs significant memory overhead. Recent work on Multi-head Latent Attention (MLA) mitigates this by compressing KV representations into a shared latent space, achieving a better trade-off between performance and cache efficiency. While MLA already achieves significant KV cache reduction, the scope for further compression remains limited without performance loss. In this paper, we propose \textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel extension of MLA that further reduces KV cache size while enhancing representational expressiveness. EG-MLA introduces a token-specific embedding gating mechanism applied in the latent space, enabling fine-grained modulation of compressed KV vectors with minimal additional computation. Compared to MHA, EG-MLA achieves over 91.6\% reduction in KV cache size with negligible performance degradation. Relative to MLA, EG-MLA consistently improves task accuracy across diverse reasoning benchmarks while achieving up to 59.9\% additional memory savings. Our theoretical analysis highlights how embedding gating induces implicit high-order interactions, and empirical evaluations demonstrate robust generalization across model scales and compression regimes. Notably, we successfully scale EG-MLA to over 1 billion parameters, demonstrating its practical viability for large-scale LLM deployment. These results establish EG-MLA as a memory- and compute-efficient attention mechanism that enables scalable, high-performance inference in modern LLMs.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2509.10502.pdf' target='_blank'>https://arxiv.org/pdf/2509.10502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sujatha Kotte, Vangala Govindakrishnan Saipradeep, Vidushi Walia, Dhandapani Nandagopal, Thomas Joseph, Naveen Sivadasan, Bhagat Singh Lali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10502">MIDOG 2025 Track 2: A Deep Learning Model for Classification of Atypical and Normal Mitotic Figures under Class and Hardness Imbalances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivation: Accurate classification of mitotic figures into normal and atypical types is crucial for tumor prognostication in digital pathology. However, developing robust deep learning models for this task is challenging due to the subtle morphological differences, as well as significant class and hardness imbalances in real-world histopathology datasets. Methods: We propose a novel deep learning approach based on a ResNet backbone with specialized classification heads. Our architecture uniquely models both the mitotic figure phenotype and the instance difficulty simultaneously. This method is specifically designed to handle the challenges of diverse tissue types, scanner variability, and imbalanced data. We employed focal loss to effectively mitigate the pronounced class imbalance, and a comprehensive data augmentation pipeline was implemented to enhance the model's robustness and generalizability. Results: Our approach demonstrated strong and consistent performance. In a 5-fold cross-validation on the MIDOG 2025 Track 2 dataset, it achieved a mean balanced accuracy of 0.8744 +/- 0.0093 and an ROC AUC of 0.9505 +/- 0.029. The model showed robust generalization across preliminary leaderboard evaluations, achieving an overall balanced accuracy of 0.8736 +/- 0.0204. Conclusion: The proposed method offers a reliable and generalizable solution for the classification of atypical and normal mitotic figures. By addressing the inherent challenges of real world data, our approach has the potential to support precise prognostic assessments in clinical practice and improve consistency in pathological diagnosis.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2509.09251.pdf' target='_blank'>https://arxiv.org/pdf/2509.09251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanyang Wang, Yuxuan Yang, Hongjun Wang, Lihui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09251">Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intelligent fault diagnosis of rotating mechanical equipment usually requires a large amount of labeled sample data. However, in practical industrial applications, acquiring enough data is both challenging and expensive in terms of time and cost. Moreover, different types of rotating mechanical equipment with different unique mechanical properties, require separate training of diagnostic models for each case. To address the challenges of limited fault samples and the lack of generalizability in prediction models for practical engineering applications, we propose a Multi-Attention Meta Transformer method for few-shot unsupervised rotating machinery fault diagnosis (MMT-FD). This framework extracts potential fault representations from unlabeled data and demonstrates strong generalization capabilities, making it suitable for diagnosing faults across various types of mechanical equipment. The MMT-FD framework integrates a time-frequency domain encoder and a meta-learning generalization model. The time-frequency domain encoder predicts status representations generated through random augmentations in the time-frequency domain. These enhanced data are then fed into a meta-learning network for classification and generalization training, followed by fine-tuning using a limited amount of labeled data. The model is iteratively optimized using a small number of contrastive learning iterations, resulting in high efficiency. To validate the framework, we conducted experiments on a bearing fault dataset and rotor test bench data. The results demonstrate that the MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled sample data, exhibiting robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2509.03137.pdf' target='_blank'>https://arxiv.org/pdf/2509.03137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yi, Qian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03137">A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is widely adopted as a standard method for radionuclide quantification because of its inherent advantages such as high precision, self-calibrating capability, and independence from radioactive reference sources. However, multiradionuclide analysis via TDCR faces the challenges of limited automation and reliance on mixture-specific standards, which may not be easily available. Here, we present an Artificial Intelligence (AI) framework that combines numerical spectral simulation and deep learning for standard-free automated analysis. $Î²$ spectra for model training were generated using Geant4 simulations coupled with statistically modeled detector response sampling. A tailored neural network architecture, trained on this dataset covering various nuclei mix ratio and quenching scenarios, enables autonomous resolution of individual radionuclide activities and detecting efficiency through end-to-end learning paradigms. The model delivers consistent high accuracy across tasks: activity proportions (mean absolute error = 0.009), detection efficiencies (mean absolute error = 0.002), and spectral reconstruction (Structural Similarity Index = 0.9998), validating its physical plausibility for quenched $Î²$ spectroscopy. This AI-driven methodology exhibits significant potential for automated safety-compliant multiradionuclide analysis with robust generalization, real-time processing capabilities, and engineering feasibility, particularly in scenarios where reference materials are unavailable or rapid field analysis is required.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2509.02630.pdf' target='_blank'>https://arxiv.org/pdf/2509.02630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Euiseop Song, Jaeyoung Park, Jaewoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02630">Challenges and Lessons from MIDOG 2025: A Two-Stage Approach to Domain-Robust Mitotic Figure Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitotic figure detection remains a challenging task in computational pathology due to domain variability and morphological complexity. This paper describes our participation in the MIDOG 2025 challenge, focusing on robust mitotic figure detection across diverse tissue domains. We developed a two-stage pipeline combining Faster R-CNN for candidate detection with an ensemble of three classifiers (DenseNet-121, EfficientNet-v2, InceptionResNet-v2) for false positive reduction. Our best submission achieved F1-score 0.2237 (Recall: 0.9528, Precision: 0.1267) using a Faster R-CNN trained solely on MIDOG++ dataset. While our high recall demonstrates effective mitotic figure detection, the critically low precision (12.67%) reveals fundamental challenges in distinguishing true mitoses from morphologically similar imposters across diverse domains. Analysis of six submission variants showed that subsequent optimization attempts were counterproductive, highlighting the omplexity of domain generalization in histopathology. This work provides valuable insights into the practical challenges of developing robust mitotic figure detection algorithms and emphasizes the importance of effective false positive suppression strategies.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2509.02589.pdf' target='_blank'>https://arxiv.org/pdf/2509.02589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Qi, Dominic Labella, Thomas Sanford, Maxwell Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02589">Normal and Atypical Mitosis Image Classifier using Efficient Vision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle atypical versus normal mitosis classification in the MIDOG 2025 challenge using EfficientViT-L2, a hybrid CNN--ViT architecture optimized for accuracy and efficiency. A unified dataset of 13,938 nuclei from seven cancer types (MIDOG++ and AMi-Br) was used, with atypical mitoses comprising ~15. To assess domain generalization, we applied leave-one-cancer-type-out cross-validation with 5-fold ensembles, using stain-deconvolution for image augmentation. For challenge submissions, we trained an ensemble with the same 5-fold split but on all cancer types. In the preliminary evaluation phase, this model achieved balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy of 0.85, demonstrating competitive and well-balanced performance across metrics.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2509.02287.pdf' target='_blank'>https://arxiv.org/pdf/2509.02287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pushpendra Dhakara, Prachi Chachodhia, Vaibhav Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02287">SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unstructured urban environments present unique challenges for scene understanding and generalization due to their complex and diverse layouts. We introduce SynthGenNet, a self-supervised student-teacher architecture designed to enable robust test-time domain generalization using synthetic multi-source imagery. Our contributions include the novel ClassMix++ algorithm, which blends labeled data from various synthetic sources while maintaining semantic integrity, enhancing model adaptability. We further employ Grounded Mask Consistency Loss (GMC), which leverages source ground truth to improve cross-domain prediction consistency and feature alignment. The Pseudo-Label Guided Contrastive Learning (PLGCL) mechanism is integrated into the student network to facilitate domain-invariant feature learning through iterative knowledge distillation from the teacher network. This self-supervised strategy improves prediction accuracy, addresses real-world variability, bridges the sim-to-real domain gap, and reliance on labeled target data, even in complex urban areas. Outcomes show our model outperforms the state-of-the-art (relying on single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value on real-world datasets like Indian Driving Dataset (IDD).
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2509.00010.pdf' target='_blank'>https://arxiv.org/pdf/2509.00010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuchang Liu, Paul A. O'Gorman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00010">CERA: A Framework for Improved Generalization of Machine Learning Models to Changed Climates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust generalization under climate change remains a major challenge for machine learning applications in climate science. Most existing approaches struggle to extrapolate beyond the climate they were trained on, leading to a strong dependence on training data from model simulations of warm climates. Use of climate-invariant inputs improves generalization but requires challenging manual feature engineering. Here, we present CERA (Climate-invariant Encoding through Representation Alignment), a machine learning framework consisting of an autoencoder with explicit latent-space alignment, followed by a predictor for downstream process estimation. We test CERA on the problem of parameterizing moist-physics processes. Without training on labeled data from a +4K climate, CERA leverages labeled control-climate data and unlabeled warmer-climate inputs to improve generalization to the warmer climate, outperforming both raw-input and physically informed baselines in predicting key moisture and energy tendencies. It captures not only the vertical and meridional structures of the moisture tendencies, but also shifts in the intensity distribution of precipitation including extremes. Ablation experiments show that latent alignment improves both accuracy and the robustness across random seeds used in training. While some reduced skill remains in the boundary layer, the framework offers a data-driven alternative to manual feature engineering of climate invariant inputs. Beyond parameterizations used in hybrid ML-physics systems, the approach holds promise for other climate applications such as statistical downscaling.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2508.19595.pdf' target='_blank'>https://arxiv.org/pdf/2508.19595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Kazemi Eskeri, Thomas Wiedemann, Ville Kyrki, Dominik Baumann, Tomasz Piotr Kucner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19595">A Lightweight Crowd Model for Robot Social Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots operating in human-populated environments must navigate safely and efficiently while minimizing social disruption. Achieving this requires estimating crowd movement to avoid congested areas in real-time. Traditional microscopic models struggle to scale in dense crowds due to high computational cost, while existing macroscopic crowd prediction models tend to be either overly simplistic or computationally intensive. In this work, we propose a lightweight, real-time macroscopic crowd prediction model tailored for human motion, which balances prediction accuracy and computational efficiency. Our approach simplifies both spatial and temporal processing based on the inherent characteristics of pedestrian flow, enabling robust generalization without the overhead of complex architectures. We demonstrate a 3.6 times reduction in inference time, while improving prediction accuracy by 3.1 %. Integrated into a socially aware planning framework, the model enables efficient and socially compliant robot navigation in dynamic environments. This work highlights that efficient human crowd modeling enables robots to navigate dense environments without costly computations.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2508.03007.pdf' target='_blank'>https://arxiv.org/pdf/2508.03007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhui Li, Xiaojie Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03007">Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalized Semantic Segmentation (DGSS) aims to improve the generalization ability of models across unseen domains without access to target data during training. Recent advances in DGSS have increasingly exploited vision foundation models (VFMs) via parameter-efficient fine-tuning strategies. However, most existing approaches concentrate on global feature fine-tuning, while overlooking hierarchical adaptation across feature levels, which is crucial for precise dense prediction. In this paper, we propose Multi-Granularity Feature Calibration (MGFC), a novel framework that performs coarse-to-fine alignment of VFM features to enhance robustness under domain shifts. Specifically, MGFC first calibrates coarse-grained features to capture global contextual semantics and scene-level structure. Then, it refines medium-grained features by promoting category-level feature discriminability. Finally, fine-grained features are calibrated through high-frequency spatial detail enhancement. By performing hierarchical and granularity-aware calibration, MGFC effectively transfers the generalization strengths of VFMs to the domain-specific task of DGSS. Extensive experiments on benchmark datasets demonstrate that our method outperforms state-of-the-art DGSS approaches, highlighting the effectiveness of multi-granularity adaptation for the semantic segmentation task of domain generalization.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2507.19881.pdf' target='_blank'>https://arxiv.org/pdf/2507.19881.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Lian, Jose L. GÃ³mez, Antonio M. LÃ³pez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19881">FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization has shown promising progress in image classification by enabling collaborative training across multiple clients without sharing raw data. However, its potential in the semantic segmentation of autonomous driving remains underexplored. In this paper, we propose FedS2R, the first one-shot federated domain generalization framework for synthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises two components: an inconsistency-driven data augmentation strategy that generates images for unstable classes, and a multi-client knowledge distillation scheme with feature fusion that distills a global model from multiple client models. Experiments on five real-world datasets, Cityscapes, BDD100K, Mapillary, IDD, and ACDC, show that the global model significantly outperforms individual client models and is only 2 mIoU points behind the model trained with simultaneous access to all client data. These results demonstrate the effectiveness of FedS2R in synthetic-to-real semantic segmentation for autonomous driving under federated learning
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2507.06111.pdf' target='_blank'>https://arxiv.org/pdf/2507.06111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad H. Danesh, Maxime Wabartha, Stanley Wu, Joelle Pineau, Hsiu-Chin Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06111">Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying reinforcement learning (RL) policies in real-world involves significant challenges, including distribution shifts, safety concerns, and the impracticality of direct interactions during policy refinement. Existing methods, such as domain randomization (DR) and off-dynamics RL, enhance policy robustness by direct interaction with the target domain, an inherently unsafe practice. We propose Uncertainty-Aware RL (UARL), a novel framework that prioritizes safety during training by addressing Out-Of-Distribution (OOD) detection and policy adaptation without requiring direct interactions in target domain. UARL employs an ensemble of critics to quantify policy uncertainty and incorporates progressive environmental randomization to prepare the policy for diverse real-world conditions. By iteratively refining over high-uncertainty regions of the state space in simulated environments, UARL enhances robust generalization to the target domain without explicitly training on it. We evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its effectiveness in reliable OOD detection, improved performance, and enhanced sample efficiency compared to baselines.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2507.05302.pdf' target='_blank'>https://arxiv.org/pdf/2507.05302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binjia Zhou, Hengrui Lou, Lizhe Chen, Haoyuan Li, Dawei Luo, Shuai Chen, Jie Lei, Zunlei Feng, Yijun Bei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05302">CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake detection.Existing techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of hallucinations.To address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias reduction.Experimental results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2506.20575.pdf' target='_blank'>https://arxiv.org/pdf/2506.20575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Itay Niv, Neta Rabin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20575">Exploring Graph-Transformer Out-of-Distribution Generalization Abilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning on graphs has shown remarkable success across numerous applications, including social networks, bio-physics, traffic networks, and recommendation systems. Regardless of their successes, current methods frequently depend on the assumption that training and testing data share the same distribution, a condition rarely met in real-world scenarios. While graph-transformer (GT) backbones have recently outperformed traditional message-passing neural networks (MPNNs) in multiple in-distribution (ID) benchmarks, their effectiveness under distribution shifts remains largely unexplored. In this work, we address the challenge of out-of-distribution (OOD) generalization for graph neural networks, with a special focus on the impact of backbone architecture. We systematically evaluate GT and hybrid backbones in OOD settings and compare them to MPNNs. To do so, we adapt several leading domain generalization (DG) algorithms to work with GTs and assess their performance on a benchmark designed to test a variety of distribution shifts. Our results reveal that GT and hybrid GT-MPNN backbones demonstrate stronger generalization ability compared to MPNNs, even without specialized DG algorithms (on four out of six benchmarks). Additionally, we propose a novel post-training analysis approach that compares the clustering structure of the entire ID and OOD test datasets, specifically examining domain alignment and class separation. Highlighting its model-agnostic design, the method yielded valuable insights into both GT and MPNN backbones and appears well suited for broader DG applications beyond graph learning, offering a deeper perspective on generalization abilities that goes beyond standard accuracy metrics. Together, our findings highlight the promise of graph-transformers for robust, real-world graph learning and set a new direction for future research in OOD generalization.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2506.16704.pdf' target='_blank'>https://arxiv.org/pdf/2506.16704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cynthia Dwork, Lunjia Hu, Han Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16704">How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a fundamental question of domain generalization: given a family of domains (i.e., data distributions), how many randomly sampled domains do we need to collect data from in order to learn a model that performs reasonably well on every seen and unseen domain in the family? We model this problem in the PAC framework and introduce a new combinatorial measure, which we call the domain shattering dimension. We show that this dimension characterizes the domain sample complexity. Furthermore, we establish a tight quantitative relationship between the domain shattering dimension and the classic VC dimension, demonstrating that every hypothesis class that is learnable in the standard PAC setting is also learnable in our setting.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2506.07378.pdf' target='_blank'>https://arxiv.org/pdf/2506.07378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuen Chen, Haozhe Si, Guojun Zhang, Han Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07378">Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) seeks to develop models that generalize well to unseen target domains, addressing the prevalent issue of distribution shifts in real-world applications. One line of research in DG focuses on aligning domain-level gradients and Hessians to enhance generalization. However, existing methods are computationally inefficient and the underlying principles of these approaches are not well understood. In this paper, we develop the theory of moment alignment for DG. Grounded in \textit{transfer measure}, a principled framework for quantifying generalizability between two domains, we first extend the definition of transfer measure to domain generalization that includes multiple source domains and establish a target error bound. Then, we prove that aligning derivatives across domains improves transfer measure both when the feature extractor induces an invariant optimal predictor across domains and when it does not. Notably, moment alignment provides a unifying understanding of Invariant Risk Minimization, gradient matching, and Hessian matching, three previously disconnected approaches to DG. We further connect feature moments and derivatives of the classifier head, and establish the duality between feature learning and classifier fitting. Building upon our theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment (CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in closed-form. Our method overcomes the computational inefficiencies of existing gradient and Hessian-based techniques by eliminating the need for repeated backpropagation or sampling-based Hessian estimation. We validate the efficacy of our approach through two sets of experiments: linear probing and full fine-tuning. CMA demonstrates superior performance in both settings compared to Empirical Risk Minimization and state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2505.17190.pdf' target='_blank'>https://arxiv.org/pdf/2505.17190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baran Hashemi, Kurt Pasque, Chris Teska, Ruriko Yoshida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17190">Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can algebraic geometry enhance the sharpness, robustness, and interpretability of modern neural reasoning models by equipping them with a mathematically grounded inductive bias? To answer this, we introduce Tropical Attention, an attention mechanism grounded in tropical geometry that lifts the attention kernel into tropical projective space, where reasoning is piecewise-linear and 1-Lipschitz, thus preserving the polyhedral decision structure inherent to combinatorial reasoning. We prove that Multi-Head Tropical Attention (MHTA) stacks universally approximate tropical circuits and realize tropical transitive closure through composition, achieving polynomial resource bounds without invoking recurrent mechanisms. These guarantees explain why the induced polyhedral decision boundaries remain sharp and scale-invariant, rather than smoothed by Softmax. Empirically, we show that Tropical Attention delivers stronger out-of-distribution generalization in both length and value, with high robustness against perturbative noise, and substantially faster inference with fewer parameters compared to Softmax-based and recurrent attention baselines. For the first time, we extend neural algorithmic reasoning beyond PTIME problems to NP-hard and NP-complete problems, paving the way toward sharper and more expressive Large Reasoning Models (LRMs) capable of tackling complex combinatorial challenges in phylogenetics, cryptography, particle physics, and mathematical discovery.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2505.08426.pdf' target='_blank'>https://arxiv.org/pdf/2505.08426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Franko Å ikiÄ, Donik VrÅ¡nak, Sven LonÄariÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08426">DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unconstrained gaze estimation is the process of determining where a subject is directing their visual attention in uncontrolled environments. Gaze estimation systems are important for a myriad of tasks such as driver distraction monitoring, exam proctoring, accessibility features in modern software, etc. However, these systems face challenges in real-world scenarios, partially due to the low resolution of in-the-wild images and partially due to insufficient modeling of head-eye interactions in current state-of-the-art (SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based method that advances gaze prediction through super-resolution (SR) and a dual head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone processes eye and multiscale SR head images, while the proposed DHECA module enables bidirectional feature refinement between the extracted visual features through cross-attention mechanisms. Furthermore, we identified critical annotation errors in one of the most diverse and widely used gaze estimation datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on Gaze360 and GFIE datasets demonstrates superior within-dataset performance of the proposed method, reducing angular error (AE) by 0.48Â° (Gaze360) and 2.95Â° (GFIE) in static configurations, and 0.59Â° (Gaze360) and 3.00Â° (GFIE) in temporal settings compared to prior SOTA methods. Cross-dataset testing shows improvements in AE of more than 1.53Â° (Gaze360) and 3.99Â° (GFIE) in both static and temporal settings, validating the robust generalization properties of our approach.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2505.06894.pdf' target='_blank'>https://arxiv.org/pdf/2505.06894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Qazi, Abdul Basit, Asim Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06894">NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts the domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures and cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows improved performance on benchmarks on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2505.06886.pdf' target='_blank'>https://arxiv.org/pdf/2505.06886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Qazi, Hamd Jalil, Asim Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06886">Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The mouse is one of the most studied animal models in the field of systems neuroscience. Understanding the generalized patterns and decoding the neural representations that are evoked by the diverse range of natural scene stimuli in the mouse visual cortex is one of the key quests in computational vision. In recent years, significant parallels have been drawn between the primate visual cortex and hierarchical deep neural networks. However, their generalized efficacy in understanding mouse vision has been limited. In this study, we investigate the functional alignment between the mouse visual cortex and deep learning models for object classification tasks. We first introduce a generalized representational learning strategy that uncovers a striking resemblance between the functional mapping of the mouse visual cortex and high-performing deep learning models on both top-down (population-level) and bottom-up (single cell-level) scenarios. Next, this representational similarity across the two systems is further enhanced by the addition of Neural Response Normalization (NeuRN) layer, inspired by the activation profile of excitatory and inhibitory neurons in the visual cortex. To test the performance effect of NeuRN on real-world tasks, we integrate it into deep learning models and observe significant improvements in their robustness against data shifts in domain generalization tasks. Our work proposes a novel framework for comparing the functional architecture of the mouse visual cortex with deep learning models. Our findings carry broad implications for the development of advanced AI models that draw inspiration from the mouse visual cortex, suggesting that these models serve as valuable tools for studying the neural representations of the mouse visual cortex and, as a result, enhancing their performance on real-world tasks.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2505.06881.pdf' target='_blank'>https://arxiv.org/pdf/2505.06881.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamd Jalil, Ahmed Qazi, Asim Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06881">NeuRN: Neuro-inspired Domain Generalization for Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization in image classification is a crucial challenge, with models often failing to generalize well across unseen datasets. We address this issue by introducing a neuro-inspired Neural Response Normalization (NeuRN) layer which draws inspiration from neurons in the mammalian visual cortex, which aims to enhance the performance of deep learning architectures on unseen target domains by training deep learning models on a source domain. The performance of these models is considered as a baseline and then compared against models integrated with NeuRN on image classification tasks. We perform experiments across a range of deep learning architectures, including ones derived from Neural Architecture Search and Vision Transformer. Additionally, in order to shortlist models for our experiment from amongst the vast range of deep neural networks available which have shown promising results, we also propose a novel method that uses the Needleman-Wunsch algorithm to compute similarity between deep learning architectures. Our results demonstrate the effectiveness of NeuRN by showing improvement against baseline in cross-domain image classification tasks. Our framework attempts to establish a foundation for future neuro-inspired deep learning models.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2505.01168.pdf' target='_blank'>https://arxiv.org/pdf/2505.01168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyang Ma, Zhihao Wu, Wang Lu, Xin Gao, Jinghang Yue, Taolin Zhang, Lipo Wang, Youfang Lin, Jing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01168">Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of model ensemble attacks has significantly improved the transferability of adversarial examples, but this progress also poses severe threats to the security of deep neural networks. Existing methods, however, face two critical challenges: insufficient capture of shared gradient directions across models and a lack of adaptive weight allocation mechanisms. To address these issues, we propose a novel method Harmonized Ensemble for Adversarial Transferability (HEAT), which introduces domain generalization into adversarial example generation for the first time. HEAT consists of two key modules: Consensus Gradient Direction Synthesizer, which uses Singular Value Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight Orchestrator which dynamically balances intra-domain coherence, stabilizing gradients within individual models, and inter-domain diversity, enhancing transferability across models. Experimental results demonstrate that HEAT significantly outperforms existing methods across various datasets and settings, offering a new perspective and direction for adversarial attack research.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2504.07415.pdf' target='_blank'>https://arxiv.org/pdf/2504.07415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyoyun Choi, Byungmu Yoon, Soobum Kim, Jonggwon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07415">Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated radiology report generation (RRG) holds potential to reduce radiologists' workload, especially as recent advancements in large language models (LLMs) enable the development of multimodal models for chest X-ray (CXR) report generation. However, multimodal LLMs (MLLMs) are resource-intensive, requiring vast datasets and substantial computational cost for training. To address these challenges, we propose a retrieval-augmented generation approach that leverages multimodal retrieval and LLMs to generate radiology reports while mitigating hallucinations and reducing computational demands. Our method uses LLMs to extract key phrases from radiology reports, effectively focusing on essential diagnostic information. Through exploring effective training strategies, including image encoder structure search, adding noise to text embeddings, and additional training objectives, we combine complementary pre-trained image encoders and adopt contrastive learning between text and semantic image embeddings. We evaluate our approach on MIMIC-CXR dataset, achieving state-of-the-art results on CheXbert metrics and competitive RadGraph F1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method demonstrates robust generalization for multi-view RRG, making it suitable for comprehensive clinical applications.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2503.18114.pdf' target='_blank'>https://arxiv.org/pdf/2503.18114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-Ning Chou, Hang Le, Yichen Wang, SueYeon Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18114">Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating task-relevant information into neural representations is a fundamental ability of both biological and artificial intelligence systems. Recent theories have categorized learning into two regimes: the rich regime, where neural networks actively learn task-relevant features, and the lazy regime, where networks behave like random feature models. Yet this simple lazy-rich dichotomy overlooks a diverse underlying taxonomy of feature learning, shaped by differences in learning algorithms, network architectures, and data properties. To address this gap, we introduce an analysis framework to study feature learning via the geometry of neural representations. Rather than inspecting individual learned features, we characterize how task-relevant representational manifolds evolve throughout the learning process. We show, in both theoretical and empirical settings, that as networks learn features, task-relevant manifolds untangle, with changes in manifold geometry revealing distinct learning stages and strategies beyond the lazy-rich dichotomy. This framework provides novel insights into feature learning across neuroscience and machine learning, shedding light on structural inductive biases in neural circuits and the mechanisms underlying out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2503.14321.pdf' target='_blank'>https://arxiv.org/pdf/2503.14321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AdriÃ¡n Javaloy, Antonio Vergari, Isabel Valera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14321">COPA: Comparing the incomparable in multi-objective model evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning (ML) practitioners, we often have hundreds of (trained) ML models at hand from which we need to choose one, based on various objectives such as accuracy, robustness, fairness, scalability, etc. However, how to compare, aggregate and, ultimately, trade-off these objectives is usually a time-consuming task that requires of expert knowledge, as they may be measured in different units or scales. In this work, we investigate how objectives can be automatically normalized and aggregated to systematically navigate their Pareto front. To do so, we make incomparable objectives comparable using their CDFs, approximated by their relative rankings. As a result, we can aggregate them while matching user-specific preferences, allowing practitioners to meaningfully navigate and search for models in the Pareto front. We demonstrate the potential impact of our approach, COPA, in both model selection and benchmarking tasks across diverse ML areas such as fair ML, domain generalization, AutoML and foundation models, where classical ways to normalize and aggregate objectives fall short.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2503.12678.pdf' target='_blank'>https://arxiv.org/pdf/2503.12678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Partho Ghosh, Raisa Bentay Hossain, Mohammad Zunaed, Taufiq Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12678">Domain Generalization for Improved Human Activity Recognition in Office Space Videos Using Adaptive Pre-processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic video activity recognition is crucial across numerous domains like surveillance, healthcare, and robotics. However, recognizing human activities from video data becomes challenging when training and test data stem from diverse domains. Domain generalization, adapting to unforeseen domains, is thus essential. This paper focuses on office activity recognition amidst environmental variability. We propose three pre-processing techniques applicable to any video encoder, enhancing robustness against environmental variations. Our study showcases the efficacy of MViT, a leading state-of-the-art video classification model, and other video encoders combined with our techniques, outperforming state-of-the-art domain adaptation methods. Our approach significantly boosts accuracy, precision, recall and F1 score on unseen domains, emphasizing its adaptability in real-world scenarios with diverse video data sources. This method lays a foundation for more reliable video activity recognition systems across heterogeneous data domains.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2503.06860.pdf' target='_blank'>https://arxiv.org/pdf/2503.06860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Gungor, Derek Eppinger, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06860">Towards Generalization of Tactile Image Generation: Reference-Free Evaluation in a Leakage-Free Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing, which relies on direct physical contact, is critical for human perception and underpins applications in computer vision, robotics, and multimodal learning. Because tactile data is often scarce and costly to acquire, generating synthetic tactile images provides a scalable solution to augment real-world measurements. However, ensuring robust generalization in synthesizing tactile images-capturing subtle, material-specific contact features-remains challenging. We demonstrate that overlapping training and test samples in commonly used datasets inflate performance metrics, obscuring the true generalizability of tactile models. To address this, we propose a leakage-free evaluation protocol coupled with novel, reference-free metrics-TMMD, I-TMMD, CI-TMMD, and D-TMMD-tailored for tactile generation. Moreover, we propose a vision-to-touch generation method that leverages text as an intermediate modality by incorporating concise, material-specific descriptions during training to better capture essential tactile features. Experiments on two popular visuo-tactile datasets, Touch and Go and HCT, show that our approach achieves superior performance and enhanced generalization in a leakage-free setting.
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2503.06026.pdf' target='_blank'>https://arxiv.org/pdf/2503.06026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Masaru Yajima, Kei Ota, Asako Kanezaki, Rei Kawakami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06026">Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving zero-shot peg insertion, where inserting an arbitrary peg into an unseen hole without task-specific training, remains a fundamental challenge in robotics. This task demands a highly generalizable perception system capable of detecting potential holes, selecting the correct mating hole from multiple candidates, estimating its precise pose, and executing insertion despite uncertainties. While learning-based methods have been applied to peg insertion, they often fail to generalize beyond the specific peg-hole pairs encountered during training. Recent advancements in Vision-Language Models (VLMs) offer a promising alternative, leveraging large-scale datasets to enable robust generalization across diverse tasks. Inspired by their success, we introduce a novel zero-shot peg insertion framework that utilizes a VLM to identify mating holes and estimate their poses without prior knowledge of their geometry. Extensive experiments demonstrate that our method achieves 90.2% accuracy, significantly outperforming baselines in identifying the correct mating hole across a wide range of previously unseen peg-hole pairs, including 3D-printed objects, toy puzzles, and industrial connectors. Furthermore, we validate the effectiveness of our approach in a real-world connector insertion task on a backpanel of a PC, where our system successfully detects holes, identifies the correct mating hole, estimates its pose, and completes the insertion with a success rate of 88.3%. These results highlight the potential of VLM-driven zero-shot reasoning for enabling robust and generalizable robotic assembly.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2503.02448.pdf' target='_blank'>https://arxiv.org/pdf/2503.02448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyi Wang, Yinning Shao, Yunlong Ma, Min Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02448">NodeNAS: Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph neural architecture search (GraphNAS) has demonstrated advantages in mitigating performance degradation of graph neural networks (GNNs) due to distribution shifts. Recent approaches introduce weight sharing across tailored architectures, generating unique GNN architectures for each graph end-to-end. However, existing GraphNAS methods do not account for distribution patterns across different graphs and heavily rely on extensive training data. With sparse or single training graphs, these methods struggle to discover optimal mappings between graphs and architectures, failing to generalize to out-of-distribution (OOD) data. In this paper, we propose node-specific graph neural architecture search(NodeNAS), which aims to tailor distinct aggregation methods for different nodes through disentangling node topology and graph distribution with limited datasets. We further propose adaptive aggregation attention based Multi-dim NodeNAS method(MNNAS), which learns an node-specific architecture customizer with good generalizability. Specifically, we extend the vertical depth of the search space, supporting simultaneous node-specific architecture customization across multiple dimensions. Moreover, we model the power-law distribution of node degrees under varying assortativity, encoding structure invariant information to guide architecture customization across each dimension. Extensive experiments across supervised and unsupervised tasks demonstrate that MNNAS surpasses state-of-the-art algorithms and achieves excellent OOD generalization.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2502.20499.pdf' target='_blank'>https://arxiv.org/pdf/2502.20499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felipe del Rio, Alain Raymond-Saez, Daniel Florea, Rodrigo Toro Icarte, Julio Hurtado, Cristian B. Calderon, Alvaro Soto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20499">Data Distributional Properties As Inductive Bias for Systematic Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) struggle at systematic generalization (SG). Several studies have evaluated the possibility to promote SG through the proposal of novel architectures, loss functions or training methodologies. Few studies, however, have focused on the role of training data properties in promoting SG. In this work, we investigate the impact of certain data distributional properties, as inductive biases for the SG ability of a multi-modal language model. To this end, we study three different properties. First, data diversity, instantiated as an increase in the possible values a latent property in the training distribution may take. Second, burstiness, where we probabilistically restrict the number of possible values of latent factors on particular inputs during training. Third, latent intervention, where a particular latent factor is altered randomly during training. We find that all three factors significantly enhance SG, with diversity contributing an 89% absolute increase in accuracy in the most affected property. Through a series of experiments, we test various hypotheses to understand why these properties promote SG. Finally, we find that Normalized Mutual Information (NMI) between latent attributes in the training distribution is strongly predictive of out-of-distribution generalization. We find that a mechanism by which lower NMI induces SG is in the geometry of representations. In particular, we find that NMI induces more parallelism in neural representations (i.e., input features coded in parallel neural vectors) of the model, a property related to the capacity of reasoning by analogy.
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2502.13522.pdf' target='_blank'>https://arxiv.org/pdf/2502.13522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastien RÃ¶cken, Julija Zavadlav
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13522">Enhancing Machine Learning Potentials through Transfer Learning across Chemical Elements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine Learning Potentials (MLPs) can enable simulations of ab initio accuracy at orders of magnitude lower computational cost. However, their effectiveness hinges on the availability of considerable datasets to ensure robust generalization across chemical space and thermodynamic conditions. The generation of such datasets can be labor-intensive, highlighting the need for innovative methods to train MLPs in data-scarce scenarios. Here, we introduce transfer learning of potential energy surfaces between chemically similar elements. Specifically, we leverage the trained MLP for silicon to initialize and expedite the training of an MLP for germanium. Utilizing classical force field and ab initio datasets, we demonstrate that transfer learning surpasses traditional training from scratch in force prediction, leading to more stable simulations and improved temperature transferability. These advantages become even more pronounced as the training dataset size decreases. The out-of-target property analysis shows that transfer learning leads to beneficial but sometimes adversarial effects. Our findings demonstrate that transfer learning across chemical elements is a promising technique for developing accurate and numerically stable MLPs, particularly in a data-scarce regime.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2501.18864.pdf' target='_blank'>https://arxiv.org/pdf/2501.18864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aodi Li, Liansheng Zhuang, Xiao Long, Minghong Yao, Shafei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18864">Test-time Loss Landscape Adaptation for Zero-Shot Generalization in Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time adaptation of pre-trained vision-language models has emerged as a technique for tackling distribution shifts during the test time. Although existing methods, especially those based on Test-time Prompt Tuning (TPT), have shown promising results, their high computational cost associated with parameter optimization presents challenges for scalability and practical application. This paper unveils the unnecessary nature of backpropagation in existing methods from a loss landscape perspective. Building on this insight, this paper proposes a simple yet effective framework called Test-time Loss Landscape Adaptation (TLLA). TLLA leverages the relative position between the training minimum and test loss landscapes to guide the adaptation process, avoiding the update of model parameters at test time. Specifically, it mainly consists of two main stages: In the prompt tuning stage, a Sharpness-Aware Prompt Tuning (SAPT) method is introduced to identify the training flat minimum, setting the foundation for the subsequent test-time adaptation; In the test stage, a Sharpness-based Test Sample Selection (STSS) approach is utilized to ensure the alignment of flat minima within the training loss landscape and each augmented test sample's loss landscape. Extensive experiments on both domain generalization and cross-dataset benchmarks demonstrate that TLLA achieves state-of-the-art performances while significantly reducing computational overhead. Notably, TLLA surpasses TPT by an average of 5.32\% and 6.98\% on four ImageNet variant datasets when employing ResNet50 and ViT-B/16 image encoders, respectively. The code will be available soon.
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2501.18801.pdf' target='_blank'>https://arxiv.org/pdf/2501.18801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhikang Dong, Weituo Hao, Ju-Chiang Wang, Peng Zhang, Pawel Polak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18801">Every Image Listens, Every Image Dances: Music-Driven Image Animation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image animation has become a promising area in multimodal research, with a focus on generating videos from reference images. While prior work has largely emphasized generic video generation guided by text, music-driven dance video generation remains underexplored. In this paper, we introduce MuseDance, an innovative end-to-end model that animates reference images using both music and text inputs. This dual input enables MuseDance to generate personalized videos that follow text descriptions and synchronize character movements with the music. Unlike existing approaches, MuseDance eliminates the need for complex motion guidance inputs, such as pose or depth sequences, making flexible and creative video generation accessible to users of all expertise levels. To advance research in this field, we present a new multimodal dataset comprising 2,904 dance videos with corresponding background music and text descriptions. Our approach leverages diffusion-based methods to achieve robust generalization, precise control, and temporal consistency, setting a new baseline for the music-driven image animation task.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2412.13573.pdf' target='_blank'>https://arxiv.org/pdf/2412.13573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aodi Li, Liansheng Zhuang, Xiao Long, Minghong Yao, Shafei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13573">Seeking Consistent Flat Minima for Better Domain Generalization via Refining Loss Landscapes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims to learn a model from multiple training domains and generalize it to unseen test domains. Recent theory has shown that seeking the deep models, whose parameters lie in the flat minima of the loss landscape, can significantly reduce the out-of-domain generalization error. However, existing methods often neglect the consistency of loss landscapes in different domains, resulting in models that are not simultaneously in the optimal flat minima in all domains, which limits their generalization ability. To address this issue, this paper proposes an iterative Self-Feedback Training (SFT) framework to seek consistent flat minima that are shared across different domains by progressively refining loss landscapes during training. It alternatively generates a feedback signal by measuring the inconsistency of loss landscapes in different domains and refines these loss landscapes for greater consistency using this feedback signal. Benefiting from the consistency of the flat minima within these refined loss landscapes, our SFT helps achieve better out-of-domain generalization. Extensive experiments on DomainBed demonstrate superior performances of SFT when compared to state-of-the-art sharpness-aware methods and other prevalent DG baselines. On average across five DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with ResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available soon.
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2412.07010.pdf' target='_blank'>https://arxiv.org/pdf/2412.07010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai V. Nguyen, Tan Bui-Thanh, Clint Dawson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07010">TAEN: A Model-Constrained Tikhonov Autoencoder Network for Forward and Inverse Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient real-time solvers for forward and inverse problems are essential in engineering and science applications. Machine learning surrogate models have emerged as promising alternatives to traditional methods, offering substantially reduced computational time. Nevertheless, these models typically demand extensive training datasets to achieve robust generalization across diverse scenarios. While physics-based approaches can partially mitigate this data dependency and ensure physics-interpretable solutions, addressing scarce data regimes remains a challenge. Both purely data-driven and physics-based machine learning approaches demonstrate severe overfitting issues when trained with insufficient data. We propose a novel Tikhonov autoencoder model-constrained framework, called TAE, capable of learning both forward and inverse surrogate models using a single arbitrary observation sample. We develop comprehensive theoretical foundations including forward and inverse inference error bounds for the proposed approach for linear cases. For comparative analysis, we derive equivalent formulations for pure data-driven and model-constrained approach counterparts. At the heart of our approach is a data randomization strategy, which functions as a generative mechanism for exploring the training data space, enabling effective training of both forward and inverse surrogate models from a single observation, while regularizing the learning process. We validate our approach through extensive numerical experiments on two challenging inverse problems: 2D heat conductivity inversion and initial condition reconstruction for time-dependent 2D Navier-Stokes equations. Results demonstrate that TAE achieves accuracy comparable to traditional Tikhonov solvers and numerical forward solvers for both inverse and forward problems, respectively, while delivering orders of magnitude computational speedups.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2412.03927.pdf' target='_blank'>https://arxiv.org/pdf/2412.03927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming-Chang Chiu, Shicheng Wen, Pin-Yu Chen, Xuezhe Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03927">MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In vision-language models (VLMs), the ability to perceive and interpret color and physical environment is crucial for achieving contextually accurate understanding and interaction. However, despite advances in multimodal modeling, there remains a significant lack of specialized datasets that rigorously evaluate a model's capacity to discern subtle color variations and spatial context -- critical elements for situational comprehension and reliable deployment across real-world applications. Toward that goal, we curate MegaCOIN, a high-quality, human-labeled dataset based on \emph{real} images with various contextual attributes. MegaCOIN consists of two parts: MegaCOIN-Instruct, which serves as a supervised fine-tuning (SFT) dataset for VLMs; and MegaCOIN-Bench, an annotated test set that can be used as a stand-alone QA dataset. MegaCOIN~provides three annotated features for 220,000 real images: foreground color, background color, and description of an object's physical environment, constituting 660k human annotations. In addition, MegaCOIN can be applied to benchmark domain generalization (DG) algorithms. We explore benchmarking DG methods in the linear probing setup for VLM and show some new insights. Last but not least, we show that VLMs, including GPT-4o, have subpar color recognition capabilities, and fine-tuning with MegaCOIN can result in improved performance on visual evaluation tasks. In certain cases, MegaCOIN fine-tuned small-scale opensource models such as LLaVA and Bunny can outperform closed-source GPT-4o. We hope the utilities of MegaCOIN can shed light on the directions VLMs can improve and provide a more complex platform for domain generalization algorithms.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2411.10063.pdf' target='_blank'>https://arxiv.org/pdf/2411.10063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Gong, Chaoran Cui, Chunyun Zhang, Wenna Wang, Xiushan Nie, Lei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10063">Federated Domain Generalization via Prompt Learning and Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization (FedDG) aims to improve the global model generalization in unseen domains by addressing data heterogeneity under privacy-preserving constraints. A common strategy in existing FedDG studies involves sharing domain-specific knowledge among clients, such as spectrum information, class prototypes, and data styles. However, this knowledge is extracted directly from local client samples, and sharing such sensitive information poses a potential risk of data leakage, which might not fully meet the requirements of FedDG. In this paper, we introduce prompt learning to adapt pre-trained vision-language models (VLMs) in the FedDG scenario, and leverage locally learned prompts as a more secure bridge to facilitate knowledge transfer among clients. Specifically, we propose a novel FedDG framework through Prompt Learning and AggregatioN (PLAN), which comprises two training stages to collaboratively generate local prompts and global prompts at each federated round. First, each client performs both text and visual prompt learning using their own data, with local prompts indirectly synchronized by regarding the global prompts as a common reference. Second, all domain-specific local prompts are exchanged among clients and selectively aggregated into the global prompts using lightweight attention-based aggregators. The global prompts are finally applied to adapt VLMs to unseen target domains. As our PLAN framework requires training only a limited number of prompts and lightweight aggregators, it offers notable advantages in computational and communication efficiency for FedDG. Extensive experiments demonstrate the superior generalization ability of PLAN across four benchmark datasets.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2411.03799.pdf' target='_blank'>https://arxiv.org/pdf/2411.03799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edvin Listo Zec, Adam Breitholtz, Fredrik D. Johansson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03799">Overcoming label shift with target-aware federated learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning enables multiple actors to collaboratively train models without sharing private data. Existing algorithms are successful and well-justified in this task when the intended target domain, where the trained model will be used, shares data distribution with the aggregate of clients, but this is often violated in practice. A common reason is label shift -- that the label distributions differ between clients and the target domain. We demonstrate empirically that this can significantly degrade performance. To address this problem, we propose FedPALS, a principled and practical model aggregation scheme that adapts to label shifts to improve performance in the target domain by leveraging knowledge of label distributions at the central server. Our approach ensures unbiased updates under federated stochastic gradient descent which yields robust generalization across clients with diverse, label-shifted data. Extensive experiments on image classification tasks demonstrate that FedPALS consistently outperforms baselines by aligning model aggregation with the target domain. Our findings reveal that conventional federated learning methods suffer severely in cases of extreme label sparsity on clients, highlighting the critical need for target-aware aggregation as offered by FedPALS.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2410.23625.pdf' target='_blank'>https://arxiv.org/pdf/2410.23625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jehan Yang, Maxwell Soh, Vivianna Lieu, Douglas J Weber, Zackory Erickson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23625">EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the first generalization and adaptation benchmark using machine learning for evaluating out-of-distribution performance of electromyography (EMG) classification algorithms. The ability of an EMG classifier to handle inputs drawn from a different distribution than the training distribution is critical for real-world deployment as a control interface. By predicting the user's intended gesture using EMG signals, we can create a wearable solution to control assistive technologies, such as computers, prosthetics, and mobile manipulator robots. This new out-of-distribution benchmark consists of two major tasks that have utility for building robust and adaptable control interfaces: 1) intersubject classification and 2) adaptation using train-test splits for time-series. This benchmark spans nine datasets--the largest collection of EMG datasets in a benchmark. Among these, a new dataset is introduced, featuring a novel, easy-to-wear high-density EMG wearable for data collection. The lack of open-source benchmarks has made comparing accuracy results between papers challenging for the EMG research community. This new benchmark provides researchers with a valuable resource for analyzing practical measures of out-of-distribution performance for EMG datasets. Our code and data from our new dataset can be found at emgbench.github.io.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2410.14821.pdf' target='_blank'>https://arxiv.org/pdf/2410.14821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mansoor Ali Teevno, Gilberto Ochoa-Ruiz, Sharib Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14821">Tackling domain generalization for out-of-distribution endoscopic imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent advances in deep learning (DL) for surgical scene segmentation have yielded promising results on single-center and single-imaging modality data, these methods usually do not generalize well to unseen distributions or modalities. Even though human experts can identify visual appearances, DL methods often fail to do so when data samples do not follow a similar distribution. Current literature addressing domain gaps in modality changes has focused primarily on natural scene data. However, these methods cannot be directly applied to endoscopic data, as visual cues in such data are more limited compared to natural scenes. In this work, we exploit both style and content information in images by performing instance normalization and feature covariance mapping techniques to preserve robust and generalizable feature representations. Additionally, to avoid the risk of removing salient feature representations associated with objects of interest, we introduce a restitution module within the feature-learning ResNet backbone that retains useful task-relevant features. Our proposed method shows a 13.7% improvement over the baseline DeepLabv3+ and nearly an 8% improvement over recent state-of-the-art (SOTA) methods for the target (different modality) set of the EndoUDA polyp dataset. Similarly, our method achieved a 19% improvement over the baseline and 6% over the best-performing SOTA method on the EndoUDA Barrett's esophagus (BE) dataset.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2410.11646.pdf' target='_blank'>https://arxiv.org/pdf/2410.11646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zahra Kadkhodaie, StÃ©phane Mallat, Eero P. Simoncelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11646">Feature-guided score diffusion for sampling conditional densities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Score diffusion methods can learn probability densities from samples. The score of the noise-corrupted density is estimated using a deep neural network, which is then used to iteratively transport a Gaussian white noise density to a target density. Variants for conditional densities have been developed, but correct estimation of the corresponding scores is difficult. We avoid these difficulties by introducing an algorithm that guides the diffusion with a projected score. The projection pushes the image feature vector towards the feature vector centroid of the target class. The projected score and the feature vectors are learned by the same network. Specifically, the image feature vector is defined as the spatial averages of the channels activations in select layers of the network. Optimizing the projected score for denoising loss encourages image feature vectors of each class to cluster around their centroids. It also leads to the separations of the centroids. We show that these centroids provide a low-dimensional Euclidean embedding of the class conditional densities. We demonstrate that the algorithm can generate high quality and diverse samples from the conditioning class. Conditional generation can be performed using feature vectors interpolated between those of the training set, demonstrating out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2410.08466.pdf' target='_blank'>https://arxiv.org/pdf/2410.08466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eugene P. W. Ang, Shan Lin, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08466">Aligned Divergent Pathways for Omni-Domain Generalized Person Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person Re-identification (Person ReID) has advanced significantly in fully supervised and domain generalized Person R e ID. However, methods developed for one task domain transfer poorly to the other. An ideal Person ReID method should be effective regardless of the number of domains involved in training or testing. Furthermore, given training data from the target domain, it should perform at least as well as state-of-the-art (SOTA) fully supervised Person ReID methods. We call this paradigm Omni-Domain Generalization Person ReID, referred to as ODG-ReID, and propose a way to achieve this by expanding compatible backbone architectures into multiple diverse pathways. Our method, Aligned Divergent Pathways (ADP), first converts a base architecture into a multi-branch structure by copying the tail of the original backbone. We design our module Dynamic Max-Deviance Adaptive Instance Normalization (DyMAIN) that encourages learning of generalized features that are robust to omni-domain directions and apply DyMAIN to the branches of ADP. Our proposed Phased Mixture-of-Cosines (PMoC) coordinates a mix of stable and turbulent learning rate schedules among branches for further diversified learning. Finally, we realign the feature space between branches with our proposed Dimensional Consistency Metric Loss (DCML). ADP outperforms the state-of-the-art (SOTA) results for multi-source domain generalization and supervised ReID within the same domain. Furthermore, our method demonstrates improvement on a wide range of single-source domain generalization benchmarks, achieving Omni-Domain Generalization over Person ReID tasks.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2410.08460.pdf' target='_blank'>https://arxiv.org/pdf/2410.08460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eugene P. W. Ang, Shan Lin, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08460">Diverse Deep Feature Ensemble Learning for Omni-Domain Generalized Person Re-identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person Re-identification (Person ReID) has progressed to a level where single-domain supervised Person ReID performance has saturated. However, such methods experience a significant drop in performance when trained and tested across different datasets, motivating the development of domain generalization techniques. However, our research reveals that domain generalization methods significantly underperform single-domain supervised methods on single dataset benchmarks. An ideal Person ReID method should be effective regardless of the number of domains involved, and when test domain data is available for training it should perform as well as state-of-the-art (SOTA) fully supervised methods. This is a paradigm that we call Omni-Domain Generalization Person ReID (ODG-ReID). We propose a way to achieve ODG-ReID by creating deep feature diversity with self-ensembles. Our method, Diverse Deep Feature Ensemble Learning (D2FEL), deploys unique instance normalization patterns that generate multiple diverse views and recombines these views into a compact encoding. To the best of our knowledge, our work is one of few to consider omni-domain generalization in Person ReID, and we advance the study of applying feature ensembles in Person ReID. D2FEL significantly improves and matches the SOTA performance for major domain generalization and single-domain supervised benchmarks.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2410.04717.pdf' target='_blank'>https://arxiv.org/pdf/2410.04717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dylan Zhang, Justin Wang, Francois Charton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04717">$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization $\textbf{only emerges}$ when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of $\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2409.18371.pdf' target='_blank'>https://arxiv.org/pdf/2409.18371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai V. Nguyen, Jau-Uei Chen, Tan Bui-Thanh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18371">A Model-Constrained Discontinuous Galerkin Network (DGNet) for Compressible Euler Equations with Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time accurate solutions of large-scale complex dynamical systems are critically needed for control, optimization, uncertainty quantification, and decision-making in practical engineering and science applications, particularly in digital twin contexts. In this work, we develop a model-constrained discontinuous Galerkin Network (DGNet) approach, a significant extension to our previous work [Model-constrained Tagent Slope Learning Approach for Dynamical Systems], for compressible Euler equations with out-of-distribution generalization. The core of DGNet is the synergy of several key strategies: (i) leveraging time integration schemes to capture temporal correlation and taking advantage of neural network speed for computation time reduction; (ii) employing a model-constrained approach to ensure the learned tangent slope satisfies governing equations; (iii) utilizing a GNN-inspired architecture where edges represent Riemann solver surrogate models and nodes represent volume integration correction surrogate models, enabling capturing discontinuity capability, aliasing error reduction, and mesh discretization generalizability; (iv) implementing the input normalization technique that allows surrogate models to generalize across different initial conditions, geometries, meshes, boundary conditions, and solution orders; and (v) incorporating a data randomization technique that not only implicitly promotes agreement between surrogate models and true numerical models up to second-order derivatives, ensuring long-term stability and prediction capacity, but also serves as a data generation engine during training, leading to enhanced generalization on unseen data. To validate the effectiveness, stability, and generalizability of our novel DGNet approach, we present comprehensive numerical results for 1D and 2D compressible Euler equation problems.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2409.14671.pdf' target='_blank'>https://arxiv.org/pdf/2409.14671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Liu, Shu Wang, Zhe Qu, Xingyu Li, Shichao Kan, Jianxin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14671">FedGCA: Global Consistent Augmentation Based Single-Source Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Domain Generalization (FedDG) aims to train the global model for generalization ability to unseen domains with multi-domain training samples. However, clients in federated learning networks are often confined to a single, non-IID domain due to inherent sampling and temporal limitations. The lack of cross-domain interaction and the in-domain divergence impede the learning of domain-common features and limit the effectiveness of existing FedDG, referred to as the single-source FedDG (sFedDG) problem. To address this, we introduce the Federated Global Consistent Augmentation (FedGCA) method, which incorporates a style-complement module to augment data samples with diverse domain styles. To ensure the effective integration of augmented samples, FedGCA employs both global guided semantic consistency and class consistency, mitigating inconsistencies from local semantics within individual clients and classes across multiple clients. The conducted extensive experiments demonstrate the superiority of FedGCA.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2409.12450.pdf' target='_blank'>https://arxiv.org/pdf/2409.12450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mansoor Ali Teevno, Rafael Martinez-Garcia-Pena, Gilberto Ochoa-Ruiz, Sharib Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12450">Domain Generalization for Endoscopic Image Segmentation by Disentangling Style-Content Information and SuperPixel Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Frequent monitoring is necessary to stratify individuals based on their likelihood of developing gastrointestinal (GI) cancer precursors. In clinical practice, white-light imaging (WLI) and complementary modalities such as narrow-band imaging (NBI) and fluorescence imaging are used to assess risk areas. However, conventional deep learning (DL) models show degraded performance due to the domain gap when a model is trained on one modality and tested on a different one. In our earlier approach, we used a superpixel-based method referred to as "SUPRA" to effectively learn domain-invariant information using color and space distances to generate groups of pixels. One of the main limitations of this earlier work is that the aggregation does not exploit structural information, making it suboptimal for segmentation tasks, especially for polyps and heterogeneous color distributions. Therefore, in this work, we propose an approach for style-content disentanglement using instance normalization and instance selective whitening (ISW) for improved domain generalization when combined with SUPRA. We evaluate our approach on two datasets: EndoUDA Barrett's Esophagus and EndoUDA polyps, and compare its performance with three state-of-the-art (SOTA) methods. Our findings demonstrate a notable enhancement in performance compared to both baseline and SOTA methods across the target domain data. Specifically, our approach exhibited improvements of 14%, 10%, 8%, and 18% over the baseline and three SOTA methods on the polyp dataset. Additionally, it surpassed the second-best method (EndoUDA) on the Barrett's Esophagus dataset by nearly 2%.
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2409.09611.pdf' target='_blank'>https://arxiv.org/pdf/2409.09611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cagri Gungor, Adriana Kovashka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09611">Integrating Audio Narrations to Strengthen Domain Generalization in Multimodal First-Person Action Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>First-person activity recognition is rapidly growing due to the widespread use of wearable cameras but faces challenges from domain shifts across different environments, such as varying objects or background scenes. We propose a multimodal framework that improves domain generalization by integrating motion, audio, and appearance features. Key contributions include analyzing the resilience of audio and motion features to domain shifts, using audio narrations for enhanced audio-text alignment, and applying consistency ratings between audio and visual narrations to optimize the impact of audio in recognition during training. Our approach achieves state-of-the-art performance on the ARGO1M dataset, effectively generalizing across unseen scenarios and locations.
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2409.07308.pdf' target='_blank'>https://arxiv.org/pdf/2409.07308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Sun, Panagiotis Kosmas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07308">Non-Invasive Glucose Prediction System Enhanced by Mixed Linear Models and Meta-Forests for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we present a non-invasive glucose prediction system that integrates Near-Infrared (NIR) spectroscopy and millimeter-wave (mm-wave) sensing. We employ a Mixed Linear Model (MixedLM) to analyze the association between mm-wave frequency S_21 parameters and blood glucose levels within a heterogeneous dataset. The MixedLM method considers inter-subject variability and integrates multiple predictors, offering a more comprehensive analysis than traditional correlation analysis. Additionally, we incorporate a Domain Generalization (DG) model, Meta-forests, to effectively handle domain variance in the dataset, enhancing the model's adaptability to individual differences. Our results demonstrate promising accuracy in glucose prediction for unseen subjects, with a mean absolute error (MAE) of 17.47 mg/dL, a root mean square error (RMSE) of 31.83 mg/dL, and a mean absolute percentage error (MAPE) of 10.88%, highlighting its potential for clinical application. This study marks a significant step towards developing accurate, personalized, and non-invasive glucose monitoring systems, contributing to improved diabetes management.
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2601.17862.pdf' target='_blank'>https://arxiv.org/pdf/2601.17862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingsong Xia, Siqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17862">Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2601.17641.pdf' target='_blank'>https://arxiv.org/pdf/2601.17641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Fang, Ryan A. Canfield, Tomohiro Ouchi, Beatrice Macagno, Eli Shlizerman, Amy L. Orsborn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17641">RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2601.16064.pdf' target='_blank'>https://arxiv.org/pdf/2601.16064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shams Nafisa Ali, Taufiq Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16064">Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2601.08404.pdf' target='_blank'>https://arxiv.org/pdf/2601.08404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Binh Duong Nguyen, Stefan Sandfeld
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08404">Out-of-distribution generalization of deep-learning surrogates for 2D PDE-generated dynamics in the small-data regime</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial differential equations (PDEs) are a central tool for modeling the dynamics of physical, engineering, and materials systems, but high-fidelity simulations are often computationally expensive. At the same time, many scientific applications can be viewed as the evolution of spatially distributed fields, making data-driven forecasting of such fields a core task in scientific machine learning. In this work we study autoregressive deep-learning surrogates for two-dimensional PDE dynamics on periodic domains, focusing on generalization to out-of-distribution initial conditions within a fixed PDE and parameter regime and on strict small-data settings with at most $\mathcal{O}(10^2)$ simulated trajectories per system. We introduce a multi-channel U-Net [...], evaluate it on five qualitatively different PDE families and compare it to ViT, AFNO, PDE-Transformer, and KAN-UNet under a common training setup. Across all datasets, me-UNet matches or outperforms these more complex architectures in terms of field-space error, spectral similarity, and physics-based metrics for in-distribution rollouts, while requiring substantially less training time. It also generalizes qualitatively to unseen initial conditions with as few as $\approx 20$ training simulations. A data-efficiency study and Grad-CAM analysis further suggest that, in small-data periodic 2D PDE settings, convolutional architectures with inductive biases aligned to locality and periodic boundary conditions remain strong contenders for accurate and moderately out-of-distribution-robust surrogate modeling.
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2601.07261.pdf' target='_blank'>https://arxiv.org/pdf/2601.07261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haomin Wu, Zhiwei Nie, Hongyu Zhang, Zhixiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07261">Pseudodata-guided Invariant Representation Learning Boosts the Out-of-Distribution Generalization in Enzymatic Kinetic Parameter Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate prediction of enzyme kinetic parameters is essential for understanding catalytic mechanisms and guiding enzyme engineering.However, existing deep learning-based enzyme-substrate interaction (ESI) predictors often exhibit performance degradation on sequence-divergent, out-of-distribution (OOD) cases, limiting robustness under biologically relevant perturbations.We propose O$^2$DENet, a lightweight, plug-and-play module that enhances OOD generalization via biologically and chemically informed perturbation augmentation and invariant representation learning.O$^2$DENet introduces enzyme-substrate perturbations and enforces consistency between original and augmented enzyme-substrate-pair representations to encourage invariance to distributional shifts.When integrated with representative ESI models, O$^2$DENet consistently improves predictive performance for both $k_{cat}$ and $K_m$ across stringent sequence-identity-based OOD benchmarks, achieving state-of-the-art results among the evaluated methods in terms of accuracy and robustness metrics.Overall, O$^2$DENet provides a general and effective strategy to enhance the stability and deployability of data-driven enzyme kinetics predictors for real-world enzyme engineering applications.
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2601.06169.pdf' target='_blank'>https://arxiv.org/pdf/2601.06169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyong Ma, Zhenpeng Li, Yuanjie Shi, Zhengping Li, Jiahao Chen, Qingyuan Chuai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06169">Think Bright, Diffuse Nice: Enhancing T2I-ICL via Inductive-Bias Hint Instruction and Query Contrastive Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Image In-Context Learning (T2I-ICL) enables customized image synthesis via interleaved text-image examples but faces two mutually reinforcing bottlenecks, compliance failure and prior-dominated hallucination, that form a vicious cycle degrading generation quality. Existing methods rely on tailored training, which limits flexibility and raises deployment costs. To address these challenges effectively, we propose TBDN, a training-free framework integrating two complementary closed-loop mechanisms: Hint Instruction (HI) and Query Contrastive Decoding (QCD). HI injects task-aware inductive bias via lightweight prompt engineering to anchor models on contextual mapping rules, thereby mitigating compliance failure. QCD adjusts the decoding distributions of language models by contrasting full-input and query-omitted distributions, suppressing prior-dominated hallucination. TBDN achieves State-of-the-Art performance on CoBSAT and Text-to-Image Fast Mini-ImageNet, with robust generalization across model backbones, prompt designs, and hyperparameters. It also maintains promising performance in concept preservation and prompt following on Dreambench++. By breaking the two bottlenecks, TBDN establishes a simple yet effective framework for efficient and reliable T2I-ICL.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2601.03056.pdf' target='_blank'>https://arxiv.org/pdf/2601.03056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Wang, Jiaojiao Zhao, Qilong Wang, Yongfeng Dong, Wenlong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03056">Fine-Grained Generalization via Structuralizing Concept and Feature Space into Commonality, Specificity and Confounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-Grained Domain Generalization (FGDG) presents greater challenges than conventional domain generalization due to the subtle inter-class differences and relatively pronounced intra-class variations inherent in fine-grained recognition tasks. Under domain shifts, the model becomes overly sensitive to fine-grained cues, leading to the suppression of critical features and a significant drop in performance. Cognitive studies suggest that humans classify objects by leveraging both common and specific attributes, enabling accurate differentiation between fine-grained categories. However, current deep learning models have yet to incorporate this mechanism effectively. Inspired by this mechanism, we propose Concept-Feature Structuralized Generalization (CFSG). This model explicitly disentangles both the concept and feature spaces into three structured components: common, specific, and confounding segments. To mitigate the adverse effects of varying degrees of distribution shift, we introduce an adaptive mechanism that dynamically adjusts the proportions of common, specific, and confounding components. In the final prediction, explicit weights are assigned to each pair of components. Extensive experiments on three single-source benchmark datasets demonstrate that CFSG achieves an average performance improvement of 9.87% over baseline models and outperforms existing state-of-the-art methods by an average of 3.08%. Additionally, explainability analysis validates that CFSG effectively integrates multi-granularity structured knowledge and confirms that feature structuralization facilitates the emergence of concept structuralization.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2601.01992.pdf' target='_blank'>https://arxiv.org/pdf/2601.01992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Zhu, Huiwen Zhang, Yujie Li, Mu He, Xiaotian Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01992">API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2512.23065.pdf' target='_blank'>https://arxiv.org/pdf/2512.23065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Melikşah Türker, A. Ebrar Kızıloğlu, Onur Güngör, Susan Üsküdarlı
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.23065">TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). The model supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories: question answering (+9.55), code retrieval (+2.41), and document retrieval (+0.60). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2512.06811.pdf' target='_blank'>https://arxiv.org/pdf/2512.06811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Lin, Weixin Li, Shu Guo, Lihong Wang, Di Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06811">RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2512.06524.pdf' target='_blank'>https://arxiv.org/pdf/2512.06524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saekwang Nam, Bowen Deng, Loong Yi Lee, Jonathan M. Rossiter, Nathan F. Lepora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06524">TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2512.05711.pdf' target='_blank'>https://arxiv.org/pdf/2512.05711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Krayani, Seyedeh Fatemeh Sadati, Lucio Marcenaro, Carlo Regazzoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05711">Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2511.19846.pdf' target='_blank'>https://arxiv.org/pdf/2511.19846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas M Metz, Matthew Q Hill, Alice J O'Toole
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19846">Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision foundation models can perform generalized object classification in zero-shot mode, and face/person recognition when they are fine-tuned. However, fine-tuned models suffer from catastrophic forgetting. We create models that perform four tasks (object recognition, face recognition from high- and low-quality images, and person recognition from whole-body images) in a single embedding space -- without incurring substantial catastrophic forgetting. To accomplish this, we introduce two variants of the Interleaved Multi-Domain Identity Curriculum (IMIC): a gradient-coupled, interleaving training schedule that fine-tunes a foundation backbone simultaneously on all four tasks. The IMIC method proved effective with three foundation model bases: DINOv3, CLIP, and EVA-02. Two of these (EVA-02 and CLIP) performed comparably with domain experts on all four tasks concurrently and were more accurate than humans at multi-tasking across face, body, and object datasets. Further, we demonstrate that our approach does not substantially harm out-of-distribution generalization, thus maintaining a key property of foundation models. Analysis of the most accurate model variants (EVA-02 + IMIC A and B) showed linearly separable representations of the four tasks in the unified embedding space, but with substantial sharing of features across tasks. Fewer than 100 PCs calculated from any one task could perform all other tasks with nearly zero performance degradation.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2511.12213.pdf' target='_blank'>https://arxiv.org/pdf/2511.12213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Xue, Haoyu Liu, Yajun Tian, Xinyu Zhong, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12213">MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2511.11368.pdf' target='_blank'>https://arxiv.org/pdf/2511.11368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Liu, Yuanzhi Liang, Sidan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11368">Free3D: 3D Human Motion Emerges from Single-View 2D Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions. This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations. Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility. Trained entirely on 2D motion data, Free3D generates diverse, temporally coherent, and semantically aligned 3D motions, achieving performance comparable to or even surpassing fully 3D-supervised counterparts. These results suggest that relaxing explicit 3D supervision encourages stronger structural reasoning and generalization, offering a scalable and data-efficient paradigm for 3D motion generation.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2511.00486.pdf' target='_blank'>https://arxiv.org/pdf/2511.00486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pooja Singh, Shashwat Bhardwaj, Vaibhav Sharma, Sandeep Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00486">Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The linguistic diversity of India poses significant machine translation challenges, especially for underrepresented tribal languages like Bhili, which lack high-quality linguistic resources. This paper addresses the gap by introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest parallel corpus worldwide comprising 110,000 meticulously curated sentences across Bhili, Hindi, and English. The corpus was created with the assistance of expert human translators. BHEPC spans critical domains such as education, administration, and news, establishing a valuable benchmark for research in low resource machine translation. To establish a comprehensive Bhili Machine Translation benchmark, we evaluated a wide range of proprietary and open-source Multilingual Large Language Models (MLLMs) on bidirectional translation tasks between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the fine-tuned NLLB-200 distilled 600M variant model outperforms others, highlighting the potential of multilingual models in low resource scenarios. Furthermore, we investigated the generative translation capabilities of multilingual LLMs on BHEPC using in-context learning, assessing performance under cross-domain generalization and quantifying distributional divergence. This work bridges a critical resource gap and promotes inclusive natural language processing technologies for low-resource and marginalized languages globally.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2510.24000.pdf' target='_blank'>https://arxiv.org/pdf/2510.24000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heethanjan Kanagalingam, Thenukan Pathmanathan, Mokeeshan Vathanakumar, Tharmakulasingam Mukunthan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24000">AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet early and accurate detection can significantly improve treatment outcomes. While numerous Deep learning (DL) models have been developed to predict DR from fundus images, many face challenges in maintaining robustness due to distributional variations caused by differences in acquisition devices, demographic disparities, and imaging conditions. This paper addresses this critical limitation by proposing a novel DR classification approach, a method called AdvBlur. Our method integrates adversarial blurred images into the dataset and employs a dual-loss function framework to address domain generalization. This approach effectively mitigates the impact of unseen distributional variations, as evidenced by comprehensive evaluations across multiple datasets. Additionally, we conduct extensive experiments to explore the effects of factors such as camera type, low-quality images, and dataset size. Furthermore, we perform ablation studies on blurred images and the loss function to ensure the validity of our choices. The experimental results demonstrate the effectiveness of our proposed method, achieving competitive performance compared to state-of-the-art domain generalization DR models on unseen external datasets.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2510.19475.pdf' target='_blank'>https://arxiv.org/pdf/2510.19475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoyang Xie, Yibo Zhao, Hui Huang, Riwei Wang, Zan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19475">PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D human pose estimation remains a fundamentally ill-posed inverse problem due to the inherent depth ambiguity in 2D-to-3D lifting. While contemporary video-based methods leverage temporal context to enhance spatial reasoning, they operate under a critical paradigm limitation: processing each sequence in isolation, thereby failing to exploit the strong structural regularities and repetitive motion patterns that pervade human movement across sequences. This work introduces the Pattern Reuse Graph Convolutional Network (PRGCN), a novel framework that formalizes pose estimation as a problem of pattern retrieval and adaptation. At its core, PRGCN features a graph memory bank that learns and stores a compact set of pose prototypes, encoded as relational graphs, which are dynamically retrieved via an attention mechanism to provide structured priors. These priors are adaptively fused with hard-coded anatomical constraints through a memory-driven graph convolution, ensuring geometrical plausibility. To underpin this retrieval process with robust spatiotemporal features, we design a dual-stream hybrid architecture that synergistically combines the linear-complexity, local temporal modeling of Mamba-based state-space models with the global relational capacity of self-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarks demonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPE of 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domain generalization capability. Our work posits that the long-overlooked mechanism of cross-sequence pattern reuse is pivotal to advancing the field, shifting the paradigm from per-sequence optimization towards cumulative knowledge learning.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2510.19305.pdf' target='_blank'>https://arxiv.org/pdf/2510.19305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chirag Padubidri, Pranesh Velmurugan, Andreas Lanitis, Andreas Kamilaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19305">FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring species distribution is vital for conservation efforts, enabling the assessment of environmental impacts and the development of effective preservation strategies. Traditional data collection methods, including citizen science, offer valuable insights but remain limited in coverage and completeness. Species Distribution Modelling (SDM) helps address these gaps by using occurrence data and environmental variables to predict species presence across large regions. In this study, we enhance SDM accuracy for frogs (Anura) by applying deep learning and data imputation techniques using data from the "EY - 2022 Biodiversity Challenge." Our experiments show that data balancing significantly improved model performance, reducing the Mean Absolute Error (MAE) from 189 to 29 in frog counting tasks. Feature selection identified key environmental factors influencing occurrence, optimizing inputs while maintaining predictive accuracy. The multimodal ensemble model, integrating land cover, NDVI, and other environmental inputs, outperformed individual models and showed robust generalization across unseen regions. The fusion of image and tabular data improved both frog counting and habitat classification, achieving 84.9% accuracy with an AUC of 0.90. This study highlights the potential of multimodal learning and data preprocessing techniques such as balancing and imputation to improve predictive ecological modeling when data are sparse or incomplete, contributing to more precise and scalable biodiversity monitoring.
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2510.18268.pdf' target='_blank'>https://arxiv.org/pdf/2510.18268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Song, Chenxi Li, Haokang Ding, Zhining Liao, Zhifang Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18268">TreeFedDG: Alleviating Global Drift in Federated Domain Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In medical image segmentation tasks, Domain Generalization (DG) under the Federated Learning (FL) framework is crucial for addressing challenges related to privacy protection and data heterogeneity. However, traditional federated learning methods fail to account for the imbalance in information aggregation across clients in cross-domain scenarios, leading to the Global Drift (GD) problem and a consequent decline in model generalization performance. This motivates us to delve deeper and define a new critical issue: global drift in federated domain generalization for medical imaging (FedDG-GD). In this paper, we propose a novel tree topology framework called TreeFedDG. First, starting from the distributed characteristics of medical images, we design a hierarchical parameter aggregation method based on a tree-structured topology to suppress deviations in the global model direction. Second, we introduce a parameter difference-based style mixing method (FedStyle), which enforces mixing among clients with maximum parameter differences to enhance robustness against drift. Third, we develop a a progressive personalized fusion strategy during model distribution, ensuring a balance between knowledge transfer and personalized features. Finally, during the inference phase, we use feature similarity to guide the retrieval of the most relevant model chain from the tree structure for ensemble decision-making, thereby fully leveraging the advantages of hierarchical knowledge. We conducted extensive experiments on two publicly available datasets. The results demonstrate that our method outperforms other state-of-the-art domain generalization approaches in these challenging tasks and achieves better balance in cross-domain performance.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2510.18052.pdf' target='_blank'>https://arxiv.org/pdf/2510.18052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arman Behnam, Binghui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18052">Measure-Theoretic Anti-Causal Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal representation learning in the anti-causal setting (labels cause features rather than the reverse) presents unique challenges requiring specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a novel measure-theoretic framework for anti-causal representation learning. ACIA employs a two-level design, low-level representations capture how labels generate observations, while high-level representations learn stable causal patterns across environment-specific variations. ACIA addresses key limitations of existing approaches by accommodating prefect and imperfect interventions through interventional kernels, eliminating dependency on explicit causal structures, handling high-dimensional data effectively, and providing theoretical guarantees for out-of-distribution generalization. Experiments on synthetic and real-world medical datasets demonstrate that ACIA consistently outperforms state-of-the-art methods in both accuracy and invariance metrics. Furthermore, our theoretical results establish tight bounds on performance gaps between training and unseen environments, confirming the efficacy of our approach for robust anti-causal learning.
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2510.16913.pdf' target='_blank'>https://arxiv.org/pdf/2510.16913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akhila Kambhatla, Ahmed R Khaled
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16913">Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thermal weapon segmentation is crucial for surveillance and security applications, enabling robust detection under lowlight and visually obscured conditions where RGB-based systems fail. While convolutional neural networks (CNNs) dominate thermal segmentation literature, their ability to capture long-range dependencies and fine structural details is limited. Vision Transformers (ViTs), with their global context modeling capabilities, have achieved state-of-the-art results in RGB segmentation tasks, yet their potential in thermal weapon segmentation remains underexplored. This work adapts and evaluates four transformer-based architectures SegFormer, DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a custom thermal dataset comprising 9,711 images collected from real world surveillance videos and automatically annotated using SAM2. We employ standard augmentation strategies within the MMSegmentation framework to ensure robust model training and fair architectural comparison. Experimental results demonstrate significant improvements in segmentation performance: SegFormer-b5 achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and 92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The transformer architectures demonstrate robust generalization capabilities for weapon detection in low-light and occluded thermal environments, with flexible accuracy-speed trade-offs suitable for diverse real-time security applications.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2508.16012.pdf' target='_blank'>https://arxiv.org/pdf/2508.16012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Circe Hsu, Claire Schlesinger, Karan Mudaliar, Jordan Leung, Robin Walters, Peter Schindler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16012">FIRE-GNN: Force-informed, Relaxed Equivariance Graph Neural Network for Rapid and Accurate Prediction of Surface Properties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The work function and cleavage energy of a surface are critical properties that determine the viability of materials in electronic emission applications, semiconductor devices, and heterogeneous catalysis. While first principles calculations are accurate in predicting these properties, their computational expense combined with the vast search space of surfaces make a comprehensive screening approach with density functional theory (DFT) infeasible. Here, we introduce FIRE-GNN (Force-Informed, Relaxed Equivariance Graph Neural Network), which integrates surface-normal symmetry breaking and machine learning interatomic potential (MLIP)-derived force information, achieving a twofold reduction in mean absolute error (down to 0.065 eV) over the previous state-of-the-art for work function prediction. We additionally benchmark recent invariant and equivariant architectures, analyze the impact of symmetry breaking, and evaluate out-of-distribution generalization, demonstrating that FIRE-GNN consistently outperforms competing models for work function predictions. This model enables accurate and rapid predictions of the work function and cleavage energy across a vast chemical space and facilitates the discovery of materials with tuned surface properties
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2508.15452.pdf' target='_blank'>https://arxiv.org/pdf/2508.15452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>UÄurcan AkyÃ¼z, Deniz Katircioglu-ÃztÃ¼rk, Emre K. SÃ¼slÃ¼, Burhan KeleÅ, Mete C. Kaya, Gamze Durhan, Meltem G. AkpÄ±nar, Figen B. DemirkazÄ±k, GÃ¶zde B. Akar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15452">DoSReMC: Domain Shift Resilient Mammography Classification using Batch Normalization Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Numerous deep learning-based solutions have been developed for the automatic recognition of breast cancer using mammography images. However, their performance often declines when applied to data from different domains, primarily due to domain shift -- the variation in data distributions between source and target domains. This performance drop limits the safe and equitable deployment of AI in real-world clinical settings. In this study, we present DoSReMC (Domain Shift Resilient Mammography Classification), a batch normalization (BN) adaptation framework designed to enhance cross-domain generalization without retraining the entire model. Using three large-scale full-field digital mammography (FFDM) datasets -- including HCTP, a newly introduced, pathologically confirmed in-house dataset -- we conduct a systematic cross-domain evaluation with convolutional neural networks (CNNs). Our results demonstrate that BN layers are a primary source of domain dependence: they perform effectively when training and testing occur within the same domain, and they significantly impair model generalization under domain shift. DoSReMC addresses this limitation by fine-tuning only the BN and fully connected (FC) layers, while preserving pretrained convolutional filters. We further integrate this targeted adaptation with an adversarial training scheme, yielding additional improvements in cross-domain generalizability. DoSReMC can be readily incorporated into existing AI pipelines and applied across diverse clinical environments, providing a practical pathway toward more robust and generalizable mammography classification systems.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2508.10026.pdf' target='_blank'>https://arxiv.org/pdf/2508.10026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhao, Yanjun Zhao, Jiaming Song, Shien He, Lusheng Zhang, Qiang Zhang, Tianjiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10026">SABER: Switchable and Balanced Training for Efficient LLM Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems. We propose SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. SABER first profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. In parallel, we incorporate no-think examples to ensure the model remains reliable even when explicit reasoning is turned off. SABER further supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling flexible trade-offs between latency and reasoning depth. Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2507.20028.pdf' target='_blank'>https://arxiv.org/pdf/2507.20028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dhruv Sarkar, Aprameyo Chakrabartty, Bibhudatta Bhanja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20028">TAPS : Frustratingly Simple Test Time Active Learning for VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-Time Optimization enables models to adapt to new data during inference by updating parameters on-the-fly. Recent advances in Vision-Language Models (VLMs) have explored learning prompts at test time to improve performance in downstream tasks. In this work, we extend this idea by addressing a more general and practical challenge: Can we effectively utilize an oracle in a continuous data stream where only one sample is available at a time, requiring an immediate query decision while respecting latency and memory constraints? To tackle this, we propose a novel Test-Time Active Learning (TTAL) framework that adaptively queries uncertain samples and updates prompts dynamically. Unlike prior methods that assume batched data or multiple gradient updates, our approach operates in a real-time streaming scenario with a single test sample per step. We introduce a dynamically adjusted entropy threshold for active querying, a class-balanced replacement strategy for memory efficiency, and a class-aware distribution alignment technique to enhance adaptation. The design choices are justified using careful theoretical analysis. Extensive experiments across 10 cross-dataset transfer benchmarks and 4 domain generalization datasets demonstrate consistent improvements over state-of-the-art methods while maintaining reasonable latency and memory overhead. Our framework provides a practical and effective solution for real-world deployment in safety-critical applications such as autonomous systems and medical diagnostics.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2507.17326.pdf' target='_blank'>https://arxiv.org/pdf/2507.17326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milena Davudova, Ziyuan Cai, Valentina Giunchiglia, Dragos C. Gruia, Giulia Sanguedolce, Adam Hampshire, Fatemeh Geranmayeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17326">Application of Whisper in Clinical Practice: the Post-Stroke Speech Assessment during a Naming Task</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detailed assessment of language impairment following stroke remains a cognitively complex and clinician-intensive task, limiting timely and scalable diagnosis. Automatic Speech Recognition (ASR) foundation models offer a promising pathway to augment human evaluation through intelligent systems, but their effectiveness in the context of speech and language impairment remains uncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASR foundation model, can be applied to transcribe and analyze speech from patients with stroke during a commonly used picture-naming task. We assess both verbatim transcription accuracy and the model's ability to support downstream prediction of language function, which has major implications for outcomes after stroke. Our results show that the baseline Whisper model performs poorly on single-word speech utterances. Nevertheless, fine-tuning Whisper significantly improves transcription accuracy (reducing Word Error Rate by 87.72% in healthy speech and 71.22% in speech from patients). Further, learned representations from the model enable accurate prediction of speech quality (average F1 Macro of 0.74 for healthy, 0.75 for patients). However, evaluations on an unseen (TORGO) dataset reveal limited generalizability, highlighting the inability of Whisper to perform zero-shot transcription of single-word utterances on out-of-domain clinical speech and emphasizing the need to adapt models to specific clinical populations. While challenges remain in cross-domain generalization, these findings highlight the potential of foundation models, when appropriately fine-tuned, to advance automated speech and language assessment and rehabilitation for stroke-related impairments.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2507.04494.pdf' target='_blank'>https://arxiv.org/pdf/2507.04494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niels Leadholm, Viviane Clay, Scott Knudstrup, Hojae Lee, Jeff Hawkins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04494">Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current AI systems achieve impressive performance on many tasks, yet they lack core attributes of biological intelligence, including rapid, continual learning, representations grounded in sensorimotor interactions, and structured knowledge that enables efficient generalization. Neuroscience theory suggests that mammals evolved flexible intelligence through the replication of a semi-independent, sensorimotor module, a functional unit known as a cortical column. To address the disparity between biological and artificial intelligence, thousand-brains systems were proposed as a means of mirroring the architecture of cortical columns and their interactions.
  In the current work, we evaluate the unique properties of Monty, the first implementation of a thousand-brains system. We focus on 3D object perception, and in particular, the combined task of object recognition and pose estimation. Utilizing the YCB dataset of household objects, we first assess Monty's use of sensorimotor learning to build structured representations, finding that these enable robust generalization. These representations include an emphasis on classifying objects by their global shape, as well as a natural ability to detect object symmetries. We then explore Monty's use of model-free and model-based policies to enable rapid inference by supporting principled movements. We find that such policies complement Monty's modular architecture, a design that can accommodate communication between modules to further accelerate inference speed via a novel `voting' algorithm. Finally, we examine Monty's use of associative, Hebbian-like binding to enable rapid, continual, and computationally efficient learning, properties that compare favorably to current deep learning architectures. While Monty is still in a nascent stage of development, these findings support thousand-brains systems as a powerful and promising new approach to AI.
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2507.02365.pdf' target='_blank'>https://arxiv.org/pdf/2507.02365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Usama, Dong Eui Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02365">Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equalizer parameter optimization for signal integrity in high-speed Dynamic Random Access Memory systems is crucial but often computationally demanding or model-reliant. This paper introduces a data-driven framework employing learned latent signal representations for efficient signal integrity evaluation, coupled with a model-free Advantage Actor-Critic reinforcement learning agent for parameter optimization. The latent representation captures vital signal integrity features, offering a fast alternative to direct eye diagram analysis during optimization, while the reinforcement learning agent derives optimal equalizer settings without explicit system models. Applied to industry-standard Dynamic Random Access Memory waveforms, the method achieved significant eye-opening window area improvements: 42.7\% for cascaded Continuous-Time Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for Decision Feedback Equalizer-only configurations. These results demonstrate superior performance, computational efficiency, and robust generalization across diverse Dynamic Random Access Memory units compared to existing techniques. Core contributions include an efficient latent signal integrity metric for optimization, a robust model-free reinforcement learning strategy, and validated superior performance for complex equalizer architectures.
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2506.08654.pdf' target='_blank'>https://arxiv.org/pdf/2506.08654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ciro Benito Raggio, Paolo Zaffino, Maria Francesca Spadea
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08654">A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise, limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield unit values and hindering direct dose calculation. Synthetic CT (sCT) generation from CBCT addresses these issues, especially using deep learning (DL) methods. Existing approaches are limited by institutional heterogeneity, scanner-dependent variations, and data privacy regulations that prevent multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region, extending our FedSynthCT framework. A conditional generative adversarial network was collaboratively trained on data from three European medical centers in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers, with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$ HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$, and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB. Notably, on an external validation dataset of 60 patients, comparable performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR: $33.52\pm2.06$ dB) without additional training, confirming robust generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT synthesis while preserving data privacy and offer a collaborative solution for developing generalizable models across institutions without centralized data sharing or site-specific fine-tuning.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2506.08379.pdf' target='_blank'>https://arxiv.org/pdf/2506.08379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yurun Yuan, Tengyang Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08379">Reinforce LLM Reasoning through Multi-Agent Reflection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2505.24592.pdf' target='_blank'>https://arxiv.org/pdf/2505.24592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weebum Yoo, Sung Whan Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24592">A Flat Minima Perspective on Understanding Augmentations and Model Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. Data augmentation is one of the prevalent and effective ways to enhance robustness. Despite the great success of augmentations in different fields, a general theoretical understanding of their efficacy in improving model robustness is lacking. We offer a unified theoretical framework to clarify how augmentations can enhance model robustness through the lens of loss surface flatness and PAC generalization bound. Our work diverges from prior studies in that our analysis i) broadly encompasses much of the existing augmentation methods, and ii) is not limited to specific types of distribution shifts like adversarial attacks. We confirm our theories through simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets, as well as domain generalization benchmarks including PACS and OfficeHome.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2505.19493.pdf' target='_blank'>https://arxiv.org/pdf/2505.19493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Zhao, Xueliang Zhang, Zhong-Qiu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19493">Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Acoustic echo cancellation (AEC) is an important speech signal processing technology that can remove echoes from microphone signals to enable natural-sounding full-duplex speech communication. While single-channel AEC is widely adopted, multi-channel AEC can leverage spatial cues afforded by multiple microphones to achieve better performance. Existing multi-channel AEC approaches typically combine beamforming with deep neural networks (DNN). This work proposes a two-stage algorithm that enhances multi-channel AEC by incorporating sound source directional cues. Specifically, a lightweight DNN is first trained to predict the sound source directions, and then the predicted directional information, multi-channel microphone signals, and single-channel far-end signal are jointly fed into an AEC network to estimate the near-end signal. Evaluation results show that the proposed algorithm outperforms baseline approaches and exhibits robust generalization across diverse acoustic environments.
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2505.08392.pdf' target='_blank'>https://arxiv.org/pdf/2505.08392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ren Zhuang, Ben Wang, Shuifa Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08392">Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2505.06831.pdf' target='_blank'>https://arxiv.org/pdf/2505.06831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaoyun Zhao, Qiang Zhang, Chenrong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06831">Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving group-robust generalization in the presence of spurious correlations remains a significant challenge, particularly when bias annotations are unavailable. Recent studies on Class-Conditional Distribution Balancing (CCDB) reveal that spurious correlations often stem from mismatches between the class-conditional and marginal distributions of bias attributes. They achieve promising results by addressing this issue through simple distribution matching in a bias-agnostic manner. However, CCDB approximates each distribution using a single Gaussian, which is overly simplistic and rarely holds in real-world applications. To address this limitation, we propose a novel method called Bias Exploration via Overfitting (BEO), which captures each distribution in greater detail by modeling it as a mixture of latent groups. Building on these group-level descriptions, we introduce a fine-grained variant of CCDB, termed FG-CCDB, which performs more precise distribution matching and balancing within each group. Through group-level reweighting, FG-CCDB learns sample weights from a global perspective, achieving stronger mitigation of spurious correlations without incurring substantial storage or computational costs. Extensive experiments demonstrate that BEO serves as a strong proxy for ground-truth bias annotations and can be seamlessly integrated with bias-supervised methods. Moreover, when combined with FG-CCDB, our method performs on par with bias-supervised approaches on binary classification tasks and significantly outperforms them in highly biased multi-class scenarios.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2505.02124.pdf' target='_blank'>https://arxiv.org/pdf/2505.02124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samidha Verma, Arushi Goyal, Ananya Mathur, Ankit Anand, Sayan Ranu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02124">GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a program that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2503.22728.pdf' target='_blank'>https://arxiv.org/pdf/2503.22728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Fu, Ziqi Ni, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22728">Dual Audio-Centric Modality Coupling for Talking Head Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of audio-driven talking head videos is a key challenge in computer vision and graphics, with applications in virtual avatars and digital media. Traditional approaches often struggle with capturing the complex interaction between audio and facial dynamics, leading to lip synchronization and visual quality issues. In this paper, we propose a novel NeRF-based framework, Dual Audio-Centric Modality Coupling (DAMC), which effectively integrates content and dynamic features from audio inputs. By leveraging a dual encoder structure, DAMC captures semantic content through the Content-Aware Encoder and ensures precise visual synchronization through the Dynamic-Sync Encoder. These features are fused using a Cross-Synchronized Fusion Module (CSFM), enhancing content representation and lip synchronization. Extensive experiments show that our method outperforms existing state-of-the-art approaches in key metrics such as lip synchronization accuracy and image quality, demonstrating robust generalization across various audio inputs, including synthetic speech from text-to-speech (TTS) systems. Our results provide a promising solution for high-quality, audio-driven talking head generation and present a scalable approach for creating realistic talking heads.
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2502.20144.pdf' target='_blank'>https://arxiv.org/pdf/2502.20144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arthur Pignet, John Klein, Genevieve Robin, Antoine Olivier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20144">Robust sensitivity control in digital pathology via tile score distribution matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying digital pathology models across medical centers is challenging due to distribution shifts. Recent advances in domain generalization improve model transferability in terms of aggregated performance measured by the Area Under Curve (AUC). However, clinical regulations often require to control the transferability of other metrics, such as prescribed sensitivity levels. We introduce a novel approach to control the sensitivity of whole slide image (WSI) classification models, based on optimal transport and Multiple Instance Learning (MIL). Validated across multiple cohorts and tasks, our method enables robust sensitivity control with only a handful of calibration samples, providing a practical solution for reliable deployment of computational pathology systems.
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2502.18735.pdf' target='_blank'>https://arxiv.org/pdf/2502.18735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Harvey Chapman, Feras Dayoub, Will Browne, Christopher Lehnert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18735">QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural Language Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A domain shift exists between the large-scale, internet data used to train a Vision-Language Model (VLM) and the raw image streams collected by a robot. Existing adaptation strategies require the definition of a closed-set of classes, which is impractical for a robot that must respond to diverse natural language queries. In response, we present QueryAdapter; a novel framework for rapidly adapting a pre-trained VLM in response to a natural language query. QueryAdapter leverages unlabelled data collected during previous deployments to align VLM features with semantic classes related to the query. By optimising learnable prompt tokens and actively selecting objects for training, an adapted model can be produced in a matter of minutes. We also explore how objects unrelated to the query should be dealt with when using real-world data for adaptation. In turn, we propose the use of object captions as negative class labels, helping to produce better calibrated confidence scores during adaptation. Extensive experiments on ScanNet++ demonstrate that QueryAdapter significantly enhances object retrieval performance compared to state-of-the-art unsupervised VLM adapters and 3D scene graph methods. Furthermore, the approach exhibits robust generalization to abstract affordance queries and other datasets, such as Ego4D.
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2502.14376.pdf' target='_blank'>https://arxiv.org/pdf/2502.14376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14376">A Similarity Paradigm Through Textual Regularization Without Forgetting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2502.08757.pdf' target='_blank'>https://arxiv.org/pdf/2502.08757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Hasanzadeh Karkan, Ahmed Ibrahim, Jean-FranÃ§ois Frigon, FranÃ§ois Leduc-Primeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08757">A Low-Complexity Plug-and-Play Deep Learning Model for Massive MIMO Precoding Across Sites</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Massive multiple-input multiple-output (mMIMO) technology has transformed wireless communication by enhancing spectral efficiency and network capacity. This paper proposes a novel deep learning-based mMIMO precoder to tackle the complexity challenges of existing approaches, such as weighted minimum mean square error (WMMSE), while leveraging meta-learning domain generalization and a teacher-student architecture to improve generalization across diverse communication environments. When deployed to a previously unseen site, the proposed model achieves excellent sum-rate performance while maintaining low computational complexity by avoiding matrix inversions and by using a simpler neural network structure. The model is trained and tested on a custom ray-tracing dataset composed of several base station locations. The experimental results indicate that our method effectively balances computational efficiency with high sum-rate performance while showcasing strong generalization performance in unseen environments. Furthermore, with fine-tuning, the proposed model outperforms WMMSE across all tested sites and SNR conditions while reducing complexity by at least 73$\times$.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2501.09527.pdf' target='_blank'>https://arxiv.org/pdf/2501.09527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oleg Somov, Elena Tutubalina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09527">Confidence Estimation for Error Detection in Text-to-SQL Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-SQL enables users to interact with databases through natural language, simplifying the retrieval and synthesis of information. Despite the success of large language models (LLMs) in converting natural language questions into SQL queries, their broader adoption is limited by two main challenges: achieving robust generalization across diverse queries and ensuring interpretative confidence in their predictions. To tackle these issues, our research investigates the integration of selective classifiers into Text-to-SQL systems. We analyse the trade-off between coverage and risk using entropy based confidence estimation with selective classifiers and assess its impact on the overall performance of Text-to-SQL models. Additionally, we explore the models' initial calibration and improve it with calibration techniques for better model alignment between confidence and accuracy. Our experimental results show that encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and decoder-only Llama 3, thus the designated external entropy-based selective classifier has better performance. The study also reveal that, in terms of error detection, selective classifier with a higher probability detects errors associated with irrelevant questions rather than incorrect query generations.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2501.07378.pdf' target='_blank'>https://arxiv.org/pdf/2501.07378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Deng, Zhe Xu, Tsuyoshi Isshiki, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07378">FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation is challenging due to the diversity of medical images and the lack of labeled data, which motivates recent developments in federated semi-supervised learning (FSSL) to leverage a large amount of unlabeled data from multiple centers for model training without sharing raw data. However, what remains under-explored in FSSL is the domain shift problem which may cause suboptimal model aggregation and low effectivity of the utilization of unlabeled data, eventually leading to unsatisfactory performance in unseen domains. In this paper, we explore this previously ignored scenario, namely domain generalized federated semi-supervised learning (FedSemiDG), which aims to learn a model in a distributed manner from multiple domains with limited labeled data and abundant unlabeled data such that the model can generalize well to unseen domains. We present a novel framework, Federated Generalization-Aware SemiSupervised Learning (FGASL), to address the challenges in FedSemiDG by effectively tackling critical issues at both global and local levels. Globally, we introduce Generalization-Aware Aggregation (GAA), assigning adaptive weights to local models based on their generalization performance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement (DR) strategy to combine global and domain-specific knowledge, generating more reliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA) enforces feature consistency under perturbations, promoting domain-invariant learning. Extensive experiments on four medical segmentation tasks (cardiac MRI, spine MRI, bladder cancer MRI and colorectal polyp) demonstrate that our method significantly outperforms state-of-the-art FSSL and domain generalization approaches, achieving robust generalization on unseen domains.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2501.00050.pdf' target='_blank'>https://arxiv.org/pdf/2501.00050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fernando Martinez-Lopez, Lesther Santana, Mohamed Rahouti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00050">Learning in Multiple Spaces: Few-Shot Network Attack Detection with Metric-Fused Prototypical Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Network intrusion detection systems face significant challenges in identifying emerging attack patterns, especially when limited data samples are available. To address this, we propose a novel Multi-Space Prototypical Learning (MSPL) framework tailored for few-shot attack detection. The framework operates across multiple metric spaces-Euclidean, Cosine, Chebyshev, and Wasserstein distances-integrated through a constrained weighting scheme to enhance embedding robustness and improve pattern recognition. By leveraging Polyak-averaged prototype generation, the framework stabilizes the learning process and effectively adapts to rare and zero-day attacks. Additionally, an episodic training paradigm ensures balanced representation across diverse attack classes, enabling robust generalization. Experimental results on benchmark datasets demonstrate that MSPL outperforms traditional approaches in detecting low-profile and novel attack types, establishing it as a robust solution for zero-day attack detection.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2412.10985.pdf' target='_blank'>https://arxiv.org/pdf/2412.10985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Deng, Yiyang Xu, Linglong Qian, Charlène Mauger, Anastasia Nasopoulou, Steven Williams, Michelle Williams, Steven Niederer, David Newby, Andrew McCulloch, Jeff Omens, Kuberan Pushprajah, Alistair Young
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10985">MorphiNet: A Graph Subdivision Network for Adaptive Bi-ventricle Surface Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cardiac Magnetic Resonance (CMR) imaging is widely used for heart model reconstruction and digital twin computational analysis because of its ability to visualize soft tissues and capture dynamic functions. However, CMR images have an anisotropic nature, characterized by large inter-slice distances and misalignments from cardiac motion. These limitations result in data loss and measurement inaccuracies, hindering the capture of detailed anatomical structures. In this work, we introduce MorphiNet, a novel network that reproduces heart anatomy learned from high-resolution Computed Tomography (CT) images, unpaired with CMR images. MorphiNet encodes the anatomical structure as gradient fields, deforming template meshes into patient-specific geometries. A multilayer graph subdivision network refines these geometries while maintaining a dense point correspondence, suitable for computational analysis. MorphiNet achieved state-of-the-art bi-ventricular myocardium reconstruction on CMR patients with tetralogy of Fallot with 0.3 higher Dice score and 2.6 lower Hausdorff distance compared to the best existing template-based methods. While matching the anatomical fidelity of comparable neural implicit function methods, MorphiNet delivered 50$\times$ faster inference. Cross-dataset validation on the Automated Cardiac Diagnosis Challenge confirmed robust generalization, achieving a 0.7 Dice score with 30\% improvement over previous template-based approaches. We validate our anatomical learning approach through the successful restoration of missing cardiac structures and demonstrate significant improvement over standard Loop subdivision. Motion tracking experiments further confirm MorphiNet's capability for cardiac function analysis, including accurate ejection fraction calculation that correctly identifies myocardial dysfunction in tetralogy of Fallot patients.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2411.17332.pdf' target='_blank'>https://arxiv.org/pdf/2411.17332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos Garrido-Munoz, Jorge Calvo-Zaragoza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17332">On the Generalization of Handwritten Text Recognition Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Handwritten Text Recognition (HTR) have led to significant reductions in transcription errors on standard benchmarks under the i.i.d. assumption, thus focusing on minimizing in-distribution (ID) errors. However, this assumption does not hold in real-world applications, which has motivated HTR research to explore Transfer Learning and Domain Adaptation techniques. In this work, we investigate the unaddressed limitations of HTR models in generalizing to out-of-distribution (OOD) data. We adopt the challenging setting of Domain Generalization, where models are expected to generalize to OOD data without any prior access. To this end, we analyze 336 OOD cases from eight state-of-the-art HTR models across seven widely used datasets, spanning five languages. Additionally, we study how HTR models leverage synthetic data to generalize. We reveal that the most significant factor for generalization lies in the textual divergence between domains, followed by visual divergence. We demonstrate that the error of HTR models in OOD scenarios can be reliably estimated, with discrepancies falling below 10 points in 70\% of cases. We identify the underlying limitations of HTR models, laying the foundation for future research to address this challenge.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2410.20088.pdf' target='_blank'>https://arxiv.org/pdf/2410.20088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atula Tejaswi, Yoonsang Lee, Sujay Sanghavi, Eunsol Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20088">RARe: Retrieval Augmented Retrieval with In-Context Examples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2410.09409.pdf' target='_blank'>https://arxiv.org/pdf/2410.09409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Jiang, Xinlong Wan, Kaiying Zhu, Xihe Qiu, Zhijun Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09409">Distribution-aware Noisy-label Crack Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road crack segmentation is critical for robotic systems tasked with the inspection, maintenance, and monitoring of road infrastructures. Existing deep learning-based methods for crack segmentation are typically trained on specific datasets, which can lead to significant performance degradation when applied to unseen real-world scenarios. To address this, we introduce the SAM-Adapter, which incorporates the general knowledge of the Segment Anything Model (SAM) into crack segmentation, demonstrating enhanced performance and generalization capabilities. However, the effectiveness of the SAM-Adapter is constrained by noisy labels within small-scale training sets, including omissions and mislabeling of cracks. In this paper, we present an innovative joint learning framework that utilizes distribution-aware domain-specific semantic knowledge to guide the discriminative learning process of the SAM-Adapter. To our knowledge, this is the first approach that effectively minimizes the adverse effects of noisy labels on the supervised learning of the SAM-Adapter. Our experimental results on two public pavement crack segmentation datasets confirm that our method significantly outperforms existing state-of-the-art techniques. Furthermore, evaluations on the completely unseen CFD dataset demonstrate the high cross-domain generalization capability of our model, underscoring its potential for practical applications in crack segmentation.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2410.08972.pdf' target='_blank'>https://arxiv.org/pdf/2410.08972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michalis Korakakis, Andreas Vlachos, Adrian Weller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08972">ALVIN: Active Learning Via INterpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active Learning aims to minimize annotation effort by selecting the most useful instances from a pool of unlabeled data. However, typical active learning methods overlook the presence of distinct example groups within a class, whose prevalence may vary, e.g., in occupation classification datasets certain demographics are disproportionately represented in specific classes. This oversight causes models to rely on shortcuts for predictions, i.e., spurious correlations between input attributes and labels occurring in well-represented groups. To address this issue, we propose Active Learning Via INterpolation (ALVIN), which conducts intra-class interpolations between examples from under-represented and well-represented groups to create anchors, i.e., artificial points situated between the example groups in the representation space. By selecting instances close to the anchors for annotation, ALVIN identifies informative examples exposing the model to regions of the representation space that counteract the influence of shortcuts. Crucially, since the model considers these examples to be of high certainty, they are likely to be ignored by typical active learning methods. Experimental results on six datasets encompassing sentiment analysis, natural language inference, and paraphrase detection demonstrate that ALVIN outperforms state-of-the-art active learning methods in both in-distribution and out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2410.06977.pdf' target='_blank'>https://arxiv.org/pdf/2410.06977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyue Li, Shuoyi Chen, Mang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06977">Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wildlife ReID involves utilizing visual technology to identify specific individuals of wild animals in different scenarios, holding significant importance for wildlife conservation, ecological research, and environmental monitoring. Existing wildlife ReID methods are predominantly tailored to specific species, exhibiting limited applicability. Although some approaches leverage extensively studied person ReID techniques, they struggle to address the unique challenges posed by wildlife. Therefore, in this paper, we present a unified, multi-species general framework for wildlife ReID. Given that high-frequency information is a consistent representation of unique features in various species, significantly aiding in identifying contours and details such as fur textures, we propose the Adaptive High-Frequency Transformer model with the goal of enhancing high-frequency information learning. To mitigate the inevitable high-frequency interference in the wilderness environment, we introduce an object-aware high-frequency selection strategy to adaptively capture more valuable high-frequency components. Notably, we unify the experimental settings of multiple wildlife datasets for ReID, achieving superior performance over state-of-the-art ReID methods. In domain generalization scenarios, our approach demonstrates robust generalization to unknown species.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2409.17899.pdf' target='_blank'>https://arxiv.org/pdf/2409.17899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Sun, Zeyu Zhao, Korin Richmond, Yuanchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17899">Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2409.12011.pdf' target='_blank'>https://arxiv.org/pdf/2409.12011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Du, Tong Niu, Rong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12011">Mixture of Prompt Learning for Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As powerful pre-trained vision-language models (VLMs) like CLIP gain prominence, numerous studies have attempted to combine VLMs for downstream tasks. Among these, prompt learning has been validated as an effective method for adapting to new tasks, which only requiring a small number of parameters. However, current prompt learning methods face two challenges: first, a single soft prompt struggles to capture the diverse styles and patterns within a dataset; second, fine-tuning soft prompts is prone to overfitting. To address these challenges, we propose a mixture of soft prompt learning method incorporating a routing module. This module is able to capture a dataset's varied styles and dynamically selects the most suitable prompts for each instance. Additionally, we introduce a novel gating mechanism to ensure the router selects prompts based on their similarity to hard prompt templates, which both retaining knowledge from hard prompts and improving selection accuracy. We also implement semantically grouped text-level supervision, initializing each soft prompt with the token embeddings of manually designed templates from its group and applied a contrastive loss between the resulted text feature and hard prompt encoded text feature. This supervision ensures that the text features derived from soft prompts remain close to those from their corresponding hard prompts, preserving initial knowledge and mitigating overfitting. Our method has been validated on 11 datasets, demonstrating evident improvements in few-shot learning, domain generalization, and base-to-new generalization scenarios compared to existing baselines. The code will be available at \url{https://anonymous.4open.science/r/mocoop-6387}
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2409.09832.pdf' target='_blank'>https://arxiv.org/pdf/2409.09832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Nanduri, Rama Chellappa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09832">Template-based Multi-Domain Face Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable performance of deep neural networks for face detection and recognition tasks in the visible spectrum, their performance on more challenging non-visible domains is comparatively still lacking. While significant research has been done in the fields of domain adaptation and domain generalization, in this paper we tackle scenarios in which these methods have limited applicability owing to the lack of training data from target domains. We focus on the problem of single-source (visible) and multi-target (SWIR, long-range/remote, surveillance, and body-worn) face recognition task. We show through experiments that a good template generation algorithm becomes crucial as the complexity of the target domain increases. In this context, we introduce a template generation algorithm called Norm Pooling (and a variant known as Sparse Pooling) and show that it outperforms average pooling across different domains and networks, on the IARPA JANUS Benchmark Multi-domain Face (IJB-MDF) dataset.
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2409.08587.pdf' target='_blank'>https://arxiv.org/pdf/2409.08587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Damiano, Thomas Dietzen, Toon van Waterschoot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08587">Frequency Tracking Features for Data-Efficient Deep Siren Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The identification of siren sounds in urban soundscapes is a crucial safety aspect for smart vehicles and has been widely addressed by means of neural networks that ensure robustness to both the diversity of siren signals and the strong and unstructured background noise characterizing traffic. Convolutional neural networks analyzing spectrogram features of incoming signals achieve state-of-the-art performance when enough training data capturing the diversity of the target acoustic scenes is available. In practice, data is usually limited and algorithms should be robust to adapt to unseen acoustic conditions without requiring extensive datasets for re-training. In this work, given the harmonic nature of siren signals, characterized by a periodically evolving fundamental frequency, we propose a low-complexity feature extraction method based on frequency tracking using a single-parameter adaptive notch filter. The features are then used to design a small-scale convolutional network suitable for training with limited data. The evaluation results indicate that the proposed model consistently outperforms the traditional spectrogram-based model when limited training data is available, achieves better cross-domain generalization and has a smaller size.
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2409.03509.pdf' target='_blank'>https://arxiv.org/pdf/2409.03509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chamuditha Jayanaga Galappaththige, Zachary Izzo, Xilin He, Honglu Zhou, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03509">Domain-Guided Weight Modulation for Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unarguably, deep learning models capable of generalizing to unseen domain data while leveraging a few labels are of great practical significance due to low developmental costs. In search of this endeavor, we study the challenging problem of semi-supervised domain generalization (SSDG), where the goal is to learn a domain-generalizable model while using only a small fraction of labeled data and a relatively large fraction of unlabeled data. Domain generalization (DG) methods show subpar performance under the SSDG setting, whereas semi-supervised learning (SSL) methods demonstrate relatively better performance, however, they are considerably poor compared to the fully-supervised DG methods. Towards handling this new, but challenging problem of SSDG, we propose a novel method that can facilitate the generation of accurate pseudo-labels under various domain shifts. This is accomplished by retaining the domain-level specialism in the classifier during training corresponding to each source domain. Specifically, we first create domain-level information vectors on the fly which are then utilized to learn a domain-aware mask for modulating the classifier's weights. We provide a mathematical interpretation for the effect of this modulation procedure on both pseudo-labeling and model training. Our method is plug-and-play and can be readily applied to different SSL baselines for SSDG. Extensive experiments on six challenging datasets in two different SSDG settings show that our method provides visible gains over the various strong SSL-based SSDG baselines.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2601.21897.pdf' target='_blank'>https://arxiv.org/pdf/2601.21897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Hasanzadeh Karkan, Ahmed Ibrahim, Jean-François Frigon, François Leduc-Primeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21897">A Low-Complexity Plug-and-Play Deep Learning Model for Generalizable Massive MIMO Precoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Massive multiple-input multiple-output (mMIMO) downlink precoding offers high spectral efficiency but remains challenging to deploy in practice because near-optimal algorithms such as the weighted minimum mean squared error (WMMSE) are computationally expensive, and sensitive to SNR and channel-estimation quality, while existing deep learning (DL)-based solutions often lack robustness and require retraining for each deployment site. This paper proposes a plug-and-play precoder (PaPP), a DL framework with a backbone that can be trained for either fully digital (FDP) or hybrid beamforming (HBF) precoding and reused across sites, transmit-power levels, and with varying amounts of channel estimation error, avoiding the need to train a new model from scratch at each deployment. PaPP combines a high-capacity teacher and a compact student with a self-supervised loss that balances teacher imitation and normalized sum-rate, trained using meta-learning domain-generalization and transmit-power-aware input normalization. Numerical results on ray-tracing data from three unseen sites show that the PaPP FDP and HBF models both outperform conventional and deep learning baselines, after fine-tuning with a small set of local unlabeled samples. Across both architectures, PaPP achieves more than 21$\times$ reduction in modeled computation energy and maintains good performance under channel-estimation errors, making it a practical solution for energy-efficient mMIMO precoding.
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2601.04672.pdf' target='_blank'>https://arxiv.org/pdf/2601.04672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Zhang, Lifei Wang, Lina Lu, MingKun Xu, Shangyang Li, Yanchao Yang, Tao Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04672">Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2512.13454.pdf' target='_blank'>https://arxiv.org/pdf/2512.13454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arpit Jadon, Joshua Niemeijer, Yuki M. Asano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13454">Test-Time Modification: Inverse Domain Transformation for Robust Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2512.11350.pdf' target='_blank'>https://arxiv.org/pdf/2512.11350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanu Singh, Pranamesh Chakraborty, Long T. Truong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11350">Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2512.09067.pdf' target='_blank'>https://arxiv.org/pdf/2512.09067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis Rangel DaCosta, Mary C. Scott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09067">Contrast transfer functions help quantify neural network out-of-distribution generalization in HRTEM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks, while effective for tackling many challenging scientific tasks, are not known to perform well out-of-distribution (OOD), i.e., within domains which differ from their training data. Understanding neural network OOD generalization is paramount to their successful deployment in experimental workflows, especially when ground-truth knowledge about the experiment is hard to establish or experimental conditions significantly vary. With inherent access to ground-truth information and fine-grained control of underlying distributions, simulation-based data curation facilitates precise investigation of OOD generalization behavior. Here, we probe generalization with respect to imaging conditions of neural network segmentation models for high-resolution transmission electron microscopy (HRTEM) imaging of nanoparticles, training and measuring the OOD generalization of over 12,000 neural networks using synthetic data generated via random structure sampling and multislice simulation. Using the HRTEM contrast transfer function, we further develop a framework to compare information content of HRTEM datasets and quantify OOD domain shifts. We demonstrate that neural network segmentation models enjoy significant performance stability, but will smoothly and predictably worsen as imaging conditions shift from the training distribution. Lastly, we consider limitations of our approach in explaining other OOD shifts, such as of the atomic structures, and discuss complementary techniques for understanding generalization in such settings.
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2512.06912.pdf' target='_blank'>https://arxiv.org/pdf/2512.06912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rushiraj Gadhvi, Sandeep Manjanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06912">Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2511.22739.pdf' target='_blank'>https://arxiv.org/pdf/2511.22739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Mohammad Ezzati, Alireza Malekhosseini, Armin Khosravi, Mohammad Hossein Rohban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22739">All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization is critical in computational pathology (CPath) due to inherent domain shifts caused by variations in staining protocols, scanner devices, and imaging settings across clinical centers. Vision-language models (VLMs), such as PLIP-a pathology-tuned CLIP-trained on image-text pairs across diverse domains, serve as strong knowledge distillation sources. However, their zero-shot performance with predefined prompts remains limited due to sensitivity to prompt variations. Moreover, unlike natural images, histopathology centers lack semantic descriptors (e.g., 'sketch'), making it difficult to define domain-specific prompts for clinical centers. This requires a data-driven approach for learning domain-specific and ultimately class-generic continuous prompts. We propose Domain Invariant Prompt Tuning (DIPT) for knowledge distillation process, a novel step that learns multiple input tokens for each domain. These tokens are trained separately for each domain and are averaged across domains, leading to domain-invariant prompts. Our student model then distills knowledge from PLIP's text encoder by leveraging the prompts learned by DIPT. This leads to alignment of visual features with domain-invariant embeddings, enhancing generalization by training on multiple domains. Our method adds a significant improvement in average F1-score to existing state-of-the-art (SOTA) knowledge distillation approaches in domain generalization with histopathology datasets. This work helps the way of deploying robust CPath models in real-world clinical problems with heterogeneous data sources.
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2511.19573.pdf' target='_blank'>https://arxiv.org/pdf/2511.19573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialiang Li, Weitong Chen, Mingyu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19573">Neural Tractability via Structure: Learning-Augmented Algorithms for Graph Combinatorial Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural models have shown promise in solving NP-hard graph combinatorial optimization (CO) problems. Once trained, they offer fast inference and reasonably high-quality solutions for in-distribution testing instances, but they generally fall short in terms of absolute solution quality compared to classical search-based algorithms that are admittedly slower but offer optimality guarantee once search finishes. We propose a novel framework that combines the inference efficiency and exploratory power of neural models with the solution quality guarantee of search-based algorithms. In particular, we use parameterized algorithms (PAs) as the search component. PAs are dedicated to identifying easy instances of generally NP-hard problems, and allow for practically efficient search by exploiting structural simplicity (of the identified easy instances). Under our framework, we use parameterized analysis to identify the structurally hard parts of a CO instance. The neural model handles the hard parts by generating advisory signals based on its data-driven understanding. The PA-based search component then integrates the advisory signals to systematically and efficiently searches through the remaining structurally easy parts. Notably, our framework is agnostic to the choice of neural model and produces strictly better solutions than neural solvers alone. We examine our framework on multiple CO tasks. Empirical results show that it achieves superior solution quality, competitive with that of commercial solvers. Furthermore, by using the neural model only for exploratory advisory signals, our framework exhibits improved out-of-distribution generalization, addressing a key limitation of existing neural CO solvers.
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2511.19149.pdf' target='_blank'>https://arxiv.org/pdf/2511.19149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Moazzam Umer Gondal, Hamad Ul Qudous, Daniya Siddiqui, Asma Ahmad Farhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19149">From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2511.18627.pdf' target='_blank'>https://arxiv.org/pdf/2511.18627.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Benedikt Ruhland, Thorsten Papenbrock, Jan-Peter Sowa, Ali Canbay, Nicole Eter, Bernd Freisleben, Dominik Heider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18627">Functional Localization Enforced Deep Anomaly Detection Using Fundus Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings. On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2511.17902.pdf' target='_blank'>https://arxiv.org/pdf/2511.17902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan He, Haodong Zhang, Qiuheng Song, Lin Lei, Zhenxuan Zeng, Haoyang He, Hongyan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17902">Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning. To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data. Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2511.17484.pdf' target='_blank'>https://arxiv.org/pdf/2511.17484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neel Sortur, Justin Goodwin, Purvik Patel, Luis Enrique Martinez, Tzofi Klinghoffer, Rajmonda S. Caceres, Robin Walters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17484">Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2511.16979.pdf' target='_blank'>https://arxiv.org/pdf/2511.16979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyun Wang, Zheng Duan, Xinyue Liao, Ke-Jia Chen, Songcan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16979">The Finer the Better: Towards Granular-aware Open-set Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2511.12937.pdf' target='_blank'>https://arxiv.org/pdf/2511.12937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoyan Wang, Yanyan Huang, Chunlin Chen, Lifeng Wang, Yuxiang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12937">Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2511.12730.pdf' target='_blank'>https://arxiv.org/pdf/2511.12730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emilien Valat, Ozan Öktem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12730">Improving the Generalisation of Learned Reconstruction Frameworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring proper generalization is a critical challenge in applying data-driven methods for solving inverse problems in imaging, as neural networks reconstructing an image must perform well across varied datasets and acquisition geometries. In X-ray Computed Tomography (CT), convolutional neural networks (CNNs) are widely used to filter the projection data but are ill-suited for this task as they apply grid-based convolutions to the sinogram, which inherently lies on a line manifold, not a regular grid. The CNNs, unaware of the geometry, are implicitly tied to it and require an excessive amount of parameters as they must infer the relations between measurements from the data rather than from prior information. The contribution of this paper is twofold. First, we introduce a graph data structure to represent CT acquisition geometries and tomographic data, providing a detailed explanation of the graph's structure for circular, cone-beam geometries. Second, we propose GLM, a hybrid neural network architecture that leverages both graph and grid convolutions to process tomographic data. We demonstrate that GLM outperforms CNNs when performance is quantified in terms of structural similarity and peak signal-to-noise ratio, despite the fact that GLM uses only a fraction of the trainable parameters. Compared to CNNs, GLM also requires significantly less training time and memory, and its memory requirements scale better. Crucially, GLM demonstrates robust generalization to unseen variations in the acquisition geometry, like when training only on fully sampled CT data and then testing on sparse-view CT data.
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2511.11388.pdf' target='_blank'>https://arxiv.org/pdf/2511.11388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanath Keshav, Felix Fritzen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11388">Robust inverse material design with physical guarantees using the Voigt-Reuss Net</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the Löwner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $>\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\approx 1.7\%$ and mean $\approx 3.4\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2>0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2511.06607.pdf' target='_blank'>https://arxiv.org/pdf/2511.06607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seshu Kumar Damarla, Xiuli Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06607">Explainable Probabilistic Machine Learning for Predicting Drilling Fluid Loss of Circulation in Marun Oil Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lost circulation remains a major and costly challenge in drilling operations, often resulting in wellbore instability, stuck pipe, and extended non-productive time. Accurate prediction of fluid loss is therefore essential for improving drilling safety and efficiency. This study presents a probabilistic machine learning framework based on Gaussian Process Regression (GPR) for predicting drilling fluid loss in complex formations. The GPR model captures nonlinear dependencies among drilling parameters while quantifying predictive uncertainty, offering enhanced reliability for high-risk decision-making. Model hyperparameters are optimized using the Limited memory Broyden Fletcher Goldfarb Shanno (LBFGS) algorithm to ensure numerical stability and robust generalization. To improve interpretability, Local Interpretable Model agnostic Explanations (LIME) are employed to elucidate how individual features influence model predictions. The results highlight the potential of explainable probabilistic learning for proactive identification of lost-circulation risks, optimized design of lost circulation materials (LCM), and reduction of operational uncertainties in drilling applications.
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2511.05393.pdf' target='_blank'>https://arxiv.org/pdf/2511.05393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05393">PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2511.04576.pdf' target='_blank'>https://arxiv.org/pdf/2511.04576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Zhang, Xiong Xiong, Sen Zhang, Yuan Zhao, Xi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04576">Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>PDEs arise ubiquitously in science and engineering, where solutions depend on parameters (physical properties, boundary conditions, geometry). Traditional numerical methods require re-solving the PDE for each parameter, making parameter space exploration prohibitively expensive. Recent machine learning advances, particularly physics-informed neural networks (PINNs) and neural operators, have revolutionized parametric PDE solving by learning solution operators that generalize across parameter spaces. We critically analyze two main paradigms: (1) PINNs, which embed physical laws as soft constraints and excel at inverse problems with sparse data, and (2) neural operators (e.g., DeepONet, Fourier Neural Operator), which learn mappings between infinite-dimensional function spaces and achieve unprecedented generalization. Through comparisons across fluid dynamics, solid mechanics, heat transfer, and electromagnetics, we show neural operators can achieve computational speedups of $10^3$ to $10^5$ times faster than traditional solvers for multi-query scenarios, while maintaining comparable accuracy. We provide practical guidance for method selection, discuss theoretical foundations (universal approximation, convergence), and identify critical open challenges: high-dimensional parameters, complex geometries, and out-of-distribution generalization. This work establishes a unified framework for understanding parametric PDE solvers via operator learning, offering a comprehensive, incrementally updated resource for this rapidly evolving field
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2511.04008.pdf' target='_blank'>https://arxiv.org/pdf/2511.04008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmoud Soliman, Omar Abdelaziz, Ahmed Radwan, Anand, Mohamed Shehata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04008">GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization (DG) seeks robust Vision Transformer (ViT) performance on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging; standard fine-tuning is costly and can impair generalization. We propose GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT, SAGE) operates on inter-patch graphs to dynamically assign patches to specialized experts. This context-aware GNN routing leverages inter-patch relationships for better adaptation to domain shifts. GNN-MoE achieves state-of-the-art or competitive DG benchmark performance with high parameter efficiency, highlighting the utility of graph-based contextual routing for robust, lightweight DG.
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2511.02417.pdf' target='_blank'>https://arxiv.org/pdf/2511.02417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Garen Boyadjian, Cyrille Pierre, Johann Laconte, Riccardo Bertoglio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02417">Synthetic Crop-Weed Image Generation and its Impact on Model Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise semantic segmentation of crops and weeds is necessary for agricultural weeding robots. However, training deep learning models requires large annotated datasets, which are costly to obtain in real fields. Synthetic data can reduce this burden, but the gap between simulated and real images remains a challenge. In this paper, we present a pipeline for procedural generation of synthetic crop-weed images using Blender, producing annotated datasets under diverse conditions of plant growth, weed density, lighting, and camera angle. We benchmark several state-of-the-art segmentation models on synthetic and real datasets and analyze their cross-domain generalization. Our results show that training on synthetic images leads to a sim-to-real gap of 10%, surpassing previous state-of-the-art methods. Moreover, synthetic data demonstrates good generalization properties, outperforming real datasets in cross-domain scenarios. These findings highlight the potential of synthetic agricultural datasets and support hybrid strategies for more efficient model training.
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2511.02415.pdf' target='_blank'>https://arxiv.org/pdf/2511.02415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02415">ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension.
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2511.01284.pdf' target='_blank'>https://arxiv.org/pdf/2511.01284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karma Phuntsho, Abdullah, Kyungmi Lee, Ickjai Lee, Euijoon Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01284">Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models (FMs) have emerged as a transformative paradigm in medical image analysis, offering the potential to provide generalizable, task-agnostic solutions across a wide range of clinical tasks and imaging modalities. Their capacity to learn transferable representations from large-scale data has the potential to address the limitations of conventional task-specific models. However, adaptation of FMs to real-world clinical practice remains constrained by key challenges, including domain shifts, limited availability of high-quality annotated data, substantial computational demands, and strict privacy requirements. This review presents a comprehensive assessment of strategies for adapting FMs to the specific demands of medical imaging. We examine approaches such as supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal or cross-modal frameworks. For each, we evaluate reported performance gains, clinical applicability, and limitations, while identifying trade-offs and unresolved challenges that prior reviews have often overlooked. Beyond these established techniques, we also highlight emerging directions aimed at addressing current gaps. These include continual learning to enable dynamic deployment, federated and privacy-preserving approaches to safeguard sensitive data, hybrid self-supervised learning to enhance data efficiency, data-centric pipelines that combine synthetic generation with human-in-the-loop validation, and systematic benchmarking to assess robust generalization under real-world clinical variability. By outlining these strategies and associated research gaps, this review provides a roadmap for developing adaptive, trustworthy, and clinically integrated FMs capable of meeting the demands of real-world medical imaging.
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2510.24503.pdf' target='_blank'>https://arxiv.org/pdf/2510.24503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mortesa Hussaini, Jan Theiß, Anthony Stein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24503">Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of Federated Learning with heterogeneous data environments, local models tend to converge to their own local model optima during local training steps, deviating from the overall data distributions. Aggregation of these local updates, e.g., with FedAvg, often does not align with the global model optimum (client drift), resulting in an update that is suboptimal for most clients. Personalized Federated Learning approaches address this challenge by exclusively focusing on the average local performances of clients' models on their own data distribution. Generalization to out-of-distribution samples, which is a substantial benefit of FedAvg and represents a significant component of robustness, appears to be inadequately incorporated into the assessment and evaluation processes. This study involves a thorough evaluation of Federated Learning approaches, encompassing both their local performance and their generalization capabilities. Therefore, we examine different stages within a single communication round to enable a more nuanced understanding of the considered metrics. Furthermore, we propose and incorporate a modified approach of FedAvg, designated as Federated Learning with Individualized Updates (FLIU), extending the algorithm by a straightforward individualization step with an adaptive personalization factor. We evaluate and compare the approaches empirically using MNIST and CIFAR-10 under various distributional conditions, including benchmark IID and pathological non-IID, as well as additional novel test environments with Dirichlet distribution specifically developed to stress the algorithms on complex data heterogeneity.
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2510.24173.pdf' target='_blank'>https://arxiv.org/pdf/2510.24173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Du, Aditi S. Krishnapriyan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24173">EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computationally resolving turbulence remains a central challenge in fluid dynamics due to its multi-scale interactions. Fully resolving large-scale turbulence through direct numerical simulation (DNS) is computationally prohibitive, motivating data-driven machine learning alternatives. In this work, we propose EddyFormer, a Transformer-based spectral-element (SEM) architecture for large-scale turbulence simulation that combines the accuracy of spectral methods with the scalability of the attention mechanism. We introduce an SEM tokenization that decomposes the flow into grid-scale and subgrid-scale components, enabling capture of both local and global features. We create a new three-dimensional isotropic turbulence dataset and train EddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x speedup over DNS. When applied to unseen domains up to 4x larger than in training, EddyFormer preserves accuracy on physics-invariant metrics-energy spectra, correlation functions, and structure functions-showing domain generalization. On The Well benchmark suite of diverse turbulent flows, EddyFormer resolves cases where prior ML models fail to converge, accurately reproducing complex dynamics across a wide range of physical conditions.
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2510.23663.pdf' target='_blank'>https://arxiv.org/pdf/2510.23663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Padmanabhan Jagannathan Prajesh, Kaliaperumal Ragunath, Miriam Gordon, Bruce Rathgeber, Suresh Neethirajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23663">AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes is essential for guiding emission mitigation strategies. We present a Spatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that reconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across southern Canada, emphasizing poultry-intensive regions. The model fuses wavelet time-frequency representations with transformer attention over meteorology, vegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT attains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions lie within +/-1 ppm. Independent validation with TCCON shows robust generalization (bias = -0.14 ppm; r = 0.928), including faithful reproduction of the late-summer drawdown. Spatial analysis across 14 poultry regions reveals a moderate positive association between facility density and XCO2 (r = 0.43); high-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced summer variability. Compared with conventional interpolation and standard machine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces with explicit uncertainties, enabling year-round coverage despite sparse observations. The approach supports integration of satellite constraints with national inventories and precision livestock platforms to benchmark emissions, refine region-specific factors, and verify interventions. Importantly, transformer-based Earth observation enables scalable, transparent, spatially explicit carbon accounting, hotspot prioritization, and policy-relevant mitigation assessment.
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2510.22119.pdf' target='_blank'>https://arxiv.org/pdf/2510.22119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lihuang Fang, Xiao Hu, Yuchen Zou, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22119">CogStereo: Neural Stereo Matching with Implicit Spatial Cognition Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep stereo matching has advanced significantly on benchmark datasets through fine-tuning but falls short of the zero-shot generalization seen in foundation models in other vision tasks. We introduce CogStereo, a novel framework that addresses challenging regions, such as occlusions or weak textures, without relying on dataset-specific priors. CogStereo embeds implicit spatial cognition into the refinement process by using monocular depth features as priors, capturing holistic scene understanding beyond local correspondences. This approach ensures structurally coherent disparity estimation, even in areas where geometry alone is inadequate. CogStereo employs a dual-conditional refinement mechanism that combines pixel-wise uncertainty with cognition-guided features for consistent global correction of mismatches. Extensive experiments on Scene Flow, KITTI, Middlebury, ETH3D, EuRoc, and real-world demonstrate that CogStereo not only achieves state-of-the-art results but also excels in cross-domain generalization, shifting stereo vision towards a cognition-driven approach.
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2508.20294.pdf' target='_blank'>https://arxiv.org/pdf/2508.20294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank RÃ¶der, Jan Benad, Manfred Eppe, Pradeep Kr. Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20294">Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI's latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2508.19604.pdf' target='_blank'>https://arxiv.org/pdf/2508.19604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhe Fan, Chaoyu Liu, Zhonghua Qiao, Xiaoqin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19604">IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalized Semantic Segmentation (DGSS) focuses on training a model using labeled data from a source domain, with the goal of achieving robust generalization to unseen target domains during inference. A common approach to improve generalization is to augment the source domain with synthetic data generated by diffusion models (DMs). However, the generated images often contain structural or semantic defects due to training imperfections. Training segmentation models with such flawed data can lead to performance degradation and error accumulation. To address this issue, we propose to integrate inverse evolution layers (IELs) into the generative process. IELs are designed to highlight spatial discontinuities and semantic inconsistencies using Laplacian-based priors, enabling more effective filtering of undesirable generative patterns. Based on this mechanism, we introduce IELDM, an enhanced diffusion-based data augmentation framework that can produce higher-quality images. Furthermore, we observe that the defect-suppression capability of IELs can also benefit the segmentation network by suppressing artifact propagation. Based on this insight, we embed IELs into the decoder of the DGSS model and propose IELFormer to strengthen generalization capability in cross-domain scenarios. To further strengthen the model's semantic consistency across scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module, which performs frequency-domain analysis to achieve structured integration of multi-resolution features, thereby improving cross-scale coherence. Extensive experiments on benchmark datasets demonstrate that our approach achieves superior generalization performance compared to existing methods.
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2508.18726.pdf' target='_blank'>https://arxiv.org/pdf/2508.18726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiroaki Aizawa, Yoshikazu Hayashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18726">Flatness-aware Curriculum Learning via Adversarial Difficulty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks trained by empirical risk minimization often suffer from overfitting, especially to specific samples or domains, which leads to poor generalization. Curriculum Learning (CL) addresses this issue by selecting training samples based on the difficulty. From the optimization perspective, methods such as Sharpness-Aware Minimization (SAM) improve robustness and generalization by seeking flat minima. However, combining CL with SAM is not straightforward. In flat regions, both the loss values and the gradient norms tend to become uniformly small, which makes it difficult to evaluate sample difficulty and design an effective curriculum. To overcome this problem, we propose the Adversarial Difficulty Measure (ADM), which quantifies adversarial vulnerability by leveraging the robustness properties of models trained toward flat minima. Unlike loss- or gradient-based measures, which become ineffective as training progresses into flatter regions, ADM remains informative by measuring the normalized loss gap between original and adversarial examples. We incorporate ADM into CL-based training with SAM to dynamically assess sample difficulty. We evaluated our approach on image classification tasks, fine-grained recognition, and domain generalization. The results demonstrate that our method preserves the strengths of both CL and SAM while outperforming existing curriculum-based and flatness-aware training strategies.
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2508.18612.pdf' target='_blank'>https://arxiv.org/pdf/2508.18612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumen Ghosh, Christine Jestin Hannan, Rajat Vashistha, Parveen Kundu, Sandra Brosda, Lauren G. Aoude, James Lonie, Andrew Nathanson, Jessica Ng, Andrew P. Barbour, Viktor Vegh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18612">Stress-testing cross-cancer generalizability of 3D nnU-Net for PET-CT tumor segmentation: multi-cohort evaluation with novel oesophageal and lung cancer datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust generalization is essential for deploying deep learning based tumor segmentation in clinical PET-CT workflows, where anatomical sites, scanners, and patient populations vary widely. This study presents the first cross cancer evaluation of nnU-Net on PET-CT, introducing two novel, expert-annotated whole-body datasets. 279 patients with oesophageal cancer (Australian cohort) and 54 with lung cancer (Indian cohort). These cohorts complement the public AutoPET dataset and enable systematic stress-testing of cross domain performance. We trained and tested 3D nnUNet models under three paradigms. Target only (oesophageal), public only (AutoPET), and combined training. For the tested sets, the oesophageal only model achieved the best in-domain accuracy (mean DSC, 57.8) but failed on external Indian lung cohort (mean DSC less than 3.4), indicating severe overfitting. The public only model generalized more broadly (mean DSC, 63.5 on AutoPET, 51.6 on Indian lung cohort) but underperformed in oesophageal Australian cohort (mean DSC, 26.7). The combined approach provided the most balanced results (mean DSC, lung (52.9), oesophageal (40.7), AutoPET (60.9)), reducing boundary errors and improving robustness across all cohorts. These findings demonstrate that dataset diversity, particularly multi demographic, multi center and multi cancer integration, outweighs architectural novelty as the key driver of robust generalization. This work presents the demography based cross cancer deep learning segmentation evaluation and highlights dataset diversity, rather than model complexity, as the foundation for clinically robust segmentation.
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2508.11354.pdf' target='_blank'>https://arxiv.org/pdf/2508.11354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyi Zhao, Muthu Rama Krishnan Mookiah, Emanuele Trucco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11354">Leveraging the RETFound foundation model for optic disc segmentation in retinal images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RETFound is a well-known foundation model (FM) developed for fundus camera and optical coherence tomography images. It has shown promising performance across multiple datasets in diagnosing diseases, both eye-specific and systemic, from retinal images. However, to our best knowledge, it has not been used for other tasks. We present the first adaptation of RETFound for optic disc segmentation, a ubiquitous and foundational task in retinal image analysis. The resulting segmentation system outperforms state-of-the-art, segmentation-specific baseline networks after training a head with only a very modest number of task-specific examples. We report and discuss results with four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private dataset, GoDARTS, achieving about 96% Dice consistently across all datasets. Overall, our method obtains excellent performance in internal verification, domain generalization and domain adaptation, and exceeds most of the state-of-the-art baseline results. We discuss the results in the framework of the debate about FMs as alternatives to task-specific architectures. The code is available at: [link to be added after the paper is accepted]
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2508.04316.pdf' target='_blank'>https://arxiv.org/pdf/2508.04316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Gui, Hongliang Ren, Shang Shi, Jin Lu, Changqiu Yu, Quanjun Cao, Guomin Gu, Qi Xuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04316">A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distributed Acoustic Sensing (DAS) technology finds growing applications across various domains. However, data distribution disparities due to heterogeneous sensing environments pose challenges for data-driven artificial intelligence (AI) models, limiting cross-domain generalization and facing a shortage of labeled training data. To address these issues, this study proposes a foundational model for DAS signal recognition based on a Masked Autoencoder, named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples, encompassing DAS gait spatiotemporal signals, 2D GASF images for perimeter security, 2D time-frequency images for pipeline leakage, and open-dataset signals including whale vocalizations and seismic activities, using a self-supervised mask reconstruction task to capture deep semantic features of DAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognition tasks. This method freezes the pretrained backbone parameters and fine-tunes only a small set of learnable visual prompt vectors inserted into the Transformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Super platform validate MAEPD using indoor gait recognition as a downstream task. The VPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322% of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT) method by 0.61% and reducing training time by 45%. The model also exhibits robust performance in pipeline leakage detection, confirming the generality, efficiency, and scalability of MAEPD as a foundational model. This approach offers a novel paradigm for addressing the limited generalization of signal recognition models in the DAS domain.
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2508.03190.pdf' target='_blank'>https://arxiv.org/pdf/2508.03190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bronya Roni Chernyak, Yael Segal, Yosi Shrem, Joseph Keshet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03190">PatchDSU: Uncertainty Modeling for Out of Distribution Generalization in Keyword Spotting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models excel at many tasks but rely on the assumption that training and test data follow the same distribution. This assumption often does not hold in real-world speech systems, where distribution shifts are common due to varying environments, recording conditions, and speaker diversity.
  The method of Domain Shifts with Uncertainty (DSU) augments the input of each neural network layer based on the input feature statistics. It addresses the problem of out-of-domain generalization by assuming feature statistics follow a multivariate Gaussian distribution and substitutes the input with sampled features from this distribution. While effective for computer vision, applying DSU to speech presents challenges due to the nature of the data. Unlike static visual data, speech is a temporal signal commonly represented by a spectrogram - the change of frequency over time. This representation cannot be treated as a simple image, and the resulting sparsity can lead to skewed feature statistics when applied to the entire input.
  To tackle out-of-distribution issues in keyword spotting, we propose PatchDSU, which extends DSU by splitting the input into patches and independently augmenting each patch. We evaluated PatchDSU and DSU alongside other methods on the Google Speech Commands, Librispeech, and TED-LIUM. Additionally, we evaluated performance under white Gaussian and MUSAN music noise conditions. We also explored out-of-domain generalization by analyzing model performance on datasets they were not trained on. Overall, in most cases, both PatchDSU and DSU outperform other methods. Notably, PatchDSU demonstrates more consistent improvements across the evaluated scenarios compared to other approaches.
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2508.01137.pdf' target='_blank'>https://arxiv.org/pdf/2508.01137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeduo Zhang, Yalda Mohsenzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01137">Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To develop a domain-agnostic, semi-supervised anomaly detection framework that integrates deep reinforcement learning (DRL) to address challenges such as large-scale data, overfitting, and class imbalance, focusing on brain MRI volumes. This retrospective study used publicly available brain MRI datasets collected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and 578 T2-weighted MRI volumes (from healthy subjects) for training, while the BraTS 2021 dataset provided 251 volumes for validation and 1000 for testing (unhealthy subjects with Glioblastomas). Preprocessing included normalization, skull-stripping, and co-registering to a uniform voxel size. Experiments were conducted on both T1- and T2-weighted modalities. Additional experiments and ablation analyses were also carried out on the industrial datasets. The proposed method integrates DRL with feature representations to handle label scarcity, large-scale data and overfitting. Statistical analysis was based on several detection and segmentation metrics including AUROC and Dice score. The proposed method achieved an AUROC of 88.7% (pixel-level) and 96.7% (image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA) methods. On industrial surface datasets, the model also showed competitive performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset, indicating strong cross-domain generalization. Studies on anomaly sample size showed a monotonic increase in AUROC as more anomalies were seen, without evidence of overfitting or additional computational cost. The domain-agnostic semi-supervised approach using DRL shows significant promise for MRI anomaly detection, achieving strong performance on both medical and industrial datasets. Its robustness, generalizability and efficiency highlight its potential for real-world clinical applications.
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2507.18542.pdf' target='_blank'>https://arxiv.org/pdf/2507.18542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Ruano, GonÃ§alo M. Correia, Leonor Barreiros, Afonso Mendes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18542">Effective Multi-Task Learning for Biomedical Named Entity Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biomedical Named Entity Recognition presents significant challenges due to the complexity of biomedical terminology and inconsistencies in annotation across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER), a novel approach designed to handle nested named entities while integrating multiple datasets through an effective multi-task learning strategy. SRU-NER mitigates annotation gaps by dynamically adjusting loss computation to avoid penalizing predictions of entity types absent in a given dataset. Through extensive experiments, including a cross-corpus evaluation and human assessment of the model's predictions, SRU-NER achieves competitive performance in biomedical and general-domain NER tasks, while improving cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2507.09440.pdf' target='_blank'>https://arxiv.org/pdf/2507.09440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Hill, Benjamin Eyre, Elliot Creager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09440">Transformers Don't In-Context Learn Least Squares Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-context learning (ICL) has emerged as a powerful capability of large pretrained transformers, enabling them to solve new tasks implicit in example input-output pairs without any gradient updates. Despite its practical success, the mechanisms underlying ICL remain largely mysterious. In this work we study synthetic linear regression to probe how transformers implement learning at inference time. Previous works have demonstrated that transformers match the performance of learning rules such as Ordinary Least Squares (OLS) regression or gradient descent and have suggested ICL is facilitated in transformers through the learned implementation of one of these techniques. In this work, we demonstrate through a suite of out-of-distribution generalization experiments that transformers trained for ICL fail to generalize after shifts in the prompt distribution, a behaviour that is inconsistent with the notion of transformers implementing algorithms such as OLS. Finally, we highlight the role of the pretraining corpus in shaping ICL behaviour through a spectral analysis of the learned representations in the residual stream. Inputs from the same distribution as the training data produce representations with a unique spectral signature: inputs from this distribution tend to have the same top two singular vectors. This spectral signature is not shared by out-of-distribution inputs, and a metric characterizing the presence of this signature is highly correlated with low loss.
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2507.03026.pdf' target='_blank'>https://arxiv.org/pdf/2507.03026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhishek Verma, Nallarasan V, Balaraman Ravindran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03026">Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning in Reinforcement Learning (RL) enables agents to leverage knowledge from source tasks to accelerate learning in target tasks. While prior work, such as the Attend, Adapt, and Transfer (A2T) framework, addresses negative transfer and selective transfer, other critical challenges remain underexplored. This paper introduces the Generalized Adaptive Transfer Network (GATN), a deep RL architecture designed to tackle task generalization across domains, robustness to environmental changes, and computational efficiency in transfer. GATN employs a domain-agnostic representation module, a robustness-aware policy adapter, and an efficient transfer scheduler to achieve these goals. We evaluate GATN on diverse benchmarks, including Atari 2600, MuJoCo, and a custom chatbot dialogue environment, demonstrating superior performance in cross-domain generalization, resilience to dynamic environments, and reduced computational overhead compared to baselines. Our findings suggest GATN is a versatile framework for real-world RL applications, such as adaptive chatbots and robotic control.
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2506.14835.pdf' target='_blank'>https://arxiv.org/pdf/2506.14835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiet Dang Vu, Trung Thai Tran, Duc Dung Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14835">MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precisely localizing 3D objects from a single image constitutes a central challenge in monocular 3D detection. While DETR-like architectures offer a powerful paradigm, their direct application in this domain encounters inherent limitations, preventing optimal performance. Our work addresses these challenges by introducing MonoVQD, a novel framework designed to fundamentally advance DETR-based monocular 3D detection. We propose three main contributions. First, we propose the Mask Separated Self-Attention mechanism that enables the integration of the denoising process into a DETR architecture. This improves the stability of Hungarian matching to achieve a consistent optimization objective. Second, we present the Variational Query Denoising technique to address the gradient vanishing problem of conventional denoising methods, which severely restricts the efficiency of the denoising process. This explicitly introduces stochastic properties to mitigate this fundamental limitation and unlock substantial performance gains. Finally, we introduce a sophisticated self-distillation strategy, leveraging insights from later decoder layers to synergistically improve query quality in earlier layers, thereby amplifying the iterative refinement process. Rigorous experimentation demonstrates that MonoVQD achieves superior performance on the challenging KITTI monocular benchmark. Highlighting its broad applicability, MonoVQD's core components seamlessly integrate into other architectures, delivering significant performance gains even in multi-view 3D detection scenarios on the nuScenes dataset and underscoring its robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2506.05443.pdf' target='_blank'>https://arxiv.org/pdf/2506.05443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyu Lin, Yan Wang, You Zhou, Xinye Ni, Jiahui Wu, Sen Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05443">UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a core mechanism of epigenetic regulation in eukaryotes, protein post-translational modifications (PTMs) require precise prediction to decipher dynamic life activity networks. To address the limitations of existing deep learning models in cross-modal feature fusion, domain generalization, and architectural optimization, this study proposes UniPTMs: the first unified framework for multi-type PTM prediction. The framework innovatively establishes a "Master-Slave" dual-path collaborative architecture: The master path dynamically integrates high-dimensional representations of protein sequences, structures, and evolutionary information through a Bidirectional Gated Cross-Attention (BGCA) module, while the slave path optimizes feature discrepancies and recalibration between structural and traditional features using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level feature integration across paths, the framework employs a Hierarchical Dynamic Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal features. Enhanced by a novel Hierarchical Contrastive loss function for feature consistency optimization, UniPTMs demonstrates significant performance improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art models across five modification types and transcends the Single-Type Prediction Paradigm. To strike a balance between model complexity and performance, we have also developed a lightweight variant named UniPTMs-mini.
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2505.22284.pdf' target='_blank'>https://arxiv.org/pdf/2505.22284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Fan, Chuanlin Liao, Yi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22284">From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to achieve image restoration caused by multiple degradation patterns via a single model with unified parameters. Although existing AiOIR approaches obtain promising performance in closed and controlled scenarios, they still suffered from considerable performance reduction in real-world scenarios since the gap of data distributions between the training samples (source domain) and real-world test samples (target domain) can lead inferior degradation awareness ability. To address this issue, a Unified Domain-Adaptive Image Restoration (UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the learned knowledge from source domain to target domain. To improve the degradation identification, a codebook is designed to learn a group of discrete embeddings to denote the degradation patterns, and the cross-sample contrastive learning mechanism is further proposed to capture shared features from different samples of certain degradation. To bridge the data gap, a domain adaptation strategy is proposed to build the feature projection between the source and target domains by dynamically aligning their codebook embeddings, and a correlation alignment-based test-time adaptation mechanism is designed to fine-tune the alignment discrepancies by tightening the degradation embeddings to the corresponding cluster center in the source domain. Experimental results on 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art performance for the AiOIR task. Most importantly, the feature cluster validate the degradation identification under unknown conditions, and qualitative comparisons showcase robust generalization to real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2505.09082.pdf' target='_blank'>https://arxiv.org/pdf/2505.09082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sophie Zhang, Zhiming Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09082">CEC-Zero: Chinese Error Correction Solution Based on LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2505.07165.pdf' target='_blank'>https://arxiv.org/pdf/2505.07165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Li, Hongzhang Zhu, Tao Chen, Xiaohua Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07165">Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, numerous pancreas segmentation methods have achieved promising performance on local single-source datasets. However, these methods don't adequately account for generalizability issues, and hence typically show limited performance and low stability on test data from other sources. Considering the limited availability of distinct data sources, we seek to improve the generalization performance of a pancreas segmentation model trained with a single-source dataset, i.e., the single source generalization task. In particular, we propose a dual self-supervised learning model that incorporates both global and local anatomical contexts. Our model aims to fully exploit the anatomical features of the intra-pancreatic and extra-pancreatic regions, and hence enhance the characterization of the high-uncertainty regions for more robust generalization. Specifically, we first construct a global-feature contrastive self-supervised learning module that is guided by the pancreatic spatial structure. This module obtains complete and consistent pancreatic features through promoting intra-class cohesion, and also extracts more discriminative features for differentiating between pancreatic and non-pancreatic tissues through maximizing inter-class separation. It mitigates the influence of surrounding tissue on the segmentation outcomes in high-uncertainty regions. Subsequently, a local-image restoration self-supervised learning module is introduced to further enhance the characterization of the high uncertainty regions. In this module, informative anatomical contexts are actually learned to recover randomly corrupted appearance patterns in those regions.
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2504.19362.pdf' target='_blank'>https://arxiv.org/pdf/2504.19362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunxuan Wang, Ray Yin, Yumei Tan, Hao Chen, Haiying Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19362">Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one of the primary causes of vision loss among retinal vascular diseases. Deep learning methods have been extensively applied in the grading of diabetic retinopathy (DR). However, their performance declines significantly when applied to data outside the training distribution due to domain shifts. Domain generalization (DG) has emerged as a solution to this challenge. However, most existing DG methods overlook lesion-specific features, resulting in insufficient accuracy. In this paper, we propose a novel approach that enhances existing DG methods by incorporating structural priors, inspired by the observation that DR grading is heavily dependent on vessel and lesion structures. We introduce Low-rank Adaptive Structural Priors (LoASP), a plug-and-play framework designed for seamless integration with existing DG models. LoASP improves generalization by learning adaptive structural representations that are finely tuned to the complexities of DR diagnosis. Extensive experiments on eight diverse datasets validate its effectiveness in both single-source and multi-source domain scenarios. Furthermore, visualizations reveal that the learned structural priors intuitively align with the intricate architecture of the vessels and lesions, providing compelling insights into their interpretability and diagnostic relevance.
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2504.19203.pdf' target='_blank'>https://arxiv.org/pdf/2504.19203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehsan Karami, Hamid Soltanian-Zadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19203">Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knee osteoarthritis (KOA) is a common joint disease that causes pain and mobility issues. While MRI-based deep learning models have demonstrated superior performance in predicting total knee replacement (TKR) and disease progression, their generalizability remains challenging, particularly when applied to imaging data from different sources. In this study, we show that replacing batch normalization with instance normalization, using data augmentation, and applying contrastive loss improves generalization. For training and evaluation, we used MRI data from the Osteoarthritis Initiative (OAI) database, considering sagittal fat-suppressed intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state (DESS) images as the target domain. The results demonstrated a statistically significant improvement in classification metrics across both domains by replacing batch normalization with instance normalization in the baseline model, generating augmented input views using the Global Intensity Non-linear (GIN) augmentation method, and incorporating a supervised contrastive loss alongside the classification loss to align representations of samples with the same label. The GIN method with contrastive loss performed better than all evaluated single-source domain generalization methods when using 3D instance normalization. Comparing GIN with and without contrastive loss (for both normalization types) showed that adding contrastive loss consistently led to better performance.
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2504.15743.pdf' target='_blank'>https://arxiv.org/pdf/2504.15743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seung Gyu Jeong, Sung Woo Nam, Seong Kwan Jung, Seong-Eun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15743">iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Respiratory auscultation is crucial for early detection of pediatric pneumonia, a condition that can quickly worsen without timely intervention. In areas with limited physician access, effective auscultation is challenging. We present a smartphone-based system that leverages built-in microphones and advanced deep learning algorithms to detect abnormal respiratory sounds indicative of pneumonia risk. Our end-to-end deep learning framework employs domain generalization to integrate a large electronic stethoscope dataset with a smaller smartphone-derived dataset, enabling robust feature learning for accurate respiratory assessments without expensive equipment. The accompanying mobile application guides caregivers in collecting high-quality lung sound samples and provides immediate feedback on potential pneumonia risks. User studies show strong classification performance and high acceptance, demonstrating the system's ability to facilitate proactive interventions and reduce preventable childhood pneumonia deaths. By seamlessly integrating into ubiquitous smartphones, this approach offers a promising avenue for more equitable and comprehensive remote pediatric care.
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2504.12966.pdf' target='_blank'>https://arxiv.org/pdf/2504.12966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanmei Wang, Xiyao Liu, Fupeng Chu, Zhi Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12966">Vision and Language Integration for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain generalization aims at training on source domains to uncover a domain-invariant feature space, allowing the model to perform robust generalization ability on unknown target domains. However, due to domain gaps, it is hard to find reliable common image feature space, and the reason for that is the lack of suitable basic units for images. Different from image in vision space, language has comprehensive expression elements that can effectively convey semantics. Inspired by the semantic completeness of language and intuitiveness of image, we propose VLCA, which combine language space and vision space, and connect the multiple image domains by using semantic space as the bridge domain. Specifically, in language space, by taking advantage of the completeness of language basic units, we tend to capture the semantic representation of the relations between categories through word vector distance. Then, in vision space, by taking advantage of the intuitiveness of image features, the common pattern of sample features with the same class is explored through low-rank approximation. In the end, the language representation is aligned with the vision representation through the multimodal space of text and image. Experiments demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2503.23605.pdf' target='_blank'>https://arxiv.org/pdf/2503.23605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kasra Jalaldoust, Alexis Bellot, Elias Bareinboim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23605">Partial Transportability for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A fundamental task in AI is providing performance guarantees for predictions made in unseen domains. In practice, there can be substantial uncertainty about the distribution of new data, and corresponding variability in the performance of existing predictors. Building on the theory of partial identification and transportability, this paper introduces new results for bounding the value of a functional of the target distribution, such as the generalization error of a classifier, given data from source domains and assumptions about the data generating mechanisms, encoded in causal diagrams. Our contribution is to provide the first general estimation technique for transportability problems, adapting existing parameterization schemes such Neural Causal Models to encode the structural constraints necessary for cross-population inference. We demonstrate the expressiveness and consistency of this procedure and further propose a gradient-based optimization scheme for making scalable inferences in practice. Our results are corroborated with experiments.
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2503.20897.pdf' target='_blank'>https://arxiv.org/pdf/2503.20897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Venuri Amarasinghe, Asini Jayakody, Isun Randila, Kalinga Bandara, Chamuditha Jayanga Galappaththige, Ranga Rodrigo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20897">Feature Modulation for Semi-Supervised Domain Generalization without Domain Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised domain generalization (SSDG) leverages a small fraction of labeled data alongside unlabeled data to enhance model generalization. Most of the existing SSDG methods rely on pseudo-labeling (PL) for unlabeled data, often assuming access to domain labels-a privilege not always available. However, domain shifts introduce domain noise, leading to inconsistent PLs that degrade model performance. Methods derived from FixMatch suffer particularly from lower PL accuracy, reducing the effectiveness of unlabeled data. To address this, we tackle the more challenging domain-label agnostic SSDG, where domain labels for unlabeled data are not available during training. First, we propose a feature modulation strategy that enhances class-discriminative features while suppressing domain-specific information. This modulation shifts features toward Similar Average Representations-a modified version of class prototypes-that are robust across domains, encouraging the classifier to distinguish between closely related classes and feature extractor to form tightly clustered, domain-invariant representations. Second, to mitigate domain noise and improve pseudo-label accuracy, we introduce a loss-scaling function that dynamically lowers the fixed confidence threshold for pseudo-labels, optimizing the use of unlabeled data. With these key innovations, our approach achieves significant improvements on four major domain generalization benchmarks-even without domain labels. We will make the code available.
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2503.19152.pdf' target='_blank'>https://arxiv.org/pdf/2503.19152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shoffan Saifullah, RafaÅ DreÅ¼ewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19152">PSO-UNet: Particle Swarm-Optimized U-Net Framework for Precise Multimodal Brain Tumor Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation, particularly for brain tumor analysis, demands precise and computationally efficient models due to the complexity of multimodal MRI datasets and diverse tumor morphologies. This study introduces PSO-UNet, which integrates Particle Swarm Optimization (PSO) with the U-Net architecture for dynamic hyperparameter optimization. Unlike traditional manual tuning or alternative optimization approaches, PSO effectively navigates complex hyperparameter search spaces, explicitly optimizing the number of filters, kernel size, and learning rate. PSO-UNet substantially enhances segmentation performance, achieving Dice Similarity Coefficients (DSC) of 0.9578 and 0.9523 and Intersection over Union (IoU) scores of 0.9194 and 0.9097 on the BraTS 2021 and Figshare datasets, respectively. Moreover, the method reduces computational complexity significantly, utilizing only 7.8 million parameters and executing in approximately 906 seconds, markedly faster than comparable U-Net-based frameworks. These outcomes underscore PSO-UNet's robust generalization capabilities across diverse MRI modalities and tumor classifications, emphasizing its clinical potential and clear advantages over conventional hyperparameter tuning methods. Future research will explore hybrid optimization strategies and validate the framework against other bio-inspired algorithms to enhance its robustness and scalability.
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2503.10354.pdf' target='_blank'>https://arxiv.org/pdf/2503.10354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nevidu Jayatilleke, Ruvan Weerasinghe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10354">A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic patent summarization approaches that help in the patent analysis and comprehension procedure are in high demand due to the colossal growth of innovations. The development of natural language processing (NLP), text mining, and deep learning has notably amplified the efficacy of text summarization models for abundant types of documents. Summarizing patent text remains a pertinent challenge due to the labyrinthine writing style of these documents, which includes technical and legal intricacies. Additionally, these patent document contents are considerably lengthier than archetypal documents, which complicates the process of extracting pertinent information for summarization. Embodying extractive and abstractive text summarization methodologies into a hybrid framework, this study proposes a system for efficiently creating abstractive summaries of patent records. The procedure involves leveraging the LexRank graph-based algorithm to retrieve the important sentences from input parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART) model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for producing text summaries. This is accompanied by methodical testing and evaluation strategies. Furthermore, the author employed certain meta-learning techniques to achieve Domain Generalization (DG) of the abstractive component across multiple patent fields.
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2503.09661.pdf' target='_blank'>https://arxiv.org/pdf/2503.09661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johnson Loh, Lyubov Dudchenko, Justus Viga, Tobias Gemmeke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09661">Towards Hardware Supported Domain Generalization in DNN-Based Edge Computing Devices for Health Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural network (DNN) models have shown remarkable success in many real-world scenarios, such as object detection and classification. Unfortunately, these models are not yet widely adopted in health monitoring due to exceptionally high requirements for model robustness and deployment in highly resource-constrained devices. In particular, the acquisition of biosignals, such as electrocardiogram (ECG), is subject to large variations between training and deployment, necessitating domain generalization (DG) for robust classification quality across sensors and patients. The continuous monitoring of ECG also requires the execution of DNN models in convenient wearable devices, which is achieved by specialized ECG accelerators with small form factor and ultra-low power consumption. However, combining DG capabilities with ECG accelerators remains a challenge. This article provides a comprehensive overview of ECG accelerators and DG methods and discusses the implication of the combination of both domains, such that multi-domain ECG monitoring is enabled with emerging algorithm-hardware co-optimized systems. Within this context, an approach based on correction layers is proposed to deploy DG capabilities on the edge. Here, the DNN fine-tuning for unknown domains is limited to a single layer, while the remaining DNN model remains unmodified. Thus, computational complexity (CC) for DG is reduced with minimal memory overhead compared to conventional fine-tuning of the whole DNN model. The DNN model-dependent CC is reduced by more than 2.5x compared to DNN fine-tuning at an average increase of F1 score by more than 20% on the generalized target domain. In summary, this article provides a novel perspective on robust DNN classification on the edge for health monitoring applications.
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2502.10277.pdf' target='_blank'>https://arxiv.org/pdf/2502.10277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin-Chih Chelsea Wang, Tsao-Lun Chen, Shankeeth Vinayahalingam, Tai-Hsien Wu, Chu Wei Chang, Hsuan Hao Chang, Hung-Jen Wei, Mu-Hsiung Chen, Ching-Chang Ko, David Anssari Moin, Bram van Ginneken, Tong Xi, Hsiao-Cheng Tsai, Min-Huey Chen, Tzu-Ming Harry Hsu, Hye Chou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10277">Artificial Intelligence to Assess Dental Findings from Panoramic Radiographs -- A Multinational Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dental panoramic radiographs (DPRs) are widely used in clinical practice for comprehensive oral assessment but present challenges due to overlapping structures and time constraints in interpretation.
  This study aimed to establish a solid baseline for the AI-automated assessment of findings in DPRs by developing, evaluating an AI system, and comparing its performance with that of human readers across multinational data sets.
  We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and Taiwan), focusing on 8 types of dental findings. The AI system combined object detection and semantic segmentation techniques for per-tooth finding identification. Performance metrics included sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC). AI generalizability was tested across data sets, and performance was compared with human dental practitioners.
  The AI system demonstrated comparable or superior performance to human readers, particularly +67.9% (95% CI: 54.0%-81.9%; p < .001) sensitivity for identifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008) sensitivity for identifying missing teeth. The AI achieved a macro-averaged AUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with the reference were comparable to inter-human agreements in 7 of 8 findings except for caries (p = .024). The AI system demonstrated robust generalization across diverse imaging and demographic settings and processed images 79 times faster (95% CI: 75-82) than human readers.
  The AI system effectively assessed findings in DPRs, achieving performance on par with or better than human experts while significantly reducing interpretation time. These results highlight the potential for integrating AI into clinical workflows to improve diagnostic efficiency and accuracy, and patient management.
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2502.08155.pdf' target='_blank'>https://arxiv.org/pdf/2502.08155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhou, Yu Cheng, Songlin Li, Hongwang Zhang, Chenxu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08155">DGSense: A Domain Generalization Framework for Wireless Sensing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wireless sensing is of great benefits to our daily lives. However, wireless signals are sensitive to the surroundings. Various factors, e.g. environments, locations, and individuals, may induce extra impact on wireless propagation. Such a change can be regarded as a domain, in which the data distribution shifts. A vast majority of the sensing schemes are learning-based. They are dependent on the training domains, resulting in performance degradation in unseen domains. Researchers have proposed various solutions to address this issue. But these solutions leverage either semi-supervised or unsupervised domain adaptation techniques. They still require some data in the target domains and do not perform well in unseen domains. In this paper, we propose a domain generalization framework DGSense, to eliminate the domain dependence problem in wireless sensing. The framework is a general solution working across diverse sensing tasks and wireless technologies. Once the sensing model is built, it can generalize to unseen domains without any data from the target domain. To achieve the goal, we first increase the diversity of the training set by a virtual data generator, and then extract the domain independent features via episodic training between the main feature extractor and the domain feature extractors. The feature extractors employ a pre-trained Residual Network (ResNet) with an attention mechanism for spatial features, and a 1D Convolutional Neural Network (1DCNN) for temporal features. To demonstrate the effectiveness and generality of DGSense, we evaluated on WiFi gesture recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall detection. All the systems exhibited high generalization capability to unseen domains, including new users, locations, and environments, free of new data and retraining.
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2501.00116.pdf' target='_blank'>https://arxiv.org/pdf/2501.00116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaozhou You, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00116">Text-to-Image GAN with Pretrained Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating desired images conditioned on given text descriptions has received lots of attention. Recently, diffusion models and autoregressive models have demonstrated their outstanding expressivity and gradually replaced GAN as the favored architectures for text-to-image synthesis. However, they still face some obstacles: slow inference speed and expensive training costs. To achieve more powerful and faster text-to-image synthesis under complex scenes, we propose TIGER, a text-to-image GAN with pretrained representations. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. (i) The vision-empowered discriminator absorbs the complex scene understanding ability and the domain generalization ability from pretrained vision models to enhance model performance. Unlike previous works, we explore stacking multiple pretrained models in our discriminator to collect multiple different representations. (ii) The high-capacity generator aims to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks (HFBlock). And the HFBlock contains several deep fusion modules and a global fusion module, which play different roles to benefit our model. Extensive experiments demonstrate the outstanding performance of our proposed TIGER both on standard and zero-shot text-to-image synthesis tasks. On the standard text-to-image synthesis task, TIGER achieves state-of-the-art performance on two challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On the zero-shot text-to-image synthesis task, we achieve comparable performance with fewer model parameters, smaller training data size and faster inference speed. Additionally, more experiments and analyses are conducted in the Supplementary Material.
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2412.12449.pdf' target='_blank'>https://arxiv.org/pdf/2412.12449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongya Wu, Xin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12449">Adversarially robust generalization theory via Jacobian regularization for deep neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Powerful deep neural networks are vulnerable to adversarial attacks. To obtain adversarially robust models, researchers have separately developed adversarial training and Jacobian regularization techniques. There are abundant theoretical and empirical studies for adversarial training, but theoretical foundations for Jacobian regularization are still lacking. In this study, we show that Jacobian regularization is closely related to adversarial training in that $\ell_{2}$ or $\ell_{1}$ Jacobian regularized loss serves as an approximate upper bound on the adversarially robust loss under $\ell_{2}$ or $\ell_{\infty}$ adversarial attack respectively. Further, we establish the robust generalization gap for Jacobian regularized risk minimizer via bounding the Rademacher complexity of both the standard loss function class and Jacobian regularization function class. Our theoretical results indicate that the norms of Jacobian are related to both standard and robust generalization. We also perform experiments on MNIST data classification to demonstrate that Jacobian regularized risk minimization indeed serves as a surrogate for adversarially robust risk minimization, and that reducing the norms of Jacobian can improve both standard and robust generalization. This study promotes both theoretical and empirical understandings to adversarially robust generalization via Jacobian regularization.
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2412.07214.pdf' target='_blank'>https://arxiv.org/pdf/2412.07214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun-Peng Zhu, Boyan Niu, Peng Cai, Zheming Ni, Jianwei Wan, Kai Xu, Jiajun Huang, Shengbo Ma, Bing Wang, Xuan Zhou, Guanglei Bao, Donghui Zhang, Liu Tang, Qi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07214">Towards Automated Cross-domain Exploratory Data Analysis through Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploratory data analysis (EDA), coupled with SQL, is essential for data analysts involved in data exploration and analysis. However, data analysts often encounter two primary challenges: (1) the need to craft SQL queries skillfully, and (2) the requirement to generate suitable visualization types that enhance the interpretation of query results. Due to its significance, substantial research efforts have been made to explore different approaches to address these challenges, including leveraging large language models (LLMs). However, existing methods fail to meet real-world data exploration requirements primarily due to (1) complex database schema; (2) unclear user intent; (3) limited cross-domain generalization capability; and (4) insufficient end-to-end text-to-visualization capability.
  This paper presents TiInsight, an automated SQL-based cross-domain exploratory data analysis system. First, we propose hierarchical data context (i.e., HDC), which leverages LLMs to summarize the contexts related to the database schema, which is crucial for open-world EDA systems to generalize across data domains. Second, the EDA system is divided into four components (i.e., stages): HDC generation, question clarification and decomposition, text-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart). Finally, we implemented an end-to-end EDA system with a user-friendly GUI interface in the production environment at PingCAP. We have also open-sourced all APIs of TiInsight to facilitate research within the EDA community. Through extensive evaluations by a real-world user study, we demonstrate that TiInsight offers remarkable performance compared to human experts. Specifically, TiSQL achieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It also demonstrates state-of-the-art performance on the Bird dataset.
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2412.05572.pdf' target='_blank'>https://arxiv.org/pdf/2412.05572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Xu, Taiping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05572">From Deterministic to Probabilistic: A Novel Perspective on Domain Generalization for Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional domain generalization methods often rely on domain alignment to reduce inter-domain distribution differences and learn domain-invariant representations. However, domain shifts are inherently difficult to eliminate, which limits model generalization. To address this, we propose an innovative framework that enhances data representation quality through probabilistic modeling and contrastive learning, reducing dependence on domain alignment and improving robustness under domain variations. Specifically, we combine deterministic features with uncertainty modeling to capture comprehensive feature distributions. Contrastive learning enforces distribution-level alignment by aligning the mean and covariance of feature distributions, enabling the model to dynamically adapt to domain variations and mitigate distribution shifts. Additionally, we design a frequency-domain-based structural enhancement strategy using discrete wavelet transforms to preserve critical structural details and reduce visual distortions caused by style variations. Experimental results demonstrate that the proposed framework significantly improves segmentation performance, providing a robust solution to domain generalization challenges in medical image segmentation.
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2411.16959.pdf' target='_blank'>https://arxiv.org/pdf/2411.16959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ezra Ameperosa, Jeremy A. Collins, Mrinal Jain, Animesh Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16959">RoCoDA: Counterfactual Data Augmentation for Data-Efficient Robot Learning from Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning in robotics faces significant challenges in generalization due to the complexity of robotic environments and the high cost of data collection. We introduce RoCoDA, a novel method that unifies the concepts of invariance, equivariance, and causality within a single framework to enhance data augmentation for imitation learning. RoCoDA leverages causal invariance by modifying task-irrelevant subsets of the environment state without affecting the policy's output. Simultaneously, we exploit SE(3) equivariance by applying rigid body transformations to object poses and adjusting corresponding actions to generate synthetic demonstrations. We validate RoCoDA through extensive experiments on five robotic manipulation tasks, demonstrating improvements in policy performance, generalization, and sample efficiency compared to state-of-the-art data augmentation methods. Our policies exhibit robust generalization to unseen object poses, textures, and the presence of distractors. Furthermore, we observe emergent behavior such as re-grasping, indicating policies trained with RoCoDA possess a deeper understanding of task dynamics. By leveraging invariance, equivariance, and causality, RoCoDA provides a principled approach to data augmentation in imitation learning, bridging the gap between geometric symmetries and causal reasoning. Project Page: https://rocoda.github.io
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2411.14883.pdf' target='_blank'>https://arxiv.org/pdf/2411.14883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Xu, Taiping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14883">Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain-invariant representation learning is a powerful method for domain generalization. Previous approaches face challenges such as high computational demands, training instability, and limited effectiveness with high-dimensional data, potentially leading to the loss of valuable features. To address these issues, we hypothesize that an ideal generalized representation should exhibit similar pattern responses within the same channel across cross-domain images. Based on this hypothesis, we use deep features from the source domain as queries, and deep features from the generated domain as keys and values. Through a cross-channel attention mechanism, the original deep features are reconstructed into robust regularization representations, forming an explicit constraint that guides the model to learn domain-invariant representations. Additionally, style augmentation is another common method. However, existing methods typically generate new styles through convex combinations of source domains, which limits the diversity of training samples by confining the generated styles to the original distribution. To overcome this limitation, we propose an Adaptive Feature Blending (AFB) method that generates out-of-distribution samples while exploring the in-distribution space, significantly expanding the domain range. Extensive experimental results demonstrate that our proposed methods achieve superior performance on two standard domain generalization benchmarks for medical image segmentation.
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2410.16337.pdf' target='_blank'>https://arxiv.org/pdf/2410.16337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Deng, Baoxing Li, Xu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16337">Disambiguating Monocular Reconstruction of 3D Clothed Human with Spatial-Temporal Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing 3D clothed humans from monocular camera data is highly challenging due to viewpoint limitations and image ambiguity. While implicit function-based approaches, combined with prior knowledge from parametric models, have made significant progress, there are still two notable problems. Firstly, the back details of human models are ambiguous due to viewpoint invisibility. The quality of the back details depends on the back normal map predicted by a convolutional neural network (CNN). However, the CNN lacks global information awareness for comprehending the back texture, resulting in excessively smooth back details. Secondly, a single image suffers from local ambiguity due to lighting conditions and body movement. However, implicit functions are highly sensitive to pixel variations in ambiguous regions. To address these ambiguities, we propose the Spatial-Temporal Transformer (STT) network for 3D clothed human reconstruction. A spatial transformer is employed to extract global information for normal map prediction. The establishment of global correlations facilitates the network in comprehending the holistic texture and shape of the human body. Simultaneously, to compensate for local ambiguity in images, a temporal transformer is utilized to extract temporal features from adjacent frames. The incorporation of temporal features can enhance the accuracy of input features in implicit networks. Furthermore, to obtain more accurate temporal features, joint tokens are employed to establish local correspondences between frames. Experimental results on the Adobe and MonoPerfCap datasets have shown that our method outperforms state-of-the-art methods and maintains robust generalization even under low-light outdoor conditions.
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2409.16984.pdf' target='_blank'>https://arxiv.org/pdf/2409.16984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>P Aditya Sreekar, Sahil Verma, Suransh Chopra, Sarik Ghazarian, Abhishek Persad, Narayanan Sadagopan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16984">AXCEL: Automated eXplainable Consistency Evaluation using LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are widely used in both industry and academia for various tasks, yet evaluating the consistency of generated text responses continues to be a challenge. Traditional metrics like ROUGE and BLEU show a weak correlation with human judgment. More sophisticated metrics using Natural Language Inference (NLI) have shown improved correlations but are complex to implement, require domain-specific training due to poor cross-domain generalization, and lack explainability. More recently, prompt-based metrics using LLMs as evaluators have emerged; while they are easier to implement, they still lack explainability and depend on task-specific prompts, which limits their generalizability. This work introduces Automated eXplainable Consistency Evaluation using LLMs (AXCEL), a prompt-based consistency metric which offers explanations for the consistency scores by providing detailed reasoning and pinpointing inconsistent text spans. AXCEL is also a generalizable metric which can be adopted to multiple tasks without changing the prompt. AXCEL outperforms both non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting inconsistencies across summarization by 8.7%, free text generation by 6.2%, and data-to-text conversion tasks by 29.4%. We also evaluate the influence of underlying LLMs on prompt based metric performance and recalibrate the SOTA prompt-based metrics with the latest LLMs for fair comparison. Further, we show that AXCEL demonstrates strong performance using open source LLMs.
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2409.16346.pdf' target='_blank'>https://arxiv.org/pdf/2409.16346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zhang, Roeland Wiersema, Juan Carrasquilla, Lukasz Cincio, Yong Baek Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16346">Scalable quantum dynamics compilation via quantum machine learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum dynamics compilation is an important task for improving quantum simulation efficiency: It aims to synthesize multi-qubit target dynamics into a circuit consisting of as few elementary gates as possible. Compared to deterministic methods such as Trotterization, variational quantum compilation (VQC) methods employ variational optimization to reduce gate costs while maintaining high accuracy. In this work, we explore the potential of a VQC scheme by making use of out-of-distribution generalization results in quantum machine learning (QML): By learning the action of a given many-body dynamics on a small data set of product states, we can obtain a unitary circuit that generalizes to highly entangled states such as the Haar random states. The efficiency in training allows us to use tensor network methods to compress such time-evolved product states by exploiting their low entanglement features. Our approach exceeds state-of-the-art compilation results in both system size and accuracy in one dimension ($1$D). For the first time, we extend VQC to systems on two-dimensional (2D) strips with a quasi-1D treatment, demonstrating a significant resource advantage over standard Trotterization methods, highlighting the method's promise for advancing quantum simulation tasks on near-term quantum processors.
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2409.02146.pdf' target='_blank'>https://arxiv.org/pdf/2409.02146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dexin Duan, Peilin liu, Bingwei Hui, Fei Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02146">Brain-Inspired Online Adaptation for Remote Sensing with Spiking Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-device computing, or edge computing, is becoming increasingly important for remote sensing, particularly in applications like deep network-based perception on on-orbit satellites and unmanned aerial vehicles (UAVs). In these scenarios, two brain-like capabilities are crucial for remote sensing models: (1) high energy efficiency, allowing the model to operate on edge devices with limited computing resources, and (2) online adaptation, enabling the model to quickly adapt to environmental variations, weather changes, and sensor drift. This work addresses these needs by proposing an online adaptation framework based on spiking neural networks (SNNs) for remote sensing. Starting with a pretrained SNN model, we design an efficient, unsupervised online adaptation algorithm, which adopts an approximation of the BPTT algorithm and only involves forward-in-time computation that significantly reduces the computational complexity of SNN adaptation learning. Besides, we propose an adaptive activation scaling scheme to boost online SNN adaptation performance, particularly in low time-steps. Furthermore, for the more challenging remote sensing detection task, we propose a confidence-based instance weighting scheme, which substantially improves adaptation performance in the detection task. To our knowledge, this work is the first to address the online adaptation of SNNs. Extensive experiments on seven benchmark datasets across classification, segmentation, and detection tasks demonstrate that our proposed method significantly outperforms existing domain adaptation and domain generalization approaches under varying weather conditions. The proposed method enables energy-efficient and fast online adaptation on edge devices, and has much potential in applications such as remote perception on on-orbit satellites and UAV.
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2409.01930.pdf' target='_blank'>https://arxiv.org/pdf/2409.01930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajesh Upadhayayaya, Manish Raj Osti, Zachary Smith, Chritopher Kottmyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01930">Efficient LLM Context Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) demonstrate proficiency across diverse tasks but often require targeted adaptations for specific applications. Various methods have been proposed to facilitate this adaptation, including fewshot fine-tuning, in-context learning, and context distillation. This paper specifically investigates context distillation a method that extends the utility of task-specific examples by internalizing them, thus augmenting the example set accessible for model inference. We conduct a comparative analysis of context distillation with in-context learning (ICL) and few-shot fine-tuning (FT), aiming to ascertain the efficacy of context distillation in adapting models using minimal in-context examples. Employing matched datasets from Mobach, our experiments leverage OPT models of various sizes. The results indicate that context distillation effectively adapts models, with student models attaining comparable in-domain and out-of-domain accuracies to in-context learning. Although context distillation surpasses ICL in out-of-domain generalization, it does not achieve the performance levels of FT. However, the reduced dataset size and computational demands position context distillation as a viable alternative, especially for smaller datasets. Overall, this study presents context distillation as an efficient and potent method for customizing LLMs to specific tasks.
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2601.21334.pdf' target='_blank'>https://arxiv.org/pdf/2601.21334.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pritika Vig, Ren-Chin Wu, William Lotter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21334">Do Pathology Foundation Models Encode Disease Progression? A Pseudotime Analysis of Visual Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision foundation models trained on discretely sampled images achieve strong performance on classification benchmarks, yet whether their representations encode the continuous processes underlying their training data remains unclear. This question is especially pertinent in computational pathology, where we posit that models whose latent representations implicitly capture continuous disease progression may better reflect underlying biology, support more robust generalization, and enable quantitative analyses of features associated with disease transitions. Using diffusion pseudotime, a method developed to infer developmental trajectories from single-cell transcriptomics, we probe whether foundation models organize disease states along coherent progression directions in representation space. Across four cancer progressions and six models, we find that all pathology-specific models recover trajectory orderings significantly exceeding null baselines, with vision-only models achieving the highest fidelities $(τ> 0.78$ on CRC-Serrated). Model rankings by trajectory fidelity on reference diseases strongly predict few-shot classification performance on held-out diseases ($ρ= 0.92$), and exploratory analysis shows cell-type composition varies smoothly along inferred trajectories in patterns consistent with known stromal remodeling. Together, these results demonstrate that vision foundation models can implicitly learn to represent continuous processes from independent static observations, and that trajectory fidelity provides a complementary measure of representation quality beyond downstream performance. While demonstrated in pathology, this framework could be applied to other domains where continuous processes are observed through static snapshots.
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2601.20176.pdf' target='_blank'>https://arxiv.org/pdf/2601.20176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Cheng, Ang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20176">Causal-Driven Feature Evaluation for Cross-Domain Image Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution (OOD) generalization remains a fundamental challenge in real-world classification, where test distributions often differ substantially from training data. Most existing approaches pursue domain-invariant representations, implicitly assuming that invariance implies reliability. However, features that are invariant across domains are not necessarily causally effective for prediction. In this work, we revisit OOD classification from a causal perspective and propose to evaluate learned representations based on their necessity and sufficiency under distribution shift. We introduce an explicit segment-level framework that directly measures causal effectiveness across domains, providing a more faithful criterion than invariance alone. Experiments on multi-domain benchmarks demonstrate consistent improvements in OOD performance, particularly under challenging domain shifts, highlighting the value of causal evaluation for robust generalization.
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2601.20131.pdf' target='_blank'>https://arxiv.org/pdf/2601.20131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deep Shah, Sanket Badhe, Nehal Kathrotia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.20131">Taxonomy of the Retrieval System Framework: Pitfalls and Paradigms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing an embedding retrieval system requires navigating a complex design space of conflicting trade-offs between efficiency and effectiveness. This work structures these decisions as a vertical traversal of the system design stack. We begin with the Representation Layer by examining how loss functions and architectures, specifically Bi-encoders and Cross-encoders, define semantic relevance and geometric projection. Next, we analyze the Granularity Layer and evaluate how segmentation strategies like Atomic and Hierarchical chunking mitigate information bottlenecks in long-context documents. Moving to the Orchestration Layer, we discuss methods that transcend the single-vector paradigm, including hierarchical retrieval, agentic decomposition, and multi-stage reranking pipelines to resolve capacity limitations. Finally, we address the Robustness Layer by identifying architectural mitigations for domain generalization failures, lexical blind spots, and the silent degradation of retrieval quality due to temporal drift. By categorizing these limitations and design choices, we provide a comprehensive framework for practitioners to optimize the efficiency-effectiveness frontier in modern neural search systems.
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2601.19561.pdf' target='_blank'>https://arxiv.org/pdf/2601.19561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayoung Kang, JongWon Kim, Jiho Park, Keonseock Lee, Ji-Woong Choi, Jinhyun So
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19561">AROMMA: Unifying Olfactory Embeddings for Single Molecules and Mixtures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Public olfaction datasets are small and fragmented across single molecules and mixtures, limiting learning of generalizable odor representations. Recent works either learn single-molecule embeddings or address mixtures via similarity or pairwise label prediction, leaving representations separate and unaligned. In this work, we propose AROMMA, a framework that learns a unified embedding space for single molecules and two-molecule mixtures. Each molecule is encoded by a chemical foundation model and the mixtures are composed by an attention-based aggregator, ensuring both permutation invariance and asymmetric molecular interactions. We further align odor descriptor sets using knowledge distillation and class-aware pseudo-labeling to enrich missing mixture annotations. AROMMA achieves state-of-the-art performance in both single-molecule and molecule-pair datasets, with up to 19.1% AUROC improvement, demonstrating a robust generalization in two domains.
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2601.17056.pdf' target='_blank'>https://arxiv.org/pdf/2601.17056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zahra Vaseqi, James Clark
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17056">Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2601.14336.pdf' target='_blank'>https://arxiv.org/pdf/2601.14336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Sharma, Vivek Yelleti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.14336">Log anomaly detection via Meta Learning and Prototypical Networks for Cross domain generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Log anomaly detection is essential for system reliability, but it is extremely challenging to do considering it involves class imbalance. Additionally, the models trained in one domain are not applicable to other domains, necessitating the need for cross-domain adaptation (such as HDFS and Linux). Traditional detection models often fail to generalize due to significant data drift and the inherent absence of labeled anomalies in new target domains. To handle the above challenges, we proposed a new end-to-end framework based on a meta-learning approach. Our methodology first gets the data ready by combining a Drain3 log parsing mechanism with a dynamic drift-based labeling technique that uses semantic and fuzzy matching to move existing anomaly knowledge from one source to another. BERT-based semantic embeddings are obtained, and the feature selection is invoked to reduce the dimensionality. Later, Model Agnostic Meta-Learning (MAML) and Prototypical Networks models are trained to adapt quickly and effectively. The SMOTE oversampling method is employed to handle imbalances in the data. All the results are obtained by employing the leave-one-out source method, and the corresponding mean F1 scores are reported. Our empirical findings validate that the proposed meta-learning-driven approach yielded the highest mean F1 score and proved to be effective for cross-domain settings.
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2601.12325.pdf' target='_blank'>https://arxiv.org/pdf/2601.12325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eli Passov, Nathan S. Netanyahu, Yosi Keller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12325">Multi-Sensor Matching with HyperNetworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground/aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2601.11724.pdf' target='_blank'>https://arxiv.org/pdf/2601.11724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muditha Fernando, Kajhanan Kailainathan, Krishnakanth Nagaratnam, Isuranga Udaravi Bandara Senavirathne, Ranga Rodrigo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.11724">SemAlign: Language Guided Semi-supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2601.07384.pdf' target='_blank'>https://arxiv.org/pdf/2601.07384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamda Hmida, Hsiu-Wen Chang Joly, Youssef Mesri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07384">CompNO: A Novel Foundation Model approach for solving Partial Differential Equations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial differential equations (PDEs) govern a wide range of physical phenomena, but their numerical solution remains computationally demanding, especially when repeated simulations are required across many parameter settings. Recent Scientific Foundation Models (SFMs) aim to alleviate this cost by learning universal surrogates from large collections of simulated systems, yet they typically rely on monolithic architectures with limited interpretability and high pretraining expense. In this work we introduce Compositional Neural Operators (CompNO), a compositional neural operator framework for parametric PDEs. Instead of pretraining a single large model on heterogeneous data, CompNO first learns a library of Foundation Blocks, where each block is a parametric Fourier neural operator specialized to a fundamental differential operator (e.g. convection, diffusion, nonlinear convection). These blocks are then assembled, via lightweight Adaptation Blocks, into task-specific solvers that approximate the temporal evolution operator for target PDEs. A dedicated boundary-condition operator further enforces Dirichlet constraints exactly at inference time. We validate CompNO on one-dimensional convection, diffusion, convection--diffusion and Burgers' equations from the PDEBench suite. The proposed framework achieves lower relative L2 error than strong baselines (PFNO, PDEFormer and in-context learning based models) on linear parametric systems, while remaining competitive on nonlinear Burgers' flows. The model maintains exact boundary satisfaction with zero loss at domain boundaries, and exhibits robust generalization across a broad range of Peclet and Reynolds numbers. These results demonstrate that compositional neural operators provide a scalable and physically interpretable pathway towards foundation models for PDEs.
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2601.07008.pdf' target='_blank'>https://arxiv.org/pdf/2601.07008.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Liang, Fang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07008">Lexicalized Constituency Parsing for Middle Dutch: Low-resource Training and Cross-Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have seen growing interest in applying neural networks and contextualized word embeddings to the parsing of historical languages. However, most advances have focused on dependency parsing, while constituency parsing for low-resource historical languages like Middle Dutch has received little attention. In this paper, we adapt a transformer-based constituency parser to Middle Dutch, a highly heterogeneous and low-resource language, and investigate methods to improve both its in-domain and cross-domain performance. We show that joint training with higher-resource auxiliary languages increases F1 scores by up to 0.73, with the greatest gains achieved from languages that are geographically and temporally closer to Middle Dutch. We further evaluate strategies for leveraging newly annotated data from additional domains, finding that fine-tuning and data combination yield comparable improvements, and our neural parser consistently outperforms the currently used PCFG-based parser for Middle Dutch. We further explore feature-separation techniques for domain adaptation and demonstrate that a minimum threshold of approximately 200 examples per domain is needed to effectively enhance cross-domain performance.
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2601.03812.pdf' target='_blank'>https://arxiv.org/pdf/2601.03812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adilkhan Alikhanov, Aidar Amangeldi, Diar Demeubay, Dilnaz Akhmetzhan, Nurbek Moldakhmetov, Omar Polat, Galymzhan Zharas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03812">AI Generated Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2601.02884.pdf' target='_blank'>https://arxiv.org/pdf/2601.02884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hana Yahia, Bruno Figliuzzi, Florent Di Meglio, Laurent Gerbaud, Stephane Menand, Mohamed Mahjoub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02884">Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on 60 second labeled sequences of 1 Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of 10% and 8%, respectively, over the baseline model. Most importantly, severe events are detected 60% of the time, against 20% for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach.
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2601.01356.pdf' target='_blank'>https://arxiv.org/pdf/2601.01356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dang H. Pham, Tu N. Nguyen, Hoa N. Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01356">Advanced Machine Learning Approaches for Enhancing Person Re-Identification Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Person re-identification (ReID) plays a critical role in intelligent surveillance systems by linking identities across multiple cameras in complex environments. However, ReID faces significant challenges such as appearance variations, domain shifts, and limited labeled data. This dissertation proposes three advanced approaches to enhance ReID performance under supervised, unsupervised domain adaptation (UDA), and fully unsupervised settings. First, SCM-ReID integrates supervised contrastive learning with hybrid loss optimization (classification, center, triplet, and centroid-triplet losses), improving discriminative feature representation and achieving state-of-the-art accuracy on Market-1501 and CUHK03 datasets. Second, for UDA, IQAGA and DAPRH combine GAN-based image augmentation, domain-invariant mapping, and pseudo-label refinement to mitigate domain discrepancies and enhance cross-domain generalization. Experiments demonstrate substantial gains over baseline methods, with mAP and Rank-1 improvements up to 12% in challenging transfer scenarios. Finally, ViTC-UReID leverages Vision Transformer-based feature encoding and camera-aware proxy learning to boost unsupervised ReID. By integrating global and local attention with camera identity constraints, this method significantly outperforms existing unsupervised approaches on large-scale benchmarks. Comprehensive evaluations across CUHK03, Market-1501, DukeMTMC-reID, and MSMT17 confirm the effectiveness of the proposed methods. The contributions advance ReID research by addressing key limitations in feature learning, domain adaptation, and label noise handling, paving the way for robust deployment in real-world surveillance systems.
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2512.19530.pdf' target='_blank'>https://arxiv.org/pdf/2512.19530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongsheng Xing, Qiuxin Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19530">Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments. Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \textbf{MSE of 0.0039} ($\pm$ 0.0003), representing a 60\% error reduction over competitive baselines and a $>25\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2512.16567.pdf' target='_blank'>https://arxiv.org/pdf/2512.16567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yin Zhang, Yongqiang Zhang, Yaoyue Zheng, Bogdan Raducanu, Dan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16567">Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2512.16271.pdf' target='_blank'>https://arxiv.org/pdf/2512.16271.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Geofrey Owino, Bernard Shibwabo Kasamani, Ahmed M. Abdelmoniem, Edem Wornyo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16271">Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework. DACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation. Experimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2512.15414.pdf' target='_blank'>https://arxiv.org/pdf/2512.15414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehab Alkhateeb, Ali Ghorbani, Arash Habibi Lashkari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15414">Packed Malware Detection Using Grayscale Binary-to-Image Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting packed executables is a critical step in malware analysis, as packing obscures the original code and complicates static inspection. This study evaluates both classical feature-based methods and deep learning approaches that transform binary executables into visual representations, specifically, grayscale byte plots, and employ convolutional neural networks (CNNs) for automated classification of packed and non-packed binaries. A diverse dataset of benign and malicious Portable Executable (PE) files, packed using various commercial and open-source packers, was curated to capture a broad spectrum of packing transformations and obfuscation techniques. Classical models using handcrafted Gabor jet features achieved intense discrimination at moderate computational cost. In contrast, CNNs based on VGG16 and DenseNet121 significantly outperformed them, achieving high detection performance with well-balanced precision, recall, and F1-scores. DenseNet121 demonstrated slightly higher precision and lower false positive rates, whereas VGG16 achieved marginally higher recall, indicating complementary strengths for practical deployment. Evaluation against unknown packers confirmed robust generalization, demonstrating that grayscale byte-plot representations combined with deep learning provide a useful and reliable approach for early detection of packed malware, enhancing malware analysis pipelines and supporting automated antivirus inspection.
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2512.14418.pdf' target='_blank'>https://arxiv.org/pdf/2512.14418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dejun Hu, Zhiming Li, Jia-Rui Shen, Jia-Ning Tu, Zi-Hao Ye, Junliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14418">Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2512.08169.pdf' target='_blank'>https://arxiv.org/pdf/2512.08169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangze Zhao, Yongzheng Zhang, Changbo Tian, Dan Xie, Hongri Liu, Bailing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08169">Information-Dense Reasoning for Efficient and Auditable Security Alert Triage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2512.07430.pdf' target='_blank'>https://arxiv.org/pdf/2512.07430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangle Li, Danli Luo, Haifeng Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07430">MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2512.04571.pdf' target='_blank'>https://arxiv.org/pdf/2512.04571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditi Naiknaware, Sanchit Singh, Hajar Homayouni, Salimeh Sekeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04571">Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2512.02831.pdf' target='_blank'>https://arxiv.org/pdf/2512.02831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Alvandi, Mina Rezaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02831">Revisiting Theory of Contrastive Learning for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2511.20701.pdf' target='_blank'>https://arxiv.org/pdf/2511.20701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nitya Tiwari, Parv Maheshwari, Vidisha Agarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20701">Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2511.05760.pdf' target='_blank'>https://arxiv.org/pdf/2511.05760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mateo Ortiz, Juan Olmos, Fabio Martínez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05760">A Second-Order Attention Mechanism For Prostate Cancer Segmentation and Detection in Bi-Parametric MRI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The detection of clinically significant prostate cancer lesions (csPCa) from biparametric magnetic resonance imaging (bp-MRI) has emerged as a noninvasive imaging technique for improving accurate diagnosis. Nevertheless, the analysis of such images remains highly dependent on the subjective expert interpretation. Deep learning approaches have been proposed for csPCa lesions detection and segmentation, but they remain limited due to their reliance on extensively annotated datasets. Moreover, the high lesion variability across prostate zones poses additional challenges, even for expert radiologists. This work introduces a second-order geometric attention (SOGA) mechanism that guides a dedicated segmentation network, through skip connections, to detect csPCa lesions. The proposed attention is modeled on the Riemannian manifold, learning from symmetric positive definitive (SPD) representations. The proposed mechanism was integrated into standard U-Net and nnU-Net backbones, and was validated on the publicly available PI-CAI dataset, achieving an Average Precision (AP) of 0.37 and an Area Under the ROC Curve (AUC-ROC) of 0.83, outperforming baseline networks and attention-based methods. Furthermore, the approach was evaluated on the Prostate158 dataset as an independent test cohort, achieving an AP of 0.37 and an AUC-ROC of 0.75, confirming robust generalization and suggesting discriminative learned representations.
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2511.02589.pdf' target='_blank'>https://arxiv.org/pdf/2511.02589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claudia Herambourg, Dawid Siuda, Julia Kopczyńska, Joao R. L. Santos, Wojciech Sas, Joanna Śmietańska-Nowak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02589">The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ORCA (Omni Research on Calculation in AI) Benchmark - a novel benchmark that evaluates large language models (LLMs) on multi-domain, real-life quantitative reasoning using verified outputs from Omni's calculator engine. In 500 natural-language tasks across domains such as finance, physics, health, and statistics, the five state-of-the-art systems (ChatGPT-5, Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only $45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$) and calculation mistakes ($33\,\%$). Results in specific domains indicate strengths in mathematics and engineering, but weaknesses in physics and natural sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the models often fail together but differ in the types of errors they make, highlighting their partial complementarity rather than redundancy. Unlike standard math datasets, ORCA evaluates step-by-step reasoning, numerical precision, and domain generalization across real problems from finance, physics, health, and statistics.
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2510.27512.pdf' target='_blank'>https://arxiv.org/pdf/2510.27512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahi Aminu, Chisom Chibuike, Fatimo Adebanjo, Omokolade Awosanya, Samuel Oyeneye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27512">Effect of Domain Generalization Techniques in Low Resource Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models typically assume that training and test data follow the same distribution, an assumption that often fails in real-world scenarios due to distribution shifts. This issue is especially pronounced in low-resource settings, where data scarcity and limited domain diversity hinder robust generalization. Domain generalization (DG) approaches address this challenge by learning features that remain invariant across domains, often using causal mechanisms to improve model robustness. In this study, we examine two distinct causal DG techniques in low-resource natural language tasks. First, we investigate a causal data augmentation (CDA) approach that automatically generates counterfactual examples to improve robustness to spurious correlations. We apply this method to sentiment classification on the NaijaSenti Twitter corpus, expanding the training data with semantically equivalent paraphrases to simulate controlled distribution shifts. Second, we explore an invariant causal representation learning (ICRL) approach using the DINER framework, originally proposed for debiasing aspect-based sentiment analysis. We adapt DINER to a multilingual setting. Our findings demonstrate that both approaches enhance robustness to unseen domains: counterfactual data augmentation yields consistent cross-domain accuracy gains in sentiment classification, while causal representation learning with DINER improves out-of-distribution performance in multilingual sentiment analysis, albeit with varying gains across languages.
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2510.25883.pdf' target='_blank'>https://arxiv.org/pdf/2510.25883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Dittrich, Jennifer Flygare Kinne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25883">The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing frameworks converge on the centrality of compression to intelligence but leave underspecified why this process enforces the discovery of causal structure rather than superficial statistical patterns. We introduce a two-level framework to address this gap. The Information-Theoretic Imperative (ITI) establishes that any system persisting in uncertain environments must minimize epistemic entropy through predictive compression: this is the evolutionary "why" linking survival pressure to information-processing demands. The Compression Efficiency Principle (CEP) specifies how efficient compression mechanically selects for generative, causal models through exception-accumulation dynamics, making reality alignment a consequence rather than a contingent achievement. Together, ITI and CEP define a causal chain: from survival pressure to prediction necessity, compression requirement, efficiency optimization, generative structure discovery, and ultimately reality alignment. Each link follows from physical, information-theoretic, or evolutionary constraints, implying that intelligence is the mechanically necessary outcome of persistence in structured environments. This framework yields empirically testable predictions: compression efficiency, measured as approach to the rate-distortion frontier, correlates with out-of-distribution generalization; exception-accumulation rates differentiate causal from correlational models; hierarchical systems exhibit increasing efficiency across abstraction layers; and biological systems demonstrate metabolic costs that track representational complexity. ITI and CEP thereby provide a unified account of convergence across biological, artificial, and multi-scale systems, addressing the epistemic and functional dimensions of intelligence without invoking assumptions about consciousness or subjective experience.
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2508.19804.pdf' target='_blank'>https://arxiv.org/pdf/2508.19804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Marzahl, Brian Napora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19804">A bag of tricks for real-time Mitotic Figure detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mitotic figure (MF) detection in histopathology images is challenging due to large variations in slide scanners, staining protocols, tissue types, and the presence of artifacts. This paper presents a collection of training techniques - a bag of tricks - that enable robust, real-time MF detection across diverse domains. We build on the efficient RTMDet single stage object detector to achieve high inference speed suitable for clinical deployment. Our method addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling, and careful augmentation. Additionally, we employ targeted, hard negative mining on necrotic and debris tissue to reduce false positives. In a grouped 5-fold cross-validation across multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025 challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81, outperforming larger models and demonstrating adaptability to new, unfamiliar domains. The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption.
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2508.08644.pdf' target='_blank'>https://arxiv.org/pdf/2508.08644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guiming Cao, Yuming Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08644">AME: Aligned Manifold Entropy for Robust Vision-Language Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation is a long-established technique for knowledge transfer, and has regained attention in the context of the recent emergence of large vision-language models (VLMs). However, vision-language knowledge distillation often requires sufficient training data to achieve robust generalization on amples with ambiguous or boundary-adjacent representations, which are associated with high predictive uncertainty. Critically, collecting such large-scale, task-specific data for training is often impractical in real-world scenarios. To address this major challenge arising from the entanglement of uncertainty and cross-modal feature representation, we propose Aligned Manifold Entropy for Robust Vision-Language Distillation (AME), aiming to achieve robust generalization under real-world conditions. AME applies entropy minimization over a reconfigured shared manifold, where multi-modal data (i.e., image and text) are bridged through a pair of projection functions, conducive to structural compression for cross-modal feature representations. This enables robust knowledge distillation under low-data regimes, while requiring no architectural modifications to the backbone. As a result, it can serve as a plug-and-play module compatible with a wide range of vision-language distillation frameworks. Notably, our theoretical analysis reveals that integrating knowledge distillation with entropy minimization over the shared manifold leads to a tighter generalization error bound. Extensive experiments across diverse distillation architectures and training settings demonstrate that AME consistently facilitates robust knowledge distillation, resulting in superior generalization performance across a wide spectrum of downstream tasks.
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2508.03555.pdf' target='_blank'>https://arxiv.org/pdf/2508.03555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Chaffin, RaphaÃ«l Sourty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03555">PyLate: Flexible Training and Retrieval for Late Interaction Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural ranking has become a cornerstone of modern information retrieval. While single vector search remains the dominant paradigm, it suffers from the shortcoming of compressing all the information into a single vector. This compression leads to notable performance degradation in out-of-domain, long-context, and reasoning-intensive retrieval tasks. Multi-vector approaches pioneered by ColBERT aim to address these limitations by preserving individual token embeddings and computing similarity via the MaxSim operator. This architecture has demonstrated superior empirical advantages, including enhanced out-of-domain generalization, long-context handling, and performance in complex retrieval scenarios. Despite these compelling empirical results and clear theoretical advantages, the practical adoption and public availability of late interaction models remain low compared to their single-vector counterparts, primarily due to a lack of accessible and modular tools for training and experimenting with such models. To bridge this gap, we introduce PyLate, a streamlined library built on top of Sentence Transformers to support multi-vector architectures natively, inheriting its efficient training, advanced logging, and automated model card generation while requiring minimal code changes to code templates users are already familiar with. By offering multi-vector-specific features such as efficient indexes, PyLate aims to accelerate research and real-world application of late interaction models, thereby unlocking their full potential in modern IR systems. Finally, PyLate has already enabled the development of state-of-the-art models, including GTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility for both research and production environments.
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2507.22485.pdf' target='_blank'>https://arxiv.org/pdf/2507.22485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Bochow, Philipp Hess, Alexander Robinson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22485">Physics-constrained generative machine learning-based high-resolution downscaling of Greenland's surface mass balance and surface temperature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate, high-resolution projections of the Greenland ice sheet's surface mass balance (SMB) and surface temperature are essential for understanding future sea-level rise, yet current approaches are either computationally demanding or limited to coarse spatial scales. Here, we introduce a novel physics-constrained generative modeling framework based on a consistency model (CM) to downscale low-resolution SMB and surface temperature fields by a factor of up to 32 (from 160 km to 5 km grid spacing) in a few sampling steps. The CM is trained on monthly outputs of the regional climate model MARv3.12 and conditioned on ice-sheet topography and insolation. By enforcing a hard conservation constraint during inference, we ensure approximate preservation of SMB and temperature sums on the coarse spatial scale as well as robust generalization to extreme climate states without retraining. On the test set, our constrained CM achieves a continued ranked probability score of 6.31 mmWE for the SMB and 0.1 K for the surface temperature, outperforming interpolation-based downscaling. Together with spatial power-spectral analysis, we demonstrate that the CM faithfully reproduces variability across spatial scales. We further apply bias-corrected outputs of the NorESM2 Earth System Model as inputs to our CM, to demonstrate the potential of our model to directly downscale ESM fields. Our approach delivers realistic, high-resolution climate forcing for ice-sheet simulations with fast inference and can be readily integrated into Earth-system and ice-sheet model workflows to improve projections of the future contribution to sea-level rise from Greenland and potentially other ice sheets and glaciers too.
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2507.20913.pdf' target='_blank'>https://arxiv.org/pdf/2507.20913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialei Cui, Jianwei Du, Yanzhe Li, Lei Gao, Hui Jiang, Chenfu Bao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20913">HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of face manipulation techniques poses a critical challenge for face forgery detection: cross-domain generalization. Conventional methods, which rely on simple classification objectives, often fail to learn domain-invariant representations. We propose HAMLET-FFD, a cognitively inspired Hierarchical Adaptive Multi-modal Learning framework that tackles this challenge via bidirectional cross-modal reasoning. Building on contrastive vision-language models such as CLIP, HAMLET-FFD introduces a knowledge refinement loop that iteratively assesses authenticity by integrating visual evidence with conceptual cues, emulating expert forensic analysis. A key innovation is a bidirectional fusion mechanism in which textual authenticity embeddings guide the aggregation of hierarchical visual features, while modulated visual features refine text embeddings to generate image-adaptive prompts. This closed-loop process progressively aligns visual observations with semantic priors to enhance authenticity assessment. By design, HAMLET-FFD freezes all pretrained parameters, serving as an external plugin that preserves CLIP's original capabilities. Extensive experiments demonstrate its superior generalization to unseen manipulations across multiple benchmarks, and visual analyses reveal a division of labor among embeddings, with distinct representations specializing in fine-grained artifact recognition.
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2507.17924.pdf' target='_blank'>https://arxiv.org/pdf/2507.17924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongrong Yang, Markus Schlaepfer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17924">UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate population flow prediction is essential for urban planning, transportation management, and public health. Yet existing methods face key limitations: traditional models rely on static spatial assumptions, deep learning models struggle with cross-city generalization, and Large Language Models (LLMs) incur high computational costs while failing to capture spatial structure. Moreover, many approaches sacrifice resolution by clustering Points of Interest (POIs) or restricting coverage to subregions, limiting their utility for city-wide analytics. We introduce UrbanPulse, a scalable deep learning framework that delivers ultra-fine-grained, city-wide OD flow predictions by treating each POI as an individual node. It combines a temporal graph convolutional encoder with a transformer-based decoder to model multi-scale spatiotemporal dependencies. To ensure robust generalization across urban contexts, UrbanPulse employs a three-stage transfer learning strategy: pretraining on large-scale urban graphs, cold-start adaptation, and reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS records from three metropolitan areas in California, UrbanPulse achieves state-of-the-art accuracy and scalability. Through efficient transfer learning, UrbanPulse takes a key step toward making high-resolution, AI-powered urban forecasting deployable in practice across diverse cities.
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2507.16571.pdf' target='_blank'>https://arxiv.org/pdf/2507.16571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>G. de RomÃ©mont, F. Renac, F. Chinesta, J. Nunez, D. Gueyffier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16571">Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume Computations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel data-driven approach for enhancing gradient reconstruction in unstructured finite volume methods for hyperbolic conservation laws, specifically for the 2D Euler equations. Our approach extends previous structured-grid methodologies to unstructured meshes through a modified DeepONet architecture that incorporates local geometry in the neural network. The architecture employs local mesh topology to ensure rotation invariance, while also ensuring first-order constraint on the learned operator. The training methodology incorporates physics-informed regularization through entropy penalization, total variation diminishing penalization, and parameter regularization to ensure physically consistent solutions, particularly in shock-dominated regions. The model is trained on high-fidelity datasets solutions derived from sine waves and randomized piecewise constant initial conditions with periodic boundary conditions, enabling robust generalization to complex flow configurations or geometries. Validation test cases from the literature, including challenging geometry configuration, demonstrates substantial improvements in accuracy compared to traditional second-order finite volume schemes. The method achieves gains of 20-60% in solution accuracy while enhancing computational efficiency. A convergence study has been conveyed and reveal improved mesh convergence rates compared to the conventional solver. The proposed algorithm is faster and more accurate than the traditional second-order finite volume solver, enabling high-fidelity simulations on coarser grids while preserving the stability and conservation properties essential for hyperbolic conservation laws. This work is a part of a new generation of solvers that are built by combining Machine-Learning (ML) tools with traditional numerical schemes, all while ensuring physical constraint on the results.
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2507.16406.pdf' target='_blank'>https://arxiv.org/pdf/2507.16406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanveer Younis, Zhanglin Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16406">Sparse-View 3D Reconstruction: Recent Advances and Open Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse-view 3D reconstruction is essential for applications in which dense image acquisition is impractical, such as robotics, augmented/virtual reality (AR/VR), and autonomous systems. In these settings, minimal image overlap prevents reliable correspondence matching, causing traditional methods, such as structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey reviews the latest advances in neural implicit models (e.g., NeRF and its regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian Splatting), and hybrid frameworks that leverage priors from diffusion and vision foundation models (VFMs).We analyze how geometric regularization, explicit shape modeling, and generative inference are used to mitigate artifacts such as floaters and pose ambiguities in sparse-view settings. Comparative results on standard benchmarks reveal key trade-offs between the reconstruction accuracy, efficiency, and generalization. Unlike previous reviews, our survey provides a unified perspective on geometry-based, neural implicit, and generative (diffusion-based) methods. We highlight the persistent challenges in domain generalization and pose-free reconstruction and outline future directions for developing 3D-native generative priors and achieving real-time, unconstrained sparse-view reconstruction.
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2507.14592.pdf' target='_blank'>https://arxiv.org/pdf/2507.14592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Liu, Jia Bi, Xiaomin Wang, Xin Yang, Ling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14592">A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance, logistics, agriculture, disaster management, and military operations. Accurate detection and classification of UAV flight states, such as hovering, cruising, ascending, or transitioning, which are essential for safe and effective operations. However, conventional time series classification (TSC) methods often lack robustness and generalization for dynamic UAV environments, while state of the art(SOTA) models like Transformers and LSTM based architectures typically require large datasets and entail high computational costs, especially with high-dimensional data streams. This paper proposes a novel framework that integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET) to address these challenges in UAV flight state classification. The Transformer encoder captures long-range temporal dependencies and complex telemetry dynamics, while the GAN module augments limited datasets with realistic synthetic samples. MIL is incorporated to focus attention on the most discriminative input segments, reducing noise and computational overhead. Experimental results show that the proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and 98.6% on the DroneRF dataset that outperforming other SOTA approaches. The framework also demonstrates strong computational efficiency and robust generalization across diverse UAV platforms and flight states, highlighting its potential for real-time deployment in resource constrained environments.
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2507.04302.pdf' target='_blank'>https://arxiv.org/pdf/2507.04302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuyu Zhang, Ning Chen, Yongshan Liu, Qinghua Zhang, Xu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04302">Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Domain Generalization (SDG) aims to develop models capable of generalizing to unseen target domains using only one source domain, a task complicated by substantial domain shifts and limited data diversity. Existing SDG approaches primarily rely on data augmentation techniques, which struggle to effectively adapt training dynamics to accommodate large domain shifts. To address this, we propose LEAwareSGD, a novel Lyapunov Exponent (LE)-guided optimization approach inspired by dynamical systems theory. By leveraging LE measurements to modulate the learning rate, LEAwareSGD encourages model training near the edge of chaos, a critical state that optimally balances stability and adaptability. This dynamic adjustment allows the model to explore a wider parameter space and capture more generalizable features, ultimately enhancing the model's generalization capability. Extensive experiments on PACS, OfficeHome, and DomainNet demonstrate that LEAwareSGD yields substantial generalization gains, achieving up to 9.47\% improvement on PACS in low-data regimes. These results underscore the effectiveness of training near the edge of chaos for enhancing model generalization capability in SDG tasks.
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2507.03146.pdf' target='_blank'>https://arxiv.org/pdf/2507.03146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ron Tsibulsky, Daniel Nevo, Uri Shalit
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03146">Set Valued Predictions For Robust Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the impressive advancements in modern machine learning, achieving robustness in Domain Generalization (DG) tasks remains a significant challenge. In DG, models are expected to perform well on samples from unseen test distributions (also called domains), by learning from multiple related training distributions. Most existing approaches to this problem rely on single-valued predictions, which inherently limit their robustness. We argue that set-valued predictors could be leveraged to enhance robustness across unseen domains, while also taking into account that these sets should be as small as possible. We introduce a theoretical framework defining successful set prediction in the DG setting, focusing on meeting a predefined performance criterion across as many domains as possible, and provide theoretical insights into the conditions under which such domain generalization is achievable. We further propose a practical optimization method compatible with modern learning architectures, that balances robust performance on unseen domains with small prediction set sizes. We evaluate our approach on several real-world datasets from the WILDS benchmark, demonstrating its potential as a promising direction for robust domain generalization.
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2506.22454.pdf' target='_blank'>https://arxiv.org/pdf/2506.22454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Luiza S. Tavares, Artur Pedro M. Neto, Francinaldo L. Gomes, Paul Rodrigo dos Reis, Arthur G. da Silva, Antonio P. Junior, Bruno D. Gomes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22454">Microelectrode Signal Dynamics as Biomarkers of Subthalamic Nucleus Entry on Deep Brain Stimulation: A Nonlinear Feature Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate intraoperative localization of the subthalamic nucleus (STN) is essential for the efficacy of Deep Brain Stimulation (DBS) in patients with Parkinson's disease. While microelectrode recordings (MERs) provide rich electrophysiological information during DBS electrode implantation, current localization practices often rely on subjective interpretation of signal features. In this study, we propose a quantitative framework that leverages nonlinear dynamics and entropy-based metrics to classify neural activity recorded inside versus outside the STN. MER data from three patients were preprocessed using a robust artifact correction pipeline, segmented, and labelled based on surgical annotations. A comprehensive set of recurrence quantification analysis, nonlinear, and entropy features were extracted from each segment. Multiple supervised classifiers were trained on every combination of feature domains using stratified 10-fold cross-validation, followed by statistical comparison using paired Wilcoxon signed-rank tests with Holm-Bonferroni correction. The combination of entropy and nonlinear features yielded the highest discriminative power, and the Extra Trees classifier emerged as the best model with a cross-validated F1-score of 0.902+/-0.027 and ROC AUC of 0.887+/-0.055. Final evaluation on a 20% hold-out test set confirmed robust generalization (F1= 0.922, ROC AUC = 0.941). These results highlight the potential of nonlinear and entropy signal descriptors in supporting real-time, data-driven decision-making during DBS surgeries
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2506.11615.pdf' target='_blank'>https://arxiv.org/pdf/2506.11615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deliang Jin, Gang Chen, Shuo Feng, Yufeng Ling, Haoran Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11615">Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and Neuron Pruning in Noisy Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have achieved remarkable success across diverse domains, but their performance can be severely degraded by noisy or corrupted training data. Conventional noise mitigation methods often rely on explicit assumptions about noise distributions or require extensive retraining, which can be impractical for large-scale models. Inspired by the principles of machine unlearning, we propose a novel framework that integrates attribution-guided data partitioning, discriminative neuron pruning, and targeted fine-tuning to mitigate the impact of noisy samples. Our approach employs gradient-based attribution to probabilistically distinguish high-quality examples from potentially corrupted ones without imposing restrictive assumptions on the noise. It then applies regression-based sensitivity analysis to identify and prune neurons that are most vulnerable to noise. Finally, the resulting network is fine-tuned on the high-quality data subset to efficiently recover and enhance its generalization performance. This integrated unlearning-inspired framework provides several advantages over conventional noise-robust learning approaches. Notably, it combines data-level unlearning with model-level adaptation, thereby avoiding the need for full model retraining or explicit noise modeling. We evaluate our method on representative tasks (e.g., CIFAR-10 image classification and speech recognition) under various noise levels and observe substantial gains in both accuracy and efficiency. For example, our framework achieves approximately a 10% absolute accuracy improvement over standard retraining on CIFAR-10 with injected label noise, while reducing retraining time by up to 47% in some settings. These results demonstrate the effectiveness and scalability of the proposed approach for achieving robust generalization in noisy environments.
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2506.07060.pdf' target='_blank'>https://arxiv.org/pdf/2506.07060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laura Cohen, Xavier Hinaut, Lilyana Petrova, Alexandre Pitti, Syd Reynal, Ichiro Tsuda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07060">Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural intelligence (NI) consistently achieves more with less. Infants learn language, develop abstract concepts, and acquire sensorimotor skills from sparse data, all within tight neural and energy limits. In contrast, today's AI relies on virtually unlimited computational power, energy, and data to reach high performance. This paper argues that constraints in NI are paradoxically catalysts for efficiency, adaptability, and creativity. We first show how limited neural bandwidth promotes concise codes that still capture complex patterns. Spiking neurons, hierarchical structures, and symbolic-like representations emerge naturally from bandwidth constraints, enabling robust generalization. Next, we discuss chaotic itinerancy, illustrating how the brain transits among transient attractors to flexibly retrieve memories and manage uncertainty. We then highlight reservoir computing, where random projections facilitate rapid generalization from small datasets. Drawing on developmental perspectives, we emphasize how intrinsic motivation, along with responsive social environments, drives infant language learning and discovery of meaning. Such active, embodied processes are largely absent in current AI. Finally, we suggest that adopting 'less is more' principles -- energy constraints, parsimonious architectures, and real-world interaction -- can foster the emergence of more efficient, interpretable, and biologically grounded artificial systems.
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2505.19971.pdf' target='_blank'>https://arxiv.org/pdf/2505.19971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kilian Sennrich, Sina Ahmadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19971">Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge graphs offer an excellent solution for representing the lexical-semantic structures of lexicographic data. However, working with the SPARQL query language represents a considerable hurdle for many non-expert users who could benefit from the advantages of this technology. This paper addresses the challenge of creating natural language interfaces for lexicographic data retrieval on knowledge graphs such as Wikidata. We develop a multidimensional taxonomy capturing the complexity of Wikidata's lexicographic data ontology module through four dimensions and create a template-based dataset with over 1.2 million mappings from natural language utterances to SPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and GPT-3.5-Turbo reveal significant differences in model capabilities. While all models perform well on familiar patterns, only GPT-3.5-Turbo demonstrates meaningful generalization capabilities, suggesting that model size and diverse pre-training are crucial for adaptability in this domain. However, significant challenges remain in achieving robust generalization, handling diverse linguistic data, and developing scalable solutions that can accommodate the full complexity of lexicographic knowledge representation.
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2505.09319.pdf' target='_blank'>https://arxiv.org/pdf/2505.09319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaustabha Ray, Nelson Mimura Gonzalez, Bruno Wassermann, Rachel Tzoref-Brill, Dean H. Lorenz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09319">Statistical Modeling and Uncertainty Estimation of LLM Inference Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM) inference systems present significant challenges in statistical performance characterization due to dynamic workload variations, diverse hardware architectures, and complex interactions between model size, batch processing, and throughput requirements. Accurate statistical characterization enables better workload scheduling, adaptive resource provisioning, and cost-aware inference optimization, making it crucial for improving efficiency in large-scale AI deployments. Traditional analytical models provide explainability but cannot cover the vast diversity of real-world workloads, making it impossible to benchmark every scenario in advance. Machine learning (ML) approaches effectively predict performance for non-benchmarked cases but struggle when extrapolating beyond their observed training space. To address these limitations for LLM inference systems, we propose an Analytical with Learning Augmentation (ALA) framework that bridges analytical modeling with \ml for robust statistical prediction and uncertainty estimation in LLM inference workloads. Our method employs an analytical throughput model with parameters estimated for benchmarked workloads, then extends to unobserved configurations using \ml predictions. We enhance this with simulated annealing to exploit subsets of the workload data point combinations and develop an error predictor. Finally, we quantify uncertainty based on vector space similarity between new and observed workloads to ensure robust generalization. Through extensive experimentation on diverse LLM inference workloads, we demonstrate that our framework achieves low median errors while maintaining adaptability to new inference scenarios.
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2505.05895.pdf' target='_blank'>https://arxiv.org/pdf/2505.05895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Raphael Ernhofer, Daniil Prokhorov, Jannica Langner, Dominik Bollmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05895">Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern automotive infotainment systems necessitate intelligent and adaptive solutions to manage frequent User Interface (UI) updates and diverse design variations. This work introduces a vision-language framework to facilitate the understanding of and interaction with automotive UIs, enabling seamless adaptation across different UI designs. To support research in this field, AutomotiveUI-Bench-4K, an open-source dataset comprising 998 images with 4,208 annotations, is also released. Additionally, a data pipeline for generating training data is presented. A Molmo-7B-based model is fine-tuned using Low-Rank Adaptation (LoRa), incorporating generated reasoning along with visual grounding and evaluation capabilities. The fine-tuned Evaluative Large Action Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and dataset are available on Hugging Face). The approach demonstrates strong cross-domain generalization, including a +5.6% improvement on ScreenSpot over the baseline model. An average accuracy of 80.8% is achieved on ScreenSpot, closely matching or surpassing specialized models for desktop, mobile, and web, despite being trained primarily on the automotive domain. This research investigates how data collection and subsequent fine-tuning can lead to AI-driven advancements in automotive UI understanding and interaction. The applied method is cost-efficient, and fine-tuned models can be deployed on consumer-grade GPUs.
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2505.03575.pdf' target='_blank'>https://arxiv.org/pdf/2505.03575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Kainz, Johannes K. Krondorfer, Malte Jaschik, Maria Jernej, Harald Ganster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03575">Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recycling textile fibers is critical to reducing the environmental impact of the textile industry. Hyperspectral near-infrared (NIR) imaging combined with advanced deep learning algorithms offers a promising solution for efficient fiber classification and sorting. In this study, we investigate supervised and unsupervised deep learning models and test their generalization capabilities on different textile structures. We show that optimized convolutional neural networks (CNNs) and autoencoder networks achieve robust generalization under varying conditions. These results highlight the potential of hyperspectral imaging and deep learning to advance sustainable textile recycling through accurate and robust classification.
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2505.01558.pdf' target='_blank'>https://arxiv.org/pdf/2505.01558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anan Yaghmour, Melba M. Crawford, Saurabh Prasad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01558">A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. Advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. Domain adaptation offers a promising solution to improve model generalization. This paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. We further provide new mathematical insights into MAE-based generative learning for domain-invariant feature learning. Experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2504.14237.pdf' target='_blank'>https://arxiv.org/pdf/2504.14237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dekang Zhang, Dan Niu, Zhou Jin, Yichao Dong, Jingweijia Tan, Changyin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14237">A Novel Frequency-Spatial Domain Aware Network for Fast Thermal Prediction in 2.5D ICs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the post-Moore era, 2.5D chiplet-based ICs present significant challenges in thermal management due to increased power density and thermal hotspots. Neural network-based thermal prediction models can perform real-time predictions for many unseen new designs. However, existing CNN-based and GCN-based methods cannot effectively capture the global thermal features, especially for high-frequency components, hindering prediction accuracy enhancement. In this paper, we propose a novel frequency-spatial dual domain aware prediction network (FSA-Heat) for fast and high-accuracy thermal prediction in 2.5D ICs. It integrates high-to-low frequency and spatial domain encoder (FSTE) module with frequency domain cross-scale interaction module (FCIFormer) to achieve high-to-low frequency and global-to-local thermal dissipation feature extraction. Additionally, a frequency-spatial hybrid loss (FSL) is designed to effectively attenuate high-frequency thermal gradient noise and spatial misalignments. The experimental results show that the performance enhancements offered by our proposed method are substantial, outperforming the newly-proposed 2.5D method, GCN+PNA, by considerable margins (over 99% RMSE reduction, 4.23X inference time speedup). Moreover, extensive experiments demonstrate that FSA-Heat also exhibits robust generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2504.11477.pdf' target='_blank'>https://arxiv.org/pdf/2504.11477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunkai Zhang, Shiyin Wei, Yong Huang, Yawu Su, Shanshan Lu, Hui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11477">SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing computer vision(CV)-based structural damage identification models demonstrate notable accuracy in categorizing and localizing damage. However, these models present several critical limitations that hinder their practical application in civil engineering(CE). Primarily, their ability to recognize damage types remains constrained, preventing comprehensive analysis of the highly varied and complex conditions encountered in real-world CE structures. Second, these models lack linguistic capabilities, rendering them unable to articulate structural damage characteristics through natural language descriptions. With the continuous advancement of artificial intelligence(AI), large multi-modal models(LMMs) have emerged as a transformative solution, enabling the unified encoding and alignment of textual and visual data. These models can autonomously generate detailed descriptive narratives of structural damage while demonstrating robust generalization across diverse scenarios and tasks. This study introduces SDIGLM, an innovative LMM for structural damage identification, developed based on the open-source VisualGLM-6B architecture. To address the challenge of adapting LMMs to the intricate and varied operating conditions in CE, this work integrates a U-Net-based semantic segmentation module to generate defect segmentation maps as visual Chain of Thought(CoT). Additionally, a multi-round dialogue fine-tuning dataset is constructed to enhance logical reasoning, complemented by a language CoT formed through prompt engineering. By leveraging this multi-modal CoT, SDIGLM surpasses general-purpose LMMs in structural damage identification, achieving an accuracy of 95.24% across various infrastructure types. Moreover, the model effectively describes damage characteristics such as hole size, crack direction, and corrosion severity.
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2504.10316.pdf' target='_blank'>https://arxiv.org/pdf/2504.10316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiqi Wu, Jianbo Mei, Yingjie Huang, Yining Xu, Jingjiao You, Yilong Liu, Li Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10316">ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content Generation with Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, significant advancements have been made in text-driven 3D content generation. However, several challenges remain. In practical applications, users often provide extremely simple text inputs while expecting high-quality 3D content. Generating optimal results from such minimal text is a difficult task due to the strong dependency of text-to-3D models on the quality of input prompts. Moreover, the generation process exhibits high variability, making it difficult to control. Consequently, multiple iterations are typically required to produce content that meets user expectations, reducing generation efficiency. To address this issue, we propose GPT-4V for self-optimization, which significantly enhances the efficiency of generating satisfactory content in a single attempt. Furthermore, the controllability of text-to-3D generation methods has not been fully explored. Our approach enables users to not only provide textual descriptions but also specify additional conditions, such as style, edges, scribbles, poses, or combinations of multiple conditions, allowing for more precise control over the generated 3D content. Additionally, during training, we effectively integrate multi-view information, including multi-view depth, masks, features, and images, to address the common Janus problem in 3D content generation. Extensive experiments demonstrate that our method achieves robust generalization, facilitating the efficient and controllable generation of high-quality 3D content.
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2504.07736.pdf' target='_blank'>https://arxiv.org/pdf/2504.07736.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Palak Patel, Luke McGuire, Abani Patra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07736">A Novel Deep Learning Approach for Emulating Computationally Expensive Postfire Debris Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional physics-based models of geophysical flows, such as debris flows and landslides that pose significant risks to human lives and infrastructure are computationally expensive, limiting their utility for large-scale parameter sweeps, uncertainty quantification, inversions or real-time applications. This study presents an efficient alternative, a deep learning-based surrogate model built using a modified U-Net architecture to predict the dynamics of runoff-generated debris flows across diverse terrain based on data from physics based simulations. The study area is divided into smaller patches for localized predictions using a patch-predict-stitch methodology (complemented by limited global data to accelerate training). The patches are then combined to reconstruct spatially continuous flow maps, ensuring scalability for large domains. To enable fast training using limited expensive simulations, the deep learning model was trained on data from an ensemble of physics based simulations using parameters generated via Latin Hypercube Sampling and validated on unseen parameter sets and terrain, achieving maximum pointwise errors below 10% and robust generalization. Uncertainty quantification using Monte Carlo methods are enabled using the validated surrogate, which can facilitate probabilistic hazard assessments. This study highlights the potential of deep learning surrogates as powerful tools for geophysical flow analysis, enabling computationally efficient and reliable probabilistic hazard map predictions.
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2504.06781.pdf' target='_blank'>https://arxiv.org/pdf/2504.06781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reiji Saito, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06781">Domain Generalization through Attenuation of Domain-Specific Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a new evaluation metric called Domain Independence (DI) and Attenuation of Domain-Specific Information (ADSI) which is specifically designed for domain-generalized semantic segmentation in automotive images. DI measures the presence of domain-specific information: a lower DI value indicates strong domain dependence, while a higher DI value suggests greater domain independence. This makes it roughly where domain-specific information exists and up to which frequency range it is present. As a result, it becomes possible to effectively suppress only the regions in the image that contain domain-specific information, enabling feature extraction independent of the domain. ADSI uses a Butterworth filter to remove the low-frequency components of images that contain inherent domain-specific information such as sensor characteristics and lighting conditions. However, since low-frequency components also contain important information such as color, we should not remove them completely. Thus, a scalar value (ranging from 0 to 1) is multiplied by the low-frequency components to retain essential information. This helps the model learn more domain-independent features. In experiments, GTA5 (synthetic dataset) was used as training images, and a real-world dataset was used for evaluation, and the proposed method outperformed conventional approaches. Similarly, in experiments that the Cityscapes (real-world dataset) was used for training and various environment datasets such as rain and nighttime were used for evaluation, the proposed method demonstrated its robustness under nighttime conditions.
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2503.23882.pdf' target='_blank'>https://arxiv.org/pdf/2503.23882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Halil Ä°brahim ÃztÃ¼rk, Muhammet Esat KalfaoÄlu, Ozsel Kilinc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23882">GLane3D : Detecting Lanes with Graph of 3D Keypoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and efficient lane detection in 3D space is essential for autonomous driving systems, where robust generalization is the foremost requirement for 3D lane detection algorithms. Considering the extensive variation in lane structures worldwide, achieving high generalization capacity is particularly challenging, as algorithms must accurately identify a wide variety of lane patterns worldwide. Traditional top-down approaches rely heavily on learning lane characteristics from training datasets, often struggling with lanes exhibiting previously unseen attributes. To address this generalization limitation, we propose a method that detects keypoints of lanes and subsequently predicts sequential connections between them to construct complete 3D lanes. Each key point is essential for maintaining lane continuity, and we predict multiple proposals per keypoint by allowing adjacent grids to predict the same keypoint using an offset mechanism. PointNMS is employed to eliminate overlapping proposal keypoints, reducing redundancy in the estimated BEV graph and minimizing computational overhead from connection estimations. Our model surpasses previous state-of-the-art methods on both the Apollo and OpenLane datasets, demonstrating superior F1 scores and a strong generalization capacity when models trained on OpenLane are evaluated on the Apollo dataset, compared to prior approaches.
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2503.23060.pdf' target='_blank'>https://arxiv.org/pdf/2503.23060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Jacob, Yanlei Diao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23060">Unsupervised Anomaly Detection in Multivariate Time Series across Heterogeneous Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread adoption of digital services, along with the scale and complexity at which they operate, has made incidents in IT operations increasingly more likely, diverse, and impactful. This has led to the rapid development of a central aspect of "Artificial Intelligence for IT Operations" (AIOps), focusing on detecting anomalies in vast amounts of multivariate time series data generated by service entities. In this paper, we begin by introducing a unifying framework for benchmarking unsupervised anomaly detection (AD) methods, and highlight the problem of shifts in normal behaviors that can occur in practical AIOps scenarios. To tackle anomaly detection under domain shift, we then cast the problem in the framework of domain generalization and propose a novel approach, Domain-Invariant VAE for Anomaly Detection (DIVAD), to learn domain-invariant representations for unsupervised anomaly detection. Our evaluation results using the Exathlon benchmark show that the two main DIVAD variants significantly outperform the best unsupervised AD method in maximum performance, with 20% and 15% improvements in maximum peak F1-scores, respectively. Evaluation using the Application Server Dataset further demonstrates the broader applicability of our domain generalization methods.
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2503.22856.pdf' target='_blank'>https://arxiv.org/pdf/2503.22856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Bai, Anna Kruspe, Xiaoxiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22856">Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tweets provides valuable semantic context for earth observation tasks and serves as a complementary modality to remote sensing imagery. In building function classification (BFC), tweets are often collected using geographic heuristics and labeled via external databases, an inherently weakly supervised process that introduces both label noise and sentence level feature noise (e.g., irrelevant or uninformative tweets). While label noise has been widely studied, the impact of sentence level feature noise remains underexplored, largely due to the lack of clean benchmark datasets for controlled analysis. In this work, we propose a method for generating a synthetic oracle dataset using LLM, designed to contain only tweets that are both correctly labeled and semantically relevant to their associated buildings. This oracle dataset enables systematic investigation of noise impacts that are otherwise difficult to isolate in real-world data. To assess its utility, we compare model performance using Naive Bayes and mBERT classifiers under three configurations: real vs. synthetic training data, and cross-domain generalization. Results show that noise in real tweets significantly degrades the contextual learning capacity of mBERT, reducing its performance to that of a simple keyword-based model. In contrast, the clean synthetic dataset allows mBERT to learn effectively, outperforming Naive Bayes Bayes by a large margin. These findings highlight that addressing feature noise is more critical than model complexity in this task. Our synthetic dataset offers a novel experimental environment for future noise injection studies and is publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2503.12215.pdf' target='_blank'>https://arxiv.org/pdf/2503.12215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amulya Reddy Maligireddy, Manohar Reddy Uppula, Nidhi Rastogi, Yaswanth Reddy Parla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12215">Gun Detection Using Combined Human Pose and Weapon Appearance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing frequency of firearm-related incidents has necessitated advancements in security and surveillance systems, particularly in firearm detection within public spaces. Traditional gun detection methods rely on manual inspections and continuous human monitoring of CCTV footage, which are labor-intensive and prone to high false positive and negative rates. To address these limitations, we propose a novel approach that integrates human pose estimation with weapon appearance recognition using deep learning techniques. Unlike prior studies that focus on either body pose estimation or firearm detection in isolation, our method jointly analyzes posture and weapon presence to enhance detection accuracy in real-world, dynamic environments. To train our model, we curated a diverse dataset comprising images from open-source repositories such as IMFDB and Monash Guns, supplemented with AI-generated and manually collected images from web sources. This dataset ensures robust generalization and realistic performance evaluation under various surveillance conditions. Our research aims to improve the precision and reliability of firearm detection systems, contributing to enhanced public safety and threat mitigation in high-risk areas.
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2503.09050.pdf' target='_blank'>https://arxiv.org/pdf/2503.09050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alvin Kimbowa, Arjun Parmar, Maziar Badii, David Liu, Matthew Harkey, Ilker Hacihaliloglu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09050">Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage Segmentation on Out-of-Distribution 2D Ultrasound Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated knee cartilage segmentation using point-of-care ultrasound devices and deep-learning networks has the potential to enhance the management of knee osteoarthritis. However, segmentation algorithms often struggle with domain shifts caused by variations in ultrasound devices and acquisition parameters, limiting their generalizability. In this paper, we propose Mono2D, a monogenic layer that extracts multi-scale, contrast- and intensity-invariant local phase features using trainable bandpass quadrature filters. This layer mitigates domain shifts, improving generalization to out-of-distribution domains. Mono2D is integrated before the first layer of a segmentation network, and its parameters jointly trained alongside the network's parameters. We evaluated Mono2D on a multi-domain 2D ultrasound knee cartilage dataset for single-source domain generalization (SSDG). Our results demonstrate that Mono2D outperforms other SSDG methods in terms of Dice score and mean average surface distance. To further assess its generalizability, we evaluate Mono2D on a multi-site prostate MRI dataset, where it continues to outperform other SSDG methods, highlighting its potential to improve domain generalization in medical imaging. Nevertheless, further evaluation on diverse datasets is still necessary to assess its clinical utility.
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2503.02311.pdf' target='_blank'>https://arxiv.org/pdf/2503.02311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kensuke Tatematsu, Akifumi Wachi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02311">Target Return Optimizer for Multi-Game Decision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving autonomous agents with robust generalization capabilities across diverse games and tasks remains one of the ultimate goals in AI research. Recent advancements in transformer-based offline reinforcement learning, exemplified by the MultiGame Decision Transformer [Lee et al., 2022], have shown remarkable performance across various games or tasks. However, these approaches depend heavily on human expertise, presenting substantial challenges for practical deployment, particularly in scenarios with limited prior game-specific knowledge. In this paper, we propose an algorithm called Multi-Game Target Return Optimizer (MTRO) to autonomously determine game-specific target returns within the Multi-Game Decision Transformer framework using solely offline datasets. MTRO addresses the existing limitations by automating the target return configuration process, leveraging environmental reward information extracted from offline datasets. Notably, MTRO does not require additional training, enabling seamless integration into existing Multi-Game Decision Transformer architectures. Our experimental evaluations on Atari games demonstrate that MTRO enhances the performance of RL policies across a wide array of games, underscoring its potential to advance the field of autonomous agent development.
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2502.19665.pdf' target='_blank'>https://arxiv.org/pdf/2502.19665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanchao Wang, Zhao-Rong Lai, Tianqi Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19665">Out-of-distribution Generalization for Total Variation based Invariant Risk Minimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invariant risk minimization is an important general machine learning framework that has recently been interpreted as a total variation model (IRM-TV). However, how to improve out-of-distribution (OOD) generalization in the IRM-TV setting remains unsolved. In this paper, we extend IRM-TV to a Lagrangian multiplier model named OOD-TV-IRM. We find that the autonomous TV penalty hyperparameter is exactly the Lagrangian multiplier. Thus OOD-TV-IRM is essentially a primal-dual optimization model, where the primal optimization minimizes the entire invariant risk and the dual optimization strengthens the TV penalty. The objective is to reach a semi-Nash equilibrium where the balance between the training loss and OOD generalization is maintained. We also develop a convergent primal-dual algorithm that facilitates an adversarial learning scheme. Experimental results show that OOD-TV-IRM outperforms IRM-TV in most situations.
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2502.18975.pdf' target='_blank'>https://arxiv.org/pdf/2502.18975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Surner, Abdelmajid Khelil, Ludwig Bothmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18975">Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Out-of-distribution generalization of machine learning models remains challenging since the models are inherently bound to the training data distribution. This especially manifests, when the learned models rely on spurious correlations. Most of the existing approaches apply data manipulation, representation learning, or learning strategies to achieve generalizable models. Unfortunately, these approaches usually require multiple training domains, group labels, specialized augmentation, or pre-processing to reach generalizable models. We propose a novel approach that addresses these limitations by providing a technique to guide the neural network through the training phase. We first establish input pairs, representing the spurious attribute and describing the invariance, a characteristic that should not affect the outcome of the model. Based on these pairs, we form a corrective gradient complementing the traditional gradient descent approach. We further make this correction mechanism adaptive based on a predefined invariance condition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasets demonstrate the effectiveness of our approach and the robustness to group shifts.
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2502.18188.pdf' target='_blank'>https://arxiv.org/pdf/2502.18188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanzi Chen, Jiying Zhang, Yang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18188">Graph Augmentation for Cross Graph Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-graph node classification, utilizing the abundant labeled nodes from one graph to help classify unlabeled nodes in another graph, can be viewed as a domain generalization problem of graph neural networks (GNNs) due to the structure shift commonly appearing among various graphs. Nevertheless, current endeavors for cross-graph node classification mainly focus on model training. Data augmentation approaches, a simple and easy-to-implement domain generalization technique, remain under-explored. In this paper, we develop a new graph structure augmentation for the crossgraph domain generalization problem. Specifically, low-weight edgedropping is applied to remove potential noise edges that may hinder the generalization ability of GNNs, stimulating the GNNs to capture the essential invariant information underlying different structures. Meanwhile, clustering-based edge-adding is proposed to generate invariant structures based on the node features from the same distribution. Consequently, with these augmentation techniques, the GNNs can maintain the domain invariant structure information that can improve the generalization ability. The experiments on out-ofdistribution citation network datasets verify our method achieves state-of-the-art performance among conventional augmentations.
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2502.10408.pdf' target='_blank'>https://arxiv.org/pdf/2502.10408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Doyoun Kim, Suin Kim, Yojan Jo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10408">Knowledge Tracing in Programming Education Integrating Students' Questions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge tracing (KT) in programming education presents unique challenges due to the complexity of coding tasks and the diverse methods students use to solve problems. Although students' questions often contain valuable signals about their understanding and misconceptions, traditional KT models often neglect to incorporate these questions as inputs to address these challenges. This paper introduces SQKT (Students' Question-based Knowledge Tracing), a knowledge tracing model that leverages students' questions and automatically extracted skill information to enhance the accuracy of predicting students' performance on subsequent problems in programming education. Our method creates semantically rich embeddings that capture not only the surface-level content of the questions but also the student's mastery level and conceptual understanding. Experimental results demonstrate SQKT's superior performance in predicting student completion across various Python programming courses of differing difficulty levels. In in-domain experiments, SQKT achieved a 33.1\% absolute improvement in AUC compared to baseline models. The model also exhibited robust generalization capabilities in cross-domain settings, effectively addressing data scarcity issues in advanced programming courses. SQKT can be used to tailor educational content to individual learning needs and design adaptive learning systems in computer science education.
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2502.03835.pdf' target='_blank'>https://arxiv.org/pdf/2502.03835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenwei He, Hongsu Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03835">Single-Domain Generalized Object Detection by Balancing Domain Diversity and Invariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-domain generalization for object detection (S-DGOD) seeks to transfer learned representations from a single source domain to unseen target domains. While recent approaches have primarily focused on achieving feature invariance, they ignore that domain diversity also presents significant challenges for the task. First, such invariance-driven strategies often lead to the loss of domain-specific information, resulting in incomplete feature representations. Second, cross-domain feature alignment forces the model to overlook domain-specific discrepancies, thereby increasing the complexity of the training process. To address these limitations, this paper proposes the Diversity Invariant Detection Model (DIDM), which achieves a harmonious integration of domain-specific diversity and domain invariance. Our key idea is to learn the invariant representations by keeping the inherent domain-specific features. Specifically, we introduce the Diversity Learning Module (DLM). This module limits the invariant semantics while explicitly enhancing domain-specific feature representation through a proposed feature diversity loss. Furthermore, to ensure cross-domain invariance without sacrificing diversity, we incorporate the Weighted Aligning Module (WAM) to enable feature alignment while maintaining the discriminative domain-specific information. Extensive experiments on multiple diverse datasets demonstrate the effectiveness of the proposed model, achieving superior performance compared to existing methods.
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2501.18957.pdf' target='_blank'>https://arxiv.org/pdf/2501.18957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alfred Bexley, Lukas Radcliffe, Giles Weatherstone, Joseph Sakau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18957">Intrinsic Tensor Field Propagation in Large Language Models: A Novel Approach to Contextual Information Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Context propagation remains a central challenge in language model architectures, particularly in tasks requiring the retention of long-range dependencies. Conventional attention mechanisms, while effective in many applications, exhibit limitations in maintaining coherent contextual representations over extended sequences due to their reliance on discrete token interactions. A novel approach is introduced through the formulation of Intrinsic Tensor Field Propagation (ITFP), which models contextual relationships as continuous tensor fields distributed across token embeddings. The propagation dynamics are governed through differential equations that enable a structured flow of contextual information, augmenting the standard attention mechanism to enhance coherence and recall. A series of experiments conducted on an open-source transformer-based model demonstrate that ITFP provides measurable improvements in contextual retention, dependency resolution, and inference stability across various linguistic structures. Comparisons with baseline models reveal a reduction in syntactic inconsistencies and factual errors, while ablation studies indicate that the choice of propagation depth and integration strength significantly impacts model performance. Additional evaluations assessing domain generalization suggest that ITFP effectively adapts across different text genres, reinforcing its applicability beyond conventional language modeling tasks. Although computational trade-offs are introduced through the inclusion of tensor field computations, empirical findings suggest that the benefits in accuracy and coherence outweigh the increased processing demands.
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2501.16848.pdf' target='_blank'>https://arxiv.org/pdf/2501.16848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ron van Bree, Diego Marcos, Ioannis Athanasiadis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16848">Hybrid Phenology Modeling for Predicting Temperature Effects on Tree Dormancy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biophysical models offer valuable insights into climate-phenology relationships in both natural and agricultural settings. However, there are substantial structural discrepancies across models which require site-specific recalibration, often yielding inconsistent predictions under similar climate scenarios. Machine learning methods offer data-driven solutions, but often lack interpretability and alignment with existing knowledge. We present a phenology model describing dormancy in fruit trees, integrating conventional biophysical models with a neural network to address their structural disparities. We evaluate our hybrid model in an extensive case study predicting cherry tree phenology in Japan, South Korea and Switzerland. Our approach consistently outperforms both traditional biophysical and machine learning models in predicting blooming dates across years. Additionally, the neural network's adaptability facilitates parameter learning for specific tree varieties, enabling robust generalization to new sites without site-specific recalibration. This hybrid model leverages both biophysical constraints and data-driven flexibility, offering a promising avenue for accurate and interpretable phenology modeling.
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2501.14119.pdf' target='_blank'>https://arxiv.org/pdf/2501.14119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Derek Yotheringhay, Alistair Kirkland, Humphrey Kirkbride, Josiah Whitesteeple
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14119">Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformative innovations in model architectures have introduced hierarchical embedding augmentation as a means to redefine the representation of tokens through multi-level semantic structures, offering enhanced adaptability to complex linguistic inputs. Autonomous structural memory manipulation further advances this paradigm through dynamic memory reallocation mechanisms that prioritize critical contextual features while suppressing less relevant information, enabling scalable and efficient performance across diverse tasks. Experimental results reveal substantial improvements in computational efficiency, with marked reductions in processing overhead for longer input sequences, achieved through memory reorganization strategies that adapt to evolving contextual requirements. Hierarchical embeddings not only improved contextual alignment but also facilitated task generalization by capturing relationships at varying semantic granularities, ensuring coherence across layers without introducing significant computational redundancies. Comparative analysis against baseline models demonstrated unique advantages in accuracy, efficiency, and interpretability, particularly in tasks requiring complex contextual understanding or domain-specific adaptability. The ability to dynamically adjust token representations and memory configurations contributed to the model's robustness under varied and unpredictable input conditions. Applications benefiting from these advancements include multi-domain generalization, interactive systems, and scenarios involving real-time decision-making, where traditional static memory architectures often face limitations. The proposed methodology combines advanced embedding and memory management strategies into a cohesive framework that addresses scalability challenges while preserving task-specific relevance.
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2501.11896.pdf' target='_blank'>https://arxiv.org/pdf/2501.11896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhong-Hua Sun, Ru-Yuan Zhang, Zonglei Zhen, Da-Hui Wang, Yong-Jie Li, Xiaohong Wan, Hongzhi You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11896">Systematic Abductive Reasoning via Diverse Relation Representations in Vector-symbolic Architecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In abstract visual reasoning, monolithic deep learning models suffer from limited interpretability and generalization, while existing neuro-symbolic approaches fall short in capturing the diversity and systematicity of attributes and relation representations. To address these challenges, we propose a Systematic Abductive Reasoning model with diverse relation representations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve Raven's Progressive Matrices (RPM). To derive attribute representations with symbolic reasoning potential, we introduce not only various types of atomic vectors that represent numeric, periodic and logical semantics, but also the structured high-dimentional representation (SHDR) for the overall Grid component. For systematic reasoning, we propose novel numerical and logical relation functions and perform rule abduction and execution in a unified framework that integrates these relation representations. Experimental results demonstrate that Rel-SAR achieves significant improvement on RPM tasks and exhibits robust out-of-distribution generalization. Rel-SAR leverages the synergy between HD attribute representations and symbolic reasoning to achieve systematic abductive reasoning with both interpretable and computable semantics.
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2501.05496.pdf' target='_blank'>https://arxiv.org/pdf/2501.05496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbing Zhou, Xiangmou Qu, Chenlong You, Jiyang Zhou, Jingyue Tang, Xin Zheng, Chunmao Cai, Yingbo Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05496">FedSA: A Unified Representation Learning via Semantic Anchors for Prototype-based Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prototype-based federated learning has emerged as a promising approach that shares lightweight prototypes to transfer knowledge among clients with data heterogeneity in a model-agnostic manner. However, existing methods often collect prototypes directly from local models, which inevitably introduce inconsistencies into representation learning due to the biased data distributions and differing model architectures among clients. In this paper, we identify that both statistical and model heterogeneity create a vicious cycle of representation inconsistency, classifier divergence, and skewed prototype alignment, which negatively impacts the performance of clients. To break the vicious cycle, we propose a novel framework named Federated Learning via Semantic Anchors (FedSA) to decouple the generation of prototypes from local representation learning. We introduce a novel perspective that uses simple yet effective semantic anchors serving as prototypes to guide local models in learning consistent representations. By incorporating semantic anchors, we further propose anchor-based regularization with margin-enhanced contrastive learning and anchor-based classifier calibration to correct feature extractors and calibrate classifiers across clients, achieving intra-class compactness and inter-class separability of prototypes while ensuring consistent decision boundaries. We then update the semantic anchors with these consistent and discriminative prototypes, which iteratively encourage clients to collaboratively learn a unified data representation with robust generalization. Extensive experiments under both statistical and model heterogeneity settings show that FedSA significantly outperforms existing prototype-based FL methods on various classification tasks.
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2501.03221.pdf' target='_blank'>https://arxiv.org/pdf/2501.03221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haosheng Zhang, Hao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03221">RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet Transform Projection-based Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of 3D object classification, a fundamental challenge lies in addressing the scarcity of labeled data, which limits the applicability of traditional data-intensive learning paradigms. This challenge is particularly pronounced in few-shot learning scenarios, where the objective is to achieve robust generalization from minimal annotated samples. To overcome these limitations, it is crucial to identify and leverage the most salient and discriminative features of 3D objects, thereby enhancing learning efficiency and reducing dependency on large-scale labeled datasets. This work introduces RW-Net, a novel framework designed to address the challenges above by integrating Rate-Distortion Explanation (RDE) and wavelet transform into a state-of-the-art projection-based 3D object classification architecture. The proposed method capitalizes on RDE to extract critical features by identifying and preserving the most informative data components while reducing redundancy. This process ensures the retention of essential information for effective decision-making, optimizing the model's ability to learn from limited data. Complementing RDE, incorporating the wavelet transform further enhances the framework's capability to generalize in low-data regimes. By emphasizing low-frequency components of the input data, the wavelet transform captures fundamental geometric and structural attributes of 3D objects. These attributes are instrumental in mitigating overfitting and improving the robustness of the learned representations across diverse tasks and domains. To validate the effectiveness of our RW-Net, we conduct extensive experiments on three datasets: ModelNet40, ModelNet40-C, and ScanObjectNN for few-shot 3D object classification. The results demonstrate that our approach achieves state-of-the-art performance and exhibits superior generalization and robustness in few-shot learning scenarios.
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2412.12349.pdf' target='_blank'>https://arxiv.org/pdf/2412.12349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madiyar Alimov, Temirlan Meiramkhanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12349">Domain Generalization in Autonomous Driving: Evaluating YOLOv8s, RT-DETR, and YOLO-NAS with the ROAD-Almaty Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates the domain generalization capabilities of three state-of-the-art object detection models - YOLOv8s, RT-DETR, and YOLO-NAS - within the unique driving environment of Kazakhstan. Utilizing the newly constructed ROAD-Almaty dataset, which encompasses diverse weather, lighting, and traffic conditions, we evaluated the models' performance without any retraining. Quantitative analysis revealed that RT-DETR achieved an average F1-score of 0.672 at IoU=0.5, outperforming YOLOv8s (0.458) and YOLO-NAS (0.526) by approximately 46% and 27%, respectively. Additionally, all models exhibited significant performance declines at higher IoU thresholds (e.g., a drop of approximately 20% when increasing IoU from 0.5 to 0.75) and under challenging environmental conditions, such as heavy snowfall and low-light scenarios. These findings underscore the necessity for geographically diverse training datasets and the implementation of specialized domain adaptation techniques to enhance the reliability of autonomous vehicle detection systems globally. This research contributes to the understanding of domain generalization challenges in autonomous driving, particularly in underrepresented regions.
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2412.11171.pdf' target='_blank'>https://arxiv.org/pdf/2412.11171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songgaojun Deng, Maarten de Rijke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11171">Learning Latent Spaces for Domain Generalization in Time Series Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time series forecasting is vital in many real-world applications, yet developing models that generalize well on unseen relevant domains -- such as forecasting web traffic data on new platforms/websites or estimating e-commerce demand in new regions -- remains underexplored. Existing forecasting models often struggle with domain shifts in time series data, as the temporal patterns involve complex components like trends, seasonality, etc. While some prior work addresses this by matching feature distributions across domains or disentangling domain-shared features using label information, they fail to reveal insights into the latent temporal dependencies, which are critical for identifying common patterns across domains and achieving generalization.
  We propose a framework for domain generalization in time series forecasting by mining the latent factors that govern temporal dependencies across domains. Our approach uses a decomposition-based architecture with a new Conditional $Î²$-Variational Autoencoder (VAE), wherein time series data is first decomposed into trend-cyclical and seasonal components, each modeled independently through separate $Î²$-VAE modules. The $Î²$-VAE aims to capture disentangled latent factors that control temporal dependencies across domains. We enhance the learning of domain-specific information with a decoder-conditional design and introduce domain regularization to improve the separation of domain-shared and domain-specific latent factors. Our proposed method is flexible and can be applied to various time series forecasting models, enabling effective domain generalization with simplicity and efficiency. We validate its effectiveness on five real-world time series datasets, covering web traffic, e-commerce, finance and power consumption, demonstrating improved generalization performance over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2412.08479.pdf' target='_blank'>https://arxiv.org/pdf/2412.08479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sumaiya Zoha, Jeong-Gun Lee, Young-Woong Ko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08479">CAT: Class Aware Adaptive Thresholding for Semi-Supervised Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) seeks to transfer knowledge from multiple source domains to unseen target domains, even in the presence of domain shifts. Achieving effective generalization typically requires a large and diverse set of labeled source data to learn robust representations that can generalize to new, unseen domains. However, obtaining such high-quality labeled data is often costly and labor-intensive, limiting the practical applicability of DG. To address this, we investigate a more practical and challenging problem: semi-supervised domain generalization (SSDG) under a label-efficient paradigm. In this paper, we propose a novel method, CAT, which leverages semi-supervised learning with limited labeled data to achieve competitive generalization performance under domain shifts. Our method addresses key limitations of previous approaches, such as reliance on fixed thresholds and sensitivity to noisy pseudo-labels. CAT combines adaptive thresholding with noisy label refinement techniques, creating a straightforward yet highly effective solution for SSDG tasks. Specifically, our approach uses flexible thresholding to generate high-quality pseudo-labels with higher class diversity while refining noisy pseudo-labels to improve their reliability. Extensive experiments across multiple benchmark datasets demonstrate the superior performance of our method, highlighting its effectiveness in achieving robust generalization under domain shift.
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2412.07226.pdf' target='_blank'>https://arxiv.org/pdf/2412.07226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingfan Wang, Guoliang Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07226">Attention Head Purification: A New Perspective to Harness CLIP for Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Domain Generalization (DG) aims to learn a model from multiple source domains to achieve satisfactory performance on unseen target domains. Recent works introduce CLIP to DG tasks due to its superior image-text alignment and zeros-shot performance. Previous methods either utilize full fine-tuning or prompt-learning paradigms to harness CLIP for DG tasks. Those works focus on avoiding catastrophic forgetting of the original knowledge encoded in CLIP but ignore that the knowledge encoded in CLIP in nature may contain domain-specific cues that constrain its domain generalization performance. In this paper, we propose a new perspective to harness CLIP for DG, i.e., attention head purification. We observe that different attention heads may encode different properties of an image and selecting heads appropriately may yield remarkable performance improvement across domains. Based on such observations, we purify the attention heads of CLIP from two levels, including task-level purification and domain-level purification. For task-level purification, we design head-aware LoRA to make each head more adapted to the task we considered. For domain-level purification, we perform head selection via a simple gating strategy. We utilize MMD loss to encourage masked head features to be more domain-invariant to emphasize more generalizable properties/heads. During training, we jointly perform task-level purification and domain-level purification. We conduct experiments on various representative DG benchmarks. Though simple, extensive experiments demonstrate that our method performs favorably against previous state-of-the-arts.
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2412.07127.pdf' target='_blank'>https://arxiv.org/pdf/2412.07127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Li, Song Wang, Chen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07127">Deep Learning-Enhanced Preconditioning for Efficient Conjugate Gradient Solvers in Large-Scale PDE Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preconditioning techniques are crucial for enhancing the efficiency of solving large-scale linear equation systems that arise from partial differential equation (PDE) discretization. These techniques, such as Incomplete Cholesky factorization (IC) and data-driven neural network methods, accelerate the convergence of iterative solvers like Conjugate Gradient (CG) by approximating the original matrices. This paper introduces a novel approach that integrates Graph Neural Network (GNN) with traditional IC, addressing the shortcomings of direct generation methods based on GNN and achieving significant improvements in computational efficiency and scalability. Experimental results demonstrate an average reduction in iteration counts by 24.8% compared to IC and a two-order-of-magnitude increase in training scale compared to previous methods. A three-dimensional static structural analysis utilizing finite element methods was validated on training sparse matrices of up to 5 million dimensions and inference scales of up to 10 million. Furthermore, the approach demon-strates robust generalization capabilities across scales, facilitating the effective acceleration of CG solvers for large-scale linear equations using small-scale data on modest hardware. The method's robustness and scalability make it a practical solution for computational science.
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2412.05169.pdf' target='_blank'>https://arxiv.org/pdf/2412.05169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Schapiro, Han Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05169">Towards Understanding the Role of Sharpness-Aware Minimization Algorithms for Out-of-Distribution Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, sharpness-aware minimization (SAM) has emerged as a promising method to improve generalization by minimizing sharpness, which is known to correlate well with generalization ability. Since the original proposal of SAM, many variants of SAM have been proposed to improve its accuracy and efficiency, but comparisons have mainly been restricted to the i.i.d. setting. In this paper we study SAM for out-of-distribution (OOD) generalization. First, we perform a comprehensive comparison of eight SAM variants on zero-shot OOD generalization, finding that the original SAM outperforms the Adam baseline by $4.76\%$ and the strongest SAM variants outperform the Adam baseline by $8.01\%$ on average. We then provide an OOD generalization bound in terms of sharpness for this setting. Next, we extend our study of SAM to the related setting of gradual domain adaptation (GDA), another form of OOD generalization where intermediate domains are constructed between the source and target domains, and iterative self-training is done on intermediate domains, to improve the overall target domain error. In this setting, our experimental results demonstrate that the original SAM outperforms the baseline of Adam on each of the experimental datasets by $0.82\%$ on average and the strongest SAM variants outperform Adam by $1.52\%$ on average. We then provide a generalization bound for SAM in the GDA setting. Asymptotically, this generalization bound is no better than the one for self-training in the literature of GDA. This highlights a further disconnection between the theoretical justification for SAM versus its empirical performance, with recent work finding that low sharpness alone does not account for all of SAM's generalization benefits. For future work, we provide several potential avenues for obtaining a tighter analysis for SAM in the OOD setting.
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2411.19451.pdf' target='_blank'>https://arxiv.org/pdf/2411.19451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Zhao, Chang Xu, Bailu Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19451">Learning Visual Abstract Reasoning through Dual-Stream Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual abstract reasoning tasks present challenges for deep neural networks, exposing limitations in their capabilities. In this work, we present a neural network model that addresses the challenges posed by Raven's Progressive Matrices (RPM). Inspired by the two-stream hypothesis of visual processing, we introduce the Dual-stream Reasoning Network (DRNet), which utilizes two parallel branches to capture image features. On top of the two streams, a reasoning module first learns to merge the high-level features of the same image. Then, it employs a rule extractor to handle combinations involving the eight context images and each candidate image, extracting discrete abstract rules and utilizing an multilayer perceptron (MLP) to make predictions. Empirical results demonstrate that the proposed DRNet achieves state-of-the-art average performance across multiple RPM benchmarks. Furthermore, DRNet demonstrates robust generalization capabilities, even extending to various out-of-distribution scenarios. The dual streams within DRNet serve distinct functions by addressing local or spatial information. They are then integrated into the reasoning module, leveraging abstract rules to facilitate the execution of visual reasoning tasks. These findings indicate that the dual-stream architecture could play a crucial role in visual abstract reasoning.
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2411.19434.pdf' target='_blank'>https://arxiv.org/pdf/2411.19434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Safaa Abdullahi Moallim Mohamud, Ho-Young Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19434">Actions and Objects Pathways for Domain Adaptation in Video Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce the Actions and Objects Pathways (AOPath) for out-of-domain generalization in video question answering tasks. AOPath leverages features from a large pretrained model to enhance generalizability without the need for explicit training on the unseen domains. Inspired by human brain, AOPath dissociates the pretrained features into action and object features, and subsequently processes them through separate reasoning pathways. It utilizes a novel module which converts out-of-domain features into domain-agnostic features without introducing any trainable weights. We validate the proposed approach on the TVQA dataset, which is partitioned into multiple subsets based on genre to facilitate the assessment of generalizability. The proposed approach demonstrates 5% and 4% superior performance over conventional classifiers on out-of-domain and in-domain datasets, respectively. It also outperforms prior methods that involve training millions of parameters, whereas the proposed approach trains very few parameters.
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2411.16877.pdf' target='_blank'>https://arxiv.org/pdf/2411.16877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zequn Chen, Jiezhi Yang, Heng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16877">PreF3R: Pose-Free Feed-Forward 3D Gaussian Splatting from Variable-length Image Sequence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PreF3R, Pose-Free Feed-forward 3D Reconstruction from an image sequence of variable length. Unlike previous approaches, PreF3R removes the need for camera calibration and reconstructs the 3D Gaussian field within a canonical coordinate frame directly from a sequence of unposed images, enabling efficient novel-view rendering. We leverage DUSt3R's ability for pair-wise 3D structure reconstruction, and extend it to sequential multi-view input via a spatial memory network, eliminating the need for optimization-based global alignment. Additionally, PreF3R incorporates a dense Gaussian parameter prediction head, which enables subsequent novel-view synthesis with differentiable rasterization. This allows supervising our model with the combination of photometric loss and pointmap regression loss, enhancing both photorealism and structural accuracy. Given a sequence of ordered images, PreF3R incrementally reconstructs the 3D Gaussian field at 20 FPS, therefore enabling real-time novel-view rendering. Empirical experiments demonstrate that PreF3R is an effective solution for the challenging task of pose-free feed-forward novel-view synthesis, while also exhibiting robust generalization to unseen scenes.
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2411.10886.pdf' target='_blank'>https://arxiv.org/pdf/2411.10886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ansh Shah, K Madhava Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10886">MetricGold: Leveraging Text-To-Image Latent Diffusion Models for Metric Depth Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recovering metric depth from a single image remains a fundamental challenge in computer vision, requiring both scene understanding and accurate scaling. While deep learning has advanced monocular depth estimation, current models often struggle with unfamiliar scenes and layouts, particularly in zero-shot scenarios and when predicting scale-ergodic metric depth. We present MetricGold, a novel approach that harnesses generative diffusion model's rich priors to improve metric depth estimation. Building upon recent advances in MariGold, DDVM and Depth Anything V2 respectively, our method combines latent diffusion, log-scaled metric depth representation, and synthetic data training. MetricGold achieves efficient training on a single RTX 3090 within two days using photo-realistic synthetic data from HyperSIM, VirtualKitti, and TartanAir. Our experiments demonstrate robust generalization across diverse datasets, producing sharper and higher quality metric depth estimates compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2411.10819.pdf' target='_blank'>https://arxiv.org/pdf/2411.10819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Yan, Zhong Chen, Cai Xu, Xinglei Shen, Jay Shiao, John Einck, Ronald C Chen, Hao Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10819">An Oversampling-enhanced Multi-class Imbalanced Classification Framework for Patient Health Status Prediction Using Patient-reported Outcomes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Patient-reported outcomes (PROs) directly collected from cancer patients being treated with radiation therapy play a vital role in assisting clinicians in counseling patients regarding likely toxicities. Precise prediction and evaluation of symptoms or health status associated with PROs are fundamental to enhancing decision-making and planning for the required services and support as patients transition into survivorship. However, the raw PRO data collected from hospitals exhibits some intrinsic challenges such as incomplete item reports and imbalance patient toxicities. To the end, in this study, we explore various machine learning techniques to predict patient outcomes related to health status such as pain levels and sleep discomfort using PRO datasets from a cancer photon/proton therapy center. Specifically, we deploy six advanced machine learning classifiers -- Random Forest (RF), XGBoost, Gradient Boosting (GB), Support Vector Machine (SVM), Multi-Layer Perceptron with Bagging (MLP-Bagging), and Logistic Regression (LR) -- to tackle a multi-class imbalance classification problem across three prevalent cancer types: head and neck, prostate, and breast cancers. To address the class imbalance issue, we employ an oversampling strategy, adjusting the training set sample sizes through interpolations of in-class neighboring samples, thereby augmenting minority classes without deviating from the original skewed class distribution. Our experimental findings across multiple PRO datasets indicate that the RF and XGB methods achieve robust generalization performance, evidenced by weighted AUC and detailed confusion matrices, in categorizing outcomes as mild, intermediate, and severe post-radiation therapy. These results underscore the models' effectiveness and potential utility in clinical settings.
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2411.06798.pdf' target='_blank'>https://arxiv.org/pdf/2411.06798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David R. Nelson, Ashish Kumar Jaiswal, Noha Ismail, Alexandra Mystikou, Kourosh Salehi-Ashtiani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06798">LA4SR: illuminating the dark proteome with generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI language models (LMs) show promise for biological sequence analysis. We re-engineered open-source LMs (GPT-2, BLOOM, DistilRoBERTa, ELECTRA, and Mamba, ranging from 70M to 12B parameters) for microbial sequence classification. The models achieved F1 scores up to 95 and operated 16,580x faster and at 2.9x the recall of BLASTP. They effectively classified the algal dark proteome - uncharacterized proteins comprising about 65% of total proteins - validated on new data including a new, complete Hi-C/Pacbio Chlamydomonas genome. Larger (>1B) LA4SR models reached high accuracy (F1 > 86) when trained on less than 2% of available data, rapidly achieving strong generalization capacity. High accuracy was achieved when training data had intact or scrambled terminal information, demonstrating robust generalization to incomplete sequences. Finally, we provide custom AI explainability software tools for attributing amino acid patterns to AI generative processes and interpret their outputs in evolutionary and biophysical contexts.
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2411.06214.pdf' target='_blank'>https://arxiv.org/pdf/2411.06214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuguang Li, Zhonglin Zuo, Zheng Dong, Yang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06214">Early Prediction of Natural Gas Pipeline Leaks Using the MKTCN Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural gas pipeline leaks pose severe risks, leading to substantial economic losses and potential hazards to human safety. In this study, we develop an accurate model for the early prediction of pipeline leaks. To the best of our knowledge, unlike previous anomaly detection, this is the first application to use internal pipeline data for early prediction of leaks. The modeling process addresses two main challenges: long-term dependencies and sample imbalance. First, we introduce a dilated convolution-based prediction model to capture long-term dependencies, as dilated convolution expands the model's receptive field without added computational cost. Second, to mitigate sample imbalance, we propose the MKTCN model, which incorporates the Kolmogorov-Arnold Network as the fully connected layer in a dilated convolution model, enhancing network generalization. Finally, we validate the MKTCN model through extensive experiments on two real-world datasets. Results demonstrate that MKTCN outperforms in generalization and classification, particularly under severe data imbalance, and effectively predicts leaks up to 5000 seconds in advance. Overall, the MKTCN model represents a significant advancement in early pipeline leak prediction, providing robust generalization and improved modeling of the long-term dependencies inherent in multi-dimensional time-series data.
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2411.04777.pdf' target='_blank'>https://arxiv.org/pdf/2411.04777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elija Deineko, Carina Kehrt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04777">Learn to Solve Vehicle Routing Problems ASAP: A Neural Optimization Approach for Time-Constrained Vehicle Routing Problems with Finite Vehicle Fleet</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding a feasible and prompt solution to the Vehicle Routing Problem (VRP) is a prerequisite for efficient freight transportation, seamless logistics, and sustainable mobility. Traditional optimization methods reach their limits when confronted with the real-world complexity of VRPs, which involve numerous constraints and objectives. Recently, the ability of generative Artificial Intelligence (AI) to solve combinatorial tasks, known as Neural Combinatorial Optimization (NCO), demonstrated promising results, offering new perspectives. In this study, we propose an NCO approach to solve a time-constrained capacitated VRP with a finite vehicle fleet size. The approach is based on an encoder-decoder architecture, formulated in line with the Policy Optimization with Multiple Optima (POMO) protocol and trained via a Proximal Policy Optimization (PPO) algorithm. We successfully trained the policy with multiple objectives (minimizing the total distance while maximizing vehicle utilization) and evaluated it on medium and large instances, benchmarking it against state-of-the-art heuristics. The method is able to find adequate and cost-efficient solutions, showing both flexibility and robust generalization. Finally, we provide a critical analysis of the solution generated by NCO and discuss the challenges and opportunities of this new branch of intelligent learning algorithms emerging in optimization science, focusing on freight transportation.
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2410.20102.pdf' target='_blank'>https://arxiv.org/pdf/2410.20102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuto Shibata, Yasunori Kudo, Yohei Sugawara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20102">Anatomical 3D Style Transfer Enabling Efficient Federated Learning with Extremely Low Communication Costs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose a novel federated learning (FL) approach that utilizes 3D style transfer for the multi-organ segmentation task. The multi-organ dataset, obtained by integrating multiple datasets, has high scalability and can improve generalization performance as the data volume increases. However, the heterogeneity of data owing to different clients with diverse imaging conditions and target organs can lead to severe overfitting of local models. To align models that overfit to different local datasets, existing methods require frequent communication with the central server, resulting in higher communication costs and risk of privacy leakage. To achieve an efficient and safe FL, we propose an Anatomical 3D Frequency Domain Generalization (A3DFDG) method for FL. A3DFDG utilizes structural information of human organs and clusters the 3D styles based on the location of organs. By mixing styles based on these clusters, it preserves the anatomical information and leads models to learn intra-organ diversity, while aligning the optimization of each local model. Experiments indicate that our method can maintain its accuracy even in cases where the communication cost is highly limited (=1.25% of the original cost) while achieving a significant difference compared to baselines, with a higher global dice similarity coefficient score of 4.3%. Despite its simplicity and minimal computational overhead, these results demonstrate that our method has high practicality in real-world scenarios where low communication costs and a simple pipeline are required. The code used in this project will be publicly available.
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2410.01213.pdf' target='_blank'>https://arxiv.org/pdf/2410.01213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arda Genc, Justin Marlowe, Anika Jalil, Libor Kovarik, Phillip Christopher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01213">A versatile machine learning workflow for high-throughput analysis of supported metal catalyst particles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and efficient characterization of nanoparticles (NPs), particularly regarding particle size distribution, is essential for advancing our understanding of their structure-property relationships and facilitating their design for various applications. In this study, we introduce a novel two-stage artificial intelligence (AI)-driven workflow for NP analysis that leverages prompt engineering techniques from state-of-the-art single-stage object detection and large-scale vision transformer (ViT) architectures. This methodology was applied to transmission electron microscopy (TEM) and scanning TEM (STEM) images of heterogeneous catalysts, enabling high-resolution, high-throughput analysis of particle size distributions for supported metal catalysts. The model's performance in detecting and segmenting NPs was validated across diverse heterogeneous catalyst systems, including various metals (Cu, Ru, Pt, and PtCo), supports (silica ($\text{SiO}_2$), $Î³$-alumina ($Î³$-$\text{Al}_2\text{O}_3$), and carbon black), and particle diameter size distributions with means and standard deviations of 2.9 $\pm$ 1.1 nm, 1.6 $\pm$ 0.2 nm, 9.7 $\pm$ 4.6 nm, and 4 $\pm$ 1.0 nm. Additionally, the proposed machine learning (ML) approach successfully detects and segments overlapping NPs anchored on non-uniform catalytic support materials, providing critical insights into their spatial arrangements and interactions. Our AI-assisted NP analysis workflow demonstrates robust generalization across diverse datasets and can be readily applied to similar NP segmentation tasks without requiring costly model retraining.
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2409.14060.pdf' target='_blank'>https://arxiv.org/pdf/2409.14060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjun Kim, Ohtae Jang, Haekang Song, Heesub Shin, Jaewoo Ok, Minyoung Back, Jaehyuk Youn, Sungho Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14060">Soft Segmented Randomization: Enhancing Domain Generalization in SAR-ATR for Synthetic-to-Measured</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic aperture radar technology is crucial for high-resolution imaging under various conditions; however, the acquisition of real-world synthetic aperture radar data for deep learning-based automatic target recognition remains challenging due to high costs and data availability issues. To overcome these challenges, synthetic data generated through simulations have been employed, although discrepancies between synthetic and real data can degrade model performance. In this study, we introduce a novel framework, soft segmented randomization, designed to reduce domain discrepancy and improve the generalize ability of synthetic aperture radar automatic target recognition models. The soft segmented randomization framework applies a Gaussian mixture model to segment target and clutter regions softly, introducing randomized variations that align the synthetic data's statistical properties more closely with those of real-world data. Experimental results demonstrate that the proposed soft segmented randomization framework significantly enhances model performance on measured synthetic aperture radar data, making it a promising approach for robust automatic target recognition in scenarios with limited or no access to measured data.
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2409.10048.pdf' target='_blank'>https://arxiv.org/pdf/2409.10048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wessel Ledder, Yuzhen Qin, Kiki van der Heijden
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10048">Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although deep reinforcement learning (DRL) approaches in audio signal processing have seen substantial progress in recent years, audio-driven DRL for tasks such as navigation, gaze control and head-orientation control in the context of human-robot interaction have received little attention. Here, we propose an audio-driven DRL framework in which we utilise deep Q-learning to develop an autonomous agent that orients towards a talker in the acoustic environment based on stereo speech recordings. Our results show that the agent learned to perform the task at a near perfect level when trained on speech segments in anechoic environments (that is, without reverberation). The presence of reverberation in naturalistic acoustic environments affected the agent's performance, although the agent still substantially outperformed a baseline, randomly acting agent. Finally, we quantified the degree of generalization of the proposed DRL approach across naturalistic acoustic environments. Our experiments revealed that policies learned by agents trained on medium or high reverb environments generalized to low reverb environments, but policies learned by agents trained on anechoic or low reverb environments did not generalize to medium or high reverb environments. Taken together, this study demonstrates the potential of audio-driven DRL for tasks such as head-orientation control and highlights the need for training strategies that enable robust generalization across environments for real-world audio-driven DRL applications.
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2409.09052.pdf' target='_blank'>https://arxiv.org/pdf/2409.09052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youzhu Jin, Yichen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09052">OrthoDoc: Multimodal Large Language Model for Assisting Diagnosis in Computed Tomography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) have achieved significant success in the general field of image processing. Their emerging task generalization and freeform conversational capabilities can greatly facilitate medical diagnostic assistance, helping patients better understand their conditions and enhancing doctor-patient trust. Computed Tomography (CT) is a non-invasive imaging technique used to capture the internal mechanisms of a patient's condition and is widely utilized. However, in past research, the complex textural features of this imaging data have made accurate interpretation by algorithms challenging, impeding the performance of general LLMs in diagnostic assistance. To address this, we developed OrthoDoc, a MLLM designed for CT diagnostics. OrthoDoc is trained on 120,000 CT images and diagnostic reports and includes a Retrieval-Augmented Generation (RAG) module capable of effectively mitigating model hallucinations. This module is informed by extensive medical literature, textbooks, and explanatory data. Thus, OrthoDoc not only processes complex CT images but also stores, understands, and reasons over medical knowledge and language. In extensive experiments, OrthoDoc outperforms commercial models led by GPT-4, demonstrating superior diagnostic capabilities and accuracy. Specifically, OrthoDoc significantly surpasses existing models in the diagnosis of common orthopedic conditions such as fractures, arthritis, and tumors. Additionally, OrthoDoc exhibits robust generalization and stability when handling rare and complex cases.
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2409.05885.pdf' target='_blank'>https://arxiv.org/pdf/2409.05885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Wu, Teng Wang, Jiaqi Nan, Lijun Yang, Jingxuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05885">A Dual-Path neural network model to construct the flame nonlinear thermoacoustic response in the time domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional numerical simulation methods require substantial computational resources to accurately determine the complete nonlinear thermoacoustic response of flames to various perturbation frequencies and amplitudes. In this paper, we have developed deep learning algorithms that can construct a comprehensive flame nonlinear response from limited numerical simulation data. To achieve this, we propose using a frequency-sweeping data type as the training dataset, which incorporates a rich array of learnable information within a constrained dataset. To enhance the precision in learning flame nonlinear response patterns from the training data, we introduce a Dual-Path neural network. This network consists of a Chronological Feature Path and a Temporal Detail Feature Path. The Dual-Path network is specifically designed to focus intensively on the temporal characteristics of velocity perturbation sequences, yielding more accurate flame response patterns and enhanced generalization capabilities. Validations confirm that our approach can accurately model flame nonlinear responses, even under conditions of significant nonlinearity, and exhibits robust generalization capabilities across various test scenarios.
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2601.07247.pdf' target='_blank'>https://arxiv.org/pdf/2601.07247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiran Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07247">Multi-environment Invariance Learning with Missing Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning models that can handle distribution shifts is a key challenge in domain generalization. Invariance learning, an approach that focuses on identifying features invariant across environments, improves model generalization by capturing stable relationships, which may represent causal effects when the data distribution is encoded within a structural equation model (SEM) and satisfies modularity conditions. This has led to a growing body of work that builds on invariance learning, leveraging the inherent heterogeneity across environments to develop methods that provide causal explanations while enhancing robust prediction. However, in many practical scenarios, obtaining complete outcome data from each environment is challenging due to the high cost or complexity of data collection. This limitation in available data hinders the development of models that fully leverage environmental heterogeneity, making it crucial to address missing outcomes to improve both causal insights and robust prediction. In this work, we derive an estimator from the invariance objective under missing outcomes. We establish non-asymptotic guarantees on variable selection property and $\ell_2$ error convergence rates, which are influenced by the proportion of missing data and the quality of imputation models across environments. We evaluate the performance of the new estimator through extensive simulations and demonstrate its application using the UCI Bike Sharing dataset to predict the count of bike rentals. The results show that despite relying on a biased imputation model, the estimator is efficient and achieves lower prediction error, provided the bias is within a reasonable range.
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2512.22774.pdf' target='_blank'>https://arxiv.org/pdf/2512.22774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Truong Son Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22774">Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce \textbf{Schrödinger AI}, a unified machine learning framework inspired by quantum mechanics. The system is defined by three tightly coupled components: (1) a {time-independent wave-energy solver} that treats perception and classification as spectral decomposition under a learned Hamiltonian; (2) a {time-dependent dynamical solver} governing the evolution of semantic wavefunctions over time, enabling context-aware decision revision, re-routing, and reasoning under environmental changes; and (3) a {low-rank operator calculus} that learns symbolic transformations such as modular arithmetic through learned quantum-like transition operators. Together, these components form a coherent physics-driven alternative to conventional cross-entropy training and transformer attention, providing robust generalization, interpretable semantics, and emergent topology. Empirically, Schrödinger AI demonstrates: (a) emergent semantic manifolds that reflect human-conceived class relations without explicit supervision; (b) dynamic reasoning that adapts to changing environments, including maze navigation with real-time potential-field perturbations; and (c) exact operator generalization on modular arithmetic tasks, where the system learns group actions and composes them across sequences far beyond training length. These results suggest a new foundational direction for machine learning, where learning is cast as discovering and navigating an underlying semantic energy landscape.
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2511.03963.pdf' target='_blank'>https://arxiv.org/pdf/2511.03963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shinto Eguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03963">Robust inference using density-powered Stein operators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a density-power weighted variant for the Stein operator, called the $γ$-Stein operator. This is a novel class of operators derived from the $γ$-divergence, designed to build robust inference methods for unnormalized probability models. The operator's construction (weighting by the model density raised to a positive power $γ$ inherently down-weights the influence of outliers, providing a principled mechanism for robustness. Applying this operator yields a robust generalization of score matching that retains the crucial property of being independent of the model's normalizing constant. We extend this framework to develop two key applications: the $γ$-kernelized Stein discrepancy for robust goodness-of-fit testing, and $γ$-Stein variational gradient descent for robust Bayesian posterior approximation. Empirical results on contaminated Gaussian and quartic potential models show our methods significantly outperform standard baselines in both robustness and statistical efficiency.
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2510.23448.pdf' target='_blank'>https://arxiv.org/pdf/2510.23448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingtu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23448">An Information-Theoretic Analysis of Out-of-Distribution Generalization in Meta-Learning with Applications to Meta-RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we study out-of-distribution generalization in meta-learning from an information-theoretic perspective. We focus on two scenarios: (i) when the testing environment mismatches the training environment, and (ii) when the training environment is broader than the testing environment. The first corresponds to the standard distribution mismatch setting, while the second reflects a broad-to-narrow training scenario. We further formalize the generalization problem in meta-reinforcement learning and establish corresponding generalization bounds. Finally, we analyze the generalization performance of a gradient-based meta-reinforcement learning algorithm.
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2510.22839.pdf' target='_blank'>https://arxiv.org/pdf/2510.22839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagnik Mukherjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22839">Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The optimization of structural parameters, such as mass(m), stiffness(k), and damping coefficient(c), is critical for designing efficient, resilient, and stable structures. Conventional numerical approaches, including Finite Element Method (FEM) and Computational Fluid Dynamics (CFD) simulations, provide high-fidelity results but are computationally expensive for iterative optimization tasks, as each evaluation requires solving the governing equations for every parameter combination. This study proposes a hybrid data-driven framework that integrates a Graph Neural Network (GNN) surrogate model with a Genetic Algorithm (GA) optimizer to overcome these challenges. The GNN is trained to accurately learn the nonlinear mapping between structural parameters and dynamic displacement responses, enabling rapid predictions without repeatedly solving the system equations. A dataset of single-degree-of-freedom (SDOF) system responses is generated using the Newmark Beta method across diverse mass, stiffness, and damping configurations. The GA then searches for globally optimal parameter sets by minimizing predicted displacements and enhancing dynamic stability. Results demonstrate that the GNN and GA framework achieves strong convergence, robust generalization, and significantly reduced computational cost compared to conventional simulations. This approach highlights the effectiveness of combining machine learning surrogates with evolutionary optimization for automated and intelligent structural design.
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2510.17469.pdf' target='_blank'>https://arxiv.org/pdf/2510.17469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17469">Layer Specialization Underlying Compositional Reasoning in Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformers exhibit compositional reasoning on sequences not observed during training, a capability often attributed to in-context learning (ICL) and skill composition. We investigate this phenomenon using the Random Hierarchy Model (RHM), a probabilistic context-free grammar that generates sequences through recursive rule application. Models are trained on subsets of sequences and evaluated across four generalization conditions: memorization, in-distribution generalization, out-of-distribution generalization with the same rules, and cross-layer transfer. Behaviorally, performance improves systematically with task complexity and the number of in-context examples, with out-of-distribution tasks requiring substantially more examples than in-distribution scenarios. Mechanistically, we identify a progressive emergence of layer specialization during training that correlates with generalization performance. Principal component analysis and attention pattern clustering reveal that transformers develop structured, hierarchically organized representations in specialized layers. These results demonstrate that transformers develop modular, interpretable mechanisms supporting compositional reasoning, linking internal algorithmic structure to observed behavioral capabilities.
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2510.15120.pdf' target='_blank'>https://arxiv.org/pdf/2510.15120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miraç Buğra Özkan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15120">Procedural Game Level Design with Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedural content generation (PCG) has become an increasingly popular technique in game development, allowing developers to generate dynamic, replayable, and scalable environments with reduced manual effort. In this study, a novel method for procedural level design using Deep Reinforcement Learning (DRL) within a Unity-based 3D environment is proposed. The system comprises two agents: a hummingbird agent, acting as a solver, and a floating island agent, responsible for generating and placing collectible objects (flowers) on the terrain in a realistic and context-aware manner. The hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm from the Unity ML-Agents toolkit. It learns to navigate through the terrain efficiently, locate flowers, and collect them while adapting to the ever-changing procedural layout of the island. The island agent is also trained using the Proximal Policy Optimization (PPO) algorithm. It learns to generate flower layouts based on observed obstacle positions, the hummingbird's initial state, and performance feedback from previous episodes. The interaction between these agents leads to emergent behavior and robust generalization across various environmental configurations. The results demonstrate that the approach not only produces effective and efficient agent behavior but also opens up new opportunities for autonomous game level design driven by machine learning. This work highlights the potential of DRL in enabling intelligent agents to both generate and solve content in virtual environments, pushing the boundaries of what AI can contribute to creative game development processes.
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2510.08588.pdf' target='_blank'>https://arxiv.org/pdf/2510.08588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritesh Mehta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08588">Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in large-scale biomedical semantic indexing and question answering), is crucial for extracting information from scientific literature but faces hurdles such as distinguishing between similar entity types like genes and chemicals. This study evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a targeted dictionary-based post-processing strategy to address common misclassifications. While this post-processing approach demonstrated notable improvement on our development set, increasing the micro F1-score from a baseline of 0.79 to 0.83, this enhancement did not generalize to the blind test set, where the post-processed model achieved a micro F1-score of 0.77 compared to the baselines 0.79. We also discuss insights gained from exploring alternative methodologies, including Conditional Random Fields. This work highlights the potential of dictionary-based refinement for pre-trained BioNER models but underscores the critical challenge of overfitting to development data and the necessity of ensuring robust generalization for real-world applicability.
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2510.07350.pdf' target='_blank'>https://arxiv.org/pdf/2510.07350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Chakravarty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07350">Out-of-Distribution Generalization in Climate-Aware Yield Prediction with Earth Observation Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate change is increasingly disrupting agricultural systems, making accurate crop yield forecasting essential for food security. While deep learning models have shown promise in yield prediction using satellite and weather data, their ability to generalize across geographic regions and years - critical for real-world deployment - remains largely untested. We benchmark two state-of-the-art models, GNN-RNN and MMST-ViT, under realistic out-of-distribution (OOD) conditions using the large-scale CropNet dataset spanning 1,200+ U.S. counties from 2017-2022. Through leave-one-cluster-out cross-validation across seven USDA Farm Resource Regions and year-ahead prediction scenarios, we identify substantial variability in cross-region transferability. GNN-RNN demonstrates superior generalization with positive correlations under geographic shifts, while MMST-ViT performs well in-domain but degrades sharply under OOD conditions. Regions like Heartland and Northern Great Plains show stable transfer dynamics (RMSE less than 10 bu/acre for soybean), whereas Prairie Gateway exhibits persistent underperformance (RMSE greater than 20 bu/acre) across both models and crops, revealing structural dissimilarities likely driven by semi-arid climate, irrigation patterns, and incomplete spectral coverage. Beyond accuracy differences, GNN-RNN achieves 135x faster training than MMST-ViT (14 minutes vs. 31.5 hours), making it more viable for sustainable deployment. Our findings underscore that spatial-temporal alignment - not merely model complexity or data scale - is key to robust generalization, and highlight the need for transparent OOD evaluation protocols to ensure equitable and reliable climate-aware agricultural forecasting.
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2510.02114.pdf' target='_blank'>https://arxiv.org/pdf/2510.02114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ding-Ruei Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02114">FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federeated Learning (FL) offers a privacy-preserving solution for Semantic Segmentation (SS) tasks to adapt to new domains, but faces significant challenges from these domain shifts, particularly when client data is unlabeled. However, most existing FL methods unrealistically assume access to labeled data on remote clients or fail to leverage the power of modern Vision Foundation Models (VFMs). Here, we propose a novel and challenging task, FFREEDG, in which a model is pretrained on a server's labeled source dataset and subsequently trained across clients using only their unlabeled data, without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a framework that leverages the knowledge of a VFM by integrating vision and language modalities. Our approach employs a Vision-Language decoder guided by CLIP-based text embeddings to improve semantic disambiguation and uses a weak-to-strong consistency learning strategy for robust local training on pseudo-labels. Our experiments on synthetic-to-real and clear-to-adverse-weather benchmarks demonstrate that our framework effectively tackles this new task, achieving competitive performance against established domain generalization and adaptation methods and setting a strong baseline for future research.
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2509.25241.pdf' target='_blank'>https://arxiv.org/pdf/2509.25241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25241">Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in training paradigms for Large Language Models (LLMs) have unlocked their remarkable capabilities in natural language processing and cross-domain generalization. While LLMs excel in tasks like programming and mathematical problem-solving, their zero-shot performance in specialized domains requiring expert knowledge, such as cybersecurity, is often suboptimal. This limitation arises because foundational LLMs are designed for general-purpose applications, constraining their ability to encapsulate domain-specific expertise within their parameter space. To address this, we explore fine-tuning strategies to embed cybersecurity knowledge into LLMs, enhancing their performance in cybersecurity question-answering (Q\&A) tasks while prioritizing computational efficiency. Specifically, we investigate Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized Low-Rank Adaptation (QLoRA) using a cybersecurity Q\&A dataset. Our results demonstrate that these fine-tuning approaches significantly outperform the foundational model in cybersecurity Q\&A tasks. Moreover, LoRA and QLoRA achieve comparable performance to SFT with substantially lower computational costs, offering an efficient pathway for adapting LLMs to specialized domains. Our work highlights the potential of low-rank fine-tuning strategies to bridge the gap between general-purpose LLMs and domain-specific applications.
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2509.19100.pdf' target='_blank'>https://arxiv.org/pdf/2509.19100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Robey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19100">Algorithms for Adversarially Robust Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given the widespread use of deep learning models in safety-critical applications, ensuring that the decisions of such models are robust against adversarial exploitation is of fundamental importance. In this thesis, we discuss recent progress toward designing algorithms that exhibit desirable robustness properties. First, we discuss the problem of adversarial examples in computer vision, for which we introduce new technical results, training paradigms, and certification algorithms. Next, we consider the problem of domain generalization, wherein the task is to train neural networks to generalize from a family of training distributions to unseen test distributions. We present new algorithms that achieve state-of-the-art generalization in medical imaging, molecular identification, and image classification. Finally, we study the setting of jailbreaking large language models (LLMs), wherein an adversarial user attempts to design prompts that elicit objectionable content from an LLM. We propose new attacks and defenses, which represent the frontier of progress toward designing robust language-based agents.
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2509.12081.pdf' target='_blank'>https://arxiv.org/pdf/2509.12081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12081">Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes deception as a mechanism for out-of-distribution (OOD) generalization: by learning data representations that make training data appear independent and identically distributed (iid) to an observer, we can identify stable features that eliminate spurious correlations and generalize to unseen domains. We refer to this principle as deceptive risk minimization (DRM) and instantiate it with a practical differentiable objective that simultaneously learns features that eliminate distribution shifts from the perspective of a detector based on conformal martingales while minimizing a task-specific loss. In contrast to domain adaptation or prior invariant representation learning methods, DRM does not require access to test data or a partitioning of training data into a finite number of data-generating domains. We demonstrate the efficacy of DRM on numerical experiments with concept shift and a simulated imitation learning setting with covariate shift in environments that a robot is deployed in.
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2509.03017.pdf' target='_blank'>https://arxiv.org/pdf/2509.03017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryandhimas E. Zezario
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03017">Non-Intrusive Intelligibility Prediction for Hearing Aids: Recent Advances, Trends, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides an overview of recent progress in non-intrusive speech intelligibility prediction for hearing aids (HA). We summarize developments in robust acoustic feature extraction, hearing loss modeling, and the use of emerging architectures for long-sequence processing. Listener-specific adaptation strategies and domain generalization approaches that aim to improve robustness in unseen acoustic environments are also discussed. Remaining challenges, such as the need for large-scale, diverse datasets and reliable cross-profile generalization, are acknowledged. Our goal is to offer a perspective on current trends, ongoing challenges, and possible future directions toward practical and reliable HA-oriented intelligibility prediction systems.
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2509.00476.pdf' target='_blank'>https://arxiv.org/pdf/2509.00476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Khalid Ali Mohamed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00476">Cross-Domain Malware Detection via Probability-Level Fusion of Lightweight Gradient Boosting Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The escalating sophistication of malware necessitates robust detection mechanisms that generalize across diverse data sources. Traditional single-dataset models struggle with cross-domain generalization and often incur high computational costs. This paper presents a novel, lightweight framework for malware detection that employs probability-level fusion across three distinct datasets: EMBER (static features), API Call Sequences (behavioral features), and CIC Obfuscated Memory (memory patterns). Our method trains individual LightGBM classifiers on each dataset, selects top predictive features to ensure efficiency, and fuses their prediction probabilities using optimized weights determined via grid search. Extensive experiments demonstrate that our fusion approach achieves a macro F1-score of 0.823 on a cross-domain validation set, significantly outperforming individual models and providing superior generalization. The framework maintains low computational overhead, making it suitable for real-time deployment, and all code and data are provided for full reproducibility.
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2508.08298.pdf' target='_blank'>https://arxiv.org/pdf/2508.08298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathan Breslow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08298">Channel-Wise MLPs Improve the Generalization of Recurrent Convolutional Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the impact of channel-wise mixing via multi-layer perceptrons (MLPs) on the generalization capabilities of recurrent convolutional networks. Specifically, we compare two architectures: DARC (Depth Aware Recurrent Convolution), which employs a simple recurrent convolutional structure, and DAMP (Depth Aware Multi-layer Perceptron), which extends DARC with a gated MLP for channel mixing. Using the Re-ARC benchmark, we find that DAMP significantly outperforms DARC in both in-distribution and out-of-distribution generalization under exact-match grading criteria. These results suggest that explicit channel mixing through MLPs enables recurrent convolutional networks to learn more robust and generalizable computational patterns. Our findings have implications for neural program synthesis and highlight the potential of DAMP as a target architecture for hypernetwork approaches.
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2508.01948.pdf' target='_blank'>https://arxiv.org/pdf/2508.01948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01948">Navigating High Dimensional Concept Space with Metalearning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapidly learning abstract concepts from limited examples is a hallmark of human intelligence. This work investigates whether gradient-based meta-learning can equip neural networks with inductive biases for efficient few-shot acquisition of discrete concepts. I compare meta-learning methods against a supervised learning baseline on Boolean concepts (logical statements) generated by a probabilistic context-free grammar (PCFG). By systematically varying concept dimensionality (number of features) and recursive compositionality (depth of grammar recursion), I delineate between complexity regimes in which meta-learning robustly improves few-shot concept learning and regimes in which it does not. Meta-learners are much better able to handle compositional complexity than featural complexity. I highlight some reasons for this with a representational analysis of the weights of meta-learners and a loss landscape analysis demonstrating how featural complexity increases the roughness of loss trajectories, allowing curvature-aware optimization to be more effective than first-order methods. I find improvements in out-of-distribution generalization on complex concepts by increasing the number of adaptation steps in meta-SGD, where adaptation acts as a way of encouraging exploration of rougher loss basins. Overall, this work highlights the intricacies of learning compositional versus featural complexity in high dimensional concept spaces and provides a road to understanding the role of 2nd order methods and extended gradient adaptation in few-shot concept learning.
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2507.15877.pdf' target='_blank'>https://arxiv.org/pdf/2507.15877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Ouellette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15877">Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We run a controlled compositional generalization experiment in the ARC-AGI domain: an open-world problem domain in which the ability to generalize out-of-distribution is, by design, an essential characteristic for success. We compare neural program synthesis and test-time fine-tuning approaches on this experiment. We find that execution-guided neural program synthesis outperforms all reference algorithms in its ability to compose novel solutions. Our empirical findings also suggest that the success of TTFT on ARC-AGI lies mainly in eliciting in-distribution knowledge that the LLM otherwise fails to rely on directly.
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2506.06024.pdf' target='_blank'>https://arxiv.org/pdf/2506.06024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deborah Pereg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06024">On Inverse Problems, Parameter Estimation, and Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Signal restoration and inverse problems are key elements in most real-world data science applications. In the past decades, with the emergence of machine learning methods, inversion of measurements has become a popular step in almost all physical applications, which is normally executed prior to downstream tasks that often involve parameter estimation. In this work, we analyze the general problem of parameter estimation in an inverse problem setting. First, we address the domain-shift problem by re-formulating it in direct relation with the discrete parameter estimation analysis. We analyze a significant vulnerability in current attempts to enforce domain generalization, which we dubbed the Double Meaning Theorem. Our theoretical findings are experimentally illustrated for domain shift examples in image deblurring and speckle suppression in medical imaging. We then proceed to a theoretical analysis of parameter estimation given observed measurements before and after data processing involving an inversion of the observations. We compare this setting for invertible and non-invertible (degradation) processes. We distinguish between continuous and discrete parameter estimation, corresponding with regression and classification problems, respectively. Our theoretical findings align with the well-known information-theoretic data processing inequality, and to a certain degree question the common misconception that data-processing for inversion, based on modern generative models that may often produce outstanding perceptual quality, will necessarily improve the following parameter estimation objective. It is our hope that this paper will provide practitioners with deeper insights that may be leveraged in the future for the development of more efficient and informed strategic system planning, critical in safety-sensitive applications.
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2505.10152.pdf' target='_blank'>https://arxiv.org/pdf/2505.10152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yikang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10152">Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated domain generalization aims to learn a generalizable model from multiple decentralized source domains for deploying on the unseen target domain. The style augmentation methods have achieved great progress on domain generalization. However, the existing style augmentation methods either explore the data styles within isolated source domain or interpolate the style information across existing source domains under the data decentralization scenario, which leads to limited style space. To address this issue, we propose a Multi-source Collaborative Style Augmentation and Domain-invariant learning method (MCSAD) for federated domain generalization. Specifically, we propose a multi-source collaborative style augmentation module to generate data in the broader style space. Furthermore, we conduct domain-invariant learning between the original data and augmented data by cross-domain feature alignment within the same class and classes relation ensemble distillation between different classes to learn a domain-invariant model. By alternatively conducting collaborative style augmentation and domain-invariant learning, the model can generalize well on unseen target domain. Extensive experiments on multiple domain generalization datasets indicate that our method significantly outperforms the state-of-the-art federated domain generalization methods.
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2504.12652.pdf' target='_blank'>https://arxiv.org/pdf/2504.12652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Sanaullah Chowdhury Lameya Sabrin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12652">AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and Scalable Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces AdaptoVision, a novel convolutional neural network (CNN) architecture designed to efficiently balance computational complexity and classification accuracy. By leveraging enhanced residual units, depth-wise separable convolutions, and hierarchical skip connections, AdaptoVision significantly reduces parameter count and computational requirements while preserving competitive performance across various benchmark and medical image datasets. Extensive experimentation demonstrates that AdaptoVision achieves state-of-the-art on BreakHis dataset and comparable accuracy levels, notably 95.3\% on CIFAR-10 and 85.77\% on CIFAR-100, without relying on any pretrained weights. The model's streamlined architecture and strategic simplifications promote effective feature extraction and robust generalization, making it particularly suitable for deployment in real-time and resource-constrained environments.
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2504.02898.pdf' target='_blank'>https://arxiv.org/pdf/2504.02898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lele Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02898">A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2503.19929.pdf' target='_blank'>https://arxiv.org/pdf/2503.19929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinhao Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19929">Robust Object Detection of Underwater Robot based on Domain Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection aims to obtain the location and the category of specific objects in a given image, which includes two tasks: classification and location. In recent years, researchers tend to apply object detection to underwater robots equipped with vision systems to complete tasks including seafood fishing, fish farming, biodiversity monitoring and so on. However, the diversity and complexity of underwater environments bring new challenges to object detection. First, aquatic organisms tend to live together, which leads to severe occlusion. Second, theaquatic organisms are good at hiding themselves, which have a similar color to the background. Third, the various water quality and changeable and extreme lighting conditions lead to the distorted, low contrast, blue or green images obtained by the underwater camera, resulting in domain shift. And the deep model is generally vulnerable to facing domain shift. Fourth, the movement of the underwater robot leads to the blur of the captured image and makes the water muddy, which results in low visibility of the water. This paper investigates the problems brought by the underwater environment mentioned above, and aims to design a high-performance and robust underwater object detector.
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2503.19564.pdf' target='_blank'>https://arxiv.org/pdf/2503.19564.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sree Bhargavi Balija
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19564">FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As artificial intelligence systems increasingly operate in Real-world environments, the integration of multi-modal data sources such as vision, language, and audio presents both unprecedented opportunities and critical challenges for achieving trustworthy intelligence. In this paper, we propose a novel framework that unifies federated learning with explainable multi-modal reasoning to ensure trustworthiness in decentralized, dynamic settings. Our approach, called FedMM-X (Federated Multi-Modal Explainable Intelligence), leverages cross-modal consistency checks, client-level interpretability mechanisms, and dynamic trust calibration to address challenges posed by data heterogeneity, modality imbalance, and out-of-distribution generalization. Through rigorous evaluation across federated multi-modal benchmarks involving vision-language tasks, we demonstrate improved performance in both accuracy and interpretability while reducing vulnerabilities to adversarial and spurious correlations. Further, we introduce a novel trust score aggregation method to quantify global model reliability under dynamic client participation. Our findings pave the way toward developing robust, interpretable, and socially responsible AI systems in Real-world environments.
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2503.06436.pdf' target='_blank'>https://arxiv.org/pdf/2503.06436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06436">Physics-Informed Residual Neural Ordinary Differential Equations for Enhanced Tropical Cyclone Intensity Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tropical cyclone (TC) intensity prediction is crucial for mitigating storm hazards, yet its complex dynamics pose challenges to traditional methods. Here, we introduce a Physics-Informed Residual Neural Ordinary Differential Equation (PIR-NODE) model to precisely forecast TC intensity evolution. This model leverages the powerful non-linear fitting capabilities of deep learning, integrates residual connections to enhance model depth and training stability, and explicitly models the continuous temporal evolution of TC intensity using Neural ODEs. Experimental results in the SHIPS dataset demonstrate that the PIR-NODE model achieves a significant improvement in 24-hour intensity prediction accuracy compared to traditional statistical models and benchmark deep learning methods, with a 25. 2\% reduction in the root mean square error (RMSE) and a 19.5\% increase in R-square (R2) relative to a baseline of neural network. Crucially, the residual structure effectively preserves initial state information, and the model exhibits robust generalization capabilities. This study details the PIR-NODE model architecture, physics-informed integration strategies, and comprehensive experimental validation, revealing the substantial potential of deep learning techniques in predicting complex geophysical systems and laying the foundation for future refined TC forecasting research.
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2501.11841.pdf' target='_blank'>https://arxiv.org/pdf/2501.11841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiuling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11841">Survey on Monocular Metric Depth Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular Depth Estimation (MDE) enables spatial understanding, 3D reconstruction, and autonomous navigation, yet deep learning approaches often predict only relative depth without a consistent metric scale. This limitation reduces reliability in applications such as visual SLAM, precise 3D modeling, and view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this challenge by producing depth maps with absolute scale, ensuring geometric consistency and enabling deployment without additional calibration. This survey reviews the evolution of MMDE, from geometry-based methods to state-of-the-art deep models, with emphasis on the datasets that drive progress. Key benchmarks, including KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of modality, scene type, and application domain. Methodological advances are analyzed, covering domain generalization, boundary preservation, and the integration of synthetic and real data. Techniques such as unsupervised and semi-supervised learning, patch-based inference, architectural innovations, and generative modeling are evaluated for their strengths and limitations. By synthesizing current progress, highlighting the importance of high-quality datasets, and identifying open challenges, this survey provides a structured reference for advancing MMDE and supporting its adoption in real-world computer vision systems.
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2501.08361.pdf' target='_blank'>https://arxiv.org/pdf/2501.08361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijian Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08361">Weight Averaging for Out-of-Distribution Generalization and Few-Shot Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empirical risk minimization (ERM) is not robust to changes in the distribution of data. When the distribution of test data is different from that of training data, the problem is known as out-of-distribution generalization. Recently, two techniques have been developed for addressing out-of-distribution generalization in computer vision: weight averaging (WA) and sharpness-aware minimization (SAM). WA involves training multiple models with different hyperparameters and then averaging the weights of these models, which can significantly improve out-of-distribution generalization performance. SAM optimizes a neural network to find minima in flat regions, which have been proven to perform well under distribution shifts. While these techniques have made great progress, there is still room for improvement and further exploration. In this thesis, we propose increasing the model diversity in WA explicitly by introducing gradient similarity as a loss regularizer to further improve out-of-distribution generalization performance. We also propose combining WA and SAM to solve the problem of few-shot domain adaptation. Our extensive experiments on digits datasets (MNIST, SVHN, USPS, MNIST-M) and other domain adaptation datasets (VLCS, PACS) show that combining WA and SAM leads to improved out-of-distribution generalization performance and significantly increases few-shot domain adaptation accuracy.
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2412.14211.pdf' target='_blank'>https://arxiv.org/pdf/2412.14211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aroj Subedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14211">Improving Generalization Performance of YOLOv8 for Camera Trap Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camera traps have become integral tools in wildlife conservation, providing non-intrusive means to monitor and study wildlife in their natural habitats. The utilization of object detection algorithms to automate species identification from Camera Trap images is of huge importance for research and conservation purposes. However, the generalization issue, where the trained model is unable to apply its learnings to a never-before-seen dataset, is prevalent. This thesis explores the enhancements made to the YOLOv8 object detection algorithm to address the problem of generalization. The study delves into the limitations of the baseline YOLOv8 model, emphasizing its struggles with generalization in real-world environments. To overcome these limitations, enhancements are proposed, including the incorporation of a Global Attention Mechanism (GAM) module, modified multi-scale feature fusion, and Wise Intersection over Union (WIoUv3) as a bounding box regression loss function. A thorough evaluation and ablation experiments reveal the improved model's ability to suppress the background noise, focus on object properties, and exhibit robust generalization in novel environments. The proposed enhancements not only address the challenges inherent in camera trap datasets but also pave the way for broader applicability in real-world conservation scenarios, ultimately aiding in the effective management of wildlife populations and habitats.
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2412.09439.pdf' target='_blank'>https://arxiv.org/pdf/2412.09439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh-Dat Truong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09439">Towards Robust and Fair Vision Learning in Open-World Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The dissertation presents four key contributions toward fairness and robustness in vision learning. First, to address the problem of large-scale data requirements, the dissertation presents a novel Fairness Domain Adaptation approach derived from two major novel research findings of Bijective Maximum Likelihood and Fairness Adaptation Learning. Second, to enable the capability of open-world modeling of vision learning, this dissertation presents a novel Open-world Fairness Continual Learning Framework. The success of this research direction is the result of two research lines, i.e., Fairness Continual Learning and Open-world Continual Learning. Third, since visual data are often captured from multiple camera views, robust vision learning methods should be capable of modeling invariant features across views. To achieve this desired goal, the research in this thesis will present a novel Geometry-based Cross-view Adaptation framework to learn robust feature representations across views. Finally, with the recent increase in large-scale videos and multimodal data, understanding the feature representations and improving the robustness of large-scale visual foundation models is critical. Therefore, this thesis will present novel Transformer-based approaches to improve the robust feature representations against multimodal and temporal data. Then, a novel Domain Generalization Approach will be presented to improve the robustness of visual foundation models. The research's theoretical analysis and experimental results have shown the effectiveness of the proposed approaches, demonstrating their superior performance compared to prior studies. The contributions in this dissertation have advanced the fairness and robustness of machine vision learning.
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2411.07556.pdf' target='_blank'>https://arxiv.org/pdf/2411.07556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07556">Multi-task Feature Enhancement Network for No-Reference Image Quality Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the scarcity of labeled samples in Image Quality Assessment (IQA) datasets, numerous recent studies have proposed multi-task based strategies, which explore feature information from other tasks or domains to boost the IQA task. Nevertheless, multi-task strategies based No-Reference Image Quality Assessment (NR-IQA) methods encounter several challenges. First, existing methods have not explicitly exploited texture details, which significantly influence the image quality. Second, multi-task methods conventionally integrate features through simple operations such as addition or concatenation, thereby diminishing the network's capacity to accurately represent distorted features. To tackle these challenges, we introduce a novel multi-task NR-IQA framework. Our framework consists of three key components: a high-frequency extraction network, a quality estimation network, and a distortion-aware network. The high-frequency extraction network is designed to guide the model's focus towards high-frequency information, which is highly related to the texture details. Meanwhile, the distortion-aware network extracts distortion-related features to distinguish different distortion types. To effectively integrate features from different tasks, a feature fusion module is developed based on an attention mechanism. Empirical results from five standard IQA databases confirm that our method not only achieves high performance but also exhibits robust generalization ability.
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2411.04892.pdf' target='_blank'>https://arxiv.org/pdf/2411.04892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04892">In the Era of Prompt Learning with Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale foundation models like CLIP have shown strong zero-shot generalization but struggle with domain shifts, limiting their adaptability. In our work, we introduce \textsc{StyLIP}, a novel domain-agnostic prompt learning strategy for Domain Generalization (DG). StyLIP disentangles visual style and content in CLIP`s vision encoder by using style projectors to learn domain-specific prompt tokens and combining them with content features. Trained contrastively, this approach enables seamless adaptation across domains, outperforming state-of-the-art methods on multiple DG benchmarks. Additionally, we propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s frozen vision backbone to learn domain-invariant prompts through image style and content features. By aligning domains in embedding space with entropy minimization, AD-CLIP effectively handles domain shifts, even when only target domain samples are available. Lastly, we outline future work on class discovery using prompt learning for semantic segmentation in remote sensing, focusing on identifying novel or rare classes in unstructured environments. This paves the way for more adaptive and generalizable models in complex, real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2410.21313.pdf' target='_blank'>https://arxiv.org/pdf/2410.21313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyue Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21313">Towards Robust Out-of-Distribution Generalization: Data Augmentation and Neural Architecture Search Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning has been demonstrated with tremendous success in recent years. Despite so, its performance in practice often degenerates drastically when encountering out-of-distribution (OoD) data, i.e. training and test data are sampled from different distributions. In this thesis, we study ways toward robust OoD generalization for deep learning, i.e., its performance is not susceptible to distribution shift in the test data.
  We first propose a novel and effective approach to disentangle the spurious correlation between features that are not essential for recognition. It employs decomposed feature representation by orthogonalizing the two gradients of losses for category and context branches. Furthermore, we perform gradient-based augmentation on context-related features (e.g., styles, backgrounds, or scenes of target objects) to improve the robustness of learned representations. Results show that our approach generalizes well for different distribution shifts.
  We then study the problem of strengthening neural architecture search in OoD scenarios. We propose to optimize the architecture parameters that minimize the validation loss on synthetic OoD data, under the condition that corresponding network parameters minimize the training loss. Moreover, to obtain a proper validation set, we learn a conditional generator by maximizing their losses computed by different neural architectures. Results show that our approach effectively discovers robust architectures that perform well for OoD generalization.
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2410.07124.pdf' target='_blank'>https://arxiv.org/pdf/2410.07124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrian Galdran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07124">Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This short abstract describes a solution to the COSAS 2024 competition on Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation from histopathological image patches. The main challenge in the task of segmenting this type of cancer is a noticeable domain shift encountered when changing acquisition devices (microscopes) and also when tissue comes from different organs. The two tasks proposed in COSAS were to train on a dataset of images from three different organs, and then predict segmentations on data from unseen organs (dataset T1), and to train on a dataset of images acquired on three different scanners and then segment images acquired with another unseen microscope. We attempted to bridge the domain shift gap by experimenting with three different strategies: standard training for each dataset, pretraining on dataset T1 and then fine-tuning on dataset T2 (and vice-versa, a strategy we call \textit{Cross-Task Pretraining}), and training on the combination of dataset A and B. Our experiments showed that Cross-Task Pre-training is a more promising approach to domain generalization.
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2409.09858.pdf' target='_blank'>https://arxiv.org/pdf/2409.09858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09858">A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph machine learning (GML) has been successfully applied across a wide range of tasks. Nonetheless, GML faces significant challenges in generalizing over out-of-distribution (OOD) data, which raises concerns about its wider applicability. Recent advancements have underscored the crucial role of causality-driven approaches in overcoming these generalization challenges. Distinct from traditional GML methods that primarily rely on statistical dependencies, causality-focused strategies delve into the underlying causal mechanisms of data generation and model prediction, thus significantly improving the generalization of GML across different environments. This paper offers a thorough review of recent progress in causality-involved GML generalization. We elucidate the fundamental concepts of employing causality to enhance graph model generalization and categorize the various approaches, providing detailed descriptions of their methodologies and the connections among them. Furthermore, we explore the incorporation of causality in other related important areas of trustworthy GML, such as explanation, fairness, and robustness. Concluding with a discussion on potential future research directions, this review seeks to articulate the continuing development and future potential of causality in enhancing the trustworthiness of graph machine learning.
